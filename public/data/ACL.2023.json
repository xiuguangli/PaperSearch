{
  "conference": "ACL",
  "year": 2023,
  "total_papers": 2150,
  "papers": [
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1,
      "original_url": "https://aclanthology.org/2023.acl-long.911/",
      "title": "Program Chairs’ Report on Peer Review at ACL 2023",
      "pdf_url": "https://aclanthology.org/2023.acl-long.report.pdf",
      "authors": [
        "Anna Rogers",
        "Marzena Karpinska",
        "Jordan Boyd-Graber",
        "Naoaki Okazaki"
      ],
      "abstract": "We present a summary of the efforts to improve conference peer review that were implemented at ACL’23. This includes work with the goal of improving review quality, clearer workflow and decision support for the area chairs, as well as our efforts to improve paper-reviewer matching for various kinds of non- mainstream NLP work, and improve the overall incentives for all participants of the peer review process. We present analysis of the factors affecting peer review, identify the most problematic issues that the authors complained about, and provide suggestions for the future chairs. We hope that publishing such reports would (a) improve transparency in decision-making, (b) help the people new to the field to understand how the *ACL conferences work, (c) provide useful data for the future chairs and workshop organizers, and also academic work on peer review, and (d) provide useful context for the final program, as a source of information for meta-research on the structure and trajectory of the field of NLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2,
      "original_url": "https://aclanthology.org/2023.acl-long.1/",
      "title": "One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-long.1.pdf",
      "authors": [
        "Yajiao Liu",
        "Xin Jiang",
        "Yichun Yin",
        "Yasheng Wang",
        "Fei Mi",
        "Qun Liu",
        "Xiang Wan",
        "Benyou Wang"
      ],
      "abstract": "User simulators are agents designed to imitate human users; recent advances have found that Task-oriented Dialogue (ToD) systems optimized toward a user simulator could better satisfy the need of human users. However, this might result in a sub-optimal ToD system if it is tailored to only one ad hoc user simulator, since human users can behave differently. In this paper, we propose a framework called MUST to optimize ToD systems via leveraging Multiple User SimulaTors. The main challenges of implementing MUST fall in 1) how to adaptively determine which user simulator to interact with the ToD system at each optimization step, since the ToD system might be over-fitted to some specific user simulators, and simultaneously under-fitted to some others; 2) how to avoid catastrophic forgetting of the adaption for a simulator that is not selected for several consecutive optimization steps.To tackle these challenges, we formulate MUST as a Multi-armed bandits (MAB) problem and provide a method called MUSTadaptive that balances i) the boosting adaption for adaptive interactions between different user simulators and the ToD system andii) the uniform adaption to avoid the catastrophic forgetting issue.With both automatic evaluations and human evaluations, our experimental results on MultiWOZ show that the dialogue system trained by MUST achieves a better performance than those trained by a single user simulator. It also has a better generalization ability when testing with unseen user simulators."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 3,
      "original_url": "https://aclanthology.org/2023.acl-long.2/",
      "title": "SafeConv: Explaining and Correcting Conversational Unsafe Behavior",
      "pdf_url": "https://aclanthology.org/2023.acl-long.2.pdf",
      "authors": [
        "Mian Zhang",
        "Lifeng Jin",
        "Linfeng Song",
        "Haitao Mi",
        "Wenliang Chen",
        "Dong Yu"
      ],
      "abstract": "One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. By virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 4,
      "original_url": "https://aclanthology.org/2023.acl-long.3/",
      "title": "Detecting and Mitigating Hallucinations in Machine Translation: Model Internal Workings Alone Do Well, Sentence Similarity Even Better",
      "pdf_url": "https://aclanthology.org/2023.acl-long.3.pdf",
      "authors": [
        "David Dale",
        "Elena Voita",
        "Loic Barrault",
        "Marta R. Costa-jussà"
      ],
      "abstract": "While the problem of hallucinations in neural machine translation has long been recognized, so far the progress on its alleviation is very little. Indeed, recently it turned out that without artificially encouraging models to hallucinate, previously existing methods fall short and even the standard sequence log-probability is more informative. It means that internal characteristics of the model can give much more information than we expect, and before using external models and measures, we first need to ask: how far can we go if we use nothing but the translation model itself ? We propose to use a method that evaluates the percentage of the source contribution to a generated translation. Intuitively, hallucinations are translations “detached” from the source, hence they can be identified by low source contribution. This method improves detection accuracy for the most severe hallucinations by a factor of 2 and is able to alleviate hallucinations at test time on par with the previous best approach that relies on external models. Next, if we move away from internal model characteristics and allow external tools, we show that using sentence similarity from cross-lingual embeddings further improves these results. We release the code of our experiments."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 5,
      "original_url": "https://aclanthology.org/2023.acl-long.4/",
      "title": "Explainable Recommendation with Personalized Review Retrieval and Aspect Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.4.pdf",
      "authors": [
        "Hao Cheng",
        "Shuo Wang",
        "Wensheng Lu",
        "Wei Zhang",
        "Mingyang Zhou",
        "Kezhong Lu",
        "Hao Liao"
      ],
      "abstract": "Explainable recommendation is a technique that combines prediction and generation tasks to produce more persuasive results. Among these tasks, textual generation demands large amounts of data to achieve satisfactory accuracy. However, historical user reviews of items are often insufficient, making it challenging to ensure the precision of generated explanation text. To address this issue, we propose a novel model, ERRA (Explainable Recommendation by personalized Review retrieval and Aspect learning). With retrieval enhancement, ERRA can obtain additional information from the training sets. With this additional information, we can generate more accurate and informative explanations. Furthermore, to better capture users’ preferences, we incorporate an aspect enhancement component into our model. By selecting the top-n aspects that users are most concerned about for different items, we can model user representation with more relevant details, making the explanation more persuasive. To verify the effectiveness of our model, extensive experiments on three datasets show that our model outperforms state-of-the-art baselines (for example, 3.4% improvement in prediction and 15.8% improvement in explanation for TripAdvisor)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 6,
      "original_url": "https://aclanthology.org/2023.acl-long.5/",
      "title": "Binary and Ternary Natural Language Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.5.pdf",
      "authors": [
        "Zechun Liu",
        "Barlas Oguz",
        "Aasish Pappu",
        "Yangyang Shi",
        "Raghuraman Krishnamoorthi"
      ],
      "abstract": "Ternary and binary neural networks enable multiplication-free computation and promise multiple orders of magnitude efficiency gains over full-precision networks if implemented on specialized hardware. However, since both the parameter and the output space are highly discretized, such networks have proven very difficult to optimize. The difficulties are compounded for the class of transformer text generation models due to the sensitivity of the attention operation to quantization and the noise-compounding effects of autoregressive decoding in the high-cardinality output space. We approach the problem with a mix of statistics-based quantization for the weights and elastic quantization of the activations and demonstrate the first ternary and binary transformer models on the downstream tasks of summarization and machine translation. Our ternary BART base achieves an R1 score of 41 on the CNN/DailyMail benchmark, which is merely 3.9 points behind the full model while being 16x more efficient. Our binary model, while less accurate, achieves a highly non-trivial score of 35.6. For machine translation, we achieved BLEU scores of 21.7 and 17.6 on the WMT16 En-Ro benchmark, compared with a full precision mBART model score of 26.8. We also compare our approach in the 8-bit activation setting, where our ternary and even binary weight models can match or outperform the best existing 8-bit weight models in the literature. Our code and models are available at: https://github.com/facebookresearch/Ternary_Binary_Transformer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 7,
      "original_url": "https://aclanthology.org/2023.acl-long.6/",
      "title": "Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.6.pdf",
      "authors": [
        "Björn Bebensee",
        "Haejun Lee"
      ],
      "abstract": "In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 8,
      "original_url": "https://aclanthology.org/2023.acl-long.7/",
      "title": "EM Pre-training for Multi-party Dialogue Response Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.7.pdf",
      "authors": [
        "Yiyang Li",
        "Hai Zhao"
      ],
      "abstract": "Dialogue response generation requires an agent to generate a response according to the current dialogue history, in terms of which two-party dialogues have been well studied, but leaving a great gap for multi-party dialogues at the same time. Different from two-party dialogues where each response is a direct reply to its previous utterance, the addressee of a response utterance should be specified before it is generated in the multi-party scenario. Thanks to the huge amount of two-party conversational data, various pre-trained language models for two-party dialogue response generation have been proposed. However, due to the lack of annotated addressee labels in multi-party dialogue datasets, it is hard to use them to pre-train a response generation model for multi-party dialogues. To tackle this obstacle, we propose an Expectation-Maximization (EM) approach that iteratively performs the expectation steps to generate addressee labels, and the maximization steps to optimize a response generation model. Theoretical analyses and extensive experiments have justified the feasibility and effectiveness of our proposed method. The official implementation of this paper is available at https://github.com/EricLee8/MPDRG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 9,
      "original_url": "https://aclanthology.org/2023.acl-long.8/",
      "title": "ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER",
      "pdf_url": "https://aclanthology.org/2023.acl-long.8.pdf",
      "authors": [
        "Sreyan Ghosh",
        "Utkarsh Tyagi",
        "Manan Suri",
        "Sonal Kumar",
        "Ramaneswaran S",
        "Dinesh Manocha"
      ],
      "abstract": "Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 10,
      "original_url": "https://aclanthology.org/2023.acl-long.9/",
      "title": "Natural Language to Code Generation in Interactive Data Science Notebooks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.9.pdf",
      "authors": [
        "Pengcheng Yin",
        "Wen-Ding Li",
        "Kefan Xiao",
        "Abhishek Rao",
        "Yeming Wen",
        "Kensen Shi",
        "Joshua Howland",
        "Paige Bailey",
        "Michele Catasta",
        "Henryk Michalewski",
        "Oleksandr Polozov",
        "Charles Sutton"
      ],
      "abstract": "Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 11,
      "original_url": "https://aclanthology.org/2023.acl-long.10/",
      "title": "Subset Retrieval Nearest Neighbor Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.10.pdf",
      "authors": [
        "Hiroyuki Deguchi",
        "Taro Watanabe",
        "Yusuke Matsui",
        "Masao Utiyama",
        "Hideki Tanaka",
        "Eiichiro Sumita"
      ],
      "abstract": "k-nearest-neighbor machine translation (kNN-MT) (Khandelwal et al., 2021) boosts the translation performance of trained neural machine translation (NMT) models by incorporating example-search into the decoding algorithm. However, decoding is seriously time-consuming, i.e., roughly 100 to 1,000 times slower than standard NMT, because neighbor tokens are retrieved from all target tokens of parallel data in each timestep. In this paper, we propose “Subset kNN-MT”, which improves the decoding speed of kNN-MT by two methods: (1) retrieving neighbor target tokens from a subset that is the set of neighbor sentences of the input sentence, not from all sentences, and (2) efficient distance computation technique that is suitable for subset neighbor search using a look-up table. Our proposed method achieved a speed-up of up to 132.2 times and an improvement in BLEU score of up to 1.6 compared with kNN-MT in the WMT’19 De-En translation task and the domain adaptation tasks in De-En and En-Ja."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 12,
      "original_url": "https://aclanthology.org/2023.acl-long.11/",
      "title": "MIL-Decoding: Detoxifying Language Models at Token-Level via Multiple Instance Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.11.pdf",
      "authors": [
        "Xu Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "Despite advances in large pre-trained neural language models, they are prone to generating toxic language, which brings security risks to their applications. We introduce MIL-Decoding, which detoxifies language models at token-level by interpolating it with a trained multiple instance learning (MIL) network.MIL model is trained on a corpus with a toxicity label for each text to predict the overall toxicity and the toxicity of each token in its context. Intuitively, the MIL network computes a toxicity distribution over next tokens according to the generated context which supplements the original language model to avoid toxicity. We evaluate MIL-Decoding with automatic metrics and human evaluation, where MIL-Decoding outperforms other baselines in detoxification while it only hurts generation fluency a little bit."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 13,
      "original_url": "https://aclanthology.org/2023.acl-long.12/",
      "title": "Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies",
      "pdf_url": "https://aclanthology.org/2023.acl-long.12.pdf",
      "authors": [
        "Iria de-Dios-Flores",
        "Juan Garcia Amboage",
        "Marcos Garcia"
      ],
      "abstract": "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as ‘José le prometió/ordenó a María ser ordenado/a’ (‘Joseph promised/ordered Mary to be tidy’). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models’ ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 14,
      "original_url": "https://aclanthology.org/2023.acl-long.13/",
      "title": "Open-ended Long Text Generation via Masked Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.13.pdf",
      "authors": [
        "Xiaobo Liang",
        "Zecheng Tang",
        "Juntao Li",
        "Min Zhang"
      ],
      "abstract": "Pre-trained autoregressive (AR) language models such as BART and GPTs have dominated OPen-ended Long Text Generation (Open-LTG).However, the AR nature will decrease the inference efficiency along with the increase of generation length, which hinder their application in Open-LTG.To improve inference efficiency, we alternatively explore the potential of the pre-trained masked language models (MLMs) along with a representative iterative non-autoregressive (NAR) decoding strategy for Open-LTG.Our preliminary study shows that pre-trained MLMs can merely generate short text and will collapse for long text modeling. To enhance the long text generation capability of MLMs, we introduce two simple yet effective strategies for the iterative NAR model: dynamic sliding window attention (DSWA) and linear temperature decay (LTD). It can alleviate long-distance collapse problems and achieve longer text generation with a flexible trade-off between performance and inference speedup. Experiments on the storytelling and multi-paragraph opinionated article writing tasks show that pre-trained MLMs can achieve more than 3 × → 13 × speedup with better performance than strong AR models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 15,
      "original_url": "https://aclanthology.org/2023.acl-long.14/",
      "title": "A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces",
      "pdf_url": "https://aclanthology.org/2023.acl-long.14.pdf",
      "authors": [
        "Gabriella Chronis",
        "Kyle Mahowald",
        "Katrin Erk"
      ],
      "abstract": "We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., ‘a beautiful three days’). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 16,
      "original_url": "https://aclanthology.org/2023.acl-long.15/",
      "title": "Holographic CCG Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.15.pdf",
      "authors": [
        "Ryosuke Yamaki",
        "Tadahiro Taniguchi",
        "Daichi Mochihashi"
      ],
      "abstract": "We propose a method for formulating CCG as a recursive composition in a continuous vector space. Recent CCG supertagging and parsing models generally demonstrate high performance, yet rely on black-box neural architectures to implicitly model phrase structure dependencies. Instead, we leverage the method of holographic embeddings as a compositional operator to explicitly model the dependencies between words and phrase structures in the embedding space. Experimental results revealed that holographic composition effectively improves the supertagging accuracy to achieve state-of-the-art parsing performance when using a C&C parser. The proposed span-based parsing algorithm using holographic composition achieves performance comparable to state-of-the-art neural parsing with Transformers. Furthermore, our model can semantically and syntactically infill text at the phrase level due to the decomposability of holographic composition."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 17,
      "original_url": "https://aclanthology.org/2023.acl-long.16/",
      "title": "Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.16.pdf",
      "authors": [
        "Zujie Liang",
        "Feng Wei",
        "Yin Jie",
        "Yuxi Qian",
        "Zhenghong Hao",
        "Bing Han"
      ],
      "abstract": "Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. Nonetheless, in real-world scenarios of UIE applications, new data of different IE tasks and domains usually come in a stream over time. A desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system. In this paper, we study the UIE system under a more challenging yet practical scenario, i.e., “lifelong learning” settings, to evaluate its abilities in three aspects, including knowledge sharing and expansion, catastrophic forgetting prevention, and rapid generalization on few-shot and unseen tasks. To achieve these three goals, we present a novel parameter- and deployment-efficient prompt tuning method namely Lottery Prompt Tuning (LPT).LPT freezes the PLM’s parameters and sequentially learns compact pruned prompt vectors for each task leveraging a binary prompt mask, while keeping the prompt parameters selected by the previous tasks insusceptible. Furthermore, we use a simple yet effective method to perform mask selection and show the powerful transferability of Lottery Prompts to novel tasks. Extensive experiments demonstrate that LPT consistently sets state-of-the-art performance on multiple lifelong learning settings of UIE, including task-incremental setting on seen tasks, few-shot adaptation, and zero-shot generalization on novel tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 18,
      "original_url": "https://aclanthology.org/2023.acl-long.17/",
      "title": "Retrieve-and-Sample: Document-level Event Argument Extraction via Hybrid Retrieval Augmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.17.pdf",
      "authors": [
        "Yubing Ren",
        "Yanan Cao",
        "Ping Guo",
        "Fang Fang",
        "Wei Ma",
        "Zheng Lin"
      ],
      "abstract": "Recent studies have shown the effectiveness of retrieval augmentation in many generative NLP tasks. These retrieval-augmented methods allow models to explicitly acquire prior external knowledge in a non-parametric manner and regard the retrieved reference instances as cues to augment text generation. These methods use similarity-based retrieval, which is based on a simple hypothesis: the more the retrieved demonstration resembles the original input, the more likely the demonstration label resembles the input label. However, due to the complexity of event labels and sparsity of event arguments, this hypothesis does not always hold in document-level EAE. This raises an interesting question: How do we design the retrieval strategy for document-level EAE? We investigate various retrieval settings from the input and label distribution views in this paper. We further augment document-level EAE with pseudo demonstrations sampled from event semantic regions that can cover adequate alternatives in the same context and event schema. Through extensive experiments on RAMS and WikiEvents, we demonstrate the validity of our newly introduced retrieval-augmented methods and analyze why they work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 19,
      "original_url": "https://aclanthology.org/2023.acl-long.18/",
      "title": "WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.18.pdf",
      "authors": [
        "Wenhao Wu",
        "Wei Li",
        "Xinyan Xiao",
        "Jiachen Liu",
        "Sujian Li",
        "Yajuan Lyu"
      ],
      "abstract": "A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs. Due to lack of annotated data, existing factual consistency metrics usually train evaluation models on synthetic texts or directly transfer from other related tasks, such as question answering (QA) and natural language inference (NLI).Bias in synthetic text or upstream tasks makes them perform poorly on text actually generated by language models, especially for general evaluation for various tasks. To alleviate this problem, we propose a weakly supervised framework named WeCheck that is directly trained on actual generated samples from language models with weakly annotated labels.WeCheck first utilizes a generative model to infer the factual labels of generated samples by aggregating weak labels from multiple resources.Next, we train a simple noise-aware classification model as the target metric using the inferred weakly supervised information.Comprehensive experiments on various tasks demonstrate the strong performance of WeCheck, achieving an average absolute improvement of 3.3% on the TRUE benchmark over 11B state-of-the-art methods using only 435M parameters.Furthermore, it is up to 30 times faster than previous evaluation methods, greatly improving the accuracy and efficiency of factual consistency evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 20,
      "original_url": "https://aclanthology.org/2023.acl-long.19/",
      "title": "AMR-based Network for Aspect-based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.19.pdf",
      "authors": [
        "Fukun Ma",
        "Xuming Hu",
        "Aiwei Liu",
        "Yawen Yang",
        "Shuang Li",
        "Philip S. Yu",
        "Lijie Wen"
      ],
      "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced self-attention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relation-enhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 21,
      "original_url": "https://aclanthology.org/2023.acl-long.20/",
      "title": "Text Adversarial Purification as Defense against Adversarial Attacks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.20.pdf",
      "authors": [
        "Linyang Li",
        "Demin Song",
        "Xipeng Qiu"
      ],
      "abstract": "Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models,using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We test our proposed adversarial purification method on several strong adversarial attack methods including Textfooler and BERT-Attack and experimental results indicate that the purification algorithm can successfully defend against strong word-substitution attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 22,
      "original_url": "https://aclanthology.org/2023.acl-long.21/",
      "title": "SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres",
      "pdf_url": "https://aclanthology.org/2023.acl-long.21.pdf",
      "authors": [
        "Shumin Deng",
        "Shengyu Mao",
        "Ningyu Zhang",
        "Bryan Hooi"
      ],
      "abstract": "Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 23,
      "original_url": "https://aclanthology.org/2023.acl-long.22/",
      "title": "Rule By Example: Harnessing Logical Rules for Explainable Hate Speech Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.22.pdf",
      "authors": [
        "Christopher Clarke",
        "Matthew Hall",
        "Gaurav Mittal",
        "Ye Yu",
        "Sandra Sajeev",
        "Jason Mars",
        "Mei Chen"
      ],
      "abstract": "Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on 3 popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 24,
      "original_url": "https://aclanthology.org/2023.acl-long.23/",
      "title": "What about “em”? How Commercial Machine Translation Fails to Handle (Neo-)Pronouns",
      "pdf_url": "https://aclanthology.org/2023.acl-long.23.pdf",
      "authors": [
        "Anne Lauscher",
        "Debora Nozza",
        "Ehm Miltersen",
        "Archie Crowley",
        "Dirk Hovy"
      ],
      "abstract": "As 3rd-person pronoun usage shifts to include novel forms, e.g., neopronouns, we need more research on identity-inclusive NLP. Exclusion is particularly harmful in one of the most popular NLP applications, machine translation (MT). Wrong pronoun translations can discriminate against marginalized groups, e.g., non-binary individuals (Dev et al., 2021). In this “reality check”, we study how three commercial MT systems translate 3rd-person pronouns. Concretely, we compare the translations of gendered vs. gender-neutral pronouns from English to five other languages (Danish, Farsi, French, German, Italian), and vice versa, from Danish to English.Our error analysis shows that the presence of a gender-neutral pronoun often leads to grammatical and semantic translation errors. Similarly, gender neutrality is often not preserved. By surveying the opinions of affected native speakers from diverse languages, we provide recommendations to address the issue in future MT research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 25,
      "original_url": "https://aclanthology.org/2023.acl-long.24/",
      "title": "What Is Overlap Knowledge in Event Argument Extraction? APE: A Cross-datasets Transfer Learning Model for EAE",
      "pdf_url": "https://aclanthology.org/2023.acl-long.24.pdf",
      "authors": [
        "Kaihang Zhang",
        "Kai Shuang",
        "Xinyue Yang",
        "Xuyang Yao",
        "Jinyu Guo"
      ],
      "abstract": "The EAE task extracts a structured event record from an event text. Most existing approaches train the EAE model on each dataset independently and ignore the overlap knowledge across datasets. However, insufficient event records in a single dataset often prevent the existing model from achieving better performance. In this paper, we clearly define the overlap knowledge across datasets and split the knowledge of the EAE task into overlap knowledge across datasets and specific knowledge of the target dataset. We propose APE model to learn the two parts of knowledge in two serial learning phases without causing catastrophic forgetting. In addition, we formulate both learning phases as conditional generation tasks and design Stressing Entity Type Prompt to close the gap between the two phases. The experiments show APE achieves new state-of-the-art with a large margin in the EAE task. When only ten records are available in the target dataset, our model dramatically outperforms the baseline model with average 27.27% F1 gain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 26,
      "original_url": "https://aclanthology.org/2023.acl-long.25/",
      "title": "Tailor: A Soft-Prompt-Based Approach to Attribute-Based Controlled Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.25.pdf",
      "authors": [
        "Kexin Yang",
        "Dayiheng Liu",
        "Wenqiang Lei",
        "Baosong Yang",
        "Mingfeng Xue",
        "Boxing Chen",
        "Jun Xie"
      ],
      "abstract": "Attribute-based Controlled Text Generation (CTG) refers to generating sentences that satisfy desirable attributes (e.g., emotions and topics). Existing work usually utilize fine-tuning or resort to extra attribute classifiers, yet suffer from increases in storage and inference time. To address these concerns, we explore attribute-based CTG in a parameter-efficient manner. In short, the proposed Tailor represents each attribute as a pre-trained continuous vector i.e., single-attribute prompt), which guides the generation of a fixed pre-trained language model (PLM) to satisfy a pre-specified attribute. These prompts can be simply concatenated as a whole for multi-attribute CTG without any re-training. Nevertheless, this may raise problems of fluency downgrading and position sensitivity. To solve this, Tailor provides two solutions to enhance the combination. The former contains a multi-attribute prompt mask and a re-indexing position sequence to bridge the gap between the training (one single-attribute prompt for each task) and the testing stage (concatenating two prompts). The latter introduces a trainable prompt connector to further enhance the combinations. Experiments demonstrate that, only requiring 0.08% extra training parameters of the GPT-2, Tailor can achieve effective and general improvements on eleven attribute-specific generation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 27,
      "original_url": "https://aclanthology.org/2023.acl-long.26/",
      "title": "Knowledge of cultural moral norms in large language models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.26.pdf",
      "authors": [
        "Aida Ramezani",
        "Yang Xu"
      ],
      "abstract": "Moral norms vary across cultures. A recent line of work suggests that English large language models contain human-like moral biases, but these studies typically do not examine moral variation in a diverse cultural setting. We investigate the extent to which monolingual English language models contain knowledge about moral norms in different countries. We consider two levels of analysis: 1) whether language models capture fine-grained moral variation across countries over a variety of topics such as “homosexuality” and “divorce”; 2) whether language models capture cultural diversity and shared tendencies in which topics people around the globe tend to diverge or agree on in their moral judgment. We perform our analyses with two public datasets from the World Values Survey (across 55 countries) and PEW global surveys (across 40 countries) on morality. We find that pre-trained English language models predict empirical moral norms across countries worse than the English moral norms reported previously. However, fine-tuning language models on the survey data improves inference across countries at the expense of a less accurate estimate of the English moral norms. We discuss the relevance and challenges of incorporating cultural knowledge into the automated inference of moral norms."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 28,
      "original_url": "https://aclanthology.org/2023.acl-long.27/",
      "title": "Songs Across Borders: Singable and Controllable Neural Lyric Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.27.pdf",
      "authors": [
        "Longshen Ou",
        "Xichu Ma",
        "Min-Yen Kan",
        "Ye Wang"
      ],
      "abstract": "The development of general-domain neural machine translation (NMT) methods has advanced significantly in recent years, but the lack of naturalness and musical constraints in the outputs makes them unable to produce singable lyric translations. This paper bridges the singability quality gap by formalizing lyric translation into a constrained translation problem, converting theoretical guidance and practical techniques from translatology literature to prompt-driven NMT approaches, exploring better adaptation methods, and instantiating them to an English-Chinese lyric translation system. Our model achieves 99.85%, 99.00%, and 95.52% on length accuracy, rhyme accuracy, and word boundary recall. In our subjective evaluation, our model shows a 75% relative enhancement on overall quality, compared against naive fine-tuning (Code available at https://github.com/Sonata165/ControllableLyricTranslation)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 29,
      "original_url": "https://aclanthology.org/2023.acl-long.28/",
      "title": "Fantastic Expressions and Where to Find Them: Chinese Simile Generation with Multiple Constraints",
      "pdf_url": "https://aclanthology.org/2023.acl-long.28.pdf",
      "authors": [
        "Kexin Yang",
        "Dayiheng Liu",
        "Wenqiang Lei",
        "Baosong Yang",
        "Xiangpeng Wei",
        "Zhengyuan Liu",
        "Jun Xie"
      ],
      "abstract": "Similes occur in the creative context of describing a concept (i.e., tenor) by making a literally false yet figuratively meaningful comparison to another (i.e., vehicle). Previous efforts form simile generation as a context-free generation task, focusing on simile-style transfer or writing a simile from a given prefix. However, generated texts under such settings might be undesirable, such as hardly meeting the simile definition (e.g., missing vehicle) or difficult to address certain preferences of content as humans wish (e.g., describe the color of apples through the simile). We believe that a simile could be more qualified and user-oriented if incorporated with pre-specified constraints. To this end, we introduce controllable simile generation (CSG), a new task that requires the model to generate a simile with multiple simile elements, e.g., context and vehicle. To facilitate this task, we present GraCe, including 61.3k simile-element annotated Chinese similes. Based on it, we propose a CSG model Similor to benchmark this task, including a vehicle retrieval module Scorer to obtain the explicable comparison for a given tenor in the vehicle-unknown situation. Both statistical and experimental analyses show that GraCe is of high quality beyond all other Chinese simile datasets, in terms of the number (8 vs. 3) of annotation elements, Is-Simile accuracy (98.9% vs. 78.7%), and increasing model-performance gains for both uncontrollable and controllable simile generation. Meanwhile, Similor can serve as a strong baseline for CSG, especially with Scorer, which beats model-based retrieval methods without any re-training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 30,
      "original_url": "https://aclanthology.org/2023.acl-long.29/",
      "title": "Revealing Single Frame Bias for Video-and-Language Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.29.pdf",
      "authors": [
        "Jie Lei",
        "Tamara Berg",
        "Mohit Bansal"
      ],
      "abstract": "Training an effective video-and-language model intuitively requires multiple frames as model inputs. However, it is unclear whether using multiple frames is beneficial to downstream tasks, and if yes, whether the performance gain is worth the drastically-increased computation and memory costs resulting from using more frames. In this work, we explore single-frame models for video-and-language learning. On a diverse set of video-and-language tasks (including text-to-video retrieval and video question answering), we show the surprising result that, with large-scale pre-training and a proper frame ensemble strategy at inference time, a single-frame trained model that does not consider temporal information can achieve better performance than existing methods that use multiple frames for training. This result reveals the existence of a strong “static appearance bias” in popular video-and-language datasets. Therefore, to allow for a more comprehensive evaluation of video-and-language models, we propose two new retrieval tasks based on existing fine-grained action recognition datasets that encourage temporal modeling. Our code is available at https://github.com/jayleicn/singularity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 31,
      "original_url": "https://aclanthology.org/2023.acl-long.30/",
      "title": "Learning with Partial Annotations for Event Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.30.pdf",
      "authors": [
        "Jian Liu",
        "Dianbo Sui",
        "Kang Liu",
        "Haoyan Liu",
        "Zhe Zhao"
      ],
      "abstract": "Event detection (ED) seeks to discover and classify event instances in plain texts. Previous methods for ED typically adopt supervised learning, requiring fully labeled and high-quality training data. However, in a real-world application, we may not obtain clean training data but only partially labeled one, which could substantially impede the learning process. In this work, we conduct a seminal study for learning with partial annotations for ED.We propose a new trigger localization formulation using contrastive learning to distinguish ground-truth triggers from contexts, showing a decent robustness for addressing partial annotation noise. Impressively, in an extreme scenario where more than 90% of events are unlabeled, our approach achieves an F1 score of over 60%.In addition, we re-annotate and make available two fully annotated subsets of ACE 2005 to serve as an unbiased benchmark for event detection. We hope our approach and data will inspire future studies on this vital yet understudied problem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 32,
      "original_url": "https://aclanthology.org/2023.acl-long.31/",
      "title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.31.pdf",
      "authors": [
        "Ziqiao Ma",
        "Jiayi Pan",
        "Joyce Chai"
      ],
      "abstract": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in open-world language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 33,
      "original_url": "https://aclanthology.org/2023.acl-long.32/",
      "title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.32.pdf",
      "authors": [
        "Alessandro Stolfo",
        "Zhijing Jin",
        "Kumar Shridhar",
        "Bernhard Schoelkopf",
        "Mrinmaya Sachan"
      ],
      "abstract": "We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 34,
      "original_url": "https://aclanthology.org/2023.acl-long.33/",
      "title": "Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information",
      "pdf_url": "https://aclanthology.org/2023.acl-long.33.pdf",
      "authors": [
        "Kun Zhao",
        "Bohao Yang",
        "Chenghua Lin",
        "Wenge Rong",
        "Aline Villavicencio",
        "Xiaohui Cui"
      ],
      "abstract": "The long-standing one-to-many issue of the open-domain dialogues poses significant challenges for automatic evaluation methods, i.e., there may be multiple suitable responses which differ in semantics for a given conversational context. To tackle this challenge, we propose a novel learning-based automatic evaluation metric (CMN), which can robustly evaluate open-domain dialogues by augmenting Conditional Variational Autoencoders (CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual Information (MI) to model the semantic similarity of text in the latent space. Experimental results on two open-domain dialogue datasets demonstrate the superiority of our method compared with a wide range of baselines, especially in handling responses which are distant to the “golden” reference responses in semantics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 35,
      "original_url": "https://aclanthology.org/2023.acl-long.34/",
      "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.34.pdf",
      "authors": [
        "John Chung",
        "Ece Kamar",
        "Saleema Amershi"
      ],
      "abstract": "Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user’s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 36,
      "original_url": "https://aclanthology.org/2023.acl-long.35/",
      "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.35.pdf",
      "authors": [
        "Ting Jiang",
        "Deqing Wang",
        "Fuzhen Zhuang",
        "Ruobing Xie",
        "Feng Xia"
      ],
      "abstract": "To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order methods. Unlike previous first-order methods, SMP is also applicable to low sparsity and outperforms zero-order methods. Meanwhile, SMP is more parameter efficient than other methods due to it does not require fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 37,
      "original_url": "https://aclanthology.org/2023.acl-long.36/",
      "title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
      "pdf_url": "https://aclanthology.org/2023.acl-long.36.pdf",
      "authors": [
        "Patrick Fernandes",
        "Kayo Yin",
        "Emmy Liu",
        "André Martins",
        "Graham Neubig"
      ],
      "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 38,
      "original_url": "https://aclanthology.org/2023.acl-long.37/",
      "title": "Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.37.pdf",
      "authors": [
        "Ziwei Chen",
        "Linmei Hu",
        "Weixin Li",
        "Yingxia Shao",
        "Liqiang Nie"
      ],
      "abstract": "Due to the rapid upgrade of social platforms, most of today’s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 39,
      "original_url": "https://aclanthology.org/2023.acl-long.38/",
      "title": "LexSym: Compositionality as Lexical Symmetry",
      "pdf_url": "https://aclanthology.org/2023.acl-long.38.pdf",
      "authors": [
        "Ekin Akyurek",
        "Jacob Andreas"
      ],
      "abstract": "In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme — a procedure for transforming examples into other well-formed examples — that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LexSym that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LexSym can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and Alchemy instruction following, and CLEVR-CoGenT visual question answering datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 40,
      "original_url": "https://aclanthology.org/2023.acl-long.39/",
      "title": "Layer-wise Fusion with Modality Independence Modeling for Multi-modal Emotion Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.39.pdf",
      "authors": [
        "Jun Sun",
        "Shoukang Han",
        "Yu-Ping Ruan",
        "Xiaoning Zhang",
        "Shu-Kai Zheng",
        "Yulong Liu",
        "Yuxin Huang",
        "Taihao Li"
      ],
      "abstract": "Multi-modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi-modal learning approaches. However, previous studies primarily focus on developing models that exploit the unification of multiple modalities. In this paper, we propose that maintaining modality independence is beneficial for the model performance. According to this principle, we construct a dataset, and devise a multi-modal transformer model. The new dataset, CHinese Emotion Recognition dataset with Modality-wise Annotions, abbreviated as CHERMA, provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. The model consists of uni-modal transformer modules that learn representations for each modality, and a multi-modal transformer module that fuses all modalities. All the modules are supervised by their corresponding labels separately, and the forward information flow is uni-directionally from the uni-modal modules to the multi-modal module. The supervision strategy and the model architecture guarantee each individual modality learns its representation independently, and meanwhile the multi-modal module aggregates all information. Extensive empirical results demonstrate that our proposed scheme outperforms state-of-the-art alternatives, corroborating the importance of modality independence in multi-modal emotion recognition. The dataset and codes are availabel at https://github.com/sunjunaimer/LFMIM"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 41,
      "original_url": "https://aclanthology.org/2023.acl-long.40/",
      "title": "CASN:Class-Aware Score Network for Textual Adversarial Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.40.pdf",
      "authors": [
        "Rong Bao",
        "Rui Zheng",
        "Liang Ding",
        "Qi Zhang",
        "Dacheng Tao"
      ],
      "abstract": "Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data and identifying adversarial ones as outliers. However, these methods suffer from significant performance degradation when the adversarial samples lie close to the non-adversarial data manifold. To address this limitation, we propose a score-based generative method to implicitly model the data distribution. Our approach utilizes the gradient of the log-density data distribution and calculates the distribution gap between adversarial and normal samples through multi-step iterations using Langevin dynamics. In addition, we use supervised contrastive learning to guide the gradient estimation using label information, which avoids collapsing to a single data manifold and better preserves the anisotropy of the different labeled data distributions. Experimental results on three text classification tasks upon four advanced attack algorithms show that our approach is a significant improvement (average +15.2 F1 score against previous SOTA) over previous detection methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 42,
      "original_url": "https://aclanthology.org/2023.acl-long.41/",
      "title": "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest",
      "pdf_url": "https://aclanthology.org/2023.acl-long.41.pdf",
      "authors": [
        "Jack Hessel",
        "Ana Marasovic",
        "Jena D. Hwang",
        "Lillian Lee",
        "Jeff Da",
        "Rowan Zellers",
        "Robert Mankoff",
        "Yejin Choi"
      ],
      "abstract": "Large neural networks can now generate jokes, but do they really “understand” humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of “understanding” a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image’s locations/entities, what’s unusual in the scene, and an explanation of the joke."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 43,
      "original_url": "https://aclanthology.org/2023.acl-long.42/",
      "title": "Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.42.pdf",
      "authors": [
        "Martijn Bartelds",
        "Nay San",
        "Bradley McDonnell",
        "Dan Jurafsky",
        "Martijn Wieling"
      ],
      "abstract": "The performance of automatic speech recognition (ASR) systems has advanced substantially in recent years, particularly for languages for which a large amount of transcribed speech is available. Unfortunately, for low-resource languages, such as minority languages, regional languages or dialects, ASR performance generally remains much lower. In this study, we investigate whether data augmentation techniques could help improve low-resource ASR performance, focusing on four typologically diverse minority languages or language variants (West Germanic: Gronings, West-Frisian; Malayo-Polynesian: Besemah, Nasal). For all four languages, we examine the use of self-training, where an ASR system trained with the available human-transcribed data is used to generate transcriptions, which are then combined with the original data to train a new ASR system. For Gronings, for which there was a pre-existing text-to-speech (TTS) system available, we also examined the use of TTS to generate ASR training data from text-only sources. We find that using a self-training approach consistently yields improved performance (a relative WER reduction up to 20.5% compared to using an ASR system trained on 24 minutes of manually transcribed speech). The performance gain from TTS augmentation for Gronings was even stronger (up to 25.5% relative reduction in WER compared to a system based on 24 minutes of manually transcribed speech). In sum, our results show the benefit of using self-training or (if possible) TTS-generated data as an efficient solution to overcome the limitations of data availability for resource-scarce languages in order to improve ASR performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 44,
      "original_url": "https://aclanthology.org/2023.acl-long.43/",
      "title": "CLCL: Non-compositional Expression Detection with Contrastive Learning and Curriculum Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.43.pdf",
      "authors": [
        "Jianing Zhou",
        "Ziheng Zeng",
        "Suma Bhat"
      ],
      "abstract": "Non-compositional expressions present a substantial challenge for natural language processing (NLP) systems, necessitating more intricate processing compared to general language tasks, even with large pre-trained language models. Their non-compositional nature and limited availability of data resources further compound the difficulties in accurately learning their representations. This paper addresses both of these challenges. By leveraging contrastive learning techniques to build improved representations it tackles the non-compositionality challenge. Additionally, we propose a dynamic curriculum learning framework specifically designed to take advantage of the scarce available data for modeling non-compositionality. Our framework employs an easy-to-hard learning strategy, progressively optimizing the model’s performance by effectively utilizing available training data. Moreover, we integrate contrastive learning into the curriculum learning approach to maximize its benefits. Experimental results demonstrate the gradual improvement in the model’s performance on idiom usage recognition and metaphor detection tasks. Our evaluation encompasses six datasets, consistently affirming the effectiveness of the proposed framework. Our models available at https://github.com/zhjjn/CLCL.git."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 45,
      "original_url": "https://aclanthology.org/2023.acl-long.44/",
      "title": "Multi-VALUE: A Framework for Cross-Dialectal English NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.44.pdf",
      "authors": [
        "Caleb Ziems",
        "William Held",
        "Jingfeng Yang",
        "Jwala Dhamala",
        "Rahul Gupta",
        "Diyi Yang"
      ],
      "abstract": "Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve the dialect robustness of existing systems. Finally, we partner with native speakers of Chicano and Indian English to release new gold-standard variants of the popular CoQA task. To execute the transformation code, run model checkpoints, and download both synthetic and gold-standard dialectal benchmark datasets, see http://value-nlp.org."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 46,
      "original_url": "https://aclanthology.org/2023.acl-long.45/",
      "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.45.pdf",
      "authors": [
        "Kechi Zhang",
        "Zhuo Li",
        "Jia Li",
        "Ge Li",
        "Zhi Jin"
      ],
      "abstract": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 47,
      "original_url": "https://aclanthology.org/2023.acl-long.46/",
      "title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.46.pdf",
      "authors": [
        "Shachar Don-Yehiya",
        "Elad Venezian",
        "Colin Raffel",
        "Noam Slonim",
        "Leshem Choshen"
      ],
      "abstract": "Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. We find ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.19 points on average without any changes to the architecture."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 48,
      "original_url": "https://aclanthology.org/2023.acl-long.47/",
      "title": "Test-time Adaptation for Machine Translation Evaluation by Uncertainty Minimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.47.pdf",
      "authors": [
        "Runzhe Zhan",
        "Xuebo Liu",
        "Derek F. Wong",
        "Cuilian Zhang",
        "Lidia S. Chao",
        "Min Zhang"
      ],
      "abstract": "The neural metrics recently received considerable attention from the research community in the automatic evaluation of machine translation. Unlike text-based metrics that have interpretable and consistent evaluation mechanisms for various data sources, the reliability of neural metrics in assessing out-of-distribution data remains a concern due to the disparity between training data and real-world data. This paper aims to address the inference bias of neural metrics through uncertainty minimization during test time, without requiring additional data. Our proposed method comprises three steps: uncertainty estimation, test-time adaptation, and inference. Specifically, the model employs the prediction uncertainty of the current data as a signal to update a small fraction of parameters during test time and subsequently refine the prediction through optimization. To validate our approach, we apply the proposed method to three representative models and conduct experiments on the WMT21 benchmarks. The results obtained from both in-domain and out-of-distribution evaluations consistently demonstrate improvements in correlation performance across different models. Furthermore, we provide evidence that the proposed method effectively reduces model uncertainty. The code is publicly available at https://github.com/NLP2CT/TaU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 49,
      "original_url": "https://aclanthology.org/2023.acl-long.48/",
      "title": "Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.48.pdf",
      "authors": [
        "Haw-Shiuan Chang",
        "Ruei-Yao Sun",
        "Kathryn Ricci",
        "Andrew McCallum"
      ],
      "abstract": "Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT_Base model can even outperform the corresponding BERT_Large model. We analyze the behavior of our Multi-CLS BERT, showing that it has many of the same characteristics and behavior as a typical BERT 5-way ensemble, but with nearly 4-times less computation and memory."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 50,
      "original_url": "https://aclanthology.org/2023.acl-long.49/",
      "title": "On-the-fly Cross-lingual Masking for Multilingual Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.49.pdf",
      "authors": [
        "Xi Ai",
        "Bin Fang"
      ],
      "abstract": "In multilingual pre-training with the objective of MLM (masked language modeling) on multiple monolingual corpora, multilingual models only learn cross-linguality implicitly from isomorphic spaces formed by overlapping different language spaces due to the lack of explicit cross-lingual forward pass. In this work, we present CLPM (Cross-lingual Prototype Masking), a dynamic and token-wise masking scheme, for multilingual pre-training, using a special token [𝒞]x to replace a random token x in the input sentence. [𝒞]x is a cross-lingual prototype for x and then forms an explicit cross-lingual forward pass. We instantiate CLPM for the multilingual pre-training phase of UNMT (unsupervised neural machine translation), and experiments show that CLPM can consistently improve the performance of UNMT models on {De, Ro, Ne } ↔ En. Beyond UNMT or bilingual tasks, we show that CLPM can consistently improve the performance of multilingual models on cross-lingual classification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 51,
      "original_url": "https://aclanthology.org/2023.acl-long.50/",
      "title": "How About Kind of Generating Hedges using End-to-End Neural Models?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.50.pdf",
      "authors": [
        "Alafate Abulimiti",
        "Chloé Clavel",
        "Justine Cassell"
      ],
      "abstract": "Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, “face threat”) to one’s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 52,
      "original_url": "https://aclanthology.org/2023.acl-long.51/",
      "title": "DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.51.pdf",
      "authors": [
        "Zijie J. Wang",
        "Evan Montoya",
        "David Munechika",
        "Haoyang Yang",
        "Benjamin Hoover",
        "Duen Horng Chau"
      ],
      "abstract": "With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 53,
      "original_url": "https://aclanthology.org/2023.acl-long.52/",
      "title": "From Key Points to Key Point Hierarchy: Structured and Expressive Opinion Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.52.pdf",
      "authors": [
        "Arie Cattan",
        "Lilach Eden",
        "Yoav Kantor",
        "Roy Bar-Haim"
      ],
      "abstract": "Key Point Analysis (KPA) has been recently proposed for deriving fine-grained insights from collections of textual comments. KPA extracts the main points in the data as a list of concise sentences or phrases, termed Key Points, and quantifies their prevalence. While key points are more expressive than word clouds and key phrases, making sense of a long, flat list of key points, which often express related ideas in varying levels of granularity, may still be challenging. To address this limitation of KPA, we introduce the task of organizing a given set of key points into a hierarchy, according to their specificity. Such hierarchies may be viewed as a novel type of Textual Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key point hierarchies for business and product reviews, obtained by consolidating multiple annotations. We compare different methods for predicting pairwise relations between key points, and for inferring a hierarchy from these pairwise predictions. In particular, for the task of computing pairwise key point relations, we achieve significant gains over existing strong baselines by applying directional distributional similarity methods to a novel distributional representation of key points, and further boost performance via weak supervision."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 54,
      "original_url": "https://aclanthology.org/2023.acl-long.53/",
      "title": "When to Use What: An In-Depth Comparative Empirical Analysis of OpenIE Systems for Downstream Applications",
      "pdf_url": "https://aclanthology.org/2023.acl-long.53.pdf",
      "authors": [
        "Kevin Pei",
        "Ishan Jindal",
        "Kevin Chen-Chuan Chang",
        "ChengXiang Zhai",
        "Yunyao Li"
      ],
      "abstract": "Open Information Extraction (OpenIE) has been used in the pipelines of various NLP tasks. Unfortunately, there is no clear consensus on which models to use in which tasks. Muddying things further is the lack of comparisons that take differing training sets into account. In this paper, we present an application-focused empirical survey of neural OpenIE models, training sets, and benchmarks in an effort to help users choose the most suitable OpenIE systems for their applications. We find that the different assumptions made by different models and datasets have a statistically significant effect on performance, making it important to choose the most appropriate model for one’s applications. We demonstrate the applicability of our recommendations on a downstream Complex QA application."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 55,
      "original_url": "https://aclanthology.org/2023.acl-long.54/",
      "title": "Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.54.pdf",
      "authors": [
        "Tharindu Cyril Weerasooriya",
        "Sarah Luger",
        "Saloni Poddar",
        "Ashiqur KhudaBukhsh",
        "Christopher Homan"
      ],
      "abstract": "Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree. Particularly when minority views are disregarded, especially among groups that may already be underrepresented in the annotator population. In this paper, we introduce CrowdOpinion, an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions. We experiment with four generative and one density-based clustering method, applied to five linear combinations of label distributions and features. We use five publicly available benchmark datasets (with varying levels of annotator disagreements) from social media (Twitter, Gab, and Reddit). We also experiment in the wild using a dataset from Facebook, where annotations come from the platform itself by users reacting to posts. We evaluate CrowdOpinion as a label distribution prediction task using KL-divergence and a single-label problem using accuracy measures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 56,
      "original_url": "https://aclanthology.org/2023.acl-long.55/",
      "title": "Post-Abstention: Towards Reliably Re-Attempting the Abstained Instances in QA",
      "pdf_url": "https://aclanthology.org/2023.acl-long.55.pdf",
      "authors": [
        "Neeraj Varshney",
        "Chitta Baral"
      ],
      "abstract": "Despite remarkable progress made in natural language processing, even the state-of-the-art models often make incorrect predictions. Such predictions hamper the reliability of systems and limit their widespread adoption in real-world applications. ‘Selective prediction’ partly addresses the above concern by enabling models to abstain from answering when their predictions are likely to be incorrect. While selective prediction is advantageous, it leaves us with a pertinent question ‘what to do after abstention’. To this end, we present an explorative study on ‘Post-Abstention’, a task that allows re-attempting the abstained instances with the aim of increasing **coverage** of the system without significantly sacrificing its **accuracy**. We first provide mathematical formulation of this task and then explore several methods to solve it. Comprehensive experiments on 11 QA datasets show that these methods lead to considerable risk improvements –performance metric of the Post-Abstention task– both in the in-domain and the out-of-domain settings. We also conduct a thorough analysis of these results which further leads to several interesting findings. Finally, we believe that our work will encourage and facilitate further research in this important area of addressing the reliability of NLP systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 57,
      "original_url": "https://aclanthology.org/2023.acl-long.56/",
      "title": "UniLG: A Unified Structure-aware Framework for Lyrics Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.56.pdf",
      "authors": [
        "Tao Qian",
        "Fan Lou",
        "Jiatong Shi",
        "Yuning Wu",
        "Shuai Guo",
        "Xiang Yin",
        "Qin Jin"
      ],
      "abstract": "As a special task of natural language generation, conditional lyrics generation needs to consider the structure of generated lyrics and the relationship between lyrics and music. Due to various forms of conditions, a lyrics generation system is expected to generate lyrics conditioned on different signals, such as music scores, music audio, or partially-finished lyrics, etc. However, most of the previous works have ignored the musical attributes hidden behind the lyrics and the structure of the lyrics. Additionally, most works only handle limited lyrics generation conditions, such as lyrics generation based on music score or partial lyrics, they can not be easily extended to other generation conditions with the same framework. In this paper, we propose a unified structure-aware lyrics generation framework named UniLG. Specifically, we design compound templates that incorporate textual and musical information to improve structure modeling and unify the different lyrics generation conditions. Extensive experiments demonstrate the effectiveness of our framework. Both objective and subjective evaluations show significant improvements in generating structural lyrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 58,
      "original_url": "https://aclanthology.org/2023.acl-long.57/",
      "title": "FC-KBQA: A Fine-to-Coarse Composition Framework for Knowledge Base Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.57.pdf",
      "authors": [
        "Lingxi Zhang",
        "Jing Zhang",
        "Yanling Wang",
        "Shulin Cao",
        "Xinmei Huang",
        "Cuiping Li",
        "Hong Chen",
        "Juanzi Li"
      ],
      "abstract": "The generalization problem on KBQA has drawn considerable attention. Existing research suffers from the generalization issue brought by the entanglement in the coarse-grained modeling of the logical expression, or inexecutability issues due to the fine-grained modeling of disconnected classes and relations in real KBs. We propose a Fine-to-Coarse Composition framework for KBQA (FC-KBQA) to both ensure the generalization ability and executability of the logical expression. The main idea of FC-KBQA is to extract relevant fine-grained knowledge components from KB and reformulate them into middle-grained knowledge pairs for generating the final logical expressions. FC-KBQA derives new state-of-the-art performance on GrailQA and WebQSP, and runs 4 times faster than the baseline. Our code is now available at GitHub https://github.com/RUCKBReasoning/FC-KBQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 59,
      "original_url": "https://aclanthology.org/2023.acl-long.58/",
      "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.58.pdf",
      "authors": [
        "Lennart Wachowiak",
        "Dagmar Gromann"
      ],
      "abstract": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor’s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT’s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 60,
      "original_url": "https://aclanthology.org/2023.acl-long.59/",
      "title": "Being Right for Whose Right Reasons?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.59.pdf",
      "authors": [
        "Terne Sasha Thorn Jakobsen",
        "Laura Cabello",
        "Anders Søgaard"
      ],
      "abstract": "Explainability methods are used to benchmark the extent to which model predictions align with human rationales i.e., are ‘right for the right reasons’. Previous work has failed to acknowledge, however, that what counts as a rationale is sometimes subjective. This paper presents what we think is a first of its kind, a collection of human rationale annotations augmented with the annotators demographic information. We cover three datasets spanning sentiment analysis and common-sense reasoning, and six demographic groups (balanced across age and ethnicity). Such data enables us to ask both what demographics our predictions align with and whose reasoning patterns our models’ rationales align with. We find systematic inter-group annotator disagreement and show how 16 Transformer-based models align better with rationales provided by certain demographic groups: We find that models are biased towards aligning best with older and/or white annotators. We zoom in on the effects of model size and model distillation, finding –contrary to our expectations– negative correlations between model size and rationale agreement as well as no evidence that either model size or model distillation improves fairness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 61,
      "original_url": "https://aclanthology.org/2023.acl-long.60/",
      "title": "ALERT: Adapt Language Models to Reasoning Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.60.pdf",
      "authors": [
        "Ping Yu",
        "Tianlu Wang",
        "Olga Golovneva",
        "Badr AlKhamissi",
        "Siddharth Verma",
        "Zhijing Jin",
        "Gargi Ghosh",
        "Mona Diab",
        "Asli Celikyilmaz"
      ],
      "abstract": "Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context. To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 62,
      "original_url": "https://aclanthology.org/2023.acl-long.61/",
      "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.61.pdf",
      "authors": [
        "Ayyoob ImaniGooghari",
        "Peiqin Lin",
        "Amir Hossein Kargaran",
        "Silvia Severini",
        "Masoud Jalili Sabet",
        "Nora Kassner",
        "Chunlan Ma",
        "Helmut Schmid",
        "André Martins",
        "François Yvon",
        "Hinrich Schütze"
      ],
      "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, “help” from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world’s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 63,
      "original_url": "https://aclanthology.org/2023.acl-long.62/",
      "title": "Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.62.pdf",
      "authors": [
        "Huawen Feng",
        "Junlong Liu",
        "Junhao Zheng",
        "Haibin Chen",
        "Xichen Shang",
        "Qianli Ma"
      ],
      "abstract": "Emotion-Cause Pair Extraction (ECPE) aims to identify the document’s emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using the same graph structure. In this paper, we propose a **J**oint **C**onstrained Learning framework with **B**oundary-adjusting for Emotion-Cause Pair Extraction (**JCB**). Specifically, through constrained learning, we summarize the prior rules existing in the data and force the model to take them into consideration in optimization, which helps the model learn a better representation from unbalanced data. Furthermore, we adjust the decision boundary of classifiers according to the relations between subtasks, which have always been ignored. No longer working independently as in the previous framework, the classifiers corresponding to three subtasks cooperate under the relation constraints. Experimental results show that **JCB** obtains competitive results compared with state-of-the-art methods and prove its robustness on unbalanced data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 64,
      "original_url": "https://aclanthology.org/2023.acl-long.63/",
      "title": "Pretrained Bidirectional Distillation for Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.63.pdf",
      "authors": [
        "Yimeng Zhuang",
        "Mei Tu"
      ],
      "abstract": "Knowledge transfer can boost neural machine translation (NMT), for example, by finetuning a pretrained masked language model (LM). However, it may suffer from the forgetting problem and the structural inconsistency between pretrained LMs and NMT models. Knowledge distillation (KD) may be a potential solution to alleviate these issues, but few studies have investigated language knowledge transfer from pretrained language models to NMT models through KD. In this paper, we propose Pretrained Bidirectional Distillation (PBD) for NMT, which aims to efficiently transfer bidirectional language knowledge from masked language pretraining to NMT models. Its advantages are reflected in efficiency and effectiveness through a globally defined and bidirectional context-aware distillation objective. Bidirectional language knowledge of the entire sequence is transferred to an NMT model concurrently during translation training. Specifically, we propose self-distilled masked language pretraining to obtain the PBD objective. We also design PBD losses to efficiently distill the language knowledge, in the form of token probabilities, to the encoder and decoder of an NMT model using the PBD objective. Extensive experiments reveal that pretrained bidirectional distillation can significantly improve machine translation performance and achieve competitive or even better results than previous pretrain-finetune or unified multilingual translation methods in supervised, unsupervised, and zero-shot scenarios. Empirically, it is concluded that pretrained bidirectional distillation is an effective and efficient method for transferring language knowledge from pretrained language models to NMT models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 65,
      "original_url": "https://aclanthology.org/2023.acl-long.64/",
      "title": "Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.64.pdf",
      "authors": [
        "Kyuyong Shin",
        "Hanock Kwak",
        "Wonjae Kim",
        "Jisu Jeong",
        "Seungjae Jung",
        "Kyungmin Kim",
        "Jung-Woo Ha",
        "Sang-Woo Lee"
      ],
      "abstract": "Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users’ behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 66,
      "original_url": "https://aclanthology.org/2023.acl-long.65/",
      "title": "Improving Continual Relation Extraction by Distinguishing Analogous Semantics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.65.pdf",
      "authors": [
        "Wenzheng Zhao",
        "Yuanning Cui",
        "Wei Hu"
      ],
      "abstract": "Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 67,
      "original_url": "https://aclanthology.org/2023.acl-long.66/",
      "title": "Improving Pretraining Techniques for Code-Switched NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.66.pdf",
      "authors": [
        "Richeek Das",
        "Sahasra Ranjan",
        "Shreya Pathak",
        "Preethi Jyothi"
      ],
      "abstract": "Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world’s languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages interspersed within a sentence) is much more scarce. Given this resource constraint, the question of how pretraining using limited amounts of code-switched text could be altered to improve performance for code-switched NLP becomes important to tackle. In this paper, we explore different masked language modeling (MLM) pretraining techniques for code-switched text that are cognizant of language boundaries prior to masking. The language identity of the tokens can either come from human annotators, trained language classifiers, or simple relative frequency-based estimates. We also present an MLM variant by introducing a residual connection from an earlier layer in the pretrained model that uniformly boosts performance on downstream tasks. Experiments on two downstream tasks, Question Answering (QA) and Sentiment Analysis (SA), involving four code-switched language pairs (Hindi-English, Spanish-English, Tamil-English, Malayalam-English) yield relative improvements of up to 5.8 and 2.7 F1 scores on QA (Hindi-English) and SA (Tamil-English), respectively, compared to standard pretraining techniques. To understand our task improvements better, we use a series of probes to study what additional information is encoded by our pretraining techniques and also introduce an auxiliary loss function that explicitly models language identification to further aid the residual MLM variants."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 68,
      "original_url": "https://aclanthology.org/2023.acl-long.67/",
      "title": "A Theory of Unsupervised Speech Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.67.pdf",
      "authors": [
        "Liming Wang",
        "Mark Hasegawa-Johnson",
        "Chang Yoo"
      ],
      "abstract": "Unsupervised speech recognition ({pasted macro ‘ASRU’}/) is the problem of learning automatic speech recognition (ASR) systems from unpaired speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and address such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of {pasted macro ‘ASRU’}/ systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of {pasted macro ‘ASRU’}/. Extensive {pasted macro ‘ASRU’}/ experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory (code available at https://github.com/cactuswiththoughts/UnsupASRTheory.gitcactuswiththoughts/UnsupASRTheory.git)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 69,
      "original_url": "https://aclanthology.org/2023.acl-long.68/",
      "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.68.pdf",
      "authors": [
        "Batu Ozturkler",
        "Nikolay Malkin",
        "Zhen Wang",
        "Nebojsa Jojic"
      ],
      "abstract": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 70,
      "original_url": "https://aclanthology.org/2023.acl-long.69/",
      "title": "NLG Evaluation Metrics Beyond Correlation Analysis: An Empirical Metric Preference Checklist",
      "pdf_url": "https://aclanthology.org/2023.acl-long.69.pdf",
      "authors": [
        "Iftitahu Nimah",
        "Meng Fang",
        "Vlado Menkovski",
        "Mykola Pechenizkiy"
      ],
      "abstract": "In this study, we analyze automatic evaluation metrics for Natural Language Generation (NLG), specifically task-agnostic metrics and human-aligned metrics. Task-agnostic metrics, such as Perplexity, BLEU, BERTScore, are cost-effective and highly adaptable to diverse NLG tasks, yet they have a weak correlation with human. Human-aligned metrics (CTC, CtrlEval, UniEval) improves correlation level by incorporating desirable human-like qualities as training objective. However, their effectiveness at discerning system-level performance and quality of system outputs remain unclear. We present metric preference checklist as a framework to assess the effectiveness of automatic metrics in three NLG tasks: Text Summarization, Dialogue Response Generation, and Controlled Generation. Our proposed framework provides access: (i) for verifying whether automatic metrics are faithful to human preference, regardless of their correlation level to human; and (ii) for inspecting the strengths and limitations of NLG systems via pairwise evaluation. We show that automatic metrics provide a better guidance than human on discriminating system-level performance in Text Summarization and Controlled Generation tasks. We also show that multi-aspect human-aligned metric (UniEval) is not necessarily dominant over single-aspect human-aligned metrics (CTC, CtrlEval) and task-agnostic metrics (BLEU, BERTScore), particularly in Controlled Generation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 71,
      "original_url": "https://aclanthology.org/2023.acl-long.70/",
      "title": "DialoGPS: Dialogue Path Sampling in Continuous Semantic Space for Data Augmentation in Multi-Turn Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.70.pdf",
      "authors": [
        "Ang Lv",
        "Jinpeng Li",
        "Yuhan Chen",
        "Gao Xing",
        "Ji Zhang",
        "Rui Yan"
      ],
      "abstract": "In open-domain dialogue generation tasks, contexts and responses in most datasets are one-to-one mapped, violating an important many-to-many characteristic: a context leads to various responses, and a response answers multiple contexts. Without such patterns, models poorly generalize and prefer responding safely. Many attempts have been made in either multi-turn settings from a one-to-many perspective or in a many-to-many perspective but limited to single-turn settings. The major challenge to many-to-many augment multi-turn dialogues is that discretely replacing each turn with semantic similarity breaks fragile context coherence. In this paper, we propose DialoGue Path Sampling (DialoGPS) method in continuous semantic space, the first many-to-many augmentation method for multi-turn dialogues. Specifically, we map a dialogue to our extended Brownian Bridge, a special Gaussian process. We sample latent variables to form coherent dialogue paths in the continuous space. A dialogue path corresponds to a new multi-turn dialogue and is used as augmented training data. We show the effect of DialoGPS with both automatic and human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 72,
      "original_url": "https://aclanthology.org/2023.acl-long.71/",
      "title": "TECHS: Temporal Logical Graph Networks for Explainable Extrapolation Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.71.pdf",
      "authors": [
        "Qika Lin",
        "Jun Liu",
        "Rui Mao",
        "Fangzhi Xu",
        "Erik Cambria"
      ],
      "abstract": "Extrapolation reasoning on temporal knowledge graphs (TKGs) aims to forecast future facts based on past counterparts. There are two main challenges: (1) incorporating the complex information, including structural dependencies, temporal dynamics, and hidden logical rules; (2) implementing differentiable logical rule learning and reasoning for explainability. To this end, we propose an explainable extrapolation reasoning framework TEemporal logiCal grapH networkS (TECHS), which mainly contains a temporal graph encoder and a logical decoder. The former employs a graph convolutional network with temporal encoding and heterogeneous attention to embed topological structures and temporal dynamics. The latter integrates propositional reasoning and first-order reasoning by introducing a reasoning graph that iteratively expands to find the answer. A forward message-passing mechanism is also proposed to update node representations, and their propositional and first-order attention scores. Experimental results demonstrate that it outperforms state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 73,
      "original_url": "https://aclanthology.org/2023.acl-long.72/",
      "title": "Consistency Regularization Training for Compositional Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.72.pdf",
      "authors": [
        "Yongjing Yin",
        "Jiali Zeng",
        "Yafu Li",
        "Fandong Meng",
        "Jie Zhou",
        "Yue Zhang"
      ],
      "abstract": "Existing neural models have difficulty generalizing to unseen combinations of seen components. To achieve compositional generalization, models are required to consistently interpret (sub)expressions across contexts. Without modifying model architectures, we improve the capability of Transformer on compositional generalization through consistency regularization training, which promotes representation consistency across samples and prediction consistency for a single sample. Experimental results on semantic parsing and machine translation benchmarks empirically demonstrate the effectiveness and generality of our method. In addition, we find that the prediction consistency scores on in-distribution validation sets can be an alternative for evaluating models during training, when commonly-used metrics are not informative."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 74,
      "original_url": "https://aclanthology.org/2023.acl-long.73/",
      "title": "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.73.pdf",
      "authors": [
        "Shengming Yin",
        "Chenfei Wu",
        "Huan Yang",
        "Jianfeng Wang",
        "Xiaodong Wang",
        "Minheng Ni",
        "Zhengyuan Yang",
        "Linjie Li",
        "Shuguang Liu",
        "Fan Yang",
        "Jianlong Fu",
        "Ming Gong",
        "Lijuan Wang",
        "Zicheng Liu",
        "Houqiang Li",
        "Nan Duan"
      ],
      "abstract": "In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a “coarse-to-fine” process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26%) at the same hardware setting when generating 1024 frames. The homepage link is [NUWA-XL](https://msra-nuwa.azurewebsites.net)"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 75,
      "original_url": "https://aclanthology.org/2023.acl-long.74/",
      "title": "Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe",
      "pdf_url": "https://aclanthology.org/2023.acl-long.74.pdf",
      "authors": [
        "Xiang Yue",
        "Huseyin Inan",
        "Xuechen Li",
        "Girish Kumar",
        "Julia McAnallen",
        "Hoda Shajari",
        "Huan Sun",
        "David Levitan",
        "Robert Sim"
      ],
      "abstract": "Privacy concerns have attracted increasing attention in data-driven products due to the tendency of machine learning models to memorize sensitive training data. Generating synthetic versions of such data with a formal privacy guarantee, such as differential privacy (DP), provides a promising path to mitigating these privacy concerns, but previous approaches in this direction have typically failed to produce synthetic data of high quality. In this work, we show that a simple and practical recipe in the text domain is effective: simply fine-tuning a pretrained generative language model with DP enables the model to generate useful synthetic text with strong privacy protection. Through extensive empirical analyses on both benchmark and private customer data, we demonstrate that our method produces synthetic text that is competitive in terms of utility with its non-private counterpart, meanwhile providing strong protection against potential privacy leakages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 76,
      "original_url": "https://aclanthology.org/2023.acl-long.75/",
      "title": "A Close Look into the Calibration of Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.75.pdf",
      "authors": [
        "Yangyi Chen",
        "Lifan Yuan",
        "Ganqu Cui",
        "Zhiyuan Liu",
        "Heng Ji"
      ],
      "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs’ calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs’ confidence in wrong predictions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 77,
      "original_url": "https://aclanthology.org/2023.acl-long.76/",
      "title": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.76.pdf",
      "authors": [
        "Yu Li",
        "Baolin Peng",
        "Pengcheng He",
        "Michel Galley",
        "Zhou Yu",
        "Jianfeng Gao"
      ],
      "abstract": "Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one from a fine-tuned summarization model and the other from important dialogue turns. We then choose one of these pseudo summaries based on information distribution differences in different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialogue corpus. Our experiments show that DIONYSUS outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 78,
      "original_url": "https://aclanthology.org/2023.acl-long.77/",
      "title": "MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.77.pdf",
      "authors": [
        "Wang Jing",
        "Aixin Sun",
        "Hao Zhang",
        "Xiaoli Li"
      ],
      "abstract": "Given a text query, the task of Natural Language Video Localization (NLVL) is to localize a temporal moment in an untrimmed video that semantically matches the query. In this paper, we adopt a proposal-based solution that generates proposals (i.e. candidate moments) and then select the best matching proposal. On top of modeling the cross-modal interaction between candidate moments and the query, our proposed Moment Sampling DETR (MS-DETR) enables efficient moment-moment relation modeling. The core idea is to sample a subset of moments guided by the learnable templates with an adopted DETR framework. To achieve this, we design a multi-scale visual-linguistic encoder, and an anchor-guided moment decoder paired with a set of learnable templates. Experimental results on three public datasets demonstrate the superior performance of MS-DETR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 79,
      "original_url": "https://aclanthology.org/2023.acl-long.78/",
      "title": "Diverse Demonstrations Improve In-context Compositional Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.78.pdf",
      "authors": [
        "Itay Levy",
        "Ben Bogin",
        "Jonathan Berant"
      ],
      "abstract": "In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 80,
      "original_url": "https://aclanthology.org/2023.acl-long.79/",
      "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.79.pdf",
      "authors": [
        "Zhiyong Wu",
        "Yaoxiang Wang",
        "Jiacheng Ye",
        "Lingpeng Kong"
      ],
      "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code will be released to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 81,
      "original_url": "https://aclanthology.org/2023.acl-long.80/",
      "title": "On the Efficacy of Sampling Adapters",
      "pdf_url": "https://aclanthology.org/2023.acl-long.80.pdf",
      "authors": [
        "Clara Meister",
        "Tiago Pimentel",
        "Luca Malagutti",
        "Ethan Wilcox",
        "Ryan Cotterell"
      ],
      "abstract": "Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model’s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the token-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 82,
      "original_url": "https://aclanthology.org/2023.acl-long.81/",
      "title": "Cross-Domain Data Augmentation with Domain-Adaptive Language Modeling for Aspect-Based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.81.pdf",
      "authors": [
        "Jianfei Yu",
        "Qiankun Zhao",
        "Rui Xia"
      ],
      "abstract": "Cross-domain Aspect-Based Sentiment Analysis (ABSA) aims to leverage the useful knowledge from a source domain to identify aspect-sentiment pairs in sentences from a target domain. To tackle the task, several recent works explore a new unsupervised domain adaptation framework, i.e., Cross-Domain Data Augmentation (CDDA), aiming to directly generate much labeled target-domain data based on the labeled source-domain data. However, these CDDA methods still suffer from several issues: 1) preserving many source-specific attributes such as syntactic structures; 2) lack of fluency and coherence; 3) limiting the diversity of generated data. To address these issues, we propose a new cross-domain Data Augmentation approach based on Domain-Adaptive Language Modeling named DA2LM, which contains three stages: 1) assigning pseudo labels to unlabeled target-domain data; 2) unifying the process of token generation and labeling with a Domain-Adaptive Language Model (DALM) to learn the shared context and annotation across domains; 3) using the trained DALM to generate labeled target-domain data. Experiments show that DA2LM consistently outperforms previous feature adaptation and CDDA methods on both ABSA and Aspect Extraction tasks. The source code is publicly released at https://github.com/NUSTM/DALM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 83,
      "original_url": "https://aclanthology.org/2023.acl-long.82/",
      "title": "Compositional Data Augmentation for Abstractive Conversation Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.82.pdf",
      "authors": [
        "Siru Ouyang",
        "Jiaao Chen",
        "Jiawei Han",
        "Diyi Yang"
      ],
      "abstract": "Recent abstractive conversation summarization systems generally rely on large-scale datasets with annotated summaries. However, collecting and annotating these conversations can be a time-consuming and labor-intensive task. To address this issue, in this work, we present a sub-structure level compositional data augmentation method, Compo, for generating diverse and high-quality pairs of conversations and summaries. Specifically, Compo first extracts conversation structures like topic splits and action triples as basic units. Then we organize these semantically meaningful conversation snippets compositionally to create new training instances. Additionally, we explore noise-tolerant settings in both self-training and joint-training paradigms to make the most of these augmented samples. Our experiments on benchmark datasets, SAMSum and DialogSum, show that Compo substantially outperforms prior baseline methods by achieving a nearly 10% increase of ROUGE scores with limited data. Code is available at https://github.com/ozyyshr/Compo."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 84,
      "original_url": "https://aclanthology.org/2023.acl-long.83/",
      "title": "PMAES: Prompt-mapping Contrastive Learning for Cross-prompt Automated Essay Scoring",
      "pdf_url": "https://aclanthology.org/2023.acl-long.83.pdf",
      "authors": [
        "Yuan Chen",
        "Xia Li"
      ],
      "abstract": "Current cross-prompt automated essay scoring (AES) is a challenging task due to the large discrepancies between different prompts, such as different genres and expressions. The main goal of current cross-prompt AES systems is to learn enough shared features between the source and target prompts to grade well on the target prompt. However, because the features are captured based on the original prompt representation, they may be limited by being extracted directly between essays. In fact, when the representations of two prompts are more similar, we can gain more shared features between them. Based on this motivation, in this paper, we propose a learning strategy called “prompt-mapping” to learn about more consistent representations of source and target prompts. In this way, we can obtain more shared features between the two prompts and use them to better represent the essays for the target prompt. Experimental results on the ASAP++ dataset demonstrate the effectiveness of our method. We also design experiments in different settings to show that our method can be applied in different scenarios. Our code is available at https://github.com/gdufsnlp/PMAES."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 85,
      "original_url": "https://aclanthology.org/2023.acl-long.84/",
      "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.84.pdf",
      "authors": [
        "Myra Cheng",
        "Esin Durmus",
        "Dan Jurafsky"
      ],
      "abstract": "To recognize and mitigate harms from large language models (LLMs), we need to understand the prevalence and nuances of stereotypes in LLM outputs. Toward this end, we present Marked Personas, a prompt-based method to measure stereotypes in LLMs for intersectional demographic groups without any lexicon or data labeling. Grounded in the sociolinguistic concept of markedness (which characterizes explicitly linguistically marked categories versus unmarked defaults), our proposed method is twofold: 1) prompting an LLM to generate personas, i.e., natural language descriptions, of the target demographic group alongside personas of unmarked, default groups; 2) identifying the words that significantly distinguish personas of the target group from corresponding unmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4 contain higher rates of racial stereotypes than human-written portrayals using the same prompts. The words distinguishing personas of marked (non-white, non-male) groups reflect patterns of othering and exoticizing these demographics. An intersectional lens further reveals tropes that dominate portrayals of marginalized groups, such as tropicalism and the hypersexualization of minoritized women. These representational harms have concerning implications for downstream applications like story generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 86,
      "original_url": "https://aclanthology.org/2023.acl-long.85/",
      "title": "On Prefix-tuning for Lightweight Out-of-distribution Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.85.pdf",
      "authors": [
        "Yawen Ouyang",
        "Yongchang Cao",
        "Yuan Gao",
        "Zhen Wu",
        "Jianbing Zhang",
        "Xinyu Dai"
      ],
      "abstract": "Out-of-distribution (OOD) detection, a fundamental task vexing real-world applications, has attracted growing attention in the NLP community. Recently fine-tuning based methods have made promising progress. However, it could be costly to store fine-tuned models for each scenario. In this paper, we depart from the classic fine-tuning based OOD detection toward a parameter-efficient alternative, and propose an unsupervised prefix-tuning based OOD detection framework termed PTO. Additionally, to take advantage of optional training data labels and targeted OOD data, two practical extensions of PTO are further proposed. Overall, PTO and its extensions offer several key advantages of being lightweight, easy-to-reproduce, and theoretically justified. Experimental results show that our methods perform comparably to, even better than, existing fine-tuning based OOD detection approaches under a wide range of metrics, detection settings, and OOD types."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 87,
      "original_url": "https://aclanthology.org/2023.acl-long.86/",
      "title": "GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.86.pdf",
      "authors": [
        "Konstantin Yakovlev",
        "Alexander Podolskiy",
        "Andrey Bout",
        "Sergey Nikolenko",
        "Irina Piontkovskaya"
      ],
      "abstract": "Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we propose a novel non-autoregressive approach to GEC that decouples the architecture into a permutation network that outputs a self-attention weight matrix that can be used in beam search to find the best permutation of input tokens (with auxiliary <ins> tokens) and a decoder network based on a step-unrolled denoising autoencoder that fills in specific tokens. This allows us to find the token permutation after only one forward pass of the permutation network, avoiding autoregressive constructions. We show that the resulting network improves over previously known non-autoregressive methods for GEC and reaches the level of autoregressive methods that do not use language-specific synthetic data generation methods. Our results are supported by a comprehensive experimental validation on the ConLL-2014 and BEA datasets and an extensive ablation study that supports our architectural and algorithmic choices."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 88,
      "original_url": "https://aclanthology.org/2023.acl-long.87/",
      "title": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.87.pdf",
      "authors": [
        "Emanuele Bugliarello",
        "Laurent Sartran",
        "Aishwarya Agrawal",
        "Lisa Anne Hendricks",
        "Aida Nematzadeh"
      ],
      "abstract": "While pretraining on large-scale image–text data from the Web has facilitated rapid progress on many vision-and-language (V&L) tasks, recent work has demonstrated that pretrained models lack “fine-grained” understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for some tasks, performance peaks early in training or significantly fluctuates, never converging."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 89,
      "original_url": "https://aclanthology.org/2023.acl-long.88/",
      "title": "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information",
      "pdf_url": "https://aclanthology.org/2023.acl-long.88.pdf",
      "authors": [
        "Sunjae Kwon",
        "Rishabh Garodia",
        "Minhwa Lee",
        "Zhichao Yang",
        "Hong Yu"
      ],
      "abstract": "Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 90,
      "original_url": "https://aclanthology.org/2023.acl-long.89/",
      "title": "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.89.pdf",
      "authors": [
        "Kaixin Ma",
        "Hao Cheng",
        "Yu Zhang",
        "Xiaodong Liu",
        "Eric Nyberg",
        "Jianfeng Gao"
      ],
      "abstract": "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 91,
      "original_url": "https://aclanthology.org/2023.acl-long.90/",
      "title": "Elaboration-Generating Commonsense Question Answering at Scale",
      "pdf_url": "https://aclanthology.org/2023.acl-long.90.pdf",
      "authors": [
        "Wenya Wang",
        "Vivek Srikumar",
        "Hannaneh Hajishirzi",
        "Noah A. Smith"
      ],
      "abstract": "In question answering requiring common sense, language models (e.g., GPT-3) have been used to generate text expressing background knowledge that helps improve performance. Yet the cost of working with such models is very high; in this work, we finetune smaller language models to generate useful intermediate context, referred to here as elaborations. Our framework alternates between updating two language models—an elaboration generator and an answer predictor—allowing each to influence the other. Using less than 0.5% of the parameters of GPT-3, our model outperforms alternatives with similar sizes and closes the gap with GPT-3 on four commonsense question answering benchmarks. Human evaluations show that the quality of the generated elaborations is high."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 92,
      "original_url": "https://aclanthology.org/2023.acl-long.91/",
      "title": "Neural Unsupervised Reconstruction of Protolanguage Word Forms",
      "pdf_url": "https://aclanthology.org/2023.acl-long.91.pdf",
      "authors": [
        "Andre He",
        "Nicholas Tomlin",
        "Dan Klein"
      ],
      "abstract": "We present a state-of-the-art neural approach to the unsupervised reconstruction of ancient word forms. Previous work in this domain used expectation-maximization to predict simple phonological changes between ancient word forms and their cognates in modern languages. We extend this work with neural models that can capture more complicated phonological and morphological changes. At the same time, we preserve the inductive biases from classical methods by building monotonic alignment constraints into the model and deliberately underfitting during the maximization step. We evaluate our performance on the task of reconstructing Latin from a dataset of cognates across five Romance languages, achieving a notable reduction in edit distance from the target word forms compared to previous methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 93,
      "original_url": "https://aclanthology.org/2023.acl-long.92/",
      "title": "DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.92.pdf",
      "authors": [
        "Menglong Lu",
        "Zhen Huang",
        "Yunxiang Zhao",
        "Zhiliang Tian",
        "Yang Liu",
        "Dongsheng Li"
      ],
      "abstract": "Self-training emerges as an important research line on domain adaptation. By taking the model’s prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanish- ment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 94,
      "original_url": "https://aclanthology.org/2023.acl-long.93/",
      "title": "On Evaluating Multilingual Compositional Generalization with Translated Datasets",
      "pdf_url": "https://aclanthology.org/2023.acl-long.93.pdf",
      "authors": [
        "Zi Wang",
        "Daniel Hershcovich"
      ],
      "abstract": "Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages? Can models compositionally generalize cross-lingually? As a first step to answering these questions, recent work used neural machine translation to translate datasets for evaluating compositional generalization in semantic parsing. However, we show that this entails critical semantic distortion. To address this limitation, we craft a faithful rule-based translation of the MCWQ dataset from English to Chinese and Japanese. Even with the resulting robust benchmark, which we call MCWQ-R, we show that the distribution of compositions still suffers due to linguistic divergences, and that multilingual models still struggle with cross-lingual compositional generalization. Our dataset and methodology will serve as useful resources for the study of cross-lingual compositional generalization in other tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 95,
      "original_url": "https://aclanthology.org/2023.acl-long.94/",
      "title": "FAA: Fine-grained Attention Alignment for Cascade Document Ranking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.94.pdf",
      "authors": [
        "Zhen Li",
        "Chongyang Tao",
        "Jiazhan Feng",
        "Tao Shen",
        "Dongyan Zhao",
        "Xiubo Geng",
        "Daxin Jiang"
      ],
      "abstract": "Document ranking aims at sorting a collection of documents with their relevance to a query. Contemporary methods explore more efficient transformers or divide long documents into passages to handle the long input. However, intensive query-irrelevant content may lead to harmful distraction and high query latency. Some recent works further propose cascade document ranking models that extract relevant passages with an efficient selector before ranking, however, their selection and ranking modules are almost independently optimized and deployed, leading to selecting error reinforcement and sub-optimal performance. In fact, the document ranker can provide fine-grained supervision to make the selector more generalizable and compatible, and the selector built upon a different structure can offer a distinct perspective to assist in document ranking. Inspired by this, we propose a fine-grained attention alignment approach to jointly optimize a cascade document ranking model. Specifically, we utilize the attention activations over the passages from the ranker as fine-grained attention feedback to optimize the selector. Meanwhile, we fuse the relevance scores from the passage selector into the ranker to assist in calculating the cooperative matching representation. Experiments on MS MARCO and TREC DL demonstrate the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 96,
      "original_url": "https://aclanthology.org/2023.acl-long.95/",
      "title": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.95.pdf",
      "authors": [
        "Zhong Zhang",
        "Bang Liu",
        "Junming Shao"
      ],
      "abstract": "Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 97,
      "original_url": "https://aclanthology.org/2023.acl-long.96/",
      "title": "Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-long.96.pdf",
      "authors": [
        "Jinfeng Zhou",
        "Zhuang Chen",
        "Bo Wang",
        "Minlie Huang"
      ],
      "abstract": "Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one’s mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., question), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose Supporter, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy’s learning for responding. Experiments verify the superiority of Supporter in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 98,
      "original_url": "https://aclanthology.org/2023.acl-long.97/",
      "title": "Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.97.pdf",
      "authors": [
        "Mingzhu Cai",
        "Siqi Bao",
        "Xin Tian",
        "Huang He",
        "Fan Wang",
        "Hua Wu"
      ],
      "abstract": "In this paper, we propose an unsupervised query enhanced approach for knowledge-intensive conversations, namely QKConv. There are three modules in QKConv: a query generator, an off-the-shelf knowledge selector, and a response generator. QKConv is optimized through joint training, which produces the response by exploring multiple candidate queries and leveraging corresponding selected knowledge. The joint training solely relies on the dialogue context and target response, getting exempt from extra query annotations or knowledge provenances. To evaluate the effectiveness of the proposed QKConv, we conduct experiments on three representative knowledge-intensive conversation datasets: conversational question-answering, task-oriented dialogue, and knowledge-grounded conversation. Experimental results reveal that QKConv performs better than all unsupervised methods across three datasets and achieves competitive performance compared to supervised methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 99,
      "original_url": "https://aclanthology.org/2023.acl-long.98/",
      "title": "Why Aren’t We NER Yet? Artifacts of ASR Errors in Named Entity Recognition in Spontaneous Speech Transcripts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.98.pdf",
      "authors": [
        "Piotr Szymański",
        "Lukasz Augustyniak",
        "Mikolaj Morzy",
        "Adrian Szymczak",
        "Krzysztof Surdyk",
        "Piotr Żelasko"
      ],
      "abstract": "Transcripts of spontaneous human speech present a significant obstacle for traditional NER models. The lack of grammatical structure of spoken utterances and word errors introduced by the ASR make downstream NLP tasks challenging. In this paper, we examine in detail the complex relationship between ASR and NER errors which limit the ability of NER models to recover entity mentions from spontaneous speech transcripts. Using publicly available benchmark datasets (SWNE, Earnings-21, OntoNotes), we present the full taxonomy of ASR-NER errors and measure their true impact on entity recognition. We find that NER models fail spectacularly even if no word errors are introduced by the ASR. We also show why the F1 score is inadequate to evaluate NER models on conversational transcripts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 100,
      "original_url": "https://aclanthology.org/2023.acl-long.99/",
      "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels",
      "pdf_url": "https://aclanthology.org/2023.acl-long.99.pdf",
      "authors": [
        "Luyu Gao",
        "Xueguang Ma",
        "Jimmy Lin",
        "Jamie Callan"
      ],
      "abstract": "While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 101,
      "original_url": "https://aclanthology.org/2023.acl-long.100/",
      "title": "White-Box Multi-Objective Adversarial Attack on Dialogue Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.100.pdf",
      "authors": [
        "Yufei Li",
        "Zexin Li",
        "Yingfan Gao",
        "Cong Liu"
      ],
      "abstract": "Pre-trained transformers are popular in state-of-the-art dialogue generation (DG) systems. Such language models are, however, vulnerable to various adversarial samples as studied in traditional tasks such as text classification, which inspires our curiosity about their robustness in DG systems. One main challenge of attacking DG models is that perturbations on the current sentence can hardly degrade the response accuracy because the unchanged chat histories are also considered for decision-making. Instead of merely pursuing pitfalls of performance metrics such as BLEU, ROUGE, we observe that crafting adversarial samples to force longer generation outputs benefits attack effectiveness—the generated responses are typically irrelevant, lengthy, and repetitive. To this end, we propose a white-box multi-objective attack method called DGSlow. Specifically, DGSlow balances two objectives—generation accuracy and length, via a gradient-based multi-objective optimizer and applies an adaptive searching mechanism to iteratively craft adversarial samples with only a few modifications. Comprehensive experiments on four benchmark datasets demonstrate that DGSlow could significantly degrade state-of-the-art DG models with a higher success rate than traditional accuracy-based methods. Besides, our crafted sentences also exhibit strong transferability in attacking other models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 102,
      "original_url": "https://aclanthology.org/2023.acl-long.101/",
      "title": "A Cautious Generalization Goes a Long Way: Learning Morphophonological Rules",
      "pdf_url": "https://aclanthology.org/2023.acl-long.101.pdf",
      "authors": [
        "Salam Khalifa",
        "Sarah Payne",
        "Jordan Kodner",
        "Ellen Broselow",
        "Owen Rambow"
      ],
      "abstract": "Explicit linguistic knowledge, encoded by resources such as rule-based morphological analyzers, continues to prove useful in downstream NLP tasks, especially for low-resource languages and dialects. Rules are an important asset in descriptive linguistic grammars. However, creating such resources is usually expensive and non-trivial, especially for spoken varieties with no written standard. In this work, we present a novel approach for automatically learning morphophonological rules of Arabic from a corpus. Motivated by classic cognitive models for rule learning, rules are generalized cautiously. Rules that are memorized for individual items are only allowed to generalize to unseen forms if they are sufficiently reliable in the training data. The learned rules are further examined to ensure that they capture true linguistic phenomena described by domain experts. We also investigate the learnability of rules in low-resource settings across different experimental setups and dialects."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 103,
      "original_url": "https://aclanthology.org/2023.acl-long.102/",
      "title": "Few-shot Adaptation Works with UnpredicTable Data",
      "pdf_url": "https://aclanthology.org/2023.acl-long.102.pdf",
      "authors": [
        "Jun Shern Chan",
        "Michael Pieler",
        "Jonathan Jao",
        "Jérémy Scheurer",
        "Ethan Perez"
      ],
      "abstract": "Prior work on language models (LMs) shows that training on a large number of diverse tasks improves few-shot learning (FSL) performance on new tasks. We take this to the extreme, automatically extracting 413,299 tasks from internet tables - orders of magnitude more than the next-largest public datasets. Finetuning on the resulting dataset leads to improved FSL performance on Natural Language Processing (NLP) tasks, but not proportionally to dataset scale. In fact, we find that narrow subsets of our dataset sometimes outperform more diverse datasets. For example, finetuning on software documentation from support.google.com raises FSL performance by a mean of +7.5% on 52 downstream tasks, which beats training on 40 human-curated NLP datasets (+6.7%). Finetuning on various narrow datasets leads to similar broad improvements across test tasks, suggesting that the gains are not from domain adaptation but adapting to FSL in general. We do not observe clear patterns between the datasets that lead to FSL gains, leaving open questions about why certain data helps with FSL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 104,
      "original_url": "https://aclanthology.org/2023.acl-long.103/",
      "title": "Cross-lingual Science Journalism: Select, Simplify and Rewrite Summaries for Non-expert Readers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.103.pdf",
      "authors": [
        "Mehwish Fatima",
        "Michael Strube"
      ],
      "abstract": "Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science journalists’ work. We analyze the performance of possible existing solutions as baselines for the CSJ task. Based on these findings, we propose to combine the three components - SELECT, SIMPLIFY and REWRITE (SSR) to produce cross-lingual simplified science summaries for non-expert readers. Our empirical evaluation on the Wikipedia dataset shows that SSR significantly outperforms the baselines for the CSJ task and can serve as a strong baseline for future work. We also perform an ablation study investigating the impact of individual components of SSR. Further, we analyze the performance of SSR on a high-quality, real-world CSJ dataset with human evaluation and in-depth analysis, demonstrating the superior performance of SSR for CSJ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 105,
      "original_url": "https://aclanthology.org/2023.acl-long.104/",
      "title": "HuCurl: Human-induced Curriculum Discovery",
      "pdf_url": "https://aclanthology.org/2023.acl-long.104.pdf",
      "authors": [
        "Mohamed Elgaar",
        "Hadi Amiri"
      ],
      "abstract": "We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 106,
      "original_url": "https://aclanthology.org/2023.acl-long.105/",
      "title": "kNN-TL: k-Nearest-Neighbor Transfer Learning for Low-Resource Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.105.pdf",
      "authors": [
        "Shudong Liu",
        "Xuebo Liu",
        "Derek F. Wong",
        "Zhaocong Li",
        "Wenxiang Jiao",
        "Lidia S. Chao",
        "Min Zhang"
      ],
      "abstract": "Transfer learning has been shown to be an effective technique for enhancing the performance of low-resource neural machine translation (NMT). This is typically achieved through either fine-tuning a child model with a pre-trained parent model, or by utilizing the out- put of the parent model during the training of the child model. However, these methods do not make use of the parent knowledge during the child inference, which may limit the translation performance. In this paper, we propose a k-Nearest-Neighbor Transfer Learning (kNN-TL) approach for low-resource NMT, which leverages the parent knowledge throughout the entire developing process of the child model. Our approach includes a parent-child representation alignment method, which ensures consistency in the output representations between the two models, and a child-aware datastore construction method that improves inference efficiency by selectively distilling the parent datastore based on relevance to the child model. Experimental results on four low-resource translation tasks show that kNN-TL outperforms strong baselines. Extensive analyses further demonstrate the effectiveness of our approach. Code and scripts are freely available at https://github.com/NLP2CT/kNN-TL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 107,
      "original_url": "https://aclanthology.org/2023.acl-long.106/",
      "title": "Do language models have coherent mental models of everyday things?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.106.pdf",
      "authors": [
        "Yuling Gu",
        "Bhavana Dalvi Mishra",
        "Peter Clark"
      ],
      "abstract": "When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 “X relation Y?” true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM’s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM’s pictures of everyday things can be significantly reduced."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 108,
      "original_url": "https://aclanthology.org/2023.acl-long.107/",
      "title": "Rogue Scores",
      "pdf_url": "https://aclanthology.org/2023.acl-long.107.pdf",
      "authors": [
        "Max Grusky"
      ],
      "abstract": "Correct, comparable, and reproducible model evaluation is essential for progress in machine learning. Over twenty years, thousands of language and vision models have been evaluated with a popular metric called ROUGE. Does this widespread benchmark metric meet these three evaluation criteria? This systematic review of over two thousand publications using ROUGE finds: (A) Critical evaluation decisions and parameters are routinely omitted, making most reported scores irreproducible. (B) Differences in evaluation protocol are common, affect scores, and impact the comparability of results reported in many papers. (C) Thousands of papers use nonstandard evaluation packages with software defects that produce provably incorrect scores. Estimating the overall impact of these findings is difficult: because software citations are rare, it is nearly impossible to distinguish between correct ROUGE scores and incorrect “rogue scores.”"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 109,
      "original_url": "https://aclanthology.org/2023.acl-long.108/",
      "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.108.pdf",
      "authors": [
        "Or Honovich",
        "Uri Shaham",
        "Samuel R. Bowman",
        "Omer Levy"
      ],
      "abstract": "Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 110,
      "original_url": "https://aclanthology.org/2023.acl-long.109/",
      "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.109.pdf",
      "authors": [
        "Xiaoyang Hu",
        "Shane Storks",
        "Richard Lewis",
        "Joyce Chai"
      ],
      "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven’s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs’ analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 111,
      "original_url": "https://aclanthology.org/2023.acl-long.110/",
      "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.110.pdf",
      "authors": [
        "Avi Caciularu",
        "Matthew Peters",
        "Jacob Goldberger",
        "Ido Dagan",
        "Arman Cohan"
      ],
      "abstract": "The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while “peeking” into other topically-related documents. In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information. This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization).Following this scheme, we pre-train our model - termed QAmden - and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 112,
      "original_url": "https://aclanthology.org/2023.acl-long.111/",
      "title": "Tailoring Instructions to Student’s Learning Levels Boosts Knowledge Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.111.pdf",
      "authors": [
        "Yuxin Ren",
        "Zihan Zhong",
        "Xingjian Shi",
        "Yi Zhu",
        "Chun Yuan",
        "Mu Li"
      ],
      "abstract": "It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student’s generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher’s learning process. By prioritizing samples that are likely to enhance the student’s generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 113,
      "original_url": "https://aclanthology.org/2023.acl-long.112/",
      "title": "REV: Information-Theoretic Evaluation of Free-Text Rationales",
      "pdf_url": "https://aclanthology.org/2023.acl-long.112.pdf",
      "authors": [
        "Hanjie Chen",
        "Faeze Brahman",
        "Xiang Ren",
        "Yangfeng Ji",
        "Yejin Choi",
        "Swabha Swayamdipta"
      ],
      "abstract": "Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models’ reasoning and prediction processes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 114,
      "original_url": "https://aclanthology.org/2023.acl-long.113/",
      "title": "ELQA: A Corpus of Metalinguistic Questions and Answers about English",
      "pdf_url": "https://aclanthology.org/2023.acl-long.113.pdf",
      "authors": [
        "Shabnam Behzad",
        "Keisuke Sakaguchi",
        "Nathan Schneider",
        "Amir Zeldes"
      ],
      "abstract": "We present ELQA, a corpus of questions and answers in and about the English language. Collected from two online forums, the >70k questions (from English learners and others) cover wide-ranging topics including grammar, meaning, fluency, and etymology. The answers include descriptions of general properties of English vocabulary and grammar as well as explanations about specific (correct and incorrect) usage examples. Unlike most NLP datasets, this corpus is metalinguistic—it consists of language about language. As such, it can facilitate investigations of the metalinguistic capabilities of NLU models, as well as educational applications in the language learning domain. To study this, we define a free-form question answering task on our dataset and conduct evaluations on multiple LLMs (Large Language Models) to analyze their capacity to generate metalinguistic answers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 115,
      "original_url": "https://aclanthology.org/2023.acl-long.114/",
      "title": "Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.114.pdf",
      "authors": [
        "Qingyue Wang",
        "Liang Ding",
        "Yanan Cao",
        "Yibing Zhan",
        "Zheng Lin",
        "Shi Wang",
        "Dacheng Tao",
        "Li Guo"
      ],
      "abstract": "Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively decouple semantics of samples, limiting the zero-shot performance of DST. In this paper, we present a simple and effective “divide, conquer and combine” solution, which explicitly disentangles the semantics of seen data, and leverages the performance and robustness with the mixture-of-experts mechanism. Specifically, we divide the seen data into semantically independent subsets and train corresponding experts, the newly unseen samples are mapped and inferred with mixture-of-experts with our designed ensemble inference. Extensive experiments on MultiWOZ2.1 upon T5-Adapter show our schema significantly and consistently improves the zero-shot performance, achieving the SOTA on settings without external knowledge, with only 10M trainable parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 116,
      "original_url": "https://aclanthology.org/2023.acl-long.115/",
      "title": "BIG-C: a Multimodal Multi-Purpose Dataset for Bemba",
      "pdf_url": "https://aclanthology.org/2023.acl-long.115.pdf",
      "authors": [
        "Claytone Sikasote",
        "Eunice Mukonde",
        "Md Mahfuz Ibn Alam",
        "Antonios Anastasopoulos"
      ],
      "abstract": "We present BIG-C (Bemba Image Grounded Conversations), a large multimodal dataset for Bemba. While Bemba is the most populous language of Zambia, it exhibits a dearth of resources which render the development of language technologies or language processing research almost impossible. The dataset is comprised of multi-turn dialogues between Bemba speakers based on images, transcribed and translated into English. There are more than 92,000 utterances/sentences, amounting to more than 180 hours of audio data with corresponding transcriptions and English translations. We also provide baselines on speech recognition (ASR), machine translation (MT) and speech translation (ST) tasks, and sketch out other potential future multimodal uses of our dataset. We hope that by making the dataset available to the research community, this work will foster research and encourage collaboration across the language, speech, and vision communities especially for languages outside the “traditionally” used high-resourced ones. All data and code are publicly available: [https://github.com/csikasote/bigc](https://github.com/csikasote/bigc)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 117,
      "original_url": "https://aclanthology.org/2023.acl-long.116/",
      "title": "Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues",
      "pdf_url": "https://aclanthology.org/2023.acl-long.116.pdf",
      "authors": [
        "Yue Feng",
        "Yunlong Jiao",
        "Animesh Prasad",
        "Nikolaos Aletras",
        "Emine Yilmaz",
        "Gabriella Kazai"
      ],
      "abstract": "User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user’s task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user’s task goals. Existing studies on USM neglect explicitly modeling the user’s task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user’s preferences regarding the task attributes are fulfilled by the system for predicting the user’s satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task attributes. Finally, it predicts the user satisfaction based on task attribute fulfillment and task attribute importance. Experimental results on benchmark datasets (i.e. MWOZ, SGD, ReDial, and JDDC) show that SG-USM consistently outperforms competitive existing methods. Our extensive analysis demonstrates that SG-USM can improve the interpretability of user satisfaction modeling, has good scalability as it can effectively deal with unseen tasks and can also effectively work in low-resource settings by leveraging unlabeled data. Code is available at https://github.com/amzn/user-satisfaction-modeling."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 118,
      "original_url": "https://aclanthology.org/2023.acl-long.117/",
      "title": "Robust Multi-bit Natural Language Watermarking through Invariant Features",
      "pdf_url": "https://aclanthology.org/2023.acl-long.117.pdf",
      "authors": [
        "KiYoon Yoo",
        "Wonhyuk Ahn",
        "Jiho Jang",
        "Nojun Kwak"
      ],
      "abstract": "Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 119,
      "original_url": "https://aclanthology.org/2023.acl-long.118/",
      "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.118.pdf",
      "authors": [
        "Shangbin Feng",
        "Zhaoxuan Tan",
        "Wenqian Zhang",
        "Zhenyu Lei",
        "Yulia Tsvetkov"
      ],
      "abstract": "With the advent of pre-trained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pre-trained LMs. While existing approaches leverage external knowledge, it remains an open question how to jointly incorporate knowledge graphs represented in varying contexts — from local (e.g., sentence), document-level, to global knowledge, to enable knowledge-rich and interpretable exchange across contexts. In addition, incorporating varying contexts can especially benefit long document understanding tasks that leverage pre-trained LMs, typically bounded by the input sequence length. In light of these challenges, we propose KALM, a language model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM firstly encodes long documents and knowledge graphs into the three knowledge-aware context representations. KALM then processes each context with context-specific layers. These context-specific layers are followed by a ContextFusion layer that facilitates knowledge exchange to derive an overarching document representation. Extensive experiments demonstrate that KALM achieves state-of-the-art performance on three long document understanding tasks across 6 datasets/settings. Further analyses reveal that the three knowledge-aware contexts are complementary and they all contribute to model performance, while the importance and information exchange patterns of different contexts vary on different tasks and datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 120,
      "original_url": "https://aclanthology.org/2023.acl-long.119/",
      "title": "AtTGen: Attribute Tree Generation for Real-World Attribute Joint Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.119.pdf",
      "authors": [
        "Yanzeng Li",
        "Bingcong Xue",
        "Ruoyu Zhang",
        "Lei Zou"
      ],
      "abstract": "Attribute extraction aims to identify attribute names and the corresponding values from descriptive texts, which is the foundation for extensive downstream applications such as knowledge graph construction, search engines, and e-Commerce. In previous studies, attribute extraction is generally treated as a classification problem for predicting attribute types or a sequence tagging problem for labeling attribute values, where two paradigms, i.e., closed-world and open-world assumption, are involved. However, both of these paradigms have limitations in terms of real-world applications. And prior studies attempting to integrate these paradigms through ensemble, pipeline, and co-training models, still face challenges like cascading errors, high computational overhead, and difficulty in training. To address these existing problems, this paper presents Attribute Tree, a unified formulation for real-world attribute extraction application, where closed-world, open-world, and semi-open attribute extraction tasks are modeled uniformly. Then a text-to-tree generation model, AtTGen, is proposed to learn annotations from different scenarios efficiently and consistently. Experiments demonstrate that our proposed paradigm well covers various scenarios for real-world applications, and the model achieves state-of-the-art, outperforming existing methods by a large margin on three datasets. Our code, pretrained model, and datasets are available at https://github.com/lsvih/AtTGen."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 121,
      "original_url": "https://aclanthology.org/2023.acl-long.120/",
      "title": "Extractive is not Faithful: An Investigation of Broad Unfaithfulness Problems in Extractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.120.pdf",
      "authors": [
        "Shiyue Zhang",
        "David Wan",
        "Mohit Bansal"
      ],
      "abstract": "The problems of unfaithful summaries have been widely discussed under the context of abstractive summarization. Though extractive summarization is less prone to the common unfaithfulness issues of abstractive summaries, does that mean extractive is equal to faithful? Turns out that the answer is no. In this work, we define a typology with five types of broad unfaithfulness problems (including and beyond not-entailment) that can appear in extractive summaries, including incorrect coreference, incomplete coreference, incorrect discourse, incomplete discourse, as well as other misleading information. We ask humans to label these problems out of 1600 English summaries produced by 16 diverse extractive systems. We find that 30% of the summaries have at least one of the five issues. To automatically detect these problems, we find that 5 existing faithfulness evaluation metrics for summarization have poor correlations with human judgment. To remedy this, we propose a new metric, ExtEval, that is designed for detecting unfaithful extractive summaries and is shown to have the best performance. We hope our work can increase the awareness of unfaithfulness problems in extractive summarization and help future work to evaluate and resolve these issues."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 122,
      "original_url": "https://aclanthology.org/2023.acl-long.121/",
      "title": "Improving Translation Quality Estimation with Bias Mitigation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.121.pdf",
      "authors": [
        "Hui Huang",
        "Shuangzhi Wu",
        "Kehai Chen",
        "Hui Di",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation performance. Our method is based on the contrastive learning between clean and noisy sentence pairs. We first introduce noise to the target side of the parallel sentence pair, forming the negative samples. With the original parallel pairs as the positive sample, the QE model is contrastively trained to distinguish the positive samples from the negative ones. This objective is jointly trained with the regression-style quality estimation, so as to prevent the QE model from overfitting to monolingual features. Experiments on WMT QE evaluation datasets demonstrate that our method improves the estimation performance by a large margin while mitigating the bias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 123,
      "original_url": "https://aclanthology.org/2023.acl-long.122/",
      "title": "Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.122.pdf",
      "authors": [
        "Josef Jon",
        "Ondřej Bojar"
      ],
      "abstract": "We propose a genetic algorithm (GA) based method for modifying n-best lists produced by a machine translation (MT) system. Our method offers an innovative approach to improving MT quality and identifying weaknesses in evaluation metrics. Using common GA operations (mutation and crossover) on a list of hypotheses in combination with a fitness function (an arbitrary MT metric), we obtain novel and diverse outputs with high metric scores. With a combination of multiple MT metrics as the fitness function, the proposed method leads to an increase in translation quality as measured by other held-out automatic metrics.With a single metric (including popular ones such as COMET) as the fitness function, we find blind spots and flaws in the metric. This allows for an automated search for adversarial examples in an arbitrary metric, without prior assumptions on the form of such example. As a demonstration of the method, we create datasets of adversarial examples and use them to show that reference-free COMET is substantially less robust than the reference-based version."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 124,
      "original_url": "https://aclanthology.org/2023.acl-long.123/",
      "title": "MoralDial: A Framework to Train and Evaluate Moral Dialogue Systems via Moral Discussions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.123.pdf",
      "authors": [
        "Hao Sun",
        "Zhexin Zhang",
        "Fei Mi",
        "Yasheng Wang",
        "Wei Liu",
        "Jianwei Cui",
        "Bin Wang",
        "Qun Liu",
        "Minlie Huang"
      ],
      "abstract": "Morality in dialogue systems has raised great attention in research recently. A moral dialogue system aligned with users’ values could enhance conversation engagement and user connections. In this paper, we propose a framework, MoralDial to train and evaluate moral dialogue systems. In our framework, we first explore the communication mechanisms of morality and resolve expressed morality into three parts, which indicate the roadmap for building a moral dialogue system. Based on that, we design a simple yet effective method: constructing moral discussions between simulated specific users and the dialogue system. The constructed discussions consist of expressing, explaining, revising, and inferring moral views in dialogue exchanges, which makes conversational models learn morality well in a natural manner. Furthermore, we propose a novel evaluation method under the framework. We evaluate the multiple aspects of morality by judging the relation between dialogue responses and human values in discussions, where the multifaceted nature of morality is particularly considered. Automatic and manual experiments demonstrate that our framework is promising to train and evaluate moral dialogue systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 125,
      "original_url": "https://aclanthology.org/2023.acl-long.124/",
      "title": "Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.124.pdf",
      "authors": [
        "Shaoxiang Wu",
        "Damai Dai",
        "Ziwei Qin",
        "Tianyu Liu",
        "Binghuai Lin",
        "Yunbo Cao",
        "Zhifang Sui"
      ],
      "abstract": "Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper will be publicly available at https://github.com/WSXRHFG/DBF"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 126,
      "original_url": "https://aclanthology.org/2023.acl-long.125/",
      "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.125.pdf",
      "authors": [
        "Liang Wang",
        "Nan Yang",
        "Xiaolong Huang",
        "Binxing Jiao",
        "Linjun Yang",
        "Daxin Jiang",
        "Rangan Majumder",
        "Furu Wei"
      ],
      "abstract": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA (Clark et al., 2020), to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to an unlabeled corpus and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 (Santhanam et al., 2021) which incurs significantly more storage cost. Our code and model checkpoints are available at https://github.com/microsoft/unilm/tree/master/simlm ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 127,
      "original_url": "https://aclanthology.org/2023.acl-long.126/",
      "title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained",
      "pdf_url": "https://aclanthology.org/2023.acl-long.126.pdf",
      "authors": [
        "Hongliang Dai",
        "Ziqian Zeng"
      ],
      "abstract": "For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly annotated training data that contains incorrect labels. However, the performance of models trained solely with such data can be limited by the errors in the automatic annotation. Recently, there are a few approaches that no longer follow this conventional way. But without using sufficient direct entity typing supervision may also cause them to yield inferior performance. In this paper, we propose a new approach that can avoid the need of creating distantly labeled data whenever there is a new type schema. We first train an entity typing model that have an extremely board type coverage by using the ultra-fine entity typing data. Then, when there is a need to produce a model for a newly designed fine-grained entity type schema. We can simply fine-tune the previously trained model with a small number of examples annotated under this schema. Experimental results show that our approach achieves outstanding performance for FET under the few-shot setting. It can also outperform state-of-the-art weak supervision based methods after fine-tuning the model with only a small size manually annotated training set."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 128,
      "original_url": "https://aclanthology.org/2023.acl-long.127/",
      "title": "Controlling Learned Effects to Reduce Spurious Correlations in Text Classifiers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.127.pdf",
      "authors": [
        "Parikshit Bansal",
        "Amit Sharma"
      ],
      "abstract": "To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model’s predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model’s prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 129,
      "original_url": "https://aclanthology.org/2023.acl-long.128/",
      "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.128.pdf",
      "authors": [
        "Jinghui Lu",
        "Dongsheng Zhu",
        "Weidong Han",
        "Rui Zhao",
        "Brian Mac Namee",
        "Fei Tan"
      ],
      "abstract": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 130,
      "original_url": "https://aclanthology.org/2023.acl-long.129/",
      "title": "Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.129.pdf",
      "authors": [
        "Xinxi Lyu",
        "Sewon Min",
        "Iz Beltagy",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 131,
      "original_url": "https://aclanthology.org/2023.acl-long.130/",
      "title": "Learning Optimal Policy for Simultaneous Machine Translation via Binary Search",
      "pdf_url": "https://aclanthology.org/2023.acl-long.130.pdf",
      "authors": [
        "Shoutao Guo",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "abstract": "Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 132,
      "original_url": "https://aclanthology.org/2023.acl-long.131/",
      "title": "Better Simultaneous Translation with Monotonic Knowledge Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.131.pdf",
      "authors": [
        "Shushu Wang",
        "Jing Wu",
        "Kai Fan",
        "Wei Luo",
        "Jun Xiao",
        "Zhongqiang Huang"
      ],
      "abstract": "Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De-En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 133,
      "original_url": "https://aclanthology.org/2023.acl-long.132/",
      "title": "StoryARG: a corpus of narratives and personal experiences in argumentative texts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.132.pdf",
      "authors": [
        "Neele Falk",
        "Gabriella Lapesa"
      ],
      "abstract": "Humans are storytellers, even in communication scenarios which are assumed to be more rationality-oriented, such as argumentation. Indeed, supporting arguments with narratives or personal experiences (henceforth, stories) is a very natural thing to do – and yet, this phenomenon is largely unexplored in computational argumentation. Which role do stories play in an argument? Do they make the argument more effective? What are their narrative properties? To address these questions, we collected and annotated StoryARG, a dataset sampled from well-established corpora in computational argumentation (ChangeMyView and RegulationRoom), and the Social Sciences (Europolis), as well as comments to New York Times articles. StoryARG contains 2451 textual spans annotated at two levels. At the argumentative level, we annotate the function of the story (e.g., clarification, disclosure of harm, search for a solution, establishing speaker’s authority), as well as its impact on the effectiveness of the argument and its emotional load. At the level of narrative properties, we annotate whether the story has a plot-like development, is factual or hypothetical, and who the protagonist is. What makes a story effective in an argument? Our analysis of the annotations in StoryARG uncover a positive impact on effectiveness for stories which illustrate a solution to a problem, and in general, annotator-specific preferences that we investigate with regression analysis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 134,
      "original_url": "https://aclanthology.org/2023.acl-long.133/",
      "title": "Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-long.133.pdf",
      "authors": [
        "Maksim Eremeev",
        "Ilya Valmianski",
        "Xavier Amatriain",
        "Anitha Kannan"
      ],
      "abstract": "Factual correctness is often the limiting factor in practical applications of natural language generation in high-stakes domains such as healthcare. An essential requirement for maintaining factuality is the ability to deal with rare tokens. This paper focuses on rare tokens that appear in both the source and the reference sequences, and which, when missed during generation, decrease the factual correctness of the output text. For high-stake domains that are also knowledge-rich, we show how to use knowledge to (a) identify which rare tokens that appear in both source and reference are important and (b) uplift their conditional probability. We introduce the “utilization rate” that encodes knowledge and serves as a regularizer by maximizing the marginal probability of selected tokens. We present a study in a knowledge-rich domain of healthcare, where we tackle the problem of generating after-visit care instructions based on patient-doctor dialogues. We verify that, in our dataset, specific medical concepts with high utilization rates are underestimated by conventionally trained sequence-to-sequence models. We observe that correcting this with our approach to knowledge injection reduces the uncertainty of the model as well as improves factuality and coherence without negatively impacting fluency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 135,
      "original_url": "https://aclanthology.org/2023.acl-long.134/",
      "title": "Sequence Parallelism: Long Sequence Training from System Perspective",
      "pdf_url": "https://aclanthology.org/2023.acl-long.134.pdf",
      "authors": [
        "Shenggui Li",
        "Fuzhao Xue",
        "Chaitanya Baranwal",
        "Yongbin Li",
        "Yang You"
      ],
      "abstract": "Transformer achieves promising results on various tasks. However, self-attention suffers from quadratic memory requirements with respect to the sequence length. Existing work focuses on reducing time and space complexity from an algorithm perspective. In this work, we propose sequence parallelism, a memory-efficient parallelism to solve this issue from system perspective instead. Our approach is compatible with most existing parallelisms (e.g., data, pipeline, and tensor parallelism), which means our sequence parallelism makes 4D parallelism possible. More importantly, we no longer require a single device to hold the whole sequence. Besides, using efficient attention with linear complexity, our sequence parallelism enables us to train transformer with infinite long sequence. Specifically, we split the input sequence into multiple chunks and feed each chunk into its corresponding device (i.e., GPU). To compute the attention output, we integrated ring-style communication with self-attention calculation and proposed Ring Self-Attention (RSA). Experiments show that sequence parallelism performs well when scaling with batch size and sequence length. Compared with tensor parallelism, our approach achieved 13.7× and 3.0× maximum batch size and sequence length respectively when scaling up to 64 NVIDIA P100 GPUs. With efficient attention, sequence can handle sequence with over 114K tokens, which is over 27× longer than existing efficient attention works holding the whole sequence on a single device."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 136,
      "original_url": "https://aclanthology.org/2023.acl-long.135/",
      "title": "MUSTIE: Multimodal Structural Transformer for Web Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.135.pdf",
      "authors": [
        "Qifan Wang",
        "Jingang Wang",
        "Xiaojun Quan",
        "Fuli Feng",
        "Zenglin Xu",
        "Shaoliang Nie",
        "Sinong Wang",
        "Madian Khabsa",
        "Hamed Firooz",
        "Dongfang Liu"
      ],
      "abstract": "The task of web information extraction is to extract target fields of an object from web pages, such as extracting the name, genre and actor from a movie page. Recent sequential modeling approaches have achieved state-of-the-art results on web information extraction. However, most of these methods only focus on extracting information from textual sources while ignoring the rich information from other modalities such as image and web layout. In this work, we propose a novel MUltimodal Structural Transformer (MUST) that incorporates multiple modalities for web information extraction. Concretely, we develop a structural encoder that jointly encodes the multimodal information based on the HTML structure of the web layout, where high-level DOM nodes, and low-level text and image tokens are introduced to represent the entire page. Structural attention patterns are designed to learn effective cross-modal embeddings for all DOM nodes and low-level tokens. An extensive set of experiments are conducted on WebSRC and Common Crawl benchmarks. Experimental results demonstrate the superior performance of MUST over several state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 137,
      "original_url": "https://aclanthology.org/2023.acl-long.136/",
      "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
      "pdf_url": "https://aclanthology.org/2023.acl-long.136.pdf",
      "authors": [
        "Zichun Yu",
        "Chenyan Xiong",
        "Shi Yu",
        "Zhiyuan Liu"
      ],
      "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 138,
      "original_url": "https://aclanthology.org/2023.acl-long.137/",
      "title": "TableVLM: Multi-modal Pre-training for Table Structure Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.137.pdf",
      "authors": [
        "Leiyuan Chen",
        "Chengsong Huang",
        "Xiaoqing Zheng",
        "Jinshu Lin",
        "Xuanjing Huang"
      ],
      "abstract": "Tables are widely used in research and business, which are suitable for human consumption, but not easily machine-processable, particularly when tables are present in images. One of the main challenges to extracting data from images of tables is accurately recognizing table structures, especially for complex tables with cross rows and columns. In this study, we propose a novel multi-modal pre-training model for table structure recognition, named TableVLM.With a two-stream multi-modal transformer-based encoder-decoder architecture, TableVLM learns to capture rich table structure-related features by multiple carefully-designed unsupervised objectives inspired by the notion of masked visual-language modeling. To pre-train this model, we also created a dataset, called ComplexTable, which consists of 1,000K samples to be released publicly. Experiment results show that the model built on pre-trained TableVLM can improve the performance up to 1.97% in tree-editing-distance-score on ComplexTable."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 139,
      "original_url": "https://aclanthology.org/2023.acl-long.138/",
      "title": "Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.138.pdf",
      "authors": [
        "Jiashu Xu",
        "Mingyu Derek Ma",
        "Muhao Chen"
      ],
      "abstract": "Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 140,
      "original_url": "https://aclanthology.org/2023.acl-long.139/",
      "title": "Dynamic Routing Transformer Network for Multimodal Sarcasm Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.139.pdf",
      "authors": [
        "Yuan Tian",
        "Nan Xu",
        "Ruike Zhang",
        "Wenji Mao"
      ],
      "abstract": "Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available at https://github.com/TIAN-viola/DynRT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 141,
      "original_url": "https://aclanthology.org/2023.acl-long.140/",
      "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary",
      "pdf_url": "https://aclanthology.org/2023.acl-long.140.pdf",
      "authors": [
        "Ori Ram",
        "Liat Bezalel",
        "Adi Zicher",
        "Yonatan Belinkov",
        "Jonathan Berant",
        "Amir Globerson"
      ],
      "abstract": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 142,
      "original_url": "https://aclanthology.org/2023.acl-long.141/",
      "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-long.141.pdf",
      "authors": [
        "Yue Yu",
        "Rongzhi Zhang",
        "Ran Xu",
        "Jieyu Zhang",
        "Jiaming Shen",
        "Chao Zhang"
      ],
      "abstract": "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 143,
      "original_url": "https://aclanthology.org/2023.acl-long.142/",
      "title": "Training-free Neural Architecture Search for RNNs and Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.142.pdf",
      "authors": [
        "Aaron Serianni",
        "Jugal Kalita"
      ],
      "abstract": "Neural architecture search (NAS) has allowed for the automatic creation of new and effective neural network architectures, offering an alternative to the laborious process of manually designing complex architectures. However, traditional NAS algorithms are slow and require immense amounts of computing power. Recent research has investigated training-free NAS metrics for image classification architectures, drastically speeding up search algorithms. In this paper, we investigate training-free NAS metrics for recurrent neural network (RNN) and BERT-based transformer architectures, targeted towards language modeling tasks. First, we develop a new training-free metric, named hidden covariance, that predicts the trained performance of an RNN architecture and significantly outperforms existing training-free metrics. We experimentally evaluate the effectiveness of the hidden covariance metric on the NAS-Bench-NLP benchmark. Second, we find that the current search space paradigm for transformer architectures is not optimized for training-free neural architecture search. Instead, a simple qualitative analysis can effectively shrink the search space to the best performing architectures. This conclusion is based on our investigation of existing training-free metrics and new metrics developed from recent transformer pruning literature, evaluated on our own benchmark of trained BERT architectures. Ultimately, our analysis shows that the architecture search space and the training-free metric must be developed together in order to achieve effective results. Our source code is available at https://github.com/aaronserianni/training-free-nas."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 144,
      "original_url": "https://aclanthology.org/2023.acl-long.143/",
      "title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.143.pdf",
      "authors": [
        "Abhik Bhattacharjee",
        "Tahmid Hasan",
        "Wasi Uddin Ahmad",
        "Yuan-Fang Li",
        "Yong-Bin Kang",
        "Rifat Shahriyar"
      ],
      "abstract": "We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever that is not centered around English. We are releasing the dataset, training and evaluation scripts, and models to spur future research on cross-lingual summarization. The resources can be found at https://github.com/csebuetnlp/CrossSum"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 145,
      "original_url": "https://aclanthology.org/2023.acl-long.144/",
      "title": "Improving Gradient Trade-offs between Tasks in Multi-task Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.144.pdf",
      "authors": [
        "Heyan Chai",
        "Jinhao Cui",
        "Ye Wang",
        "Min Zhang",
        "Binxing Fang",
        "Qing Liao"
      ],
      "abstract": "Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. However, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since different tasks might conflict with each other. Existing MTL methods for alleviating this issue is to leverage heuristics or gradient-based algorithm to achieve an arbitrary Pareto optimal trade-off among different tasks. In this paper, we present a novel gradient trade-off approach to mitigate the task conflict problem, dubbed GetMTL, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification (MTC), so as to improve the performance of each task simultaneously. The results of extensive experiments on two benchmark datasets back up our theoretical analysis and validate the superiority of our proposed GetMTL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 146,
      "original_url": "https://aclanthology.org/2023.acl-long.145/",
      "title": "Bi-Phone: Modeling Inter Language Phonetic Influences in Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.145.pdf",
      "authors": [
        "Abhirut Gupta",
        "Ananya B. Sai",
        "Richard Sproat",
        "Yuri Vasilevski",
        "James Ren",
        "Ambarish Jash",
        "Sukhdeep Sodhi",
        "Aravindan Raghuveer"
      ],
      "abstract": "A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Written text in the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1).We propose a method to mine phoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2.These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2 text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web.We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the SuperGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions in text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 147,
      "original_url": "https://aclanthology.org/2023.acl-long.146/",
      "title": "Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.146.pdf",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Wei Ji",
        "Tat-Seng Chua"
      ],
      "abstract": "Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual&cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English-Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 148,
      "original_url": "https://aclanthology.org/2023.acl-long.147/",
      "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.147.pdf",
      "authors": [
        "Lei Wang",
        "Wanyu Xu",
        "Yihuai Lan",
        "Zhiqiang Hu",
        "Yunshi Lan",
        "Roy Ka-Wei Lee",
        "Ee-Peng Lim"
      ],
      "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 149,
      "original_url": "https://aclanthology.org/2023.acl-long.148/",
      "title": "RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.148.pdf",
      "authors": [
        "Zheng Liu",
        "Shitao Xiao",
        "Yingxia Shao",
        "Zhao Cao"
      ],
      "abstract": "To better support information retrieval tasks such as web search and open-domain question answering, growing effort is made to develop retrieval-oriented language models, e.g., RetroMAE and many others. Most of the existing works focus on improving the semantic representation capability for the contextualized embedding of the [CLS] token. However, recent study shows that the ordinary tokens besides [CLS] may provide extra information, which help to produce a better representation effect. As such, it’s necessary to extend the current methods where all contextualized embeddings can be jointly pre-trained for the retrieval tasks. In this work, we propose a novel pre-training method called Duplex Masked Auto-Encoder, a.k.a. DupMAE. It is designed to improve the quality of semantic representation where all contextualized embeddings of the pre-trained model can be leveraged. It takes advantage of two complementary auto-encoding tasks: one reconstructs the input sentence on top of the [CLS] embedding; the other one predicts the bag-of-words feature of the input sentence based on the ordinary tokens’ embeddings. The two tasks are jointly conducted to train a unified encoder, where the whole contextualized embeddings are aggregated in a compact way to produce the final semantic representation. DupMAE is simple but empirically competitive: it substantially improves the pre-trained model’s representation capability and transferability, where superior retrieval performances can be achieved on popular benchmarks, like MS MARCO and BEIR. We make our code publicly available at https://github.com/staoxiao/RetroMAE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 150,
      "original_url": "https://aclanthology.org/2023.acl-long.149/",
      "title": "DecompX: Explaining Transformers Decisions by Propagating Token Decomposition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.149.pdf",
      "authors": [
        "Ali Modarressi",
        "Mohsen Fayyaz",
        "Ehsan Aghazadeh",
        "Yadollah Yaghoobzadeh",
        "Mohammad Taher Pilehvar"
      ],
      "abstract": "An emerging solution for explaining Transformer-based models is to use vector-based analysis on how the representations are formed. However, providing a faithful vector-based explanation for a multi-layer model could be challenging in three aspects: (1) Incorporating all components into the analysis, (2) Aggregating the layer dynamics to determine the information flow and mixture throughout the entire model, and (3) Identifying the connection between the vector-based analysis and the model’s predictions. In this paper, we present DecompX to tackle these challenges. DecompX is based on the construction of decomposed token representations and their successive propagation throughout the model without mixing them in between layers. Additionally, our proposal provides multiple advantages over existing solutions for its inclusion of all encoder components (especially nonlinear feed-forward networks) and the classification head. The former allows acquiring precise vectors while the latter transforms the decomposition into meaningful prediction-based values, eliminating the need for norm- or summation-based vector aggregation. According to the standard faithfulness evaluations, DecompX consistently outperforms existing gradient-based and vector-based approaches on various datasets. Our code is available at https://github.com/mohsenfayyaz/DecompX."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 151,
      "original_url": "https://aclanthology.org/2023.acl-long.150/",
      "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step",
      "pdf_url": "https://aclanthology.org/2023.acl-long.150.pdf",
      "authors": [
        "Liunian Harold Li",
        "Jack Hessel",
        "Youngjae Yu",
        "Xiang Ren",
        "Kai-Wei Chang",
        "Yejin Choi"
      ],
      "abstract": "Chain-of-thought prompting (e.g., “Let’s think step-by-ste”) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M—1.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 152,
      "original_url": "https://aclanthology.org/2023.acl-long.151/",
      "title": "Generating EDU Extracts for Plan-Guided Summary Re-Ranking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.151.pdf",
      "authors": [
        "Griffin Adams",
        "Alex Fabbri",
        "Faisal Ladhak",
        "Noémie Elhadad",
        "Kathleen McKeown"
      ],
      "abstract": "Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues. We ground each candidate abstract on its own unique content plan and generate distinct plan-guided abstracts using a model’s top beam. More concretely, a standard language model (a BART LM) auto-regressively generates elemental discourse unit (EDU) content plans with an extractive copy mechanism. The top K beams from the content plan generator are then used to guide a separate LM, which produces a single abstractive candidate for each distinct plan. We apply an existing re-ranker (BRIO) to abstractive candidates generated from our method, as well as baseline decoding methods. We show large relevance improvements over previously published methods on widely used single document news article corpora, with ROUGE-2 F1 gains of 0.88, 2.01, and 0.38 on CNN / Dailymail, NYT, and Xsum, respectively. A human evaluation on CNN / DM validates these results. Similarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow EDU plans outperforms sampling-based methods by by 1.05 ROUGE-2 F1 points. Code to generate and realize plans is available at https://github.com/griff4692/edu-sum."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 153,
      "original_url": "https://aclanthology.org/2023.acl-long.152/",
      "title": "A Survey on Asking Clarification Questions Datasets in Conversational Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-long.152.pdf",
      "authors": [
        "Hossein A. Rahmani",
        "Xi Wang",
        "Yue Feng",
        "Qiang Zhang",
        "Emine Yilmaz",
        "Aldo Lipani"
      ],
      "abstract": "The ability to understand a user’s underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users’ true intent from their queries or utterances arise as an essential task. However, it is noticeable that a key limitation of the existing ACQs studies is their incomparability, from inconsistent use of data, distinct experimental setups and evaluation strategies. Therefore, in this paper, to assist the development of ACQs techniques, we comprehensively analyse the current ACQs research status, which offers a detailed comparison of publicly available datasets, and discusses the applied evaluation metrics, joined with benchmarks for multiple ACQs-related tasks. In particular, given a thorough analysis of the ACQs task, we discuss a number of corresponding research directions for the investigation of ACQs as well as the development of conversational systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 154,
      "original_url": "https://aclanthology.org/2023.acl-long.153/",
      "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
      "pdf_url": "https://aclanthology.org/2023.acl-long.153.pdf",
      "authors": [
        "Boshi Wang",
        "Sewon Min",
        "Xiang Deng",
        "Jiaming Shen",
        "You Wu",
        "Luke Zettlemoyer",
        "Huan Sun"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 155,
      "original_url": "https://aclanthology.org/2023.acl-long.154/",
      "title": "Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.154.pdf",
      "authors": [
        "Jean Maillard",
        "Cynthia Gao",
        "Elahe Kalbassi",
        "Kaushik Ram Sadagopan",
        "Vedanuj Goswami",
        "Philipp Koehn",
        "Angela Fan",
        "Francisco Guzman"
      ],
      "abstract": "For many languages, machine translation progress is hindered by the lack of reliable training data. Models are trained on whatever pre-existing datasets may be available and then augmented with synthetic data, because it is often not economical to pay for the creation of large-scale datasets. But for the case of low-resource languages, would the creation of a few thousand professionally translated sentence pairs give any benefit? In this paper, we show that it does. We describe a broad data collection effort involving around 6k professionally translated sentence pairs for each of 39 low-resource languages, which we make publicly available. We analyse the gains of models trained on this small but high-quality data, showing that it has significant impact even when larger but lower quality pre-existing corpora are used, or when data is augmented with millions of sentences through backtranslation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 156,
      "original_url": "https://aclanthology.org/2023.acl-long.155/",
      "title": "RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.155.pdf",
      "authors": [
        "Zhaoyang Wang",
        "Zhiyue Liu",
        "Xiaopeng Zheng",
        "Qinliang Su",
        "Jiahai Wang"
      ],
      "abstract": "Adversarial attacks on deep neural networks keep raising security concerns in natural language processing research. Existing defenses focus on improving the robustness of the victim model in the training stage. However, they often neglect to proactively mitigate adversarial attacks during inference. Towards this overlooked aspect, we propose a defense framework that aims to mitigate attacks by confusing attackers and correcting adversarial contexts that are caused by malicious perturbations. Our framework comprises three components: (1) a synonym-based transformation to randomly corrupt adversarial contexts in the word level, (2) a developed BERT defender to correct abnormal contexts in the representation level, and (3) a simple detection method to filter out adversarial examples, any of which can be flexibly combined. Additionally, our framework helps improve the robustness of the victim model during training. Extensive experiments demonstrate the effectiveness of our framework in defending against word-level adversarial attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 157,
      "original_url": "https://aclanthology.org/2023.acl-long.156/",
      "title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.156.pdf",
      "authors": [
        "Ziqing Yang",
        "Yiming Cui",
        "Xin Yao",
        "Shijin Wang"
      ],
      "abstract": "Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves 6 7x speedups while maintaining 93% 99% performance. Under extreme compression where only 3% transformer weights remain, the pruned model is still competitive compared to larger models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 158,
      "original_url": "https://aclanthology.org/2023.acl-long.157/",
      "title": "Learning to Substitute Spans towards Improving Compositional Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.157.pdf",
      "authors": [
        "Zhaoyi Li",
        "Ying Wei",
        "Defu Lian"
      ],
      "abstract": "Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases only) or differentiation of training sequences in an imbalanced difficulty distribution. To address the two challenges, we first propose a novel compositional augmentation strategy dubbed Span Substitution (SpanSub) that enables multi-grained composition of substantial substructures in the whole training set. Over and above that, we introduce the Learning to Substitute Span (L2S2) framework which empowers the learning of span substitution probabilities in SpanSub in an end-to-end manner by maximizing the loss of neural sequence models, so as to outweigh those challenging compositions with elusive concepts and novel surroundings. Our empirical results on three standard compositional generalization benchmarks, including SCAN, COGS and GeoQuery (with an improvement of at most 66.5%, 10.3%, 1.2%, respectively), demonstrate the superiority of SpanSub, L2S2 and their combination."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 159,
      "original_url": "https://aclanthology.org/2023.acl-long.158/",
      "title": "DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.158.pdf",
      "authors": [
        "Guanqun Bi",
        "Lei Shen",
        "Yanan Cao",
        "Meng Chen",
        "Yuqiang Xie",
        "Zheng Lin",
        "Xiaodong He"
      ],
      "abstract": "Empathy is a crucial factor in open-domain conversations, which naturally shows one’s caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outperforms competitive baselines in terms of controllability, informativeness, and diversity without the loss of context-relatedness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 160,
      "original_url": "https://aclanthology.org/2023.acl-long.159/",
      "title": "BREAK: Breaking the Dialogue State Tracking Barrier with Beam Search and Re-ranking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.159.pdf",
      "authors": [
        "Seungpil Won",
        "Heeyoung Kwak",
        "Joongbo Shin",
        "Janghoon Han",
        "Kyomin Jung"
      ],
      "abstract": "Despite the recent advances in dialogue state tracking (DST), the joint goal accuracy (JGA) of the existing methods on MultiWOZ 2.1 still remains merely 60%. In our preliminary error analysis, we find that beam search produces a pool of candidates that is likely to include the correct dialogue state. Motivated by this observation, we introduce a novel framework, called BREAK (Beam search and RE-rAnKing), that achieves outstanding performance on DST. BREAK performs DST in two stages: (i) generating k-best dialogue state candidates with beam search and (ii) re-ranking the candidates to select the correct dialogue state. This simple yet powerful framework shows state-of-the-art performance on all versions of MultiWOZ and M2M datasets. Most notably, we push the joint goal accuracy to 80-90% on MultiWOZ 2.1-2.4, which is an improvement of 23.6%, 26.3%, 21.7%, and 10.8% over the previous best-performing models, respectively. The data and code will be available at https://github.com/tony-won/DST-BREAK"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 161,
      "original_url": "https://aclanthology.org/2023.acl-long.160/",
      "title": "Faithful Low-Resource Data-to-Text Generation through Cycle Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.160.pdf",
      "authors": [
        "Zhuoer Wang",
        "Marcus Collins",
        "Nikhita Vedula",
        "Simone Filice",
        "Shervin Malmasi",
        "Oleg Rokhlenko"
      ],
      "abstract": "Methods to generate text from structured data have advanced significantly in recent years, primarily due to fine-tuning of pre-trained language models on large datasets. However, such models can fail to produce output faithful to the input data, particularly on out-of-domain data. Sufficient annotated data is often not available for specific domains, leading us to seek an unsupervised approach to improve the faithfulness of output text. Since the problem is fundamentally one of consistency between the representations of the structured data and text, we evaluate the effectiveness of cycle training in this work. Cycle training uses two models which are inverses of each other: one that generates text from structured data, and one which generates the structured data from natural language text. We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the data-to-text generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform extensive empirical analysis with automated evaluation metrics and a newly designed human evaluation schema to reveal different cycle training strategies’ effectiveness of reducing various types of generation errors. Our code is publicly available at https://github.com/Edillower/CycleNLG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 162,
      "original_url": "https://aclanthology.org/2023.acl-long.161/",
      "title": "Towards Stable Natural Language Understanding via Information Entropy Guided Debiasing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.161.pdf",
      "authors": [
        "Li Du",
        "Xiao Ding",
        "Zhouhao Sun",
        "Ting Liu",
        "Bing Qin",
        "Jingshuo Liu"
      ],
      "abstract": "Although achieving promising performance, current Natural Language Understanding models tend to utilize dataset biases instead of learning the intended task, which always leads to performance degradation on out-of-distribution (OOD) samples. Toincrease the performance stability, previous debiasing methods empirically capture bias features from data to prevent the model from corresponding biases. However, our analyses show that the empirical debiasing methods may fail to capture part of the potential dataset biases and mistake semantic information of input text as biases, which limits the effectiveness of debiasing. To address these issues, we propose a debiasing framework IEGDB that comprehensively detects the dataset biases to induce a set of biased features, and then purifies the biased features with the guidance of information entropy. Experimental results show that IEGDB can consistently improve the stability of performance on OOD datasets for a set of widely adopted NLU models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 163,
      "original_url": "https://aclanthology.org/2023.acl-long.162/",
      "title": "Dynamic and Efficient Inference for Text Generation via BERT Family",
      "pdf_url": "https://aclanthology.org/2023.acl-long.162.pdf",
      "authors": [
        "Xiaobo Liang",
        "Juntao Li",
        "Lijun Wu",
        "Ziqiang Cao",
        "Min Zhang"
      ],
      "abstract": "Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 → 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks. Our code will be publicly available at GitHub https://github.com/dropreg/DEER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 164,
      "original_url": "https://aclanthology.org/2023.acl-long.163/",
      "title": "Learning to Generate Equitable Text in Dialogue from Biased Training Data",
      "pdf_url": "https://aclanthology.org/2023.acl-long.163.pdf",
      "authors": [
        "Anthony Sicilia",
        "Malihe Alikhani"
      ],
      "abstract": "The ingrained principles of fairness in a dialogue system’s decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 165,
      "original_url": "https://aclanthology.org/2023.acl-long.164/",
      "title": "Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.164.pdf",
      "authors": [
        "Ke Ji",
        "Yixin Lian",
        "Jingsheng Gao",
        "Baoyuan Wang"
      ],
      "abstract": "Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (PLMs), which has exhibited effectiveness in the few-shot flat text classification tasks. However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce. In this work, we define a path-based few-shot setting and establish a strict path-based evaluation metric to further explore few-shot HTC tasks. To address the issue, we propose the hierarchical verbalizer (“HierVerb”), a multi-verbalizer framework treating HTC as a single- or multi-label classification problem at multiple layers and learning vectors as verbalizers constrained by hierarchical structure and hierarchical contrastive learning. In this manner, HierVerb fuses label hierarchy knowledge into verbalizers and remarkably outperforms those who inject hierarchy through graph encoders, maximizing the benefits of PLMs. Extensive experiments on three popular HTC datasets under the few-shot settings demonstrate that prompt with HierVerb significantly boosts the HTC performance, meanwhile indicating an elegant way to bridge the gap between the large pre-trained model and downstream hierarchical classification tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 166,
      "original_url": "https://aclanthology.org/2023.acl-long.165/",
      "title": "Summary-Oriented Vision Modeling for Multimodal Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.165.pdf",
      "authors": [
        "Yunlong Liang",
        "Fandong Meng",
        "Jinan Xu",
        "Jiaan Wang",
        "Yufeng Chen",
        "Jie Zhou"
      ],
      "abstract": "The goal of multimodal abstractive summarization (MAS) is to produce a concise summary given the multimodal data (text and vision). Existing studies on MAS mainly focus on how to effectively use the extracted visual features, having achieved impressive success on the high-resource English dataset. However, less attention has been paid to the quality of the visual features to the summary, which may limit the model performance, especially in the low- and zero-resource scenarios. In this paper, we propose to improve the summary quality through summary-oriented visual features. To this end, we devise two auxiliary tasks including vision to summary task and masked image modeling task. Together with the main summarization task, we optimize the MAS model via the training objectives of all these tasks. By these means, the MAS model can be enhanced by capturing the summary-oriented visual features, thereby yielding more accurate summaries. Experiments on 44 languages, covering mid-high-, low-, and zero-resource scenarios, verify the effectiveness and superiority of the proposed approach, which achieves state-of-the-art performance under all scenarios. Additionally, we will contribute a large-scale multilingual multimodal abstractive summarization (MM-Sum) dataset to the research community."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 167,
      "original_url": "https://aclanthology.org/2023.acl-long.166/",
      "title": "Helping a Friend or Supporting a Cause? Disentangling Active and Passive Cosponsorship in the U.S. Congress",
      "pdf_url": "https://aclanthology.org/2023.acl-long.166.pdf",
      "authors": [
        "Giuseppe Russo",
        "Christoph Gote",
        "Laurence Brandenberger",
        "Sophia Schlosser",
        "Frank Schweitzer"
      ],
      "abstract": "In the U.S. Congress, legislators can use active and passive cosponsorship to support bills. We show that these two types of cosponsorship are driven by two different motivations: the backing of political colleagues and the backing of the bill’s content. To this end, we develop an Encoder+RGCN based model that learns legislator representations from bill texts and speech transcripts. These representations predict active and passive cosponsorship with an F1-score of 0.88.Applying our representations to predict voting decisions, we show that they are interpretable and generalize to unseen tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 168,
      "original_url": "https://aclanthology.org/2023.acl-long.167/",
      "title": "TREA: Tree-Structure Reasoning Schema for Conversational Recommendation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.167.pdf",
      "authors": [
        "Wendi Li",
        "Wei Wei",
        "Xiaoye Qu",
        "Xian-Ling Mao",
        "Ye Yuan",
        "Wenfeng Xie",
        "Dangyang Chen"
      ],
      "abstract": "Conversational recommender systems (CRS) aim to timely trace the dynamic interests of users through dialogues and generate relevant responses for item recommendations. Recently, various external knowledge bases (especially knowledge graphs) are incorporated into CRS to enhance the understanding of conversation contexts. However, recent reasoning-based models heavily rely on simplified structures such as linear structures or fixed-hierarchical structures for causality reasoning, hence they cannot fully figure out sophisticated relationships among utterances with external knowledge. To address this, we propose a novel Tree structure Reasoning schEmA named TREA. TREA constructs a multi-hierarchical scalable tree as the reasoning structure to clarify the causal relationships between mentioned entities, and fully utilizes historical conversations to generate more reasonable and suitable responses for recommended results. Extensive experiments on two public CRS datasets have demonstrated the effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 169,
      "original_url": "https://aclanthology.org/2023.acl-long.168/",
      "title": "CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality",
      "pdf_url": "https://aclanthology.org/2023.acl-long.168.pdf",
      "authors": [
        "Liang Li",
        "Ruiying Geng",
        "Chengyang Fang",
        "Bing Li",
        "Can Ma",
        "Rongyu Cao",
        "Binhua Li",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while leaving other languages underexplored.To alleviate these limitations, in this paper, we present CATS, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality. The dataset aims to generate textual descriptions for the answer in the practical TableQA system. Further, to bridge the structural gap between the input SQL and table and establish better semantic alignments, we propose a Unified Graph Transformation approach to establish a joint encoding space for the two hybrid knowledge resources and convert this task to a graph-to-text problem. The experiment results demonstrate the effectiveness of our proposed method. Further analysis on CATS attests to both the high quality and challenges of the dataset"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 170,
      "original_url": "https://aclanthology.org/2023.acl-long.169/",
      "title": "Multilingual Multifaceted Understanding of Online News in Terms of Genre, Framing, and Persuasion Techniques",
      "pdf_url": "https://aclanthology.org/2023.acl-long.169.pdf",
      "authors": [
        "Jakub Piskorski",
        "Nicolas Stefanovitch",
        "Nikolaos Nikolaidis",
        "Giovanni Da San Martino",
        "Preslav Nakov"
      ],
      "abstract": "We present a new multilingual multifacet dataset of news articles, each annotated for genre (objective news reporting vs. opinion vs. satire), framing (what key aspects are highlighted), and persuasion techniques (logical fallacies, emotional appeals, ad hominem attacks, etc.). The persuasion techniques are annotated at the span level, using a taxonomy of 23 fine-grained techniques grouped into 6 coarse categories. The dataset contains 1,612 news articles covering recent news on current topics of public interest in six European languages (English, French, German, Italian, Polish, and Russian), with more than 37k annotated spans of persuasion techniques. We describe the dataset and the annotation process, and we report the evaluation results of multilabel classification experiments using state-of-the-art multilingual transformers at different levels of granularity: token-level, sentence-level, paragraph-level, and document-level."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 171,
      "original_url": "https://aclanthology.org/2023.acl-long.170/",
      "title": "Learning Action Conditions from Instructional Manuals for Instruction Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.170.pdf",
      "authors": [
        "Te-Lin Wu",
        "Caiqi Zhang",
        "Qingyuan Hu",
        "Alexander Spangher",
        "Nanyun Peng"
      ],
      "abstract": "The ability to infer pre- and postconditions of an action is vital for comprehending complex instructions, and is essential for applications such as autonomous instruction-guided agents and assistive AI that supports humans to perform physical tasks. In this work, we propose a task dubbed action condition inference, which extracts mentions of preconditions and postconditions of actions in instructional manuals. We propose a weakly supervised approach utilizing automatically constructed large-scale training instances from online instructions, and curate a densely human-annotated and validated dataset to study how well the current NLP models do on the proposed task. We design two types of models differ by whether contextualized and global information is leveraged, as well as various combinations of heuristics to construct the weak supervisions.Our experiments show a > 20% F1-score improvement with considering the entire instruction contexts and a > 6% F1-score benefit with the proposed heuristics. However, the best performing model is still well-behind human performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 172,
      "original_url": "https://aclanthology.org/2023.acl-long.171/",
      "title": "StoryWars: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.171.pdf",
      "authors": [
        "Yulun Du",
        "Lydia Chilton"
      ],
      "abstract": "Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora. To address this, we introduce StoryWars, a new dataset of over 40,000 collaborative stories written by 9,400 different authors from an online platform. We design 12 task types, comprising 7 understanding and 5 generation task types, on {pasted macro ‘STORYWARS’}, deriving 101 diverse story-related tasks in total as a multi-task benchmark covering all fully-supervised, few-shot, and zero-shot scenarios. Furthermore, we present our instruction-tuned model, InstructStory, for the story tasks showing that instruction tuning, in addition to achieving superior results in zero-shot and few-shot scenarios, can also obtain the best performance on the fully-supervised tasks in StoryWars, establishing strong multi-task benchmark performances on StoryWars."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 173,
      "original_url": "https://aclanthology.org/2023.acl-long.172/",
      "title": "Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.172.pdf",
      "authors": [
        "Fan Yin",
        "Jesse Vig",
        "Philippe Laban",
        "Shafiq Joty",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common structured format, and (2) adding a meta-tuning stage to help the model better understand the definitions. With these two strategies, we achieve a 4.2 Rouge-L improvement over 119 unseen test tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 174,
      "original_url": "https://aclanthology.org/2023.acl-long.173/",
      "title": "Do PLMs Know and Understand Ontological Knowledge?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.173.pdf",
      "authors": [
        "Weiqi Wu",
        "Chengyue Jiang",
        "Yong Jiang",
        "Pengjun Xie",
        "Kewei Tu"
      ],
      "abstract": "Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a system- atic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic un- derstanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of ; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. How- ever, both the memorizing and reasoning per- formances are less than perfect, indicating in- complete knowledge and understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 175,
      "original_url": "https://aclanthology.org/2023.acl-long.174/",
      "title": "CORE: Cooperative Training of Retriever-Reranker for Effective Dialogue Response Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.174.pdf",
      "authors": [
        "Chongyang Tao",
        "Jiazhan Feng",
        "Tao Shen",
        "Chang Liu",
        "Juntao Li",
        "Xiubo Geng",
        "Daxin Jiang"
      ],
      "abstract": "Establishing retrieval-based dialogue systems that can select appropriate responses from the pre-built index has gained increasing attention. Recent common practice is to construct a two-stage pipeline with a fast retriever (e.g., bi-encoder) for first-stage recall followed by a smart response reranker (e.g., cross-encoder) for precise ranking. However, existing studies either optimize the retriever and reranker in independent ways, or distill the knowledge from a pre-trained reranker into the retriever in an asynchronous way, leading to sub-optimal performance of both modules. Thus, an open question remains about how to train them for a better combination of the best of both worlds. To this end, we present a cooperative training of the response retriever and the reranker whose parameters are dynamically optimized by the ground-truth labels as well as list-wise supervision signals from each other. As a result, the two modules can learn from each other and evolve together throughout the training. Experimental results on two benchmarks demonstrate the superiority of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 176,
      "original_url": "https://aclanthology.org/2023.acl-long.175/",
      "title": "Exploring How Generative Adversarial Networks Learn Phonological Representations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.175.pdf",
      "authors": [
        "Jingyi Chen",
        "Micha Elsner"
      ],
      "abstract": "This paper explores how Generative Adversarial Networks (GANs) learn representations of phonological phenomena. We analyze how GANs encode contrastive and non-contrastive nasality in French and English vowels by applying the ciwGAN architecture (Begus, 2021). Begus claims that ciwGAN encodes linguistically meaningful representations with categorical variables in its latent space and manipulating the latent variables shows an almost one to one corresponding control of the phonological features in ciwGAN’s generated outputs. However, our results show an interactive effect of latent variables on the features in the generated outputs, which suggests the learned representations in neural networks are different from the phonological representations proposed by linguists. On the other hand, ciwGAN is able to distinguish contrastive and noncontrastive features in English and French by encoding them differently. Comparing the performance of GANs learning from different languages results in a better understanding of what language specific features contribute to developing language specific phonological representations. We also discuss the role of training data frequencies in phonological feature learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 177,
      "original_url": "https://aclanthology.org/2023.acl-long.176/",
      "title": "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.176.pdf",
      "authors": [
        "Mario Giulianelli",
        "Iris Luden",
        "Raquel Fernandez",
        "Andrey Kutuzov"
      ],
      "abstract": "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users — historical linguists, lexicographers, or social scientists — to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the ‘definitions as representations’ paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 178,
      "original_url": "https://aclanthology.org/2023.acl-long.177/",
      "title": "Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.177.pdf",
      "authors": [
        "Hao Yan",
        "Saurabh Srivastava",
        "Yintao Tai",
        "Sida I. Wang",
        "Wen-tau Yih",
        "Ziyu Yao"
      ],
      "abstract": "Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full set of human annotations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 179,
      "original_url": "https://aclanthology.org/2023.acl-long.178/",
      "title": "InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.178.pdf",
      "authors": [
        "Anwen Hu",
        "Shizhe Chen",
        "Liang Zhang",
        "Qin Jin"
      ],
      "abstract": "Automatic image captioning evaluation is critical for benchmarking and promoting advances in image captioning research. Existing metrics only provide a single score to measure caption qualities, which are less explainable and informative. Instead, we humans can easily identify the problems of captions in details, e.g., which words are inaccurate and which salient objects are not described, and then rate the caption quality. To support such informative feedback, we propose an Informative Metric for Reference-free Image Caption evaluation (InfoMetIC). Given an image and a caption, InfoMetIC is able to report incorrect words and unmentioned image regions at fine-grained level, and also provide a text precision score, a vision recall score and an overall quality score at coarse-grained level. The coarse-grained score of InfoMetIC achieves significantly better correlation with human judgements than existing metrics on multiple benchmarks. We also construct a token-level evaluation dataset and demonstrate the effectiveness of InfoMetIC in fine-grained evaluation. Our code and datasets are publicly available at https://github.com/HAWLYQ/InfoMetIC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 180,
      "original_url": "https://aclanthology.org/2023.acl-long.179/",
      "title": "An Invariant Learning Characterization of Controlled Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.179.pdf",
      "authors": [
        "Carolina Zheng",
        "Claudia Shi",
        "Keyon Vafa",
        "Amir Feder",
        "David Blei"
      ],
      "abstract": "Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions. In this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments. We study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 181,
      "original_url": "https://aclanthology.org/2023.acl-long.180/",
      "title": "HistRED: A Historical Document-Level Relation Extraction Dataset",
      "pdf_url": "https://aclanthology.org/2023.acl-long.180.pdf",
      "authors": [
        "Soyoung Yang",
        "Minseok Choi",
        "Youngwoo Cho",
        "Jaegul Choo"
      ],
      "abstract": "Despite the extensive applications of relation extraction (RE) tasks in various domains, little has been explored in the historical context, which contains promising data across hundreds and thousands of years. To promote the historical RE research, we present HistRED constructed from Yeonhaengnok. Yeonhaengnok is a collection of records originally written in Hanja, the classical Chinese writing, which has later been translated into Korean. HistRED provides bilingual annotations such that RE can be performed on Korean and Hanja texts. In addition, HistRED supports various self-contained subtexts with different lengths, from a sentence level to a document level, supporting diverse context settings for researchers to evaluate the robustness of their RE models. To demonstrate the usefulness of our dataset, we propose a bilingual RE model that leverages both Korean and Hanja contexts to predict relations between entities. Our model outperforms monolingual baselines on HistRED, showing that employing multiple language contexts supplements the RE predictions. The dataset is publicly available at: https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 182,
      "original_url": "https://aclanthology.org/2023.acl-long.181/",
      "title": "A Critical Evaluation of Evaluations for Long-form Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.181.pdf",
      "authors": [
        "Fangyuan Xu",
        "Yixiao Song",
        "Mohit Iyyer",
        "Eunsol Choi"
      ],
      "abstract": "Long-form question answering (LFQA) enables answering a wide range of questions, but its flexibility poses enormous challenges for evaluation. We perform the first targeted study of the evaluation of long-form answers, covering both human and automatic evaluation practices. We hire domain experts in seven areas to provide preference judgments over pairs of answers, along with free-form justifications for their choices. We present a careful analysis of experts’ evaluation, which focuses on new aspects such as the comprehensiveness of the answer. Next, we examine automatic text generation metrics, finding that no existing metrics are predictive of human preference judgments. However, some metrics correlate with fine-grained aspects of answers (e.g., coherence). We encourage future work to move away from a single “overall score” of the answer and adopt a multi-faceted evaluation, targeting aspects such as factuality and completeness. We publicly release all of our annotations and code to spur future work into LFQA evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 183,
      "original_url": "https://aclanthology.org/2023.acl-long.182/",
      "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.182.pdf",
      "authors": [
        "Hongyi Yuan",
        "Zheng Yuan",
        "Chuanqi Tan",
        "Fei Huang",
        "Songfang Huang"
      ],
      "abstract": "Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In addition, HyPe acquires negligible computational overheads, and is better than and compatible with previous state-of-the-art fine-tuning techniques."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 184,
      "original_url": "https://aclanthology.org/2023.acl-long.183/",
      "title": "Generating User-Engaging News Headlines",
      "pdf_url": "https://aclanthology.org/2023.acl-long.183.pdf",
      "authors": [
        "Pengshan Cai",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Hongwei Wang",
        "Xiaoyang Wang",
        "Hong Yu",
        "Fei Liu",
        "Dong Yu"
      ],
      "abstract": "The potential choices for news article headlines are enormous, and finding the right balance between conveying the essential message and capturing the reader’s attention is key to effective headlining. However, presenting the same news headline to all readers is a suboptimal strategy, because it does not take into account the different preferences and interests of diverse readers, who may be confused about why a particular article has been recommended to them and do not see a clear connection between their interests and the recommended article. In this paper, we present a novel framework that addresses these challenges by incorporating user profiling to generate personalized headlines, and a combination of automated and human evaluation methods to determine user preference for personalized headlines. Our framework utilizes a learnable relevance function to assign personalized signature phrases to users based on their reading histories, which are then used to personalize headline generation. Through extensive evaluation, we demonstrate the effectiveness of our proposed framework in generating personalized headlines that meet the needs of a diverse audience. Our framework has the potential to improve the efficacy of news recommendations and facilitate creation of personalized content."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 185,
      "original_url": "https://aclanthology.org/2023.acl-long.184/",
      "title": "Word sense extension",
      "pdf_url": "https://aclanthology.org/2023.acl-long.184.pdf",
      "authors": [
        "Lei Yu",
        "Yang Xu"
      ],
      "abstract": "Humans often make creative use of words to expressnovel senses. A long-standing effort in natural language processing hasbeen focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) thatenables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitivemodels of chaining with a learning scheme that transforms a language model embedding space to supportvarious types of word sense extension. We evaluate our frameworkagainst several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 186,
      "original_url": "https://aclanthology.org/2023.acl-long.185/",
      "title": "PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism",
      "pdf_url": "https://aclanthology.org/2023.acl-long.185.pdf",
      "authors": [
        "Yongkang Liu",
        "Shi Feng",
        "Daling Wang",
        "Yifei Zhang",
        "Hinrich Schütze"
      ],
      "abstract": "We investigate response generation for multi-turn dialogue in generative chatbots. Existing generative modelsbased on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the history, which makesmodels unable to capture the subtle variability observed in different dialogues and cannot distinguish the differencesbetween dialogues that are similar in composition. In this paper, we propose Pseudo-Variational Gated Recurrent Unit (PVGRU). The key novelty of PVGRU is a recurrent summarizing variable thataggregates the accumulated distribution variations of subsequences. We train PVGRU without relying on posterior knowledge, thus avoiding the training-inference inconsistency problem. PVGRU can perceive subtle semantic variability through summarizing variables that are optimized by two objectives we employ for training: distribution consistency and reconstruction. In addition, we build a Pseudo-Variational Hierarchical Dialogue(PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the diversity andrelevance of responses on two benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 187,
      "original_url": "https://aclanthology.org/2023.acl-long.186/",
      "title": "Decoding Symbolism in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.186.pdf",
      "authors": [
        "Meiqi Guo",
        "Rebecca Hwa",
        "Adriana Kovashka"
      ],
      "abstract": "This work explores the feasibility of eliciting knowledge from language models (LMs) to decode symbolism, recognizing something (e.g.,roses) as a stand-in for another (e.g., love). We present our evaluative framework, Symbolism Analysis (SymbA), which compares LMs (e.g., RoBERTa, GPT-J) on different types of symbolism and analyze the outcomes along multiple metrics. Our findings suggest that conventional symbols are more reliably elicited from LMs while situated symbols are more challenging. Results also reveal the negative impact of the bias in pre-trained corpora. We further demonstrate that a simple re-ranking strategy can mitigate the bias and significantly improve model performances to be on par with human performances in some cases."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 188,
      "original_url": "https://aclanthology.org/2023.acl-long.187/",
      "title": "A Survey on Zero Pronoun Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.187.pdf",
      "authors": [
        "Longyue Wang",
        "Siyou Liu",
        "Mingzhou Xu",
        "Linfeng Song",
        "Shuming Shi",
        "Zhaopeng Tu"
      ],
      "abstract": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution so that researchers can recognize the current state and future directions of this field. We provide an organization of the literature based on evolution, dataset, method, and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 189,
      "original_url": "https://aclanthology.org/2023.acl-long.188/",
      "title": "We Understand Elliptical Sentences, and Language Models should Too: A New Dataset for Studying Ellipsis and its Interaction with Thematic Fit",
      "pdf_url": "https://aclanthology.org/2023.acl-long.188.pdf",
      "authors": [
        "Davide Testa",
        "Emmanuele Chersoni",
        "Alessandro Lenci"
      ],
      "abstract": "Ellipsis is a linguistic phenomenon characterized by the omission of one or more sentence elements. Solving such a linguistic construction is not a trivial issue in natural language processing since it involves the retrieval of non-overtly expressed verbal material, which might in turn require the model to integrate human-like syntactic and semantic knowledge. In this paper, we explored the issue of how the prototypicality of event participants affects the ability of Language Models (LMs) to handle elliptical sentences and to identify the omitted arguments at different degrees of thematic fit, ranging from highly typical participants to semantically anomalous ones. With this purpose in mind, we built ELLie, the first dataset composed entirely of utterances containing different types of elliptical constructions, and structurally suited for evaluating the effect of argument thematic fit in solving ellipsis and reconstructing the missing element. Our tests demonstrated that the probability scores assigned by the models are higher for typical events than for atypical and impossible ones in different elliptical contexts, confirming the influence of prototypicality of the event participants in interpreting such linguistic structures. Finally, we conducted a retrieval task of the elided verb in the sentence in which the low performance of LMs highlighted a considerable difficulty in reconstructing the correct event."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 190,
      "original_url": "https://aclanthology.org/2023.acl-long.189/",
      "title": "MPCHAT: Towards Multimodal Persona-Grounded Conversation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.189.pdf",
      "authors": [
        "Jaewoo Ahn",
        "Yeda Song",
        "Sangdoo Yun",
        "Gunhee Kim"
      ],
      "abstract": "In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker’s personal characteristics and experiences in episodic memory (Rubin et al., 2003; Conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we present the first multimodal persona-based dialogue dataset named MPCHAT, which extends persona with both text and images to contain episodic memories. Second, we empirically show that incorporating multimodal persona, as measured by three proposed multimodal persona-grounded dialogue tasks (i.e., next response prediction, grounding persona prediction, and speaker identification), leads to statistically significant performance improvements across all tasks. Thus, our work highlights that multimodal persona is crucial for improving multimodal dialogue comprehension, and our MPCHAT serves as a high-quality resource for this research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 191,
      "original_url": "https://aclanthology.org/2023.acl-long.190/",
      "title": "DOC: Improving Long Story Coherence With Detailed Outline Control",
      "pdf_url": "https://aclanthology.org/2023.acl-long.190.pdf",
      "authors": [
        "Kevin Yang",
        "Dan Klein",
        "Nanyun Peng",
        "Yuandong Tian"
      ],
      "abstract": "We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 192,
      "original_url": "https://aclanthology.org/2023.acl-long.191/",
      "title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.191.pdf",
      "authors": [
        "Ziheng Li",
        "Shaohan Huang",
        "Zihan Zhang",
        "Zhi-Hong Deng",
        "Qiang Lou",
        "Haizhen Huang",
        "Jian Jiao",
        "Furu Wei",
        "Weiwei Deng",
        "Qi Zhang"
      ],
      "abstract": "Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive experiments on three sentence-level cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https://github.com/ChillingDream/DAP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 193,
      "original_url": "https://aclanthology.org/2023.acl-long.192/",
      "title": "Exploring Better Text Image Translation with Multimodal Codebook",
      "pdf_url": "https://aclanthology.org/2023.acl-long.192.pdf",
      "authors": [
        "Zhibin Lan",
        "Jiawei Yu",
        "Xiang Li",
        "Wen Zhang",
        "Jian Luan",
        "Bin Wang",
        "Degen Huang",
        "Jinsong Su"
      ],
      "abstract": "Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 194,
      "original_url": "https://aclanthology.org/2023.acl-long.193/",
      "title": "FEDLEGAL: The First Real-World Federated Learning Benchmark for Legal NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.193.pdf",
      "authors": [
        "Zhuo Zhang",
        "Xiangjing Hu",
        "Jingyuan Zhang",
        "Yating Zhang",
        "Hui Wang",
        "Lizhen Qu",
        "Zenglin Xu"
      ],
      "abstract": "The inevitable private information in legal data necessitates legal artificial intelligence to study privacy-preserving and decentralized learning methods. Federated learning (FL) has merged as a promising technique for multiple participants to collaboratively train a shared model while efficiently protecting the sensitive data of participants. However, to the best of our knowledge, there is no work on applying FL to legal NLP. To fill this gap, this paper presents the first real-world FL benchmark for legal NLP, coined FEDLEGAL, which comprises five legal NLP tasks and one privacy task based on the data from Chinese courts. Based on the extensive experiments on these datasets, our results show that FL faces new challenges in terms of real-world non-IID data. The benchmark also encourages researchers to investigate privacy protection using real-world data in the FL setting, as well as deploying models in resource-constrained scenarios. The code and datasets of FEDLEGAL are available here."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 195,
      "original_url": "https://aclanthology.org/2023.acl-long.194/",
      "title": "A Gradient Control Method for Backdoor Attacks on Parameter-Efficient Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.194.pdf",
      "authors": [
        "Naibin Gu",
        "Peng Fu",
        "Xiyu Liu",
        "Zhengxiao Liu",
        "Zheng Lin",
        "Weiping Wang"
      ],
      "abstract": "Parameter-Efficient Tuning (PET) has shown remarkable performance by fine-tuning only a small number of parameters of the pre-trained language models (PLMs) for the downstream tasks, while it is also possible to construct backdoor attacks due to the vulnerability of pre-trained weights. However, a large reduction in the number of attackable parameters in PET will cause the user’s fine-tuning to greatly affect the effectiveness of backdoor attacks, resulting in backdoor forgetting. We find that the backdoor injection process can be regarded as multi-task learning, which has a convergence imbalance problem between the training of clean and poisoned data. And this problem might result in forgetting the backdoor. Based on this finding, we propose a gradient control method to consolidate the attack effect, comprising two strategies. One controls the gradient magnitude distribution cross layers within one task and the other prevents the conflict of gradient directions between tasks. Compared with previous backdoor attack methods in the scenario of PET, our method improve the effect of the attack on sentiment classification and spam detection respectively, which shows that our method is widely applicable to different tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 196,
      "original_url": "https://aclanthology.org/2023.acl-long.195/",
      "title": "History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.195.pdf",
      "authors": [
        "Hao Sun",
        "Yang Li",
        "Liwei Deng",
        "Bowen Li",
        "Binyuan Hui",
        "Binhua Li",
        "Yunshi Lan",
        "Yan Zhang",
        "Yongbin Li"
      ],
      "abstract": "Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models context at different levels of granularity. We evaluate HSGE on a widely used benchmark dataset for complex sequential question answering. Experimental results demonstrate that it outperforms existing baselines averaged on all question types."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 197,
      "original_url": "https://aclanthology.org/2023.acl-long.196/",
      "title": "From the One, Judge of the Whole: Typed Entailment Graph Construction with Predicate Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.196.pdf",
      "authors": [
        "Zhibin Chen",
        "Yansong Feng",
        "Dongyan Zhao"
      ],
      "abstract": "Entailment Graphs (EGs) have been constructed based on extracted corpora as a strong and explainable form to indicate context-independent entailment relation in natural languages. However, EGs built by previous methods often suffer from the severe sparsity issues, due to limited corpora available and the long-tail phenomenon of predicate distributions. In this paper, we propose a multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to tackle this problem. Given several seed predicates, TP-EGG builds the graphs by generating new predicates and detecting entailment relations among them. The generative nature of TP-EGG helps us leverage the recent advances from large pretrained language models (PLMs), while avoiding the reliance on carefully prepared corpora. Experiments on benchmark datasets show that TP-EGG can generate high-quality and scale-controllable entailment graphs, achieving significant in-domain improvement over state-of-the-art EGs and boosting the performance of down-stream inference tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 198,
      "original_url": "https://aclanthology.org/2023.acl-long.197/",
      "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.197.pdf",
      "authors": [
        "Nuo Chen",
        "Linjun Shou",
        "Jian Pei",
        "Ming Gong",
        "Bowen Cao",
        "Jianhui Chang",
        "Jia Li",
        "Daxin Jiang"
      ],
      "abstract": "Currently, learning better unsupervised sentence representations is the pursuit of many natural language processing communities. Lots of approaches based on pre-trained language models (PLMs) and contrastive learning have achieved promising results on this task. Experimentally, we observe that the over-smoothing problem reduces the capacity of these powerful PLMs, leading to sub-optimal sentence representations. In this paper, we present a Simple method named Self-Contrastive Learning (SSCL) to alleviate this issue, which samples negatives from PLMs intermediate layers, improving the quality of the sentence representation. Our proposed method is quite simple and can be easily extended to various state-of-the-art models for performance boosting, which can be seen as a plug-and-play contrastive framework for learning unsupervised sentence representation. Extensive results prove that SSCL brings the superior performance improvements of different strong baselines (e.g., BERT and SimCSE) on Semantic Textual Similarity and Transfer datasets"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 199,
      "original_url": "https://aclanthology.org/2023.acl-long.198/",
      "title": "Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.198.pdf",
      "authors": [
        "Yeskendir Koishekenov",
        "Alexandre Berard",
        "Vassilina Nikoulina"
      ],
      "abstract": "The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80% of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 200,
      "original_url": "https://aclanthology.org/2023.acl-long.199/",
      "title": "DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-long.199.pdf",
      "authors": [
        "William Held",
        "Christopher Hidey",
        "Fei Liu",
        "Eric Zhu",
        "Rahul Goel",
        "Diyi Yang",
        "Rushin Shah"
      ],
      "abstract": "Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improves both English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 201,
      "original_url": "https://aclanthology.org/2023.acl-long.200/",
      "title": "From Characters to Words: Hierarchical Pre-trained Language Model for Open-vocabulary Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.200.pdf",
      "authors": [
        "Li Sun",
        "Florian Luisier",
        "Kayhan Batmanghelich",
        "Dinei Florencio",
        "Cha Zhang"
      ],
      "abstract": "Current state-of-the-art models for natural language understanding require a preprocessing step to convert raw text into discrete tokens. This process known as tokenization relies on a pre-built vocabulary of words or sub-word morphemes. This fixed vocabulary limits the model’s robustness to spelling errors and its capacity to adapt to new domains. In this work, we introduce a novel open-vocabulary language model that adopts a hierarchical two-level approach: one at the word level and another at the sequence level. Concretely, we design an intra-word module that uses a shallow Transformer architecture to learn word representations from their characters, and a deep inter-word Transformer module that contextualizes each word representation by attending to the entire word sequence. Our model thus directly operates on character sequences with explicit awareness of word boundaries, but without biased sub-word or word-level vocabulary. Experiments on various downstream tasks show that our method outperforms strong baselines. We also demonstrate that our hierarchical model is robust to textual corruption and domain shift."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 202,
      "original_url": "https://aclanthology.org/2023.acl-long.201/",
      "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.201.pdf",
      "authors": [
        "Yu Song",
        "Santiago Miret",
        "Bang Liu"
      ],
      "abstract": "We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on {pasted macro ‘BENCHMARK’} and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 203,
      "original_url": "https://aclanthology.org/2023.acl-long.202/",
      "title": "Code4Struct: Code Generation for Few-Shot Event Structure Prediction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.202.pdf",
      "authors": [
        "Xingyao Wang",
        "Sha Li",
        "Heng Ji"
      ],
      "abstract": "Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12% absolute F1."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 204,
      "original_url": "https://aclanthology.org/2023.acl-long.203/",
      "title": "GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles",
      "pdf_url": "https://aclanthology.org/2023.acl-long.203.pdf",
      "authors": [
        "Tanmay Parekh",
        "I-Hung Hsu",
        "Kuan-Hao Huang",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "abstract": "Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites aimed at evaluating models’ ability to handle limited data and unseen event type generalization. We benchmark six EAE models from various families. The results show that owing to non-entity argument roles, even the best-performing model can only achieve 39% F1 score, indicating how GENEVA provides new challenges for generalization in EAE. Overall, our large and diverse EAE ontology can aid in creating more comprehensive future resources, while GENEVA is a challenging benchmarking dataset encouraging further research for improving generalizability in EAE. The code and data can be found at https://github.com/PlusLabNLP/GENEVA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 205,
      "original_url": "https://aclanthology.org/2023.acl-long.204/",
      "title": "Efficient Semiring-Weighted Earley Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.204.pdf",
      "authors": [
        "Andreas Opedal",
        "Ran Zmigrod",
        "Tim Vieira",
        "Ryan Cotterell",
        "Jason Eisner"
      ],
      "abstract": "We present Earley’s (1970) context-free parsing algorithm as a deduction system, incorporating various known and new speed-ups. In particular, our presentation supports a known worst-case runtime improvement from Earley’s (1970) O(N3|G||R|), which is unworkable for the large grammars that arise in natural language processing, to O(N3|G|), which matches the complexity of CKY on a binarized version of the grammar G. Here N is the length of the sentence, |R| is the number of productions in G, and |G| is the total length of those productions. We also provide a version that achieves runtime of O(N3|M|) with |M| ≤ |G| when the grammar is represented compactly as a single finite-state automaton M (this is partly novel). We carefully treat the generalization to semiring-weighted deduction, preprocessing the grammar like Stolcke (1995) to eliminate the possibility of deduction cycles, and further generalize Stolcke’s method to compute the weights of sentence prefixes. We also provide implementation details for efficient execution, ensuring that on a preprocessed grammar, the semiring-weighted versions of our methods have the same asymptotic runtime and space requirements as the unweighted methods, including sub-cubic runtime on some grammars."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 206,
      "original_url": "https://aclanthology.org/2023.acl-long.205/",
      "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
      "pdf_url": "https://aclanthology.org/2023.acl-long.205.pdf",
      "authors": [
        "Alexander Scarlatos",
        "Andrew Lan"
      ],
      "abstract": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 207,
      "original_url": "https://aclanthology.org/2023.acl-long.206/",
      "title": "ParaLS: Lexical Substitution via Pretrained Paraphraser",
      "pdf_url": "https://aclanthology.org/2023.acl-long.206.pdf",
      "authors": [
        "Jipeng Qiang",
        "Kang Liu",
        "Yun Li",
        "Yunhao Yuan",
        "Yi Zhu"
      ],
      "abstract": "Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence’s meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence’s meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 208,
      "original_url": "https://aclanthology.org/2023.acl-long.207/",
      "title": "Peer-Label Assisted Hierarchical Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.207.pdf",
      "authors": [
        "Junru Song",
        "Feifei Wang",
        "Yang Yang"
      ],
      "abstract": "Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in our work to improve the classification performance. To fully explore the peer-label relationship, we develop a PeerHTC method. This method innovatively measures the latent relevancy of peer labels through several metrics and then encodes the relevancy with a Graph Convolutional Neural Network. We also propose a sample importance learning method to ameliorate the side effects raised by modelling the peer label relevancy. Our experiments on several standard datasets demonstrate the evidence of peer labels and the superiority of PeerHTC over other state-of-the-art HTC methods in terms of classification accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 209,
      "original_url": "https://aclanthology.org/2023.acl-long.208/",
      "title": "Free Lunch for Efficient Textual Commonsense Integration in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.208.pdf",
      "authors": [
        "Wanyun Cui",
        "Xingran Chen"
      ],
      "abstract": "Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. Our idea is to group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. We theoretically investigate this problem and demonstrate that its upper bound can be reduced to the classic graph k-cut problem. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the computational cost while preserving performance. The efficiency improvement is more pronounced on larger datasets and on devices with more memory capacity, attesting to its practical utility for large-scale applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 210,
      "original_url": "https://aclanthology.org/2023.acl-long.209/",
      "title": "A Probabilistic Framework for Discovering New Intents",
      "pdf_url": "https://aclanthology.org/2023.acl-long.209.pdf",
      "authors": [
        "Yunhua Zhou",
        "Guofeng Quan",
        "Xipeng Qiu"
      ],
      "abstract": "Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not deeply explore the intrinsic structure of unlabeled data, and as a result, cannot seek out the characteristics that define an intent in general. In this paper, starting from the intuition that discovering intents could be beneficial for identifying known intents, we propose a probabilistic framework for discovering intents where intent assignments are treated as latent variables. We adopt the Expectation Maximization framework for optimization. Specifically, In the E-step, we conduct intent discovery and explore the intrinsic structure of unlabeled data by the posterior of intent assignments. In the M-step, we alleviate the forgetting of prior knowledge transferred from known intents by optimizing the discrimination of labeled data. Extensive experiments conducted on three challenging real-world datasets demonstrate the generality and effectiveness of the proposed framework and implementation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 211,
      "original_url": "https://aclanthology.org/2023.acl-long.210/",
      "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
      "pdf_url": "https://aclanthology.org/2023.acl-long.210.pdf",
      "authors": [
        "Leonhard Hennig",
        "Philippe Thomas",
        "Sebastian Möller"
      ],
      "abstract": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 212,
      "original_url": "https://aclanthology.org/2023.acl-long.211/",
      "title": "Towards Higher Pareto Frontier in Multilingual Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.211.pdf",
      "authors": [
        "Yichong Huang",
        "Xiaocheng Feng",
        "Xinwei Geng",
        "Baohang Li",
        "Bing Qin"
      ],
      "abstract": "Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontierIn Pareto optimization, Pareto optimal solutions refer to solutions in which none of the objectives can be improved without sacrificing at least one of the other objectives. The set of all Pareto optimal solutions forms a Pareto frontier..In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED datasets show that our method significantly pushes the Pareto frontier and outperforms baselines by up to +2.46 BLEUOur code will be released upon acceptance.."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 213,
      "original_url": "https://aclanthology.org/2023.acl-long.212/",
      "title": "Small Pre-trained Language Models Can be Fine-tuned as Large Models via Over-Parameterization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.212.pdf",
      "authors": [
        "Ze-Feng Gao",
        "Kun Zhou",
        "Peiyu Liu",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "By scaling the model size, large pre-trained language models (PLMs) have shown remarkable performance in various natural language processing tasks, mostly outperforming small PLMs by a large margin. However, due to the high computational cost, the huge number of parameters also restricts the applicability of large PLMs in real-world systems. In this paper, we focus on scaling up the parameters of PLMs only during fine-tuning, to benefit from the over-parameterization, while without increasing the inference latency. Given a relatively small PLM, we over-parameterize it by employing a matrix product operator, an efficient and almost lossless decomposition method to factorize its contained parameter matrices into a set of higher-dimensional tensors.Considering the efficiency, we further propose both static and dynamic strategies to select the most important parameter matrices for over-parameterization.Extensive experiments have demonstrated that our approach can significantly boost the fine-tuning performance of small PLMs and even help small PLMs outperform 3× parameterized larger ones.Our code is publicly available at https://github.com/zfgao66/OPF."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 214,
      "original_url": "https://aclanthology.org/2023.acl-long.213/",
      "title": "Entity Tracking in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.213.pdf",
      "authors": [
        "Najoung Kim",
        "Sebastian Schuster"
      ],
      "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 215,
      "original_url": "https://aclanthology.org/2023.acl-long.214/",
      "title": "A Textual Dataset for Situated Proactive Response Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.214.pdf",
      "authors": [
        "Naoki Otani",
        "Jun Araki",
        "HyeongSik Kim",
        "Eduard Hovy"
      ],
      "abstract": "Recent data-driven conversational models are able to return fluent, consistent, and informative responses to many kinds of requests and utterances in task-oriented scenarios. However, these responses are typically limited to just the immediate local topic instead of being wider-ranging and proactively taking the conversation further, for example making suggestions to help customers achieve their goals. This inadequacy reflects a lack of understanding of the interlocutor’s situation and implicit goal. To address the problem, we introduce a task of proactive response selection based on situational information. We present a manually-curated dataset of 1.7k English conversation examples that include situational background information plus for each conversation a set of responses, only some of which are acceptable in the situation. A responsive and informed conversation system should select the appropriate responses and avoid inappropriate ones; doing so demonstrates the ability to adequately understand the initiating request and situation. Our benchmark experiments show that this is not an easy task even for strong neural models, offering opportunities for future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 216,
      "original_url": "https://aclanthology.org/2023.acl-long.215/",
      "title": "DiffusionNER: Boundary Diffusion for Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.215.pdf",
      "authors": [
        "Yongliang Shen",
        "Kaitao Song",
        "Xu Tan",
        "Dongsheng Li",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "abstract": "In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process. The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DiffusionNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 217,
      "original_url": "https://aclanthology.org/2023.acl-long.216/",
      "title": "WACO: Word-Aligned Contrastive Learning for Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.216.pdf",
      "authors": [
        "Siqi Ouyang",
        "Rong Ye",
        "Lei Li"
      ],
      "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model’s performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 218,
      "original_url": "https://aclanthology.org/2023.acl-long.217/",
      "title": "Cross-lingual Continual Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.217.pdf",
      "authors": [
        "Meryem M’hamdi",
        "Xiang Ren",
        "Jonathan May"
      ],
      "abstract": "The longstanding goal of multi-lingual learning has been to develop a universal cross-lingual model that can withstand the changes in multi-lingual data distributions. There has been a large amount of work to adapt such multi-lingual models to unseen target languages. However, the majority of work in this direction focuses on the standard one-hop transfer learning pipeline from source to target languages, whereas in realistic scenarios, new languages can be incorporated at any time in a sequential manner. In this paper, we present a principled Cross-lingual Continual Learning (CCL) evaluation paradigm, where we analyze different categories of approaches used to continually adapt to emerging data from different languages. We provide insights into what makes multilingual sequential learning particularly challenging. To surmount such challenges, we benchmark a representative set of cross-lingual continual learning algorithms and analyze their knowledge preservation, accumulation, and generalization capabilities compared to baselines on carefully curated datastreams. The implications of this analysis include a recipe for how to measure and balance different cross-lingual continual learning desiderata, which go beyond conventional transfer learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 219,
      "original_url": "https://aclanthology.org/2023.acl-long.218/",
      "title": "Faithful Question Answering with Monte-Carlo Planning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.218.pdf",
      "authors": [
        "Ruixin Hong",
        "Hongming Zhang",
        "Hong Zhao",
        "Dong Yu",
        "Changshui Zhang"
      ],
      "abstract": "Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves advanced performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 220,
      "original_url": "https://aclanthology.org/2023.acl-long.219/",
      "title": "Unbalanced Optimal Transport for Unbalanced Word Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.219.pdf",
      "authors": [
        "Yuki Arase",
        "Han Bao",
        "Sho Yokoi"
      ],
      "abstract": "Monolingual word alignment is crucial to model semantic interactions between sentences. In particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. Identification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. To achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques. Our extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 221,
      "original_url": "https://aclanthology.org/2023.acl-long.220/",
      "title": "Guiding Computational Stance Detection with Expanded Stance Triangle Framework",
      "pdf_url": "https://aclanthology.org/2023.acl-long.220.pdf",
      "authors": [
        "Zhengyuan Liu",
        "Yong Keong Yap",
        "Hai Leong Chieu",
        "Nancy Chen"
      ],
      "abstract": "Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational solutions to model semantic features and infer the corresponding implications from a literal statement. Moreover, the limited amount of available training data leads to subpar performance in out-of-domain and cross-target scenarios, as data-driven approaches are prone to rely on superficial and domain-specific features. In this work, we decompose the stance detection task from a linguistic perspective, and investigate key components and inference paths in this task. The stance triangle is a generic linguistic framework previously proposed to describe the fundamental ways people express their stance. We further expand it by characterizing the relationship between explicit and implicit objects. We then use the framework to extend one single training corpus with additional annotation. Experimental results show that strategically-enriched data can significantly improve the performance on out-of-domain and cross-target evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 222,
      "original_url": "https://aclanthology.org/2023.acl-long.221/",
      "title": "Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast",
      "pdf_url": "https://aclanthology.org/2023.acl-long.221.pdf",
      "authors": [
        "Yiduo Guo",
        "Yaobo Liang",
        "Dongyan Zhao",
        "Bing Liu",
        "Nan Duan"
      ],
      "abstract": "Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 223,
      "original_url": "https://aclanthology.org/2023.acl-long.222/",
      "title": "Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.222.pdf",
      "authors": [
        "Ran Zhou",
        "Xin Li",
        "Lidong Bing",
        "Erik Cambria",
        "Chunyan Miao"
      ],
      "abstract": "In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. In this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework. Our proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. Our contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. We evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 224,
      "original_url": "https://aclanthology.org/2023.acl-long.223/",
      "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.223.pdf",
      "authors": [
        "Letitia Parcalabescu",
        "Anette Frank"
      ],
      "abstract": "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models – LXMERT, CLIP and four ALBEF variants – on four VL tasks highlight that unimodal collapse can occur to different degrees and in different directions, contradicting the wide-spread assumption that unimodal collapse is one-sided. Based on our results, we recommend MM-SHAP for analysing multimodal tasks, to diagnose and guide progress towards multimodal integration. Code available at https://github.com/Heidelberg-NLP/MM-SHAP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 225,
      "original_url": "https://aclanthology.org/2023.acl-long.224/",
      "title": "Towards Boosting the Open-Domain Chatbot with Human Feedback",
      "pdf_url": "https://aclanthology.org/2023.acl-long.224.pdf",
      "authors": [
        "Hua Lu",
        "Siqi Bao",
        "Huang He",
        "Fan Wang",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Many open-domain dialogue models pre-trained with social media comments can generate coherent replies but have difficulties producing engaging responses. This phenomenon might mainly result from the deficiency of annotated human-human conversations and the misalignment with human preference. In this paper, we propose a novel and efficient framework Diamante to boost the open-domain chatbot, where two kinds of human feedback (including explicit demonstration and implicit preference) are collected and leveraged. By asking annotators to select or amend the model-generated candidate responses, Diamante efficiently collects the human demonstrated responses and constructs a Chinese chit-chat dataset. To enhance the alignment with human preference, Diamante leverages the implicit preference in the data collection process and introduces the generation-evaluation joint training. Comprehensive experiments indicate that the Diamante dataset and joint training paradigm can significantly boost the performance of pre-trained dialogue models. The overall engagingness of the previous state-of-the-art model has been improved remarkably by 50% in Chinese open-domain conversations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 226,
      "original_url": "https://aclanthology.org/2023.acl-long.225/",
      "title": "Knowledge-enhanced Mixed-initiative Dialogue System for Emotional Support Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.225.pdf",
      "authors": [
        "Yang Deng",
        "Wenxuan Zhang",
        "Yifei Yuan",
        "Wai Lam"
      ],
      "abstract": "Unlike empathetic dialogues, the system in emotional support conversations (ESC) is expected to not only convey empathy for comforting the help-seeker, but also proactively assist in exploring and addressing their problems during the conversation. In this work, we study the problem of mixed-initiative ESC where the user and system can both take the initiative in leading the conversation. Specifically, we conduct a novel analysis on mixed-initiative ESC systems with a tailor-designed schema that divides utterances into different types with speaker roles and initiative types. Four emotional support metrics are proposed to evaluate the mixed-initiative interactions. The analysis reveals the necessity and challenges of building mixed-initiative ESC systems. In the light of this, we propose a knowledge-enhanced mixed-initiative framework (KEMI) for ESC, which retrieves actual case knowledge from a large-scale mental health knowledge graph for generating mixed-initiative responses. Experimental results on two ESC datasets show the superiority of KEMI in both content-preserving evaluation and mixed initiative related analyses."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 227,
      "original_url": "https://aclanthology.org/2023.acl-long.226/",
      "title": "UTC-IE: A Unified Token-pair Classification Architecture for Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.226.pdf",
      "authors": [
        "Hang Yan",
        "Yu Sun",
        "Xiaonan Li",
        "Yunhua Zhou",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "Information Extraction (IE) spans several tasks with different output structures, such as named entity recognition, relation extraction and event extraction. Previously, those tasks were solved with different models because of diverse task output structures. Through re-examining IE tasks, we find that all of them can be interpreted as extracting spans and span relations. They can further be decomposed into token-pair classification tasks by using the start and end token of a span to pinpoint the span, and using the start-to-start and end-to-end token pairs of two spans to determine the relation. Based on the reformulation, we propose a Unified Token-pair Classification architecture for Information Extraction (UTC-IE), where we introduce Plusformer on top of the token-pair feature matrix. Specifically, it models axis-aware interaction with plus-shaped self-attention and local interaction with Convolutional Neural Network over token pairs. Experiments show that our approach outperforms task-specific and unified models on all tasks in 10 datasets, and achieves better or comparable results on 2 joint IE datasets. Moreover, UTC-IE speeds up over state-of-the-art models on IE tasks significantly in most datasets, which verifies the effectiveness of our architecture."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 228,
      "original_url": "https://aclanthology.org/2023.acl-long.227/",
      "title": "Social-Group-Agnostic Bias Mitigation via the Stereotype Content Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.227.pdf",
      "authors": [
        "Ali Omrani",
        "Alireza Salkhordeh Ziabari",
        "Charles Yu",
        "Preni Golazizian",
        "Brendan Kennedy",
        "Mohammad Atari",
        "Heng Ji",
        "Morteza Dehghani"
      ],
      "abstract": "Existing bias mitigation methods require social-group-specific word pairs (e.g., “man” – “woman”) for each social attribute (e.g., gender), restricting the bias mitigation to only one specified social attribute. Further, this constraint renders such methods impractical and costly for mitigating bias in understudied and/or unmarked social groups. We propose that the Stereotype Content Model (SCM) — a theoretical framework developed in social psychology for understanding the content of stereotyping — can help debiasing efforts to become social-group-agnostic by capturing the underlying connection between bias and stereotypes. SCM proposes that the content of stereotypes map to two psychological dimensions of warmth and competence. Using only pairs of terms for these two dimensions (e.g., warmth: “genuine” – “fake”; competence: “smart” – “stupid”), we perform debiasing with established methods on both pre-trained word embeddings and large language models. We demonstrate that our social-group-agnostic, SCM-based debiasing technique performs comparably to group-specific debiasing on multiple bias benchmarks, but has theoretical and practical advantages over existing approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 229,
      "original_url": "https://aclanthology.org/2023.acl-long.228/",
      "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.228.pdf",
      "authors": [
        "Yixin Liu",
        "Alex Fabbri",
        "Pengfei Liu",
        "Yilun Zhao",
        "Linyong Nan",
        "Ruilin Han",
        "Simeng Han",
        "Shafiq Joty",
        "Chien-Sheng Wu",
        "Caiming Xiong",
        "Dragomir Radev"
      ],
      "abstract": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators’ prior, input-agnostic preferences, calling for more robust, targeted evaluation methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 230,
      "original_url": "https://aclanthology.org/2023.acl-long.229/",
      "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
      "pdf_url": "https://aclanthology.org/2023.acl-long.229.pdf",
      "authors": [
        "Andrew Zhu",
        "Karmanya Aggarwal",
        "Alexander Feng",
        "Lara Martin",
        "Chris Callison-Burch"
      ],
      "abstract": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D&D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 231,
      "original_url": "https://aclanthology.org/2023.acl-long.230/",
      "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.230.pdf",
      "authors": [
        "Jennifer Hu",
        "Sammy Floyd",
        "Olessia Jouravlev",
        "Evelina Fedorenko",
        "Edward Gibson"
      ],
      "abstract": "Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 232,
      "original_url": "https://aclanthology.org/2023.acl-long.231/",
      "title": "Counterfactual Multihop QA: A Cause-Effect Approach for Reducing Disconnected Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.231.pdf",
      "authors": [
        "Wangzhen Guo",
        "Qinkang Gong",
        "Yanghui Rao",
        "Hanjiang Lai"
      ],
      "abstract": "Multi-hop QA requires reasoning over multiple supporting facts to answer the question. However, the existing QA models always rely on shortcuts, e.g., providing the true answer by only one fact, rather than multi-hop reasoning, which is referred as disconnected reasoning problem. To alleviate this issue, we propose a novel counterfactual multihop QA, a causal-effect approach that enables to reduce the disconnected reasoning. It builds upon explicitly modeling of causality: 1) the direct causal effects of disconnected reasoning and 2) the causal effect of true multi-hop reasoning from the total causal effect. With the causal graph, a counterfactual inference is proposed to disentangle the disconnected reasoning from the total causal effect, which provides us a new perspective and technology to learn a QA model that exploits the true multi-hop reasoning instead of shortcuts. Extensive experiments have been conducted on the benchmark HotpotQA dataset, which demonstrate that the proposed method can achieve notable improvement on reducing disconnected reasoning. For example, our method achieves 5.8% higher points of its Supps score on HotpotQA through true multihop reasoning. The code is available at https://github.com/guowzh/CFMQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 233,
      "original_url": "https://aclanthology.org/2023.acl-long.232/",
      "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.232.pdf",
      "authors": [
        "Fan Zhou",
        "Yuzhou Mao",
        "Liu Yu",
        "Yi Yang",
        "Ting Zhong"
      ],
      "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 234,
      "original_url": "https://aclanthology.org/2023.acl-long.233/",
      "title": "Parameter-Efficient Fine-Tuning without Introducing New Latency",
      "pdf_url": "https://aclanthology.org/2023.acl-long.233.pdf",
      "authors": [
        "Baohao Liao",
        "Yan Meng",
        "Christof Monz"
      ],
      "abstract": "Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03% parameters of full fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 235,
      "original_url": "https://aclanthology.org/2023.acl-long.234/",
      "title": "MANNER: A Variational Memory-Augmented Model for Cross Domain Few-Shot Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.234.pdf",
      "authors": [
        "Jinyuan Fang",
        "Xiaobin Wang",
        "Zaiqiao Meng",
        "Pengjun Xie",
        "Fei Huang",
        "Yong Jiang"
      ],
      "abstract": "This paper focuses on the task of cross domain few-shot named entity recognition (NER), which aims to adapt the knowledge learned from source domain to recognize named entities in target domain with only a few labeled examples. To address this challenging task, we propose MANNER, a variational memory-augmented few-shot NER model. Specifically, MANNER uses a memory module to store information from the source domain and then retrieve relevant information from the memory to augment few-shot task in the target domain. In order to effectively utilize the information from memory, MANNER uses optimal transport to retrieve and process information from memory, which can explicitly adapt the retrieved information from source domain to target domain and improve the performance in the cross domain few-shot setting. We conduct experiments on English and Chinese cross domain few-shot NER datasets, and the experimental results demonstrate that MANNER can achieve superior performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 236,
      "original_url": "https://aclanthology.org/2023.acl-long.235/",
      "title": "MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.235.pdf",
      "authors": [
        "Jack FitzGerald",
        "Christopher Hench",
        "Charith Peris",
        "Scott Mackie",
        "Kay Rottmann",
        "Ana Sanchez",
        "Aaron Nash",
        "Liam Urbach",
        "Vishesh Kakarala",
        "Richa Singh",
        "Swetha Ranganath",
        "Laurie Crist",
        "Misha Britan",
        "Wouter Leeuwis",
        "Gokhan Tur",
        "Prem Natarajan"
      ],
      "abstract": "We present the MASSIVE dataset–Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 237,
      "original_url": "https://aclanthology.org/2023.acl-long.236/",
      "title": "Distilling Script Knowledge from Large Language Models for Constrained Language Planning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.236.pdf",
      "authors": [
        "Siyu Yuan",
        "Jiangjie Chen",
        "Ziquan Fu",
        "Xuyang Ge",
        "Soham Shah",
        "Charles Jankowski",
        "Yanghua Xiao",
        "Deqing Yang"
      ],
      "abstract": "In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., “make a cake”), but leaves more specific goals with multi-facet constraints understudied (e.g., “make a cake for diabetics”). In this paper, we define the task of constrained language planning for the first time. We propose an over-generate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, Coscript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, Coscript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 238,
      "original_url": "https://aclanthology.org/2023.acl-long.237/",
      "title": "REDFM: a Filtered and Multilingual Relation Extraction Dataset",
      "pdf_url": "https://aclanthology.org/2023.acl-long.237.pdf",
      "authors": [
        "Pere-Lluís Huguet Cabot",
        "Simone Tedeschi",
        "Axel-Cyrille Ngonga Ngomo",
        "Roberto Navigli"
      ],
      "abstract": "Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English.In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems. First, we present SREDFM, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose REDFM, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. To demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, that extracts triplets, including entity types, in multiple languages. We release our resources and model checkpoints at [https://www.github.com/babelscape/rebel](https://www.github.com/babelscape/rebel)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 239,
      "original_url": "https://aclanthology.org/2023.acl-long.238/",
      "title": "Modeling Appropriate Language in Argumentation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.238.pdf",
      "authors": [
        "Timon Ziegenbein",
        "Shahbaz Syed",
        "Felix Lange",
        "Martin Potthast",
        "Henning Wachsmuth"
      ],
      "abstract": "Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the first time. In particular, we model appropriateness through the absence of flaws, grounded in research on argument quality assessment, especially in aspects from rhetoric. From these, we derive a new taxonomy of 14 dimensions that determine inappropriate language in online discussions. Building on three argument quality corpora, we then create a corpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses support that the taxonomy covers the concept of appropriateness comprehensively, showing several plausible correlations with argument quality dimensions. Moreover, results of baseline approaches to assessing appropriateness suggest that all dimensions can be modeled computationally on the corpus."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 240,
      "original_url": "https://aclanthology.org/2023.acl-long.239/",
      "title": "CELDA: Leveraging Black-box Language Model as Enhanced Classifier without Labels",
      "pdf_url": "https://aclanthology.org/2023.acl-long.239.pdf",
      "authors": [
        "Hyunsoo Cho",
        "Youna Kim",
        "Sang-goo Lee"
      ],
      "abstract": "Utilizing language models (LMs) without internal access is becoming an attractive paradigm in the field of NLP as many cutting-edge LMs are released through APIs and boast a massive scale. The de-facto method in this type of black-box scenario is known as prompting, which has shown progressive performance enhancements in situations where data labels are scarce or unavailable. Despite their efficacy, they still fall short in comparison to fully supervised counterparts and are generally brittle to slight modifications. In this paper, we propose Clustering-enhanced Linear Discriminative Analysis (CELDA), a novel approach that improves the text classification accuracy with a very weak-supervision signal (i.e., name of the labels).Our framework draws a precise decision boundary without accessing weights or gradients of the LM model or data labels. The core ideas of CELDA are twofold:(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and (2) training a lightweight and robust model on the top of LM, which learns an accurate decision boundary from an extracted noisy dataset. Throughout in-depth investigations on various datasets, we demonstrated that CELDA reaches new state-of-the-art in weakly-supervised text classification and narrows the gap with a fully-supervised model. Additionally, our proposed methodology can be applied universally to any LM and has the potential to scale to larger models, making it a more viable option for utilizing large LMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 241,
      "original_url": "https://aclanthology.org/2023.acl-long.240/",
      "title": "MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.240.pdf",
      "authors": [
        "Zhibin Gou",
        "Qingyan Guo",
        "Yujiu Yang"
      ],
      "abstract": "Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 242,
      "original_url": "https://aclanthology.org/2023.acl-long.241/",
      "title": "ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-long.241.pdf",
      "authors": [
        "Sarik Ghazarian",
        "Yijia Shao",
        "Rujun Han",
        "Aram Galstyan",
        "Nanyun Peng"
      ],
      "abstract": "Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct the first public event commonsense evaluation dataset for open-domain dialogues.Our experiments show that ACCENT is an efficient metric for event commonsense evaluation, which achieves higher correlations with human judgments than existing baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 243,
      "original_url": "https://aclanthology.org/2023.acl-long.242/",
      "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
      "pdf_url": "https://aclanthology.org/2023.acl-long.242.pdf",
      "authors": [
        "Josh Magnus Ludan",
        "Yixuan Meng",
        "Tai Nguyen",
        "Saurabh Shah",
        "Qing Lyu",
        "Marianna Apidianaki",
        "Chris Callison-Burch"
      ],
      "abstract": "Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a general approach to mitigate LLMs’ reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). The efficacy generalizes across multiple model families and scales, with greater gains for larger models. Finally, our method also works well with explanations generated by the model, implying its applicability to more datasets without human-written explanations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 244,
      "original_url": "https://aclanthology.org/2023.acl-long.243/",
      "title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.243.pdf",
      "authors": [
        "Yang Luo",
        "Xiaozhe Ren",
        "Zangwei Zheng",
        "Zhuo Jiang",
        "Xin Jiang",
        "Yang You"
      ],
      "abstract": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 245,
      "original_url": "https://aclanthology.org/2023.acl-long.244/",
      "title": "On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.244.pdf",
      "authors": [
        "Omar Shaikh",
        "Hongxin Zhang",
        "William Held",
        "Michael Bernstein",
        "Diyi Yang"
      ],
      "abstract": "Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 246,
      "original_url": "https://aclanthology.org/2023.acl-long.245/",
      "title": "Solving Math Word Problems via Cooperative Reasoning induced Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.245.pdf",
      "authors": [
        "Xinyu Zhu",
        "Junjie Wang",
        "Lin Zhang",
        "Yuxiang Zhang",
        "Yongfeng Huang",
        "Ruyi Gan",
        "Jiaxing Zhang",
        "Yujiu Yang"
      ],
      "abstract": "Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6% increase over best baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 247,
      "original_url": "https://aclanthology.org/2023.acl-long.246/",
      "title": "Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.246.pdf",
      "authors": [
        "Chantal Amrhein",
        "Florian Schottmann",
        "Rico Sennrich",
        "Samuel Läubli"
      ],
      "abstract": "Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data through linguistic rules. However, this approach is not practical for languages with more complex morphology than English. We hypothesise that creating training data in the reverse direction, i.e. starting from gender-fair text, is easier for morphologically complex languages and show that it matches the performance of state-of-the-art rewriting models for English. To eliminate the rule-based nature of data creation, we instead propose using machine translation models to create gender-biased text from real gender-fair text via round-trip translation. Our approach allows us to train a rewriting model for German without the need for elaborate handcrafted rules. The outputs of this model increased gender-fairness as shown in a human evaluation study."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 248,
      "original_url": "https://aclanthology.org/2023.acl-long.247/",
      "title": "Early Discovery of Disappearing Entities in Microblogs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.247.pdf",
      "authors": [
        "Satoshi Akasaki",
        "Naoki Yoshinaga",
        "Masashi Toyoda"
      ],
      "abstract": "We make decisions by reacting to changes in the real world, particularly the emergence and disappearance of impermanent entities such as restaurants, services, and events. Because we want to avoid missing out on opportunities or making fruitless actions after those entities have disappeared, it is important to know when entities disappear as early as possible. We thus tackle the task of detecting disappearing entities from microblogs where various information is shared timely. The major challenge is detecting uncertain contexts of disappearing entities from noisy microblog posts. To collect such disappearing contexts, we design time-sensitive distant supervision, which utilizes entities from the knowledge base and time-series posts. Using this method, we actually build large-scale Twitter datasets of disappearing entities. To ensure robust detection in noisy environments, we refine pretrained word embeddings for the detection model on microblog streams in a timely manner. Experimental results on the Twitter datasets confirmed the effectiveness of the collected labeled data and refined word embeddings; the proposed method outperformed a baseline in terms of accuracy, and more than 70% of the detected disappearing entities in Wikipedia are discovered earlier than the update on Wikipedia, with the average lead-time is over one month."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 249,
      "original_url": "https://aclanthology.org/2023.acl-long.248/",
      "title": "DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.248.pdf",
      "authors": [
        "Zhengfu He",
        "Tianxiang Sun",
        "Qiong Tang",
        "Kuanning Wang",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 250,
      "original_url": "https://aclanthology.org/2023.acl-long.249/",
      "title": "Lifting the Curse of Capacity Gap in Distilling Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.249.pdf",
      "authors": [
        "Chen Zhang",
        "Yang Yang",
        "Jiahao Liu",
        "Jingang Wang",
        "Yunsen Xian",
        "Benyou Wang",
        "Dawei Song"
      ],
      "abstract": "Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ~50×, MiniMoE preserves ~95% GLUE score of the teacher."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 251,
      "original_url": "https://aclanthology.org/2023.acl-long.250/",
      "title": "Towards Faithful Dialogues via Focus Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.250.pdf",
      "authors": [
        "Yifan Deng",
        "Xingsheng Zhang",
        "Heyan Huang",
        "Yue Hu"
      ],
      "abstract": "Maintaining faithfulness between responses and knowledge is an important research topic for building reliable knowledge-grounded dialogue systems. Existing models heavily rely on elaborate data engineering or increasing the model’s parameters ignoring to track the tokens that significantly influence losses, which is decisive for the optimization direction of the model in each iteration. To address this issue, we propose Focus Learning (FocusL), a novel learning approach that adjusts the contribution of each token to the optimization direction by directly scaling the corresponding objective loss. Specifically, we first introduce a positioning method by utilizing similarity distributions between knowledge and each response token to locate knowledge-aware tokens. Then, we further design a similarity-to-weight transformation to provide dynamic token-level weights for the cross-entropy loss. Finally, we use the weighted loss to encourage the model to pay special attention to the knowledge utilization. Experimental results demonstrate that our method achieves the new state-of-the-art results and generates more reliable responses while maintaining training stability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 252,
      "original_url": "https://aclanthology.org/2023.acl-long.251/",
      "title": "Back Translation for Speech-to-text Translation Without Transcripts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.251.pdf",
      "authors": [
        "Qingkai Fang",
        "Yang Feng"
      ],
      "abstract": "The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the remarkable success of back translation in MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo ST data from monolingual target data. To ease the challenges posed by short-to-long generation and one-to-many mapping, we introduce self-supervised discrete units and achieve back translation by cascading a target-to-unit model and a unit-to-speech model. With our synthetic ST data, we achieve an average boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments show that our method is especially effective in low-resource scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 253,
      "original_url": "https://aclanthology.org/2023.acl-long.252/",
      "title": "Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.252.pdf",
      "authors": [
        "Ibrahim Taha Aksu",
        "Min-Yen Kan",
        "Nancy Chen"
      ],
      "abstract": "A challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data — zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. Our method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer’s self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter’s gains are due to its improved ability to distinguish ”none”-valued dialogue slots, compared against baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 254,
      "original_url": "https://aclanthology.org/2023.acl-long.253/",
      "title": "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.253.pdf",
      "authors": [
        "Chen Tang",
        "Hongbo Zhang",
        "Tyler Loakman",
        "Chenghua Lin",
        "Frank Guerin"
      ],
      "abstract": "Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 255,
      "original_url": "https://aclanthology.org/2023.acl-long.254/",
      "title": "Multi-modal Action Chain Abductive Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.254.pdf",
      "authors": [
        "Mengze Li",
        "Tianbao Wang",
        "Jiahe Xu",
        "Kairong Han",
        "Shengyu Zhang",
        "Zhou Zhao",
        "Jiaxu Miao",
        "Wenqiao Zhang",
        "Shiliang Pu",
        "Fei Wu"
      ],
      "abstract": "Abductive Reasoning, has long been considered to be at the core ability of humans, which enables us to infer the most plausible explanation of incomplete known phenomena in daily life. However, such critical reasoning capability is rarely investigated for contemporary AI systems under such limited observations. To facilitate this research community, this paper sheds new light on Abductive Reasoning by studying a new vision-language task, Multi-modal Action chain abductive Reasoning (MAR), together with a large-scale Abductive Reasoning dataset: Given an incomplete set of language described events, MAR aims to imagine the most plausible event by spatio-temporal grounding in past video and then infer the hypothesis of subsequent action chain that can best explain the language premise. To solve this task, we propose a strong baseline model that realizes MAR from two perspectives: (i) we first introduce the transformer, which learns to encode the observation to imagine the plausible event with explicitly interpretable event grounding in the video based on the commonsense knowledge recognition ability. (ii) To complete the assumption of a follow-up action chain, we design a novel symbolic module that can complete strict derivation of the progressive action chain layer by layer. We conducted extensive experiments on the proposed dataset, and the experimental study shows that the proposed model significantly outperforms existing video-language models in terms of effectiveness on our newly created MAR dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 256,
      "original_url": "https://aclanthology.org/2023.acl-long.255/",
      "title": "Exploring the Capacity of Pretrained Language Models for Reasoning about Actions and Change",
      "pdf_url": "https://aclanthology.org/2023.acl-long.255.pdf",
      "authors": [
        "Weinan He",
        "Canming Huang",
        "Zhanhao Xiao",
        "Yongmei Liu"
      ],
      "abstract": "Reasoning about actions and change (RAC) is essential to understand and interact with the ever-changing environment. Previous AI research has shown the importance of fundamental and indispensable knowledge of actions, i.e., preconditions and effects. However, traditional methods rely on logical formalization which hinders practical applications. With recent transformer-based language models (LMs), reasoning over text is desirable and seemingly feasible, leading to the question of whether LMs can effectively and efficiently learn to solve RAC problems. We propose four essential RAC tasks as a comprehensive textual benchmark and generate problems in a way that minimizes the influence of other linguistic requirements (e.g., grounding) to focus on RAC. The resulting benchmark, TRAC, encompassing problems of various complexities, facilitates a more granular evaluation of LMs, precisely targeting the structural generalization ability much needed for RAC. Experiments with three high-performing transformers indicate that additional efforts are needed to tackle challenges raised by TRAC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 257,
      "original_url": "https://aclanthology.org/2023.acl-long.256/",
      "title": "Unified Demonstration Retriever for In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.256.pdf",
      "authors": [
        "Xiaonan Li",
        "Kai Lv",
        "Hang Yan",
        "Tianyang Lin",
        "Wei Zhu",
        "Yuan Ni",
        "Guotong Xie",
        "Xiaoling Wang",
        "Xipeng Qiu"
      ],
      "abstract": "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks’ training signals into a unified list-wise ranking formulation by language model’s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks’ signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR’s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 258,
      "original_url": "https://aclanthology.org/2023.acl-long.257/",
      "title": "Movie101: A New Movie Understanding Benchmark",
      "pdf_url": "https://aclanthology.org/2023.acl-long.257.pdf",
      "authors": [
        "Zihao Yue",
        "Qi Zhang",
        "Anwen Hu",
        "Liang Zhang",
        "Ziheng Wang",
        "Qin Jin"
      ],
      "abstract": "To help the visually impaired enjoy movies, automatic movie narrating systems are expected to narrate accurate, coherent, and role-aware plots when there are no speaking lines of actors. Existing works benchmark this challenge as a normal video captioning task via some simplifications, such as removing role names and evaluating narrations with ngram-based metrics, which makes it difficult for automatic systems to meet the needs of real application scenarios. To narrow this gap, we construct a large-scale Chinese movie benchmark, named Movie101. Closer to real scenarios, the Movie Clip Narrating (MCN) task in our benchmark asks models to generate role-aware narration paragraphs for complete movie clips where no actors are speaking. External knowledge, such as role information and movie genres, is also provided for better movie understanding. Besides, we propose a new metric called Movie Narration Score (MNScore) for movie narrating evaluation, which achieves the best correlation with human evaluation. Our benchmark also supports the Temporal Narration Grounding (TNG) task to investigate clip localization given text descriptions. For both two tasks, our proposed methods well leverage external knowledge and outperform carefully designed baselines. The dataset and codes are released at https://github.com/yuezih/Movie101."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 259,
      "original_url": "https://aclanthology.org/2023.acl-long.258/",
      "title": "Enhancing Language Representation with Constructional Information for Natural Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.258.pdf",
      "authors": [
        "Lvxiaowei Xu",
        "Jianwang Wu",
        "Jiawei Peng",
        "Zhilin Gong",
        "Ming Cai",
        "Tianxiang Wang"
      ],
      "abstract": "Natural language understanding (NLU) is an essential branch of natural language processing, which relies on representations generated by pre-trained language models (PLMs). However, PLMs primarily focus on acquiring lexico-semantic information, while they may be unable to adequately handle the meaning of constructions. To address this issue, we introduce construction grammar (CxG), which highlights the pairings of form and meaning, to enrich language representation. We adopt usage-based construction grammar as the basis of our work, which is highly compatible with statistical models such as PLMs. Then a HyCxG framework is proposed to enhance language representation through a three-stage solution. First, all constructions are extracted from sentences via a slot-constraints approach. As constructions can overlap with each other, bringing redundancy and imbalance, we formulate the conditional max coverage problem for selecting the discriminative constructions. Finally, we propose a relational hypergraph attention network to acquire representation from constructional information by capturing high-order word interactions among constructions. Extensive experiments demonstrate the superiority of the proposed model on a variety of NLU tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 260,
      "original_url": "https://aclanthology.org/2023.acl-long.259/",
      "title": "Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.259.pdf",
      "authors": [
        "Siyuan Wang",
        "Zhongyu Wei",
        "Meng Han",
        "Zhihao Fan",
        "Haijun Shan",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Logical reasoning over incomplete knowledge graphs to answer complex logical queries is a challenging task. With the emergence of new entities and relations in constantly evolving KGs, inductive logical reasoning over KGs has become a crucial problem. However, previous PLMs-based methods struggle to model the logical structures of complex queries, which limits their ability to generalize within the same structure. In this paper, we propose a structure-modeled textual encoding framework for inductive logical reasoning over KGs. It encodes linearized query structures and entities using pre-trained language models to find answers. For structure modeling of complex queries, we design stepwise instructions that implicitly prompt PLMs on the execution order of geometric operations in each query. We further separately model different geometric operations (i.e., projection, intersection, and union) on the representation space using a pre-trained encoder with additional attention and maxout layers to enhance structured modeling. We conduct experiments on two inductive logical reasoning datasets and three transductive datasets. The results demonstrate the effectiveness of our method on logical reasoning over KGs in both inductive and transductive settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 261,
      "original_url": "https://aclanthology.org/2023.acl-long.260/",
      "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
      "pdf_url": "https://aclanthology.org/2023.acl-long.260.pdf",
      "authors": [
        "Chenzhengyi Liu",
        "Jie Huang",
        "Kerui Zhu",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 262,
      "original_url": "https://aclanthology.org/2023.acl-long.261/",
      "title": "Incorporating Attribution Importance for Improving Faithfulness Metrics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.261.pdf",
      "authors": [
        "Zhixue Zhao",
        "Nikolaos Aletras"
      ],
      "abstract": "Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our soft-sufficiency and soft-comprehensiveness metrics consistently prefer more faithful explanations compared to hard sufficiency and comprehensiveness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 263,
      "original_url": "https://aclanthology.org/2023.acl-long.262/",
      "title": "Reward Gaming in Conditional Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.262.pdf",
      "authors": [
        "Richard Yuanzhe Pang",
        "Vishakh Padmakumar",
        "Thibault Sellam",
        "Ankur Parikh",
        "He He"
      ],
      "abstract": "To align conditional text generation model outputs with desired behaviors, there has been an increasing focus on training the model using reinforcement learning (RL) with reward functions learned from human annotations. Under this framework, we identify three common cases where high rewards are incorrectly assigned to undesirable patterns: noise-induced spurious correlation, naturally occurring spurious correlation, and covariate shift. We show that even though learned metrics achieve high performance on the distribution of the data used to train the reward function, the undesirable patterns may be amplified during RL training of the text generation model. While there has been discussion about reward gaming in the RL or safety community, in this discussion piece, we would like to highlight reward gaming in the natural language generation (NLG) community using concrete conditional text generation examples and discuss potential fixes and areas for future work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 264,
      "original_url": "https://aclanthology.org/2023.acl-long.263/",
      "title": "Hidden Schema Networks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.263.pdf",
      "authors": [
        "Ramses Sanchez",
        "Lukas Conrads",
        "Pascal Welke",
        "Kostadin Cvejoski",
        "Cesar Ojeda Marin"
      ],
      "abstract": "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk “reasoning” models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 265,
      "original_url": "https://aclanthology.org/2023.acl-long.264/",
      "title": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.264.pdf",
      "authors": [
        "Linlin Liu",
        "Xingxuan Li",
        "Megh Thakkar",
        "Xin Li",
        "Shafiq Joty",
        "Luo Si",
        "Lidong Bing"
      ],
      "abstract": "Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level lowresource NLP tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 266,
      "original_url": "https://aclanthology.org/2023.acl-long.265/",
      "title": "An Ordinal Latent Variable Model of Conflict Intensity",
      "pdf_url": "https://aclanthology.org/2023.acl-long.265.pdf",
      "authors": [
        "Niklas Stoehr",
        "Lucas Torroba Hennigen",
        "Josef Valvoda",
        "Robert West",
        "Ryan Cotterell",
        "Aaron Schein"
      ],
      "abstract": "Measuring the intensity of events is crucial for monitoring and tracking armed conflict. Advances in automated event extraction have yielded massive data sets of “who did what to whom” micro-records that enable data-driven approaches to monitoring conflict. The Goldstein scale is a widely-used expert-based measure that scores events on a conflictual–cooperative scale. It is based only on the action category (“what”) and disregards the subject (“who”) and object (“to whom”) of an event, as well as contextual information, like associated casualty count, that should contribute to the perception of an event’s “intensity”. This paper takes a latent variable-based approach to measuring conflict intensity. We introduce a probabilistic generative model that assumes each observed event is associated with a latent intensity class. A novel aspect of this model is that it imposes an ordering on the classes, such that higher-valued classes denote higher levels of intensity. The ordinal nature of the latent variable is induced from naturally ordered aspects of the data (e.g., casualty counts) where higher values naturally indicate higher intensity. We evaluate the proposed model both intrinsically and extrinsically, showing that it obtains comparatively good held-out predictive performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 267,
      "original_url": "https://aclanthology.org/2023.acl-long.266/",
      "title": "Multilingual Conceptual Coverage in Text-to-Image Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.266.pdf",
      "authors": [
        "Michael Saxon",
        "William Yang Wang"
      ],
      "abstract": "We propose “Conceptual Coverage Across Languages” (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess “conceptual coverage” of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under translation in the target language. This technique allows us to estimate how well-suited a model is to a target language as well as identify model-specific weaknesses, spurious correlations, and biases without a-priori assumptions. We demonstrate how it can be used to benchmark T2I models in terms of multilinguality, and how despite its simplicity it is a good proxy for impressive generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 268,
      "original_url": "https://aclanthology.org/2023.acl-long.267/",
      "title": "Pre-Training to Learn in Context",
      "pdf_url": "https://aclanthology.org/2023.acl-long.267.pdf",
      "authors": [
        "Yuxian Gu",
        "Li Dong",
        "Furu Wei",
        "Minlie Huang"
      ],
      "abstract": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models’ in-context learning ability by pre-training the model on a large collection of “intrinsic tasks” in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 269,
      "original_url": "https://aclanthology.org/2023.acl-long.268/",
      "title": "Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.268.pdf",
      "authors": [
        "Manuel Mager",
        "Elisabeth Mager",
        "Katharina Kann",
        "Ngoc Thang Vu"
      ],
      "abstract": "In recent years machine translation has become very successful for high-resource language pairs. This has also sparked new interest in research on the automatic translation of low-resource languages, including Indigenous languages. However, the latter are deeply related to the ethnic and cultural groups that speak (or used to speak) them. The data collection, modeling and deploying machine translation systems thus result in new ethical questions that must be addressed. Motivated by this, we first survey the existing literature on ethical considerations for the documentation, translation, and general natural language processing for Indigenous languages. Afterward, we conduct and analyze an interview study to shed light on the positions of community leaders, teachers, and language activists regarding ethical concerns for the automatic translation of their languages. Our results show that the inclusion, at different degrees, of native speakers and community members is vital to performing better and more ethical research on Indigenous languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 270,
      "original_url": "https://aclanthology.org/2023.acl-long.269/",
      "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
      "pdf_url": "https://aclanthology.org/2023.acl-long.269.pdf",
      "authors": [
        "Michael Ryan",
        "Tarek Naous",
        "Wei Xu"
      ],
      "abstract": "Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 271,
      "original_url": "https://aclanthology.org/2023.acl-long.270/",
      "title": "Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",
      "pdf_url": "https://aclanthology.org/2023.acl-long.270.pdf",
      "authors": [
        "Yu Gu",
        "Xiang Deng",
        "Yu Su"
      ],
      "abstract": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 272,
      "original_url": "https://aclanthology.org/2023.acl-long.271/",
      "title": "Privacy-Preserving Domain Adaptation of Semantic Parsers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.271.pdf",
      "authors": [
        "Fatemehsadat Mireshghallah",
        "Yu Su",
        "Tatsunori Hashimoto",
        "Jason Eisner",
        "Richard Shin"
      ],
      "abstract": "Task-oriented dialogue systems often assist users with personal or confidential matters. For this reason, the developers of such a system are generally prohibited from observing actual usage. So how can they know where the system is failing and needs more training data or new functionality? In this work, we study ways in which realistic user utterances can be generated synthetically, to help increase the linguistic and functional coverage of the system, without compromising the privacy of actual users. To this end, we propose a two-stage Differentially Private (DP) generation method which first generates latent semantic parses, and then generates utterances based on the parses. Our proposed approach improves MAUVE by 2.5X and parse tree function-type overlap by 1.3X relative to current approaches for private synthetic data generation, improving both on fluency and semantic coverage. We further validate our approach on a realistic domain adaptation task of adding new functionality from private user data to a semantic parser, and show overall gains of 8.5% points on its accuracy with the new feature."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 273,
      "original_url": "https://aclanthology.org/2023.acl-long.272/",
      "title": "Guide the Many-to-One Assignment: Open Information Extraction via IoU-aware Optimal Transport",
      "pdf_url": "https://aclanthology.org/2023.acl-long.272.pdf",
      "authors": [
        "Kaiwen Wei",
        "Yiran Yang",
        "Li Jin",
        "Xian Sun",
        "Zequn Zhang",
        "Jingyuan Zhang",
        "Xiao Li",
        "Linhao Zhang",
        "Jintao Liu",
        "Guo Zhi"
      ],
      "abstract": "Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how to assign ground truth labels to the parallelly generated tuple proposals remains under-exploited. The commonly utilized Hungarian algorithm for this procedure is restricted to handling one-to-one assignment among the desired tuples and tuple proposals, which ignores the correlation between proposals and affects the recall of the models. To solve this problem, we propose a dynamic many-to-one label assignment strategy named IOT. Concretely, the label assignment process in OIE is formulated as an Optimal Transport (OT) problem. We leverage the intersection-over-union (IoU) as the assignment quality measurement, and convert the problem of finding the best assignment solution to the one of solving the optimal transport plan by maximizing the IoU values. To further utilize the knowledge from the assignment, we design an Assignment-guided Multi-granularity loss (AM) by simultaneously considering word-level and tuple-level information. Experiment results show the proposed method outperforms the state-of-the-art models on three benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 274,
      "original_url": "https://aclanthology.org/2023.acl-long.273/",
      "title": "Actively Supervised Clustering for Open Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.273.pdf",
      "authors": [
        "Jun Zhao",
        "Yongxin Zhang",
        "Qi Zhang",
        "Tao Gui",
        "Zhongyu Wei",
        "Minlong Peng",
        "Mingming Sun"
      ],
      "abstract": "Current clustering-based Open Relation Extraction (OpenRE) methods usually adopt a two-stage pipeline, which simultaneously learns relation representations and assignments in the first stage, then manually labels relation for each cluster. However, unsupervised objectives struggle to explicitly optimize clusters to align with relational semantics, and the number of clusters K has to be supplied in advance. In this paper, we present a novel setting, named actively supervised clustering for OpenRE. Our insight lies in that clustering learning and relation labeling can be performed simultaneously, which provides the necessary guidance for clustering without a significant increase in human effort. Along with this setting, we propose an active labeling strategy tailored for clustering. Instead of only focusing on improving the clustering of relations that have been discovered, our strategy is encouraged to discover new relations through diversity regularization. This is particularly beneficial for long-tail relations in the real world. Experimental results show that our method is able to discover almost all relational clusters in the data and improve the SOTA methods by 13.8% and 10.6%, on two datasets respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 275,
      "original_url": "https://aclanthology.org/2023.acl-long.274/",
      "title": "ConvGQR: Generative Query Reformulation for Conversational Search",
      "pdf_url": "https://aclanthology.org/2023.acl-long.274.pdf",
      "authors": [
        "Fengran Mo",
        "Kelong Mao",
        "Yutao Zhu",
        "Yihong Wu",
        "Kaiyu Huang",
        "Jian-Yun Nie"
      ],
      "abstract": "In conversational search, the user’s real search intent for the current conversation turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Thus, training a rewriting model on them would lead to sub-optimal queries. Another useful information to enhance the search query is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to the retrieval task, we propose a knowledge infusion mechanism to optimize both query reformulation and retrieval. Extensive experiments on four conversational search datasets demonstrate the effectiveness of ConvGQR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 276,
      "original_url": "https://aclanthology.org/2023.acl-long.275/",
      "title": "KILM: Knowledge Injection into Encoder-Decoder Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.275.pdf",
      "authors": [
        "Yan Xu",
        "Mahdi Namazifar",
        "Devamanyu Hazarika",
        "Aishwarya Padmakumar",
        "Yang Liu",
        "Dilek Hakkani-Tur"
      ],
      "abstract": "Large pre-trained language models (PLMs) have been shown to retain implicit knowledge within their parameters. To enhance this implicit knowledge, we propose Knowledge Injection into Language Models (KILM), a novel approach that injects entity-related knowledge into encoder-decoder PLMs, via a generative knowledge infilling objective through continued pre-training. This is done without architectural modifications to the PLMs or adding additional parameters. Experimental results over a suite of knowledge-intensive tasks spanning numerous datasets show that KILM enables models to retain more knowledge and hallucinate less while preserving their original performance on general NLU and NLG tasks. KILM also demonstrates improved zero-shot performances on tasks such as entity disambiguation, outperforming state-of-the-art models having 30x more parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 277,
      "original_url": "https://aclanthology.org/2023.acl-long.276/",
      "title": "VSTAR: A Video-grounded Dialogue Dataset for Situated Semantic Understanding with Scene and Topic Transitions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.276.pdf",
      "authors": [
        "Yuxuan Wang",
        "Zilong Zheng",
        "Xueliang Zhao",
        "Jinpeng Li",
        "Yueqian Wang",
        "Dongyan Zhao"
      ],
      "abstract": "Video-grounded dialogue understanding is a challenging problem that requires machine to perceive, parse and reason over situated semantics extracted from weakly aligned video and dialogues. Most existing benchmarks treat both modalities the same as a frame-independent visual understanding task, while neglecting the intrinsic attributes in multimodal dialogues, such as scene and topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe dialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding dataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for video-grounded dialogue understanding: scene segmentation and topic segmentation, and one benchmark for video-grounded dialogue generation. Comprehensive experiments are performed on these benchmarks to demonstrate the importance of multimodal information and segments in video-grounded dialogue understanding and generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 278,
      "original_url": "https://aclanthology.org/2023.acl-long.277/",
      "title": "NLPeer: A Unified Resource for the Computational Study of Peer Review",
      "pdf_url": "https://aclanthology.org/2023.acl-long.277.pdf",
      "authors": [
        "Nils Dycke",
        "Ilia Kuznetsov",
        "Iryna Gurevych"
      ],
      "abstract": "Peer review constitutes a core component of scholarly publishing; yet it demands substantial expertise and training, and is susceptible to errors and biases. Various applications of NLP for peer reviewing assistance aim to support reviewers in this complex process, but the lack of clearly licensed datasets and multi-domain corpora prevent the systematic study of NLP for peer review. To remedy this, we introduce NLPeer– the first ethically sourced multidomain corpus of more than 5k papers and 11k review reports from five different venues. In addition to the new datasets of paper drafts, camera-ready versions and peer reviews from the NLP community, we establish a unified data representation and augment previous peer review datasets to include parsed and structured paper representations, rich metadata and versioning information. We complement our resource with implementations and analysis of three reviewing assistance tasks, including a novel guided skimming task. Our work paves the path towards systematic, multi-faceted, evidence-based study of peer review in NLP and beyond. The data and code are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 279,
      "original_url": "https://aclanthology.org/2023.acl-long.278/",
      "title": "IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures",
      "pdf_url": "https://aclanthology.org/2023.acl-long.278.pdf",
      "authors": [
        "Mingyu Zheng",
        "Yang Hao",
        "Wenbin Jiang",
        "Zheng Lin",
        "Yajuan Lyu",
        "QiaoQiao She",
        "Weiping Wang"
      ],
      "abstract": "Various datasets have been proposed to promote the development of Table Question Answering (TQA) technique. However, the problem setting of existing TQA benchmarks suffers from two limitations. First, they directly provide models with explicit table structures where row headers and column headers of the table are explicitly annotated and treated as model input during inference. Second, they only consider tables of limited types and ignore other tables especially complex tables with flexible header locations. Such simplified problem setting cannot cover practical scenarios where models need to process tables without header annotations in the inference phase or tables of different types. To address above issues, we construct a new TQA dataset with implicit and multi-type table structures, named IM-TQA, which not only requires the model to understand tables without directly available header annotations but also to handle multi-type tables including previously neglected complex tables. We investigate the performance of recent methods on our dataset and find that existing methods struggle in processing implicit and multi-type table structures. Correspondingly, we propose an RGCN-RCI framework outperforming recent baselines. We will release our dataset to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 280,
      "original_url": "https://aclanthology.org/2023.acl-long.279/",
      "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.279.pdf",
      "authors": [
        "Pengcheng He",
        "Baolin Peng",
        "Song Wang",
        "Yang Liu",
        "Ruochen Xu",
        "Hany Hassan",
        "Yu Shi",
        "Chenguang Zhu",
        "Wayne Xiong",
        "Michael Zeng",
        "Jianfeng Gao",
        "Xuedong Huang"
      ],
      "abstract": "This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state-of-the-art encoder-decoder model using three techniques. First, we use a two-phase pre-training to improve the model’s performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ createsa new state-of-the-art on 9 of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM540B on XSum, and the finetuned 200x larger GPT3175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 281,
      "original_url": "https://aclanthology.org/2023.acl-long.280/",
      "title": "Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories",
      "pdf_url": "https://aclanthology.org/2023.acl-long.280.pdf",
      "authors": [
        "Shizhe Diao",
        "Tianyang Xu",
        "Ruijia Xu",
        "Jiawei Wang",
        "Tong Zhang"
      ],
      "abstract": "Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT).Further analyses demonstrate the reliability, scalability, and efficiency of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 282,
      "original_url": "https://aclanthology.org/2023.acl-long.281/",
      "title": "Unsupervised Graph-Text Mutual Conversion with a Unified Pretrained Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.281.pdf",
      "authors": [
        "Yi Xu",
        "Shuqian Sheng",
        "Jiexing Qi",
        "Luoyi Fu",
        "Zhouhan Lin",
        "Xinbing Wang",
        "Chenghu Zhou"
      ],
      "abstract": "Graph-to-text (G2T) generation and text-to-graph (T2G) triple extraction are two essential tasks for knowledge graphs. Existing unsupervised approaches become suitable candidates for jointly learning the two tasks due to their avoidance of using graph-text parallel data. However, they adopt multiple complex modules and still require entity information or relation type for training. To this end, we propose INFINITY, a simple yet effective unsupervised method with a unified pretrained language model that does not introduce external annotation tools or additional parallel information. It achieves fully unsupervised graph-text mutual conversion for the first time. Specifically, INFINITY treats both G2T and T2G as a bidirectional sequence generation task by fine-tuning only one pretrained seq2seq model. A novel back-translation-based framework is then designed to generate synthetic parallel data automatically. Besides, we investigate the impact of graph linearization and introduce the structure-aware fine-tuning strategy to alleviate possible performance deterioration via retaining structural information in graph sequences. As a fully unsupervised framework, INFINITY is empirically verified to outperform state-of-the-art baselines for G2T and T2G tasks. Additionally, we also devise a new training setting called cross learning for low-resource unsupervised information extraction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 283,
      "original_url": "https://aclanthology.org/2023.acl-long.282/",
      "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
      "pdf_url": "https://aclanthology.org/2023.acl-long.282.pdf",
      "authors": [
        "Han Cheol Moon",
        "Shafiq Joty",
        "Ruochen Zhao",
        "Megh Thakkar",
        "Chi Xu"
      ],
      "abstract": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 284,
      "original_url": "https://aclanthology.org/2023.acl-long.283/",
      "title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes",
      "pdf_url": "https://aclanthology.org/2023.acl-long.283.pdf",
      "authors": [
        "Wenda Xu",
        "Xian Qian",
        "Mingxuan Wang",
        "Lei Li",
        "William Yang Wang"
      ],
      "abstract": "Is it possible to train a general metric for evaluating text generation quality without human-annotated ratings? Existing learned metrics either perform unsatisfactory across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SEScore2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. We evaluate SEScore2 and previous methods on four text generation tasks across three languages. SEScore2 outperforms all prior unsupervised metrics on four text generation evaluation benchmarks, with an average Kendall improvement of 0.158. Surprisingly, SEScore2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 285,
      "original_url": "https://aclanthology.org/2023.acl-long.284/",
      "title": "Tokenization and the Noiseless Channel",
      "pdf_url": "https://aclanthology.org/2023.acl-long.284.pdf",
      "authors": [
        "Vilém Zouhar",
        "Clara Meister",
        "Juan Gastaldi",
        "Li Du",
        "Mrinmaya Sachan",
        "Ryan Cotterell"
      ],
      "abstract": "Subword tokenization is a key part of most NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution. Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of Rényi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance. In machine translation, we find that across multiple tokenizers, the Rényi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 286,
      "original_url": "https://aclanthology.org/2023.acl-long.285/",
      "title": "Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.285.pdf",
      "authors": [
        "Jiaxi Li",
        "Wei Lu"
      ],
      "abstract": "Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of perturbations motivated by the linguistic concept of constituency tests, and use these to score each span by aggregating the distortion scores. To produce a parse tree, we use chart parsing to find the tree with the minimum score. Our method consistently outperforms previous state-of-the-art methods on English with masked LMs, and also demonstrates superior performance in a multilingual setting, outperforming the state-of-the-art in 6 out of 8 languages. Notably, although our method does not involve parameter updates or extensive hyperparameter search, its performance can even surpass some unsupervised parsing methods that require fine-tuning. Our analysis highlights that the distortion of contextual representation resulting from syntactic perturbation can serve as an effective indicator of constituency across languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 287,
      "original_url": "https://aclanthology.org/2023.acl-long.286/",
      "title": "MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.286.pdf",
      "authors": [
        "Zhenrui Yue",
        "Huimin Zeng",
        "Yang Zhang",
        "Lanyu Shang",
        "Dong Wang"
      ],
      "abstract": "With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose MetaAdapt, a meta learning based approach for domain adaptive few-shot misinformation detection. MetaAdapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). In particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. Based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. As such, MetaAdapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. To demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 288,
      "original_url": "https://aclanthology.org/2023.acl-long.287/",
      "title": "Tackling Modality Heterogeneity with Multi-View Calibration Network for Multimodal Sentiment Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.287.pdf",
      "authors": [
        "Yiwei Wei",
        "Shaozu Yuan",
        "Ruosong Yang",
        "Lei Shen",
        "Zhangmeizhi Li",
        "Longbiao Wang",
        "Meng Chen"
      ],
      "abstract": "With the popularity of social media, detecting sentiment from multimodal posts (e.g. image-text pairs) has attracted substantial attention recently. Existing works mainly focus on fusing different features but ignore the challenge of modality heterogeneity. Specifically, different modalities with inherent disparities may bring three problems: 1) introducing redundant visual features during feature fusion; 2) causing feature shift in the representation space; 3) leading to inconsistent annotations for different modal data. All these issues will increase the difficulty in understanding the sentiment of the multimodal content. In this paper, we propose a novel Multi-View Calibration Network (MVCN) to alleviate the above issues systematically. We first propose a text-guided fusion module with novel Sparse-Attention to reduce the negative impacts of redundant visual elements. We then devise a sentiment-based congruity constraint task to calibrate the feature shift in the representation space. Finally, we introduce an adaptive loss calibration strategy to tackle inconsistent annotated labels. Extensive experiments demonstrate the competitiveness of MVCN against previous approaches and achieve state-of-the-art results on two public benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 289,
      "original_url": "https://aclanthology.org/2023.acl-long.288/",
      "title": "COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective",
      "pdf_url": "https://aclanthology.org/2023.acl-long.288.pdf",
      "authors": [
        "Zhaowei Wang",
        "Quyet V. Do",
        "Hongming Zhang",
        "Jiayao Zhang",
        "Weiqi Wang",
        "Tianqing Fang",
        "Yangqiu Song",
        "Ginny Wong",
        "Simon See"
      ],
      "abstract": "Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 290,
      "original_url": "https://aclanthology.org/2023.acl-long.289/",
      "title": "MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.289.pdf",
      "authors": [
        "Shivam Sharma",
        "Ramaneswaran S",
        "Udit Arora",
        "Md. Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "abstract": "Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena renders them an ideal vehicle for communication. To comprehend the subtle message conveyed within a meme, one must understand the relevant background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme’s context dynamically. In this work, we propose a novel task, MEMEX - given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses external knowledge-enriched meme representation and a multi-level approach to capture the cross-modal semantic dependencies between the meme and the context. MIME surpasses several unimodal and multimodal systems and yields an absolute improvement of 4% F1-score over the best baseline. Lastly, we conduct detailed analyses of MIME’s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 291,
      "original_url": "https://aclanthology.org/2023.acl-long.290/",
      "title": "WikiHowQA: A Comprehensive Benchmark for Multi-Document Non-Factoid Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.290.pdf",
      "authors": [
        "Valeriia Bolotova-Baranova",
        "Vladislav Blinov",
        "Sofya Filippova",
        "Falk Scholer",
        "Mark Sanderson"
      ],
      "abstract": "Answering non-factoid questions (NFQA) is a challenging task, requiring passage-level answers that are difficult to construct and evaluate. Search engines may provide a summary of a single web page, but many questions require reasoning across multiple documents. Meanwhile, modern models can generate highly coherent and fluent, but often factually incorrect answers that can deceive even non-expert humans. There is a critical need for high-quality resources for multi-document NFQA (MD-NFQA) to train new models and evaluate answers’ grounding and factual consistency in relation to supporting documents. To address this gap, we introduce WikiHowQA, a new multi-document NFQA benchmark built on WikiHow, a website dedicated to answering “how-to” questions. The benchmark includes 11,746 human-written answers along with 74,527 supporting documents. We describe the unique challenges of the resource, provide strong baselines, and propose a novel human evaluation framework that utilizes highlighted relevant supporting passages to mitigate issues such as assessor unfamiliarity with the question topic. All code and data, including the automatic code for preparing the human evaluation, are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 292,
      "original_url": "https://aclanthology.org/2023.acl-long.291/",
      "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
      "pdf_url": "https://aclanthology.org/2023.acl-long.291.pdf",
      "authors": [
        "Yifei Li",
        "Zeqi Lin",
        "Shizhuo Zhang",
        "Qiang Fu",
        "Bei Chen",
        "Jian-Guang Lou",
        "Weizhu Chen"
      ],
      "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 293,
      "original_url": "https://aclanthology.org/2023.acl-long.292/",
      "title": "Distributed Marker Representation for Ambiguous Discourse Markers and Entangled Relations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.292.pdf",
      "authors": [
        "Dongyu Ru",
        "Lin Qiu",
        "Xipeng Qiu",
        "Yue Zhang",
        "Zheng Zhang"
      ],
      "abstract": "Discourse analysis is an important task because it models intrinsic semantic structures between sentences in a document. Discourse markers are natural representations of discourse in our daily language. One challenge is that the markers as well as pre-defined and human-labeled discourse relations can be ambiguous when describing the semantics between sentences. We believe that a better approach is to use a contextual-dependent distribution over the markers to express discourse information. In this work, we propose to learn a Distributed Marker Representation (DMR) by utilizing the (potentially) unlimited discourse marker data with a latent discourse sense, thereby bridging markers with sentence pairs. Such representations can be learned automatically from data without supervision, and in turn provide insights into the data itself. Experiments show the SOTA performance of our DMR on the implicit discourse relation recognition task and strong interpretability. Our method also offers a valuable tool to understand complex ambiguity and entanglement among discourse markers and manually defined discourse relations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 294,
      "original_url": "https://aclanthology.org/2023.acl-long.293/",
      "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
      "pdf_url": "https://aclanthology.org/2023.acl-long.293.pdf",
      "authors": [
        "Tamanna Hossain",
        "Sunipa Dev",
        "Sameer Singh"
      ],
      "abstract": "Content Warning: This paper contains examples of misgendering and erasure that could be offensive and potentially triggering. Gender bias in language technologies has been widely studied, but research has mostly been restricted to a binary paradigm of gender. It is essential also to consider non-binary gender identities, as excluding them can cause further harm to an already marginalized group. In this paper, we comprehensively evaluate popular language models for their ability to correctly use English gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze, xe, thon) that are used by individuals whose gender identity is not represented by binary pronouns. We introduce Misgendered, a framework for evaluating large language models’ ability to correctly use preferred pronouns, consisting of (i) instances declaring an individual’s pronoun, followed by a sentence with a missing pronoun, and (ii) an experimental setup for evaluating masked and auto-regressive language models using a unified method. When prompted out-of-the-box, language models perform poorly at correctly predicting neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging 31.0% accuracy). This inability to generalize results from a lack of representation of non-binary pronouns in training data and memorized associations. Few-shot adaptation with explicit examples in the prompt improves the performance but plateaus at only 45.4% for neo-pronouns. We release the full dataset, code, and demo at https://tamannahossainkay.github.io/misgendered/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 295,
      "original_url": "https://aclanthology.org/2023.acl-long.294/",
      "title": "Reasoning with Language Model Prompting: A Survey",
      "pdf_url": "https://aclanthology.org/2023.acl-long.294.pdf",
      "authors": [
        "Shuofei Qiao",
        "Yixin Ou",
        "Ningyu Zhang",
        "Xiang Chen",
        "Yunzhi Yao",
        "Shumin Deng",
        "Chuanqi Tan",
        "Fei Huang",
        "Huajun Chen"
      ],
      "abstract": "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 296,
      "original_url": "https://aclanthology.org/2023.acl-long.295/",
      "title": "Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.295.pdf",
      "authors": [
        "Matthieu Futeral",
        "Cordelia Schmid",
        "Ivan Laptev",
        "Benoît Sagot",
        "Rachel Bawden"
      ],
      "abstract": "One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard English→French, English→German and English→Czech benchmarks and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set. Our code and CoMMuTE are freely available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 297,
      "original_url": "https://aclanthology.org/2023.acl-long.296/",
      "title": "Hybrid Knowledge Transfer for Improved Cross-Lingual Event Detection via Hierarchical Sample Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.296.pdf",
      "authors": [
        "Luis Guzman Nateras",
        "Franck Dernoncourt",
        "Thien Nguyen"
      ],
      "abstract": "In this paper, we address the Event Detection task under a zero-shot cross-lingual setting where a model is trained on a source language but evaluated on a distinct target language for which there is no labeled data available. Most recent efforts in this field follow a direct transfer approach in which the model is trained using language-invariant features and then directly applied to the target language. However, we argue that these methods fail to take advantage of the benefits of the data transfer approach where a cross-lingual model is trained on target-language data and is able to learn task-specific information from syntactical features or word-label relations in the target language. As such, we propose a hybrid knowledge-transfer approach that leverages a teacher-student framework where the teacher and student networks are trained following the direct and data transfer approaches, respectively. Our method is complemented by a hierarchical training-sample selection scheme designed to address the issue of noisy labels being generated by the teacher model. Our model achieves state-of-the-art results on 9 morphologically-diverse target languages across 3 distinct datasets, highlighting the importance of exploiting the benefits of hybrid transfer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 298,
      "original_url": "https://aclanthology.org/2023.acl-long.297/",
      "title": "BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.297.pdf",
      "authors": [
        "Yiming Yan",
        "Tao Wang",
        "Chengqi Zhao",
        "Shujian Huang",
        "Jiajun Chen",
        "Mingxuan Wang"
      ],
      "abstract": "Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhance the robustness of evaluation metrics, which in turn leads to an improvement in the performance of machine translation systems. Codes are available at https://github.com/powerpuffpomelo/fairseq_mrt."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 299,
      "original_url": "https://aclanthology.org/2023.acl-long.298/",
      "title": "Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.298.pdf",
      "authors": [
        "Rohan Pandey",
        "Rulin Shao",
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "abstract": "Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., ‘mug in grass’) with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the language attention from ‘mug’ to ‘grass’ (capturing the semantic relation ‘in’) to match the visual attention from the mug to the grass (capturing the corresponding physical relation). Tokens and their corresponding objects are softly identified using a weighted mean of cross-modal attention. We prove that this notion of soft cross-modal equivalence is equivalent to enforcing congruence between vision and language attention matrices under a ‘change of basis’ provided by the cross-modal attention matrix. Intuitively, our approach projects visual attention into the language attention space to calculate its divergence from the actual language attention, and vice versa. We apply our Cross-modal Attention Congruence Regularization (CACR) loss to fine-tune UNITER and improve its Winoground Group score by 5.75 points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 300,
      "original_url": "https://aclanthology.org/2023.acl-long.299/",
      "title": "Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona",
      "pdf_url": "https://aclanthology.org/2023.acl-long.299.pdf",
      "authors": [
        "Yihong Tang",
        "Bo Wang",
        "Miao Fang",
        "Dongming Zhao",
        "Kun Huang",
        "Ruifang He",
        "Yuexian Hou"
      ],
      "abstract": "The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model’s superiority in personalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 301,
      "original_url": "https://aclanthology.org/2023.acl-long.300/",
      "title": "Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge",
      "pdf_url": "https://aclanthology.org/2023.acl-long.300.pdf",
      "authors": [
        "Yasumasa Onoe",
        "Michael Zhang",
        "Shankar Padmanabhan",
        "Greg Durrett",
        "Eunsol Choi"
      ],
      "abstract": "Pre-trained language models (LMs) are used for knowledge intensive tasks like question answering, but their knowledge gets continuously outdated as the world changes. Prior work has studied targeted updates to LMs, injecting individual facts and evaluating whether the model learns these facts while not changing predictions on other contexts. We take a step forward and study LMs’ abilities to make inferences based on injected facts (or propagate those facts): for example, after learning that something is a TV show, does an LM predict that you can watch it? We study this with two cloze-style tasks: an existing dataset of real-world sentences about novel entities (ECBD) as well as a new controlled benchmark with manually designed templates requiring varying levels of inference about injected knowledge. Surprisingly, we find that existing methods for updating knowledge (gradient-based fine-tuning and modifications of this approach) show little propagation of injected knowledge. These methods improve performance on cloze instances only when there is lexical overlap between injected facts and target inferences. Yet, prepending entity definitions in an LM’s context improves performance across all settings, suggesting that there is substantial headroom for parameter-updating approaches for knowledge injection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 302,
      "original_url": "https://aclanthology.org/2023.acl-long.301/",
      "title": "Explaining How Transformers Use Context to Build Predictions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.301.pdf",
      "authors": [
        "Javier Ferrando",
        "Gerard I. Gállego",
        "Ioannis Tsiamas",
        "Marta R. Costa-jussà"
      ],
      "abstract": "Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model’s prediction, it is still unclear how prior words affect the model’s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 303,
      "original_url": "https://aclanthology.org/2023.acl-long.302/",
      "title": "DISCO: Distilling Counterfactuals with Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.302.pdf",
      "authors": [
        "Zeming Chen",
        "Qiyue Gao",
        "Antoine Bosselut",
        "Ashish Sabharwal",
        "Kyle Richardson"
      ],
      "abstract": "Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high-quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DISCO generated counterfactuals are more robust (6% absolute) and generalize better across distributions (2%) compared to models trained without data augmentation. Furthermore, DISCO augmented models are 10% more consistent between counterfactual pairs on three evaluation sets, demonstrating that DISCO augmentation enables models to more reliably learn causal representations. Our repository are available at: https://github.com/eric11eca/disco"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 304,
      "original_url": "https://aclanthology.org/2023.acl-long.303/",
      "title": "Non-Sequential Graph Script Induction via Multimedia Grounding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.303.pdf",
      "authors": [
        "Yu Zhou",
        "Sha Li",
        "Manling Li",
        "Xudong Lin",
        "Shih-Fu Chang",
        "Mohit Bansal",
        "Heng Ji"
      ],
      "abstract": "Online resources such as WikiHow compile a wide range of scripts for performing everyday tasks, which can assist models in learning to reason about procedures. However, the scripts are always presented in a linear manner, which does not reflect the flexibility displayed by people executing tasks in real life. For example, in the CrossTask Dataset, 64.5% of consecutive step pairs are also observed in the reverse order, suggesting their ordering is not fixed. In addition, each step has an average of 2.56 frequent next steps, demonstrating “branching”. In this paper, we propose the new challenging task of non-sequential graph script induction, aiming to capture optional and interchangeable steps in procedural planning. To automate the induction of such graph scripts for given tasks, we propose to take advantage of loosely aligned videos of people performing the tasks. In particular, we design a multimodal framework to ground procedural videos to WikiHow textual steps and thus transform each video into an observed step path on the latent ground truth graph script. This key transformation enables us to train a script knowledge model capable of both generating explicit graph scripts for learnt tasks and predicting future steps given a partial step sequence. Our best model outperforms the strongest pure text/vision baselines by 17.52% absolute gains on F1@3 for next step prediction and 13.8% absolute gains on Acc@1 for partial sequence completion. Human evaluation shows our model outperforming the WikiHow linear baseline by 48.76% absolute gains in capturing sequential and non-sequential step relationships."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 305,
      "original_url": "https://aclanthology.org/2023.acl-long.304/",
      "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.304.pdf",
      "authors": [
        "Peifeng Wang",
        "Zhengyang Wang",
        "Zheng Li",
        "Yifan Gao",
        "Bing Yin",
        "Xiang Ren"
      ],
      "abstract": "Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM’s predictions or faithfully justify the decisions. In this work, we propose SCOTT, a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance, our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 306,
      "original_url": "https://aclanthology.org/2023.acl-long.305/",
      "title": "Clinical Note Owns its Hierarchy: Multi-Level Hypergraph Neural Networks for Patient-Level Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.305.pdf",
      "authors": [
        "Nayeon Kim",
        "Yinhua Piao",
        "Sun Kim"
      ],
      "abstract": "Leveraging knowledge from electronic health records (EHRs) to predict a patient’s condition is essential to the effective delivery of appropriate care. Clinical notes of patient EHRs contain valuable information from healthcare professionals, but have been underused due to their difficult contents and complex hierarchies. Recently, hypergraph-based methods have been proposed for document classifications. Directly adopting existing hypergraph methods on clinical notes cannot sufficiently utilize the hierarchy information of the patient, which can degrade clinical semantic information by (1) frequent neutral words and (2) hierarchies with imbalanced distribution. Thus, we propose a taxonomy-aware multi-level hypergraph neural network (TM-HGNN), where multi-level hypergraphs assemble useful neutral words with rare keywords via note and taxonomy level hyperedges to retain the clinical semantic information. The constructed patient hypergraphs are fed into hierarchical message passing layers for learning more balanced multi-level knowledge at the note and taxonomy levels. We validate the effectiveness of TM-HGNN by conducting extensive experiments with MIMIC-III dataset on benchmark in-hospital-mortality prediction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 307,
      "original_url": "https://aclanthology.org/2023.acl-long.306/",
      "title": "Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.306.pdf",
      "authors": [
        "Dongqi Pu",
        "Yifan Wang",
        "Vera Demberg"
      ],
      "abstract": "For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the ‘RSTformer’, a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 308,
      "original_url": "https://aclanthology.org/2023.acl-long.307/",
      "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.307.pdf",
      "authors": [
        "Ehsan Kamalloo",
        "Nouha Dziri",
        "Charles Clarke",
        "Davood Rafiei"
      ],
      "abstract": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 309,
      "original_url": "https://aclanthology.org/2023.acl-long.308/",
      "title": "No clues good clues: out of context Lexical Relation Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.308.pdf",
      "authors": [
        "Lucia Pitarch",
        "Jordi Bernad",
        "Lacramioara Dranca",
        "Carlos Bobed Lisbona",
        "Jorge Gracia"
      ],
      "abstract": "The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs “well-formed” verbalized text to interact with it, either to fine-tune it or to exploit it. However, there are indications that commonly used PTLMs already encode enough linguistic knowledge to allow the use of minimal (or none) textual context for some linguistically motivated tasks, thus notably reducing human effort, the need for data pre-processing, and favoring techniques that are language neutral since do not rely on syntactic structures. In this work, we explore this idea for the tasks of lexical relation classification (LRC) and graded Lexical Entailment (LE). After fine-tuning PTLMs for LRC with different verbalizations, our evaluation results show that very simple prompts are competitive for LRC and significantly outperform graded LE SoTA. In order to gain a better insight into this phenomenon, we perform a number of quantitative statistical analyses on the results, as well as a qualitative visual exploration based on embedding projections."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 310,
      "original_url": "https://aclanthology.org/2023.acl-long.309/",
      "title": "Won’t Get Fooled Again: Answering Questions with False Premises",
      "pdf_url": "https://aclanthology.org/2023.acl-long.309.pdf",
      "authors": [
        "Shengding Hu",
        "Yifan Luo",
        "Huadong Wang",
        "Xingyi Cheng",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Pre-trained language models (PLMs) have shown unprecedented potential in various fields, especially as the backbones for question-answering (QA) systems. However, they tend to be easily deceived by tricky questions such as “How many eyes does the sun have?”. Such frailties of PLMs often allude to the lack of knowledge within them. In this paper, we find that the PLMs already possess the knowledge required to rebut such questions, and the key is how to activate the knowledge. To systematize this observation, we investigate the PLMs’ responses to one kind of tricky questions, i.e., the false premises questions (FPQs). We annotate a FalseQA dataset containing 2365 human-written FPQs, with the corresponding explanations for the false premises and the revised true premise questions. Using FalseQA, we discover that PLMs are capable of discriminating FPQs by fine-tuning on moderate numbers (e.g., 256) of examples. PLMs also generate reasonable explanations for the false premise, which serve as rebuttals. Further replaying a few general questions during training allows PLMs to excel on FPQs and general questions simultaneously. Our work suggests that once the rebuttal ability is stimulated, knowledge inside the PLMs can be effectively utilized to handle FPQs, which incentivizes the research on PLM-based QA systems. The FalseQA dataset and code are available at https://github.com/thunlp/FalseQA ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 311,
      "original_url": "https://aclanthology.org/2023.acl-long.310/",
      "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
      "pdf_url": "https://aclanthology.org/2023.acl-long.310.pdf",
      "authors": [
        "Raphael Tang",
        "Linqing Liu",
        "Akshat Pandey",
        "Zhiying Jiang",
        "Gefei Yang",
        "Karun Kumar",
        "Pontus Stenetorp",
        "Jimmy Lin",
        "Ferhan Ture"
      ],
      "abstract": "Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 312,
      "original_url": "https://aclanthology.org/2023.acl-long.311/",
      "title": "Zero-shot Faithful Factual Error Correction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.311.pdf",
      "authors": [
        "Kung-Hsiang Huang",
        "Hou Pong Chan",
        "Heng Ji"
      ],
      "abstract": "Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans’ ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 313,
      "original_url": "https://aclanthology.org/2023.acl-long.312/",
      "title": "Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.312.pdf",
      "authors": [
        "Sha Li",
        "Ruining Zhao",
        "Manling Li",
        "Heng Ji",
        "Chris Callison-Burch",
        "Jiawei Han"
      ],
      "abstract": "Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method IncPrompt to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, IncSchema can generate large and complex schemas with 7.2% F1 improvement in temporal relations and 31.0% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover ~10% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 314,
      "original_url": "https://aclanthology.org/2023.acl-long.313/",
      "title": "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.313.pdf",
      "authors": [
        "Mohna Chakraborty",
        "Adithya Kulkarni",
        "Qi Li"
      ],
      "abstract": "Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt and the prompts generated using few-shot learning for the binary sentence-level sentiment classification task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 315,
      "original_url": "https://aclanthology.org/2023.acl-long.314/",
      "title": "Free Lunch: Robust Cross-Lingual Transfer via Model Checkpoint Averaging",
      "pdf_url": "https://aclanthology.org/2023.acl-long.314.pdf",
      "authors": [
        "Fabian David Schmidt",
        "Ivan Vulić",
        "Goran Glavaš"
      ],
      "abstract": "Massively multilingual language models have displayed strong performance in zero-shot (ZS-XLT) and few-shot (FS-XLT) cross-lingual transfer setups, where models fine-tuned on task data in a source language are transferred without any or with only a few annotated instances to the target language(s). However, current work typically overestimates model performance as fine-tuned models are frequently evaluated at model checkpoints that generalize best to validation instances in the target languages. This effectively violates the main assumptions of ‘true’ ZS-XLT and FS-XLT. Such XLT setups require robust methods that do not depend on labeled target language data for validation and model selection. In this work, aiming to improve the robustness of ‘true’ ZS-XLT and FS-XLT, we propose a simple and effective method that averages different checkpoints (i.e., model snapshots) during task fine-tuning. We conduct exhaustive ZS-XLT and FS-XLT experiments across higher-level semantic tasks (NLI, extractive QA) and lower-level token classification tasks (NER, POS). The results indicate that averaging model checkpoints yields systematic and consistent performance gains across diverse target languages in all tasks. Importantly, it simultaneously substantially desensitizes XLT to varying hyperparameter choices in the absence of target language validation. We also show that checkpoint averaging benefits performance when further combined with run averaging (i.e., averaging the parameters of models fine-tuned over independent runs)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 316,
      "original_url": "https://aclanthology.org/2023.acl-long.315/",
      "title": "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.315.pdf",
      "authors": [
        "Yan Zeng",
        "Wangchunshu Zhou",
        "Ao Luo",
        "Ziming Cheng",
        "Xinsong Zhang"
      ],
      "abstract": "In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval datasets show that while conceptually simpler, CCLM significantly outperforms the prior state-of-the-art with an average absolute improvement of over 10%. Moreover, CCLM is the first multi-lingual multi-modal pre-trained model that surpasses the translate-test performance of representative English vision-language models by zero-shot cross-lingual transfer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 317,
      "original_url": "https://aclanthology.org/2023.acl-long.316/",
      "title": "Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars",
      "pdf_url": "https://aclanthology.org/2023.acl-long.316.pdf",
      "authors": [
        "Songlin Yang",
        "Roger Levy",
        "Yoon Kim"
      ],
      "abstract": "We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To reduce the computational complexity of both parsing and parameter estimation, we restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two) and further discard rules that require O(l6) time to parse, reducing inference to O(l5). We find that using a large number of nonterminals is beneficial and thus make use of tensor decomposition-based rank-space dynamic programming with an embedding-based parameterization of rule probabilities to scale up the number of nonterminals. Experiments on German and Dutch show that our approach is able to induce linguistically meaningful trees with continuous and discontinuous structures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 318,
      "original_url": "https://aclanthology.org/2023.acl-long.317/",
      "title": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.317.pdf",
      "authors": [
        "Satwik Bhattamishra",
        "Arkil Patel",
        "Varun Kanade",
        "Phil Blunsom"
      ],
      "abstract": "Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence that suggests differences in the inductive biases of Transformers and recurrent models which may help explain Transformer’s effective generalization performance despite relatively limited expressiveness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 319,
      "original_url": "https://aclanthology.org/2023.acl-long.318/",
      "title": "Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.318.pdf",
      "authors": [
        "Rishabh Gupta",
        "Shaily Desai",
        "Manvi Goel",
        "Anil Bandhakavi",
        "Tanmoy Chakraborty",
        "Md. Shad Akhtar"
      ],
      "abstract": "Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore intent-conditioned counterspeech generation. At first, we develop IntentCONAN, a diversified intent-specific counterspeech dataset with 6831 counterspeeches conditioned on five intents, i.e., informative, denouncing, question, positive, and humour. Subsequently, we propose QUARC, a two-stage framework for intent-conditioned counterspeech generation. QUARC leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific information into the model. Our evaluation demonstrates that QUARC outperforms several baselines by an average of ~10% across evaluation metrics. An extensive human evaluation supplements our hypothesis of better and more appropriate responses than comparative systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 320,
      "original_url": "https://aclanthology.org/2023.acl-long.319/",
      "title": "DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.319.pdf",
      "authors": [
        "Suraj Kothawade",
        "Anmol Mekala",
        "D.Chandra Sekhara Hetha Havya",
        "Mayank Kothyari",
        "Rishabh Iyer",
        "Ganesh Ramakrishnan",
        "Preethi Jyothi"
      ],
      "abstract": "State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring large amounts of labeled speech for specific target accents is challenging. Choosing an informative subset of speech samples that are most representative of the target accents becomes important for effective ASR finetuning. To address this problem, we propose DITTO (Data-efficient and faIr Targeted subseT selectiOn that uses Submodular Mutual Information (SMI) functions as acquisition functions to find the most informative set of utterances matching a target accent within a fixed budget. An important feature of DITTO is that it supports fair targeting for multiple accents, i.e. it can automatically select representative data points from multiple accents when the ASR model needs to perform well on more than one accent. We show that compared to other speech selection methods, DITTO is 3-5 times as label-efficient for its improvements on the Indic-TTS and L2 datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 321,
      "original_url": "https://aclanthology.org/2023.acl-long.320/",
      "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
      "pdf_url": "https://aclanthology.org/2023.acl-long.320.pdf",
      "authors": [
        "Ruochen Zhao",
        "Xingxuan Li",
        "Shafiq Joty",
        "Chengwei Qin",
        "Lidong Bing"
      ],
      "abstract": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 322,
      "original_url": "https://aclanthology.org/2023.acl-long.321/",
      "title": "Bridging the Domain Gaps in Context Representations for k-Nearest Neighbor Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.321.pdf",
      "authors": [
        "Zhiwei Cao",
        "Baosong Yang",
        "Huan Lin",
        "Suhang Wu",
        "Xiangpeng Wei",
        "Dayiheng Liu",
        "Jun Xie",
        "Min Zhang",
        "Jinsong Su"
      ],
      "abstract": "k-Nearest neighbor machine translation (kNN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation.However, there often exists a significant gap between upstream and downstream domains, which hurts the datastore retrieval and the final translation quality.To deal with this issue, we propose a novel approach to boost the datastore retrieval of kNN-MT by reconstructing the original datastore.Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semantically related to its corresponding queries, and the other is an L2-norm loss encouraging revised key representations to effectively retain the knowledge learned by the upstream NMT model. Extensive experiments on domain adaptation tasks demonstrate that our method can effectively boost the datastore retrieval and translation quality of kNN-MT.Our code is available at https://github.com/DeepLearnXMU/Revised-knn-mt."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 323,
      "original_url": "https://aclanthology.org/2023.acl-long.322/",
      "title": "Node Placement in Argument Maps: Modeling Unidirectional Relations in High & Low-Resource Scenarios",
      "pdf_url": "https://aclanthology.org/2023.acl-long.322.pdf",
      "authors": [
        "Iman Jundi",
        "Neele Falk",
        "Eva Maria Vecchi",
        "Gabriella Lapesa"
      ],
      "abstract": "Argument maps structure discourse into nodes in a tree with each node being an argument that supports or opposes its parent argument. This format is more comprehensible and less redundant compared to an unstructured one. Exploring those maps and maintaining their structure by placing new arguments under suitable parents is more challenging for users with huge maps that are typical in online discussions. To support those users, we introduce the task of node placement: suggesting candidate nodes as parents for a new contribution. We establish an upper-bound of human performance, and conduct experiments with models of various sizes and training strategies. We experiment with a selection of maps from Kialo, drawn from a heterogeneous set of domains. Based on an annotation study, we highlight the ambiguity of the task that makes it challenging for both humans and models. We examine the unidirectional relation between tree nodes and show that encoding a node into different embeddings for each of the parent and child cases improves performance. We further show the few-shot effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 324,
      "original_url": "https://aclanthology.org/2023.acl-long.323/",
      "title": "Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review",
      "pdf_url": "https://aclanthology.org/2023.acl-long.323.pdf",
      "authors": [
        "Fred Philippy",
        "Siwen Guo",
        "Shohreh Haddadan"
      ],
      "abstract": "In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining the cross-lingual potential of MLLMs. This review provides, first, an aligned reference point for future research and, second, guidance for a better-informed and more efficient way of leveraging the cross-lingual capacity of MLLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 325,
      "original_url": "https://aclanthology.org/2023.acl-long.324/",
      "title": "Toward Human-Like Evaluation for Natural Language Generation with Error Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.324.pdf",
      "authors": [
        "Qingyu Lu",
        "Liang Ding",
        "Liping Xie",
        "Kanjian Zhang",
        "Derek F. Wong",
        "Dacheng Tao"
      ],
      "abstract": "The pretrained language model (PLM) based metrics have been successfully used in evaluating language generation tasks. Recent studies of the human evaluation community show that considering both major errors (e.g. mistranslated tokens) and minor errors (e.g. imperfections in fluency) can produce high-quality judgments. This inspires us to approach the final goal of the automatic metrics (human-like evaluations) by fine-grained error analysis. In this paper, we argue that the ability to estimate sentence confidence is the tip of the iceberg for PLM-based metrics. And it can be used to refine the generated sentence toward higher confidence and more reference-grounded, where the costs of refining and approaching reference are used to determine the major and minor errors, respectively. To this end, we take BARTScore as the testbed and present an innovative solution to marry the unexploited sentence refining capacity of BARTScore and human-like error analysis, where the final score consists of both the evaluations of major and minor errors. Experiments show that our solution consistently and significantly improves BARTScore, and outperforms top-scoring metrics in 19/25 test settings. Analyses demonstrate our method robustly and efficiently approaches human-like evaluations, enjoying better interpretability. Our code and scripts will be publicly released in https://github.com/Coldmist-Lu/ErrorAnalysis_NLGEvaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 326,
      "original_url": "https://aclanthology.org/2023.acl-long.325/",
      "title": "Connective Prediction for Implicit Discourse Relation Recognition via Knowledge Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.325.pdf",
      "authors": [
        "Hongyi Wu",
        "Hao Zhou",
        "Man Lan",
        "Yuanbin Wu",
        "Yadong Zhang"
      ],
      "abstract": "Implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis due to the absence of connectives. Most existing methods utilize one-hot labels as the sole optimization target, ignoring the internal association among connectives. Besides, these approaches spend lots of effort on template construction, negatively affecting the generalization capability. To address these problems,we propose a novel Connective Prediction via Knowledge Distillation (CP-KD) approach to instruct large-scale pre-trained language models (PLMs) mining the latent correlations between connectives and discourse relations, which is meaningful for IDRR. Experimental results on the PDTB 2.0/3.0 and CoNLL2016 datasets show that our method significantly outperforms the state-of-the-art models on coarse-grained and fine-grained discourse relations. Moreover, our approach can be transferred to explicit discourse relation recognition(EDRR) and achieve acceptable performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 327,
      "original_url": "https://aclanthology.org/2023.acl-long.326/",
      "title": "What is the best recipe for character-level encoder-only modelling?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.326.pdf",
      "authors": [
        "Kris Cao"
      ],
      "abstract": "This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations (Clark et al., 2022, Jaegle et al., 2022, Tay et al., 2021) and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level tokeniser during pretraining, and final model performance is highly dependent on tokeniser quality. We believe our results demonstrate the readiness of character-level models for multilingual language representation, and encourage NLP practitioners to try them as drop-in replacements for token-based models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 328,
      "original_url": "https://aclanthology.org/2023.acl-long.327/",
      "title": "Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.327.pdf",
      "authors": [
        "Zejun Li",
        "Zhihao Fan",
        "Jingjing Chen",
        "Qi Zhang",
        "Xuanjing Huang",
        "Zhongyu Wei"
      ],
      "abstract": "Multilingual Vision-Language Pre-training (VLP) is a promising but challenging topic due to the lack of large-scale multilingual image-text pairs. Existing works address the problem by translating English data into other languages, which is intuitive and the generated data is usually limited in form and scale. In this paper, we explore a more practical and scalable setting: weakly supervised multilingual VLP with only English image-text pairs and multilingual text corpora. We argue that the universal multilingual representation learned from texts allows the cross-modal interaction learned in English to be transferable to other languages. To this end, we propose a framework to effectively unify cross-lingual and cross-modal pre-training. For unified modeling on different data, we design an architecture with flexible modules to learn different interactions. Moreover, two unified tasks are introduced to efficiently guide the unified cross-lingual cross-modal learning. Extensive experiments demonstrate that our pre-trained model learns universal multilingual multimodal representations, allowing effective cross-lingual transfer on multimodal tasks. Code and models are available at https://github.com/FudanDISC/weakly-supervised-mVLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 329,
      "original_url": "https://aclanthology.org/2023.acl-long.328/",
      "title": "Learning “O” Helps for Learning More: Handling the Unlabeled Entity Problem for Class-incremental NER",
      "pdf_url": "https://aclanthology.org/2023.acl-long.328.pdf",
      "authors": [
        "Ruotian Ma",
        "Xuanting Chen",
        "Zhang Lin",
        "Xin Zhou",
        "Junzhe Wang",
        "Tao Gui",
        "Qi Zhang",
        "Xiang Gao",
        "Yun Wen Chen"
      ],
      "abstract": "As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are regarded as “Non-entity” (or “O”). In this work, we conduct an empirical study on the “Unlabeled Entity Problem” and find that it leads to severe confusion between “O” and entities, decreasing class discrimination of old classes and declining the model’s ability to learn new classes. To solve the Unlabeled Entity Problem, we propose a novel representation learning method to learn discriminative representations for the entity classes and “O”. Specifically, we propose an entity-aware contrastive learning method that adaptively detects entity clusters in “O”. Furthermore, we propose two effective distance-based relabeling strategies for better learning the old classes. We introduce a more realistic and challenging benchmark for class-incremental NER, and the proposed method achieves up to 10.62% improvement over the baseline methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 330,
      "original_url": "https://aclanthology.org/2023.acl-long.329/",
      "title": "Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination",
      "pdf_url": "https://aclanthology.org/2023.acl-long.329.pdf",
      "authors": [
        "Hao Fei",
        "Qian Liu",
        "Meishan Zhang",
        "Min Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene hallucination mechanism that dynamically generates pseudo visual SG from the given textual SG. Several SG-pivoting based learning objectives are introduced for unsupervised translation training. On the benchmark Multi30K data, our SG-based method outperforms the best-performing baseline by significant BLEU scores on the task and setup, helping yield translations with better completeness, relevance and fluency without relying on paired images. Further in-depth analyses reveal how our model advances in the task setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 331,
      "original_url": "https://aclanthology.org/2023.acl-long.330/",
      "title": "CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.330.pdf",
      "authors": [
        "Tingting Ma",
        "Qianhui Wu",
        "Huiqiang Jiang",
        "Börje Karlsson",
        "Tiejun Zhao",
        "Chin-Yew Lin"
      ],
      "abstract": "Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge distillation on target-language unlabeled data. However, these methods may suffer from label noise due to the automatic labeling process. In this paper, we propose CoLaDa, a Collaborative Label Denoising Framework, to address this problem. Specifically, we first explore a model-collaboration-based denoising scheme that enables models trained on different data sources to collaboratively denoise pseudo labels used by each other. We then present an instance-collaboration-based strategy that considers the label consistency of each token’s neighborhood in the representation space for denoising. Experiments on different benchmark datasets show that the proposed CoLaDa achieves superior results compared to previous methods, especially when generalizing to distant languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 332,
      "original_url": "https://aclanthology.org/2023.acl-long.331/",
      "title": "Dialect-robust Evaluation of Generated Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.331.pdf",
      "authors": [
        "Jiao Sun",
        "Thibault Sellam",
        "Elizabeth Clark",
        "Tu Vu",
        "Timothy Dozat",
        "Dan Garrette",
        "Aditya Siddhant",
        "Jacob Eisenstein",
        "Sebastian Gehrmann"
      ],
      "abstract": "Text generation metrics that are not robust to dialect variation make it impossible to tell how well systems perform for many groups of users, and can even penalize systems for producing text in lower-resource dialects. In this paper, we introduce a suite of methods to assess whether metrics are dialect robust. These methods show that state-of-the-art metrics are not dialect robust: they often prioritize dialect similarity over semantics, preferring outputs that are semantically incorrect over outputs that match the semantics of the reference but contain dialect differences. As a step towards dialect-robust metrics for text generation, we propose NANO, which introduces regional and language information to the metric’s pretraining. NANO significantly improves dialect robustness while preserving the correlation between automated metrics and human ratings. It also enables a more ambitious approach to evaluation, dialect awareness, in which system outputs are scored by both semantic match to the reference and appropriateness in any specified dialect."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 333,
      "original_url": "https://aclanthology.org/2023.acl-long.332/",
      "title": "Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.332.pdf",
      "authors": [
        "Huaao Zhang",
        "Qiang Wang",
        "Bo Qin",
        "Zelin Shi",
        "Haibo Wang",
        "Ming Chen"
      ],
      "abstract": "In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. We identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO, are blind to this issue due to oversimplified constraint settings. To solve it, we create a new challenging test set of English-German, increasing the average constraint count per sentence from 1.1~1.7 to 6.1 and the length per target constraint from 1.1~1.2 words to 3.4 words. Then we find that PH and CS methods degrade as the number of constraints increases, but they have complementary strengths. Specifically, PH is better at retaining high constraint accuracy but lower translation quality as measured by BLEU and COMET scores. In contrast, CS has the opposite results. Based on these observations, we propose a simple but effective method combining the advantages of PH and CS. This approach involves training a model like PH to predict the term labels, and then during inference replacing those labels with target terminology text like CS, so that the subsequent generation is aware of the target term content. Extensive experimental results show that this approach can achieve high constraint accuracy and translation quality simultaneously, regardless of the number or length of constraints."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 334,
      "original_url": "https://aclanthology.org/2023.acl-long.333/",
      "title": "Language model acceptability judgements are not always robust to context",
      "pdf_url": "https://aclanthology.org/2023.acl-long.333.pdf",
      "authors": [
        "Koustuv Sinha",
        "Jon Gauthier",
        "Aaron Mueller",
        "Kanishka Misra",
        "Keren Fuentes",
        "Roger Levy",
        "Adina Williams"
      ],
      "abstract": "Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Our best syntactic evaluation datasets, however, provide substantially less linguistic context than models receive during pretraining. This mismatch raises an important question: how robust are models’ syntactic judgements across different contexts? In this paper, we vary the input contexts based on: length, the types of syntactic phenomena it contains, and whether or not there are grammatical violations. We find that model judgements are generally robust when placed in randomly sampled linguistic contexts, but are unstable when contexts match the test stimuli in syntactic structure. Among all tested models (GPT-2 and five variants of OPT), we find that model performance is affected when we provided contexts with matching syntactic structure: performance significantly improves when contexts are acceptable, and it significantly declines when they are unacceptable. This effect is amplified by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by acceptability-preserving syntactic perturbations. This sensitivity to highly specific syntactic features of the context can only be explained by the models’ implicit in-context learning abilities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 335,
      "original_url": "https://aclanthology.org/2023.acl-long.334/",
      "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.334.pdf",
      "authors": [
        "Yilun Zhao",
        "Chen Zhao",
        "Linyong Nan",
        "Zhenting Qi",
        "Wenlin Zhang",
        "Xiangru Tang",
        "Boyu Mi",
        "Dragomir Radev"
      ],
      "abstract": "Despite significant progress having been made in question answering on tabular data (Table QA), it’s unclear whether, and to what extent existing Table QA models are robust to task-specific perturbations, e.g., replacing key question entities or shuffling table columns. To systematically study the robustness of Table QA models, we propose a benchmark called RobuT, which builds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and includes human-annotated adversarial perturbations in terms of table header, table content, and question. Our results indicate that both state-of-the-art Table QA models and large language models (e.g., GPT-3) with few-shot learning falter in these adversarial sets. We propose to address this problem by using large language models to generate adversarial examples to enhance training, which significantly improves the robustness of Table QA models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 336,
      "original_url": "https://aclanthology.org/2023.acl-long.335/",
      "title": "Morphological Inflection: A Reality Check",
      "pdf_url": "https://aclanthology.org/2023.acl-long.335.pdf",
      "authors": [
        "Jordan Kodner",
        "Sarah Payne",
        "Salam Khalifa",
        "Zoey Liu"
      ],
      "abstract": "Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high variability; we find several aspects of data set creation and evaluation which systematically inflate performance and obfuscate differences between languages. To improve generalizability and reliability of results, we propose new data sampling and evaluation strategies that better reflect likely use-cases. Using these new strategies, we make new observations on the generalization abilities of current inflection systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 337,
      "original_url": "https://aclanthology.org/2023.acl-long.336/",
      "title": "TOME: A Two-stage Approach for Model-based Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.336.pdf",
      "authors": [
        "Ruiyang Ren",
        "Wayne Xin Zhao",
        "Jing Liu",
        "Hua Wu",
        "Ji-Rong Wen",
        "Haifeng Wang"
      ],
      "abstract": "Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 338,
      "original_url": "https://aclanthology.org/2023.acl-long.337/",
      "title": "Using Neural Machine Translation for Generating Diverse Challenging Exercises for Language Learner",
      "pdf_url": "https://aclanthology.org/2023.acl-long.337.pdf",
      "authors": [
        "Frank Palma Gomez",
        "Subhadarshi Panda",
        "Michael Flor",
        "Alla Rozovskaya"
      ],
      "abstract": "We propose a novel approach to automatically generate distractors for cloze exercises for English language learners, using round-trip neural machine translation. A carrier sentence is translated from English into another (pivot) language and back, and distractors are produced by aligning the original sentence with its round-trip translation. We make use of 16 linguistically-diverse pivots and generate hundreds of translation hypotheses in each direction. We show that using hundreds of translations allows us to generate a rich set of challenging distractors. Moreover, we find that typologically unrelated language pivots contribute more diverse candidate distractors, compared to language pivots that are closely related. We further evaluate the use of machine translation systems of varying quality and find that better quality MT systems produce more challenging distractors. Finally, we conduct a study with language learners, demonstrating that the automatically generated distractors are of the same difficulty as the gold distractors produced by human experts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 339,
      "original_url": "https://aclanthology.org/2023.acl-long.338/",
      "title": "Similarity-weighted Construction of Contextualized Commonsense Knowledge Graphs for Knowledge-intense Argumentation Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.338.pdf",
      "authors": [
        "Moritz Plenz",
        "Juri Opitz",
        "Philipp Heinisch",
        "Philipp Cimiano",
        "Anette Frank"
      ],
      "abstract": "Arguments often do not make explicit how a conclusion follows from its premises. To compensate for this lack, we enrich arguments with structured background knowledge to support knowledge-intense argumentation tasks. We present a new unsupervised method for constructing Contextualized Commonsense Knowledge Graphs (CCKGs) that selects contextually relevant knowledge from large knowledge graphs (KGs) efficiently and at high quality. Our work goes beyond context-insensitive knowledge extraction heuristics by computing semantic similarity between KG triplets and textual arguments. Using these triplet similarities as weights, we extract contextualized knowledge paths that connect a conclusion to its premise, while maximizing similarity to the argument. We combine multiple paths into a CCKG that we optionally prune to reduce noise and raise precision. Intrinsic evaluation of the quality of our graphs shows that our method is effective for (re)constructing human explanation graphs. Manual evaluations in a large-scale knowledge selection setup verify high recall and precision of implicit CSK in the CCKGs. Finally, we demonstrate the effectiveness of CCKGs in a knowledge-insensitive argument quality rating task, outperforming strong baselines and rivaling a GPT-3 based system."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 340,
      "original_url": "https://aclanthology.org/2023.acl-long.339/",
      "title": "miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-long.339.pdf",
      "authors": [
        "Tassilo Klein",
        "Moin Nabi"
      ],
      "abstract": "This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 341,
      "original_url": "https://aclanthology.org/2023.acl-long.340/",
      "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
      "pdf_url": "https://aclanthology.org/2023.acl-long.340.pdf",
      "authors": [
        "Mandar Sharma",
        "Nikhil Muralidhar",
        "Naren Ramakrishnan"
      ],
      "abstract": "The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the leaning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information-theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 342,
      "original_url": "https://aclanthology.org/2023.acl-long.341/",
      "title": "Forgotten Knowledge: Examining the Citational Amnesia in NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.341.pdf",
      "authors": [
        "Janvijay Singh",
        "Mukund Rungta",
        "Diyi Yang",
        "Saif Mohammad"
      ],
      "abstract": "Citing papers is the primary method through which modern scientific writing discusses and builds on past work. Collectively, citing a diverse set of papers (in time and area of study) is an indicator of how widely the community is reading. Yet, there is little work looking at broad temporal patterns of citation. This work systematically and empirically examines: How far back in time do we tend to go to cite papers? How has that changed over time, and what factors correlate with this citational attention/amnesia? We chose NLP as our domain of interest and analyzed approximately 71.5K papers to show and quantify several key trends in citation. Notably, around 62% of cited papers are from the immediate five years prior to publication, whereas only about 17% are more than ten years old. Furthermore, we show that the median age and age diversity of cited papers were steadily increasing from 1990 to 2014, but since then, the trend has reversed, and current NLP papers have an all-time low temporal citation diversity. Finally, we show that unlike the 1990s, the highly cited papers in the last decade were also papers with the least citation diversity, likely contributing to the intense (and arguably harmful) recency focus. Code, data, and a demo are available on the project homepage."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 343,
      "original_url": "https://aclanthology.org/2023.acl-long.342/",
      "title": "Measuring the Instability of Fine-Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.342.pdf",
      "authors": [
        "Yupei Du",
        "Dong Nguyen"
      ],
      "abstract": "Fine-tuning pre-trained language models on downstream tasks with varying random seeds has been shown to be unstable, especially on small datasets. Many previous studies have investigated this instability and proposed methods to mitigate it. However, most of these studies only used the standard deviation of performance scores (SD) as their measure, which is a narrow characterization of instability. In this paper, we analyze SD and six other measures quantifying instability of different granularity levels. Moreover, we propose a systematic evaluation framework of these measures’ validity. Finally, we analyze the consistency and difference between different measures by reassessing existing instability mitigation methods. We hope our results will inform better measurements of the fine-tuning instability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 344,
      "original_url": "https://aclanthology.org/2023.acl-long.343/",
      "title": "FairPrism: Evaluating Fairness-Related Harms in Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.343.pdf",
      "authors": [
        "Eve Fleisig",
        "Aubrie Amstutz",
        "Chad Atalla",
        "Su Lin Blodgett",
        "Hal Daumé III",
        "Alexandra Olteanu",
        "Emily Sheng",
        "Dan Vann",
        "Hanna Wallach"
      ],
      "abstract": "It is critical to measure and mitigate fairness-related harms caused by AI text generation systems, including stereotyping and demeaning harms. To that end, we introduce FairPrism, a dataset of 5,000 examples of AI-generated English text with detailed human annotations covering a diverse set of harms relating to gender and sexuality. FairPrism aims to address several limitations of existing datasets for measuring and mitigating fairness-related harms, including improved transparency, clearer specification of dataset coverage, and accounting for annotator disagreement and harms that are context-dependent. FairPrism’s annotations include the extent of stereotyping and demeaning harms, the demographic groups targeted, and appropriateness for different applications. The annotations also include specific harms that occur in interactive contexts and harms that raise normative concerns when the “speaker” is an AI system. Due to its precision and granularity, FairPrism can be used to diagnose (1) the types of fairness-related harms that AI text generation systems cause, and (2) the potential limitations of mitigation methods, both of which we illustrate through case studies. Finally, the process we followed to develop FairPrism offers a recipe for building improved datasets for measuring and mitigating harms caused by AI systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 345,
      "original_url": "https://aclanthology.org/2023.acl-long.344/",
      "title": "Factually Consistent Summarization via Reinforcement Learning with Textual Entailment Feedback",
      "pdf_url": "https://aclanthology.org/2023.acl-long.344.pdf",
      "authors": [
        "Paul Roit",
        "Johan Ferret",
        "Lior Shani",
        "Roee Aharoni",
        "Geoffrey Cideron",
        "Robert Dadashi",
        "Matthieu Geist",
        "Sertan Girgin",
        "Leonard Hussenot",
        "Orgad Keller",
        "Nikola Momchev",
        "Sabela Ramos Garea",
        "Piotr Stanczyk",
        "Nino Vieillard",
        "Olivier Bachem",
        "Gal Elidan",
        "Avinatan Hassidim",
        "Olivier Pietquin",
        "Idan Szpektor"
      ],
      "abstract": "Despite the seeming success of contemporary grounded text generation systems, they often tend to generate factually inconsistent text with respect to their input. This phenomenon is emphasized in tasks like summarization, in which the generated summaries should be corroborated by their source article. In this work we leverage recent progress on textual entailment models to directly address this problem for abstractive summarization systems. We use reinforcement learning with reference-free, textual-entailment rewards to optimize for factual consistency and explore the ensuing trade-offs, as improved consistency may come at the cost of less informative or more extractive summaries. Our results, according to both automatic metrics and human evaluation, show that our method considerably improves the faithfulness, salience and conciseness of the generated summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 346,
      "original_url": "https://aclanthology.org/2023.acl-long.345/",
      "title": "SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams",
      "pdf_url": "https://aclanthology.org/2023.acl-long.345.pdf",
      "authors": [
        "Te-Lin Wu",
        "Satwik Kottur",
        "Andrea Madotto",
        "Mahmoud Azab",
        "Pedro Rodriguez",
        "Babak Damavandi",
        "Nanyun Peng",
        "Seungwhan Moon"
      ],
      "abstract": "Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities:(1) spatial and temporal understanding of the situated and real-time user scenes,(2) capability of grounding the actively perceived visuals of users to conversation contexts,and (3) conversational reasoning over past utterances to perform just-in-time assistance. However, we currently lack a large-scale benchmark that captures user–assistant interactions with all of the aforementioned features. To this end, we propose SIMMC-VR, an extension of the SIMMC-2.0 dataset, to a video-grounded task-oriented dialog dataset that captures real-world AI-assisted user scenarios in VR.We propose a novel data collection paradigm that involves(1) generating object-centric multimodal dialog flows with egocentric visual streams and visually-grounded templates,and (2) manually paraphrasing the simulated dialogs for naturalness and diversity while preserving multimodal dependencies. To measure meaningful progress in the field, we propose four tasks to address the new challenges in SIMMC-VR, which require complex spatial-temporal dialog reasoning in active egocentric scenes. We benchmark the proposed tasks with strong multimodal models, and highlight the key capabilities that current models lack for future research directions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 347,
      "original_url": "https://aclanthology.org/2023.acl-long.346/",
      "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.346.pdf",
      "authors": [
        "Eshaan Tanwar",
        "Subhabrata Dutta",
        "Manish Borthakur",
        "Tanmoy Chakraborty"
      ],
      "abstract": "In-context learning (ICL) unfolds as large language models become capable of inferring test labels conditioned on a few labeled samples without any gradient update. ICL-enabled large language models provide a promising step forward toward bypassing recurrent annotation costs in a low-resource setting. Yet, only a handful of past studies have explored ICL in a cross-lingual setting, in which the need for transferring label-knowledge from a high-resource language to a low-resource one is immensely crucial. To bridge the gap, we provide the first in-depth analysis of ICL for cross-lingual text classification. We find that the prevalent mode of selecting random input-label pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy — Cross-lingual In-context Source Target Alignment (X-InSTA). With an injected coherence in the semantics of the input examples and a task-based alignment across the source and target languages, X-InSTA is able to outperform random prompt selection by a large margin across three different tasks using 44 different cross-lingual pairs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 348,
      "original_url": "https://aclanthology.org/2023.acl-long.347/",
      "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.347.pdf",
      "authors": [
        "Soumya Sanyal",
        "Yichong Xu",
        "Shuohang Wang",
        "Ziyi Yang",
        "Reid Pryzant",
        "Wenhao Yu",
        "Chenguang Zhu",
        "Xiang Ren"
      ],
      "abstract": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 349,
      "original_url": "https://aclanthology.org/2023.acl-long.348/",
      "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.348.pdf",
      "authors": [
        "Vaishali Pal",
        "Andrew Yates",
        "Evangelos Kanoulas",
        "Maarten de Rijke"
      ],
      "abstract": "Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 350,
      "original_url": "https://aclanthology.org/2023.acl-long.349/",
      "title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.349.pdf",
      "authors": [
        "Rui Li",
        "Xu Chen",
        "Chaozhuo Li",
        "Yanming Shen",
        "Jianan Zhao",
        "Yujing Wang",
        "Weihao Han",
        "Hao Sun",
        "Weiwei Deng",
        "Qi Zhang",
        "Xing Xie"
      ],
      "abstract": "Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 351,
      "original_url": "https://aclanthology.org/2023.acl-long.350/",
      "title": "CoAD: Automatic Diagnosis through Symptom and Disease Collaborative Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.350.pdf",
      "authors": [
        "Huimin Wang",
        "Wai Chung Kwan",
        "Kam-Fai Wong",
        "Yefeng Zheng"
      ],
      "abstract": "Automatic diagnosis (AD), a critical application of AI in healthcare, employs machine learning techniques to assist doctors in gathering patient symptom information for precise disease diagnosis. The Transformer-based method utilizes an input symptom sequence, predicts itself through auto-regression, and employs the hidden state of the final symptom to determine the disease. Despite its simplicity and superior performance demonstrated, a decline in disease diagnosis accuracy is observed caused by 1) a mismatch between symptoms observed during training and generation, and 2) the effect of different symptom orders on disease prediction. To address the above obstacles, we introduce the CoAD, a novel disease and symptom collaborative generation framework, which incorporates several key innovations to improve AD: 1) aligning sentence-level disease labels with multiple possible symptom inquiry steps to bridge the gap between training and generation; 2) expanding symptom labels for each sub-sequence of symptoms to enhance annotation and eliminate the effect of symptom order; 3) developing a repeated symptom input schema to effectively and efficiently learn the expanded disease and symptom labels. We evaluate the CoAD framework using four datasets, including three public and one private, and demonstrate that it achieves an average 2.3% improvement over previous state-of-the-art results in automatic disease diagnosis. For reproducibility, we release the code and data at https://github.com/KwanWaiChung/coad."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 352,
      "original_url": "https://aclanthology.org/2023.acl-long.351/",
      "title": "Long-Tailed Question Answering in an Open World",
      "pdf_url": "https://aclanthology.org/2023.acl-long.351.pdf",
      "authors": [
        "Yi Dai",
        "Hao Lang",
        "Yinhe Zheng",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM).Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing.A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 353,
      "original_url": "https://aclanthology.org/2023.acl-long.352/",
      "title": "Parallel Context Windows for Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.352.pdf",
      "authors": [
        "Nir Ratner",
        "Yoav Levine",
        "Yonatan Belinkov",
        "Ori Ram",
        "Inbal Magar",
        "Omri Abend",
        "Ehud Karpas",
        "Amnon Shashua",
        "Kevin Leyton-Brown",
        "Yoav Shoham"
      ],
      "abstract": "When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (“windows”), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ai21labs/parallel-context-windows."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 354,
      "original_url": "https://aclanthology.org/2023.acl-long.353/",
      "title": "Efficient Transformers with Dynamic Token Pooling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.353.pdf",
      "authors": [
        "Piotr Nawrot",
        "Jan Chorowski",
        "Adrian Lancucki",
        "Edoardo Maria Ponti"
      ],
      "abstract": "Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vanilla Transformers and fixed-length pooling within the same computational budget."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 355,
      "original_url": "https://aclanthology.org/2023.acl-long.354/",
      "title": "Did the Models Understand Documents? Benchmarking Models for Language Understanding in Document-Level Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.354.pdf",
      "authors": [
        "Haotian Chen",
        "Bingsheng Chen",
        "Xiangdong Zhou"
      ],
      "abstract": "Document-level relation extraction (DocRE) attracts more research interest recently. While models achieve consistent performance gains in DocRE, their underlying decision rules are still understudied: Do they make the right predictions according to rationales? In this paper, we take the first step toward answering this question and then introduce a new perspective on comprehensively evaluating a model. Specifically, we first conduct annotations to provide the rationales considered by humans in DocRE. Then, we conduct investigations and discover the fact that: In contrast to humans, the representative state-of-the-art (SOTA) models in DocRE exhibit different reasoning processes. Through our proposed RE-specific attacks, we next demonstrate that the significant discrepancy in decision rules between models and humans severely damages the robustness of models. After that, we introduce mean average precision (MAP) to evaluate the understanding and reasoning capabilities of models. According to the extensive experimental results, we finally appeal to future work to consider evaluating the understanding ability of models because the improved ability renders models more trustworthy and robust to be deployed in real-world scenarios. We make our annotations and code publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 356,
      "original_url": "https://aclanthology.org/2023.acl-long.355/",
      "title": "ContraCLM: Contrastive Learning For Causal Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.355.pdf",
      "authors": [
        "Nihal Jain",
        "Dejiao Zhang",
        "Wasi Uddin Ahmad",
        "Zijian Wang",
        "Feng Nan",
        "Xiaopeng Li",
        "Ming Tan",
        "Ramesh Nallapati",
        "Baishakhi Ray",
        "Parminder Bhatia",
        "Xiaofei Ma",
        "Bing Xiang"
      ],
      "abstract": "Despite exciting progress in causal language models, the expressiveness of their representations is largely limited due to poor discrimination ability. To remedy this issue, we present CONTRACLM, a novel contrastive learning framework at both the token-level and the sequence-level. We assess CONTRACLM on a variety of downstream tasks. We show that CONTRACLM enhances the discrimination of representations and bridges the gap with encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain 44% relative improvement on the Semantic Textual Similarity tasks and 34% on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of representations, CONTRACLM also boosts the source code generation capability with 9% relative improvement on execution accuracy on the HumanEval benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 357,
      "original_url": "https://aclanthology.org/2023.acl-long.356/",
      "title": "Advancing Multi-Criteria Chinese Word Segmentation Through Criterion Classification and Denoising",
      "pdf_url": "https://aclanthology.org/2023.acl-long.356.pdf",
      "authors": [
        "Tzu Hsuan Chou",
        "Chun-Yi Lin",
        "Hung-Yu Kao"
      ],
      "abstract": "Recent research on multi-criteria Chinese word segmentation (MCCWS) mainly focuses on building complex private structures, adding more handcrafted features, or introducing complex optimization processes. In this work, we show that through a simple yet elegant input-hint-based MCCWS model, we can achieve state-of-the-art (SoTA) performances on several datasets simultaneously. We further propose a novel criterion-denoising objective that hurts slightly on F1 score but achieves SoTA recall on out-of-vocabulary words. Our result establishes a simple yet strong baseline for future MCCWS research. Source code is available at https://github.com/IKMLab/MCCWS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 358,
      "original_url": "https://aclanthology.org/2023.acl-long.357/",
      "title": "Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.357.pdf",
      "authors": [
        "Haodong Zhao",
        "Ruifang He",
        "Mengnan Xiao",
        "Jing Xu"
      ],
      "abstract": "Multi-level implicit discourse relation recognition (MIDRR) aims at identifying hierarchical discourse relations among arguments. Previous methods achieve the promotion through fine-tuning PLMs. However, due to the data scarcity and the task gap, the pre-trained feature space cannot be accurately tuned to the task-specific space, which even aggravates the collapse of the vanilla space. Besides, the comprehension of hierarchical semantics for MIDRR makes the conversion much harder. In this paper, we propose a prompt-based Parameter-Efficient Multi-level IDRR (PEMI) framework to solve the above problems. First, we leverage parameter-efficient prompt tuning to drive the inputted arguments to match the pre-trained space and realize the approximation with few parameters. Furthermore, we propose a hierarchical label refining (HLR) method for the prompt verbalizer to deeply integrate hierarchical guidance into the prompt tuning. Finally, our model achieves comparable results on PDTB 2.0 and 3.0 using about 0.1% trainable parameters compared with baselines and the visualization demonstrates the effectiveness of our HLR method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 359,
      "original_url": "https://aclanthology.org/2023.acl-long.358/",
      "title": "Contrastive Learning with Adversarial Examples for Alleviating Pathology of Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.358.pdf",
      "authors": [
        "Pengwei Zhan",
        "Jing Yang",
        "Xiao Huang",
        "Chunlei Jing",
        "Jingying Li",
        "Liming Wang"
      ],
      "abstract": "Neural language models have achieved superior performance. However, these models also suffer from the pathology of overconfidence in the out-of-distribution examples, potentially making the model difficult to interpret and making the interpretation methods fail to provide faithful attributions. In this paper, we explain the model pathology from the view of sentence representation and argue that the counter-intuitive bias degree and direction of the out-of-distribution examples’ representation cause the pathology. We propose a Contrastive learning regularization method using Adversarial examples for Alleviating the Pathology (ConAAP), which calibrates the sentence representation of out-of-distribution examples. ConAAP generates positive and negative examples following the attribution results and utilizes adversarial examples to introduce direction information in regularization. Experiments show that ConAAP effectively alleviates the model pathology while slightly impacting the generalization ability on in-distribution examples and thus helps interpretation methods obtain more faithful results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 360,
      "original_url": "https://aclanthology.org/2023.acl-long.359/",
      "title": "Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains of Children’s Fairy Tales",
      "pdf_url": "https://aclanthology.org/2023.acl-long.359.pdf",
      "authors": [
        "Paulina Toro Isaza",
        "Guangxuan Xu",
        "Toye Oloko",
        "Yufang Hou",
        "Nanyun Peng",
        "Dakuo Wang"
      ],
      "abstract": "Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing (NLP) methods that examine social bias in models and data corpora. Our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories. We propose a computational pipeline that automatically extracts a story’s temporal narrative verb-based event chain for each of its characters as well as character attributes such as gender. We also present a verb-based event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes. Through a case study analyzing gender bias in fairy tales, we demonstrate that our framework can reveal bias in not only the unigram verb-based events in which female and male characters participate but also in the temporal narrative order of such event participation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 361,
      "original_url": "https://aclanthology.org/2023.acl-long.360/",
      "title": "FutureTOD: Teaching Future Knowledge to Pre-trained Language Model for Task-Oriented Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-long.360.pdf",
      "authors": [
        "Weihao Zeng",
        "Keqing He",
        "Yejie Wang",
        "Chen Zeng",
        "Jingang Wang",
        "Yunsen Xian",
        "Weiran Xu"
      ],
      "abstract": "Pre-trained language models based on general text enable huge success in the NLP scenario. But the intrinsical difference of linguistic patterns between general text and task-oriented dialogues makes existing pre-trained language models less useful in practice. Current dialogue pre-training methods rely on a contrastive framework and face the challenges of both selecting true positives and hard negatives. In this paper, we propose a novel dialogue pre-training model, FutureTOD, which distills future knowledge to the representation of the previous dialogue context using a self-training framework. Our intuition is that a good dialogue representation both learns local context information and predicts future information. Extensive experiments on diverse downstream dialogue tasks demonstrate the effectiveness of our model, especially the generalization, robustness, and learning discriminative dialogue representations capabilities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 362,
      "original_url": "https://aclanthology.org/2023.acl-long.361/",
      "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language",
      "pdf_url": "https://aclanthology.org/2023.acl-long.361.pdf",
      "authors": [
        "Mehran Kazemi",
        "Najoung Kim",
        "Deepti Bhatia",
        "Xin Xu",
        "Deepak Ramachandran"
      ],
      "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 363,
      "original_url": "https://aclanthology.org/2023.acl-long.362/",
      "title": "PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives",
      "pdf_url": "https://aclanthology.org/2023.acl-long.362.pdf",
      "authors": [
        "Silin Gao",
        "Beatriz Borges",
        "Soyoung Oh",
        "Deniz Bayazit",
        "Saya Kanno",
        "Hiromi Wakaki",
        "Yuki Mitsufuji",
        "Antoine Bosselut"
      ],
      "abstract": "Sustaining coherent and engaging narratives requires dialogue or storytelling agents to understandhow the personas of speakers or listeners ground the narrative. Specifically, these agents must infer personas of their listeners to produce statements that cater to their interests. They must also learn to maintain consistent speaker personas for themselves throughout the narrative, so that their counterparts feel involved in a realistic conversation or story. However, personas are diverse and complex: they entail large quantities of rich interconnected world knowledge that is challenging to robustly represent in general narrative systems (e.g., a singer is good at singing, and may have attended conservatoire). In this work, we construct a new large-scale persona commonsense knowledge graph, PeaCoK, containing ~100K human-validated persona facts. Our knowledge graph schematizes five dimensions of persona knowledge identified in previous studies of human interactive behaviours, and distils facts in this schema from both existing commonsense knowledge graphs and large-scale pretrained language models. Our analysis indicates that PeaCoK contains rich and precise world persona inferences that help downstream systems generate more consistent and engaging narratives."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 364,
      "original_url": "https://aclanthology.org/2023.acl-long.363/",
      "title": "OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.363.pdf",
      "authors": [
        "Xize Cheng",
        "Tao Jin",
        "Linjun Li",
        "Wang Lin",
        "Xinyu Duan",
        "Zhou Zhao"
      ],
      "abstract": "Speech Recognition builds a bridge between the multimedia streaming (audio-only, visual-only or audio-visual) and the corresponding text transcription. However, when training the specific model of new domain, it often gets stuck in the lack of new-domain utterances, especially the labeled visual utterances. To break through this restriction, we attempt to achieve zero-shot modality transfer by maintaining the multi-modality alignment in phoneme space learned with unlabeled multimedia utterances in the high resource domain during the pre-training, and propose a training system Open-modality Speech Recognition (OpenSR) that enables the models trained on a single modality (e.g., audio-only) applicable to more modalities (e.g., visual-only and audio-visual). Furthermore, we employ a cluster-based prompt tuning strategy to handle the domain shift for the scenarios with only common words in the new domain utterances. We demonstrate that OpenSR enables modality transfer from one to any in three different settings (zero-, few- and full-shot), and achieves highly competitive zero-shot performance compared to the existing few-shot and full-shot lip-reading methods. To the best of our knowledge, OpenSR achieves the state-of-the-art performance of word error rate in LRS2 on audio-visual speech recognition and lip-reading with 2.7% and 25.0%, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 365,
      "original_url": "https://aclanthology.org/2023.acl-long.364/",
      "title": "Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.364.pdf",
      "authors": [
        "Rui Wang",
        "Jianzhu Bao",
        "Fei Mi",
        "Yi Chen",
        "Hongru Wang",
        "Yasheng Wang",
        "Yitong Li",
        "Lifeng Shang",
        "Kam-Fai Wong",
        "Ruifeng Xu"
      ],
      "abstract": "Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline. Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it costly to transfer. To tackle this challenge, this paper proposed a retrieval-free approach, KiDG, by automatically turning knowledge documents into simulated multi-turn dialogues through a Multi-Document Traversal algorithm. The simulated knowledge-intensive dialogues constructed by KiDG in one domain can be easily used to train and enhance pre-trained dialogue models’ knowledge w.r.t. this domain without costly annotation. We conduct extensive experiments comparing retrieval-augmented models and a variety of retrieval-free models. We found that dialogue models enhanced with data simulated with KiDG largely outperform state-of-the-art retrieval-free methods, and it achieves comparable performance compared to retrieval-augmented methods while being better, and cheaper at domain transfer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 366,
      "original_url": "https://aclanthology.org/2023.acl-long.365/",
      "title": "BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.365.pdf",
      "authors": [
        "Shicheng Xu",
        "Liang Pang",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context. Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining and knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 367,
      "original_url": "https://aclanthology.org/2023.acl-long.366/",
      "title": "Multiview Identifiers Enhanced Generative Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.366.pdf",
      "authors": [
        "Yongqi Li",
        "Nan Yang",
        "Liang Wang",
        "Furu Wei",
        "Wenjie Li"
      ],
      "abstract": "Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage’s content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative retrieval, demonstrating its effectiveness and robustness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 368,
      "original_url": "https://aclanthology.org/2023.acl-long.367/",
      "title": "Prompting Language Models for Linguistic Structure",
      "pdf_url": "https://aclanthology.org/2023.acl-long.367.pdf",
      "authors": [
        "Terra Blevins",
        "Hila Gonen",
        "Luke Zettlemoyer"
      ],
      "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 369,
      "original_url": "https://aclanthology.org/2023.acl-long.368/",
      "title": "Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.368.pdf",
      "authors": [
        "Agam Shah",
        "Suvan Paturi",
        "Sudheer Chava"
      ],
      "abstract": "Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 370,
      "original_url": "https://aclanthology.org/2023.acl-long.369/",
      "title": "RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.369.pdf",
      "authors": [
        "Jun Zhao",
        "WenYu Zhan",
        "Xin Zhao",
        "Qi Zhang",
        "Tao Gui",
        "Zhongyu Wei",
        "Junzhe Wang",
        "Minlong Peng",
        "Mingming Sun"
      ],
      "abstract": "Semantic matching is a mainstream paradigm of zero-shot relation extraction, which matches a given input with a corresponding label description. The entities in the input should exactly match their hypernyms in the description, while the irrelevant contexts should be ignored when matching. However, general matching methods lack explicit modeling of the above matching pattern. In this work, we propose a fine-grained semantic matching method tailored for zero-shot relation extraction. Guided by the above matching pattern, we decompose the sentence-level similarity score into the entity matching score and context matching score. Considering that not all contextual words contribute equally to the relation semantics, we design a context distillation module to reduce the negative impact of irrelevant components on context matching. Experimental results show that our method achieves higher matching accuracy and more than 10 times faster inference speed, compared with the state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 371,
      "original_url": "https://aclanthology.org/2023.acl-long.370/",
      "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration",
      "pdf_url": "https://aclanthology.org/2023.acl-long.370.pdf",
      "authors": [
        "Hwaran Lee",
        "Seokhee Hong",
        "Joonsuk Park",
        "Takyoung Kim",
        "Meeyoung Cha",
        "Yejin Choi",
        "Byoungpil Kim",
        "Gunhee Kim",
        "Eun-Ju Lee",
        "Yong Lim",
        "Alice Oh",
        "Sangchul Park",
        "Jung-Woo Ha"
      ],
      "abstract": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 372,
      "original_url": "https://aclanthology.org/2023.acl-long.371/",
      "title": "Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.371.pdf",
      "authors": [
        "Soyoung Yoon",
        "Sungjoon Park",
        "Gyuwan Kim",
        "Junhee Cho",
        "Kihyo Park",
        "Gyu Tae Kim",
        "Minjoon Seo",
        "Alice Oh"
      ],
      "abstract": "Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS (Korean Automatic Grammatical error Annotation System), which can automatically annotate error types from parallel corpora. We use KAGAS on our datasets to make an evaluation benchmark for Korean, and present baseline models trained from our datasets. We show that the model trained with our datasets significantly outperforms the currently used statistical Korean GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets. The implementations and datasets are open-sourced."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 373,
      "original_url": "https://aclanthology.org/2023.acl-long.372/",
      "title": "FLamE: Few-shot Learning from Natural Language Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.372.pdf",
      "authors": [
        "Yangqiaoyu Zhou",
        "Yiming Zhang",
        "Chenhao Tan"
      ],
      "abstract": "Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then fine-tunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI.Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., “not know” for the neutral label) in generated explanations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 374,
      "original_url": "https://aclanthology.org/2023.acl-long.373/",
      "title": "Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.373.pdf",
      "authors": [
        "Subhajit Chaudhury",
        "Sarathkrishna Swaminathan",
        "Daiki Kimura",
        "Prithviraj Sen",
        "Keerthiram Murugesan",
        "Rosario Uceda-Sosa",
        "Michiaki Tatsubori",
        "Achille Fokoue",
        "Pavan Kapanipathi",
        "Asim Munawar",
        "Alexander Gray"
      ],
      "abstract": "Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from fewer training interactions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 375,
      "original_url": "https://aclanthology.org/2023.acl-long.374/",
      "title": "Counterfactual Debiasing for Fact Verification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.374.pdf",
      "authors": [
        "Weizhi Xu",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "Fact verification aims to automatically judge the veracity of a claim according to several pieces of evidence. Due to the manual construction of datasets, spurious correlations between claim patterns and its veracity (i.e., biases) inevitably exist. Recent studies show that models usually learn such biases instead of understanding the semantic relationship between the claim and evidence. Existing debiasing works can be roughly divided into data-augmentation-based and weight-regularization-based pipeline, where the former is inflexible and the latter relies on the uncertain output on the training stage. Unlike previous works, we propose a novel method from a counterfactual view, namely CLEVER, which is augmentation-free and mitigates biases on the inference stage. Specifically, we train a claim-evidence fusion model and a claim-only model independently. Then, we obtain the final prediction via subtracting output of the claim-only model from output of the claim-evidence fusion model, which counteracts biases in two outputs so that the unbiased part is highlighted. Comprehensive experiments on several datasets have demonstrated the effectiveness of CLEVER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 376,
      "original_url": "https://aclanthology.org/2023.acl-long.375/",
      "title": "What social attitudes about gender does BERT encode? Leveraging insights from psycholinguistics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.375.pdf",
      "authors": [
        "Julia Watson",
        "Barend Beekhuizen",
        "Suzanne Stevenson"
      ],
      "abstract": "Much research has sought to evaluate the degree to which large language models reflect social biases. We complement such work with an approach to elucidating the connections between language model predictions and people’s social attitudes. We show how word preferences in a large language model reflect social attitudes about gender, using two datasets from human experiments that found differences in gendered or gender neutral word choices by participants with differing views on gender (progressive, moderate, or conservative). We find that the language model BERT takes into account factors that shape human lexical choice of such language, but may not weigh those factors in the same way people do. Moreover, we show that BERT’s predictions most resemble responses from participants with moderate to conservative views on gender. Such findings illuminate how a language model: (1) may differ from people in how it deploys words that signal gender, and (2) may prioritize some social attitudes over others."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 377,
      "original_url": "https://aclanthology.org/2023.acl-long.376/",
      "title": "Rethinking Multimodal Entity and Relation Extraction from a Translation Point of View",
      "pdf_url": "https://aclanthology.org/2023.acl-long.376.pdf",
      "authors": [
        "Changmeng Zheng",
        "Junhao Feng",
        "Yi Cai",
        "Xiaoyong Wei",
        "Qing Li"
      ],
      "abstract": "We revisit the multimodal entity and relation extraction from a translation point of view. Special attention is paid on the misalignment issue in text-image datasets which may mislead the learning. We are motivated by the fact that the cross-modal misalignment is a similar problem of cross-lingual divergence issue in machine translation. The problem can then be transformed and existing solutions can be borrowed by treating a text and its paired image as the translation to each other. We implement a multimodal back-translation using diffusion-based generative models for pseudo-paralleled pairs and a divergence estimator by constructing a high-resource corpora as a bridge for low-resource learners. Fine-grained confidence scores are generated to indicate both types and degrees of alignments with which better representations are obtained. The method has been validated in the experiments by outperforming 14 state-of-the-art methods in both entity and relation extraction tasks. The source code is available at https://github.com/thecharm/TMR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 378,
      "original_url": "https://aclanthology.org/2023.acl-long.377/",
      "title": "Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.377.pdf",
      "authors": [
        "Rongxin Zhu",
        "Jianzhong Qi",
        "Jey Han Lau"
      ],
      "abstract": "A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and weevaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 379,
      "original_url": "https://aclanthology.org/2023.acl-long.378/",
      "title": "Improving the Robustness of Summarization Systems with Dual Augmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.378.pdf",
      "authors": [
        "Xiuying Chen",
        "Guodong Long",
        "Chongyang Tao",
        "Mingzhe Li",
        "Xin Gao",
        "Chengqi Zhang",
        "Xiangliang Zhang"
      ],
      "abstract": "A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models’ robustness against perturbations including word-level synonym substitution and noise. To create semantic-consistent substitutes, we propose a SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models. Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets. Next, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation. Specifically, the first vulnerability factor we found is the low diversity of the training inputs. Correspondingly, we expose the encoder to more diverse cases created by SummAttacker in the input space. The second factor is the vulnerability of the decoder, and we propose an augmentation in the latent space of the decoder to improve its robustness. Concretely, we create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings. Experimental results on Gigaword and CNN/DM datasets demonstrate that our approach achieves significant improvements over strong baselines and exhibits higher robustness on noisy, attacked, and clean datasets"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 380,
      "original_url": "https://aclanthology.org/2023.acl-long.379/",
      "title": "Interpretable Math Word Problem Solution Generation via Step-by-step Planning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.379.pdf",
      "authors": [
        "Mengxue Zhang",
        "Zichao Wang",
        "Zhichao Yang",
        "Weiqi Feng",
        "Andrew Lan"
      ],
      "abstract": "Solutions to math word problems (MWPs) with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches only focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning approach for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed, given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our approach improves the accuracy and interpretability of the solution on both automatic metrics and human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 381,
      "original_url": "https://aclanthology.org/2023.acl-long.380/",
      "title": "TemplateGEC: Improving Grammatical Error Correction with Detection Template",
      "pdf_url": "https://aclanthology.org/2023.acl-long.380.pdf",
      "authors": [
        "Yinghao Li",
        "Xuebo Liu",
        "Shuo Wang",
        "Peiyuan Gong",
        "Derek F. Wong",
        "Yang Gao",
        "Heyan Huang",
        "Min Zhang"
      ],
      "abstract": "Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC.Further analysis reveals the potential of our method in performing human-in-the-loop GEC. Source code and scripts are available at https://github.com/li-aolong/TemplateGEC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 382,
      "original_url": "https://aclanthology.org/2023.acl-long.381/",
      "title": "Deep Model Compression Also Helps Models Capture Ambiguity",
      "pdf_url": "https://aclanthology.org/2023.acl-long.381.pdf",
      "authors": [
        "Hancheol Park",
        "Jong Park"
      ],
      "abstract": "Natural language understanding (NLU) tasks face a non-trivial amount of ambiguous samples where veracity of their labels is debatable among annotators. NLU models should thus account for such ambiguity, but they approximate the human opinion distributions quite poorly and tend to produce over-confident predictions. To address this problem, we must consider how to exactly capture the degree of relationship between each sample and its candidate classes. In this work, we propose a novel method with deep model compression and show how such relationship can be accounted for. We see that more reasonably represented relationships can be discovered in the lower layers and that validation accuracies are converging at these layers, which naturally leads to layer pruning. We also see that distilling the relationship knowledge from a lower layer helps models produce better distribution. Experimental results demonstrate that our method makes substantial improvement on quantifying ambiguity without gold distribution labels. As positive side-effects, our method is found to reduce the model size significantly and improve latency, both attractive aspects of NLU products."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 383,
      "original_url": "https://aclanthology.org/2023.acl-long.382/",
      "title": "Are Experts Needed? On Human Evaluation of Counselling Reflection Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.382.pdf",
      "authors": [
        "Zixiu Wu",
        "Simone Balloccu",
        "Ehud Reiter",
        "Rim Helaoui",
        "Diego Reforgiato Recupero",
        "Daniele Riboni"
      ],
      "abstract": "Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 384,
      "original_url": "https://aclanthology.org/2023.acl-long.383/",
      "title": "PairSpanBERT: An Enhanced Language Model for Bridging Resolution",
      "pdf_url": "https://aclanthology.org/2023.acl-long.383.pdf",
      "authors": [
        "Hideo Kobayashi",
        "Yufang Hou",
        "Vincent Ng"
      ],
      "abstract": "We present PairSpanBERT, a SpanBERT-based pre-trained model specialized for bridging resolution. To this end, we design a novel pre-training objective that aims to learn the contexts in which two mentions are implicitly linked to each other from a large amount of data automatically generated either heuristically or via distance supervision with a knowledge graph. Despite the noise inherent in the automatically generated data, we achieve the best results reported to date on three evaluation datasets for bridging resolution when replacing SpanBERT with PairSpanBERT in a state-of-the-art resolver that jointly performs entity coreference resolution and bridging resolution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 385,
      "original_url": "https://aclanthology.org/2023.acl-long.384/",
      "title": "Compounding Geometric Operations for Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.384.pdf",
      "authors": [
        "Xiou Ge",
        "Yun Cheng Wang",
        "Bin Wang",
        "C.-C. Jay Kuo"
      ],
      "abstract": "Geometric transformations including translation, rotation, and scaling are commonly used operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE). Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a composite one, the new model is named CompoundE. By casting CompoundE in the framework of group theory, we show that quite a few distanced-based KGE models are special cases of CompoundE. CompoundE extends the simple distance-based scoring functions to relation-dependent compound operations on head and/or tail entities. To demonstrate the effectiveness of CompoundE, we perform three prevalent KG prediction tasks including link prediction, path query answering, and entity typing, on a range of datasets. CompoundE outperforms extant models consistently, demonstrating its effectiveness and flexibility."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 386,
      "original_url": "https://aclanthology.org/2023.acl-long.385/",
      "title": "Few-shot In-context Learning on Knowledge Base Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.385.pdf",
      "authors": [
        "Tianle Li",
        "Xueguang Ma",
        "Alex Zhuang",
        "Yu Gu",
        "Yu Su",
        "Wenhu Chen"
      ],
      "abstract": "Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 387,
      "original_url": "https://aclanthology.org/2023.acl-long.386/",
      "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.386.pdf",
      "authors": [
        "Liangming Pan",
        "Xiaobao Wu",
        "Xinyuan Lu",
        "Anh Tuan Luu",
        "William Yang Wang",
        "Min-Yen Kan",
        "Preslav Nakov"
      ],
      "abstract": "Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 388,
      "original_url": "https://aclanthology.org/2023.acl-long.387/",
      "title": "Patton: Language Model Pretraining on Text-Rich Networks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.387.pdf",
      "authors": [
        "Bowen Jin",
        "Wentao Zhang",
        "Yu Zhang",
        "Yu Meng",
        "Xinyang Zhang",
        "Qi Zhu",
        "Jiawei Han"
      ],
      "abstract": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 389,
      "original_url": "https://aclanthology.org/2023.acl-long.388/",
      "title": "Soft Language Clustering for Multilingual Model Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.388.pdf",
      "authors": [
        "Jiali Zeng",
        "Yufan Jiang",
        "Yongjing Yin",
        "Yi Jing",
        "Fandong Meng",
        "Binghuai Lin",
        "Yunbo Cao",
        "Jie Zhou"
      ],
      "abstract": "Multilingual pre-trained language models have demonstrated impressive (zero-shot) cross-lingual transfer abilities, however, their performance is hindered when the target language has distant typologyfrom the source language or when pre-training data is limited in size. In this paper, we propose XLM-P, a method that contextually retrieves prompts as flexible guidance for encoding instances conditionally. Our space-efficient and model-agnostic XLM-P approach enables (1) lightweight modeling of language-invariant and language-specific knowledge across languages, and (2) easy integration with other multilingual pre-training methods. On the tasks of XTREME, which include text classification, sequence labeling, question answering, and sentence retrieval, both base- and large-size language models pre-trained with our proposed method exhibit consistent performance improvement. Furthermore, it provides substantial advantages for low-resource languages in unsupervised sentence retrieval and for target languages that differ greatly from the source language in cross-lingual transfer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 390,
      "original_url": "https://aclanthology.org/2023.acl-long.389/",
      "title": "Curriculum Learning for Graph Neural Networks: A Multiview Competence-based Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-long.389.pdf",
      "authors": [
        "Nidhi Vakil",
        "Hadi Amiri"
      ],
      "abstract": "A curriculum is a planned sequence of learning materials and an effective one can make learning efficient and effective for both humans and machines. Recent studies developed effective data-driven curriculum learning approaches for training graph neural networks in language applications. However, existing curriculum learning approaches often employ a single criterion of difficulty in their training paradigms. In this paper, we propose a new perspective on curriculum learning by introducing a novel approach that builds on graph complexity formalisms (as difficulty criteria) and model competence during training. The model consists of a scheduling scheme which derives effective curricula by accounting for different views of sample difficulty and model competence during training. The proposed solution advances existing research in curriculum learning for graph neural networks with the ability to incorporate a fine-grained spectrum of graph difficulty criteria in their training paradigms. Experimental results on real-world link prediction and node classification tasks illustrate the effectiveness of the proposed approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 391,
      "original_url": "https://aclanthology.org/2023.acl-long.390/",
      "title": "When and how to paraphrase for named entity recognition?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.390.pdf",
      "authors": [
        "Saket Sharma",
        "Aviral Joshi",
        "Yiyun Zhao",
        "Namrata Mukhija",
        "Hanoz Bhathena",
        "Prateek Singh",
        "Sashank Santhanam"
      ],
      "abstract": "While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of span-level label preservation. In this paper, we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, and dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength, while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 392,
      "original_url": "https://aclanthology.org/2023.acl-long.391/",
      "title": "UniEvent: Unified Generative Model with Multi-Dimensional Prefix for Zero-Shot Event-Relational Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.391.pdf",
      "authors": [
        "Zhengwei Tao",
        "Zhi Jin",
        "Haiyan Zhao",
        "Chengfeng Dou",
        "Yongqiang Zhao",
        "Tao Shen",
        "Chongyang Tao"
      ],
      "abstract": "Reasoning about events and their relations attracts surging research efforts since it is regarded as an indispensable ability to fulfill various event-centric or common-sense reasoning tasks. However, these tasks often suffer from limited data availability due to the labor-intensive nature of their annotations. Consequently, recent studies have explored knowledge transfer approaches within a multi-task learning framework to address this challenge. Although such methods have achieved acceptable results, such brute-force solutions struggle to effectively transfer event-relational knowledge due to the vast array of inter-event relations (e.g. temporal, causal, conditional) and reasoning formulations (e.g. discriminative, abductive, ending prediction). To enhance knowledge transfer and enable zero-shot generalization among various combinations, in this work we propose a novel unified framework, called UNIEVENT. Inspired by prefix-based multitask learning, our approach organizes event relational reasoning tasks into a coordinate system with multiple axes, representing inter-event relations and reasoning formulations. We then train a unified text-to-text generative model that utilizes coordinate-assigning prefixes for each task. By leveraging our adapted prefixes, our unified model achieves state-of-the-art or competitive performance on both zero-shot and supervised reasoning tasks, as demonstrated in extensive experiments"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 393,
      "original_url": "https://aclanthology.org/2023.acl-long.392/",
      "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales",
      "pdf_url": "https://aclanthology.org/2023.acl-long.392.pdf",
      "authors": [
        "Brihi Joshi",
        "Ziyi Liu",
        "Sahana Ramnath",
        "Aaron Chan",
        "Zhewei Tong",
        "Shaoliang Nie",
        "Qifan Wang",
        "Yejin Choi",
        "Xiang Ren"
      ],
      "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale’s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs’ ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 394,
      "original_url": "https://aclanthology.org/2023.acl-long.393/",
      "title": "Automatic Annotation of Direct Speech in Written French Narratives",
      "pdf_url": "https://aclanthology.org/2023.acl-long.393.pdf",
      "authors": [
        "Noé Durandard",
        "Viet Anh Tran",
        "Gaspard Michel",
        "Elena Epure"
      ],
      "abstract": "The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many works exist. Our goal is to create a unified framework to design and evaluate AADS models in French. For this, we consolidated the largest-to-date French narrative dataset annotated with DS per word; we adapted various baselines for sequence labelling or from AADS in other languages; and we designed and conducted an extensive evaluation focused on generalisation. Results show that the task still requires substantial efforts and emphasise characteristics of each baseline. Although this framework could be improved, it is a step further to encourage more research on the topic."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 395,
      "original_url": "https://aclanthology.org/2023.acl-long.394/",
      "title": "Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.394.pdf",
      "authors": [
        "Hyunjae Kim",
        "Jaehyo Yoo",
        "Seunghyun Yoon",
        "Jaewoo Kang"
      ],
      "abstract": "Most weakly supervised named entity recognition (NER) models rely on domain-specific dictionaries provided by experts. This approach is infeasible in many domains where dictionaries do not exist. While a phrase retrieval model was used to construct pseudo-dictionaries with entities retrieved from Wikipedia automatically in a recent study, these dictionaries often have limited coverage because the retriever is likely to retrieve popular entities rather than rare ones. In this study, we present a novel framework, HighGEN, that generates NER datasets with high-coverage pseudo-dictionaries. Specifically, we create entity-rich dictionaries with a novel search method, called phrase embedding search, which encourages the retriever to search a space densely populated with various entities. In addition, we use a new verification process based on the embedding distance between candidate entity mentions and entity types to reduce the false-positive noise in weak labels generated by high-coverage dictionaries. We demonstrate that HighGEN outperforms the previous best model by an average F1 score of 4.7 across five NER benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 396,
      "original_url": "https://aclanthology.org/2023.acl-long.395/",
      "title": "Dynamic Transformers Provide a False Sense of Efficiency",
      "pdf_url": "https://aclanthology.org/2023.acl-long.395.pdf",
      "authors": [
        "Yiming Chen",
        "Simin Chen",
        "Zexin Li",
        "Wei Yang",
        "Cong Liu",
        "Robby Tan",
        "Haizhou Li"
      ],
      "abstract": "Despite much success in natural language processing (NLP), pre-trained language models typically lead to a high computational cost during inference. Multi-exit is a mainstream approach to address this issue by making a trade-off between efficiency and accuracy, where the saving of computation comes from an early exit. However, whether such saving from early-exiting is robust remains unknown. Motivated by this, we first show that directly adapting existing adversarial attack approaches targeting model accuracy cannot significantly reduce inference efficiency. To this end, we propose a simple yet effective attacking framework, SAME, a novel slowdown attack framework on multi-exit models, which is specially tailored to reduce the efficiency of the multi-exit models. By leveraging the multi-exit models’ design characteristics, we utilize all internal predictions to guide the adversarial sample generation instead of merely considering the final prediction. Experiments on the GLUE benchmark show that SAME can effectively diminish the efficiency gain of various multi-exit models by 80% on average, convincingly validating its effectiveness and generalization ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 397,
      "original_url": "https://aclanthology.org/2023.acl-long.396/",
      "title": "Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features",
      "pdf_url": "https://aclanthology.org/2023.acl-long.396.pdf",
      "authors": [
        "Ester Hlavnova",
        "Sebastian Ruder"
      ],
      "abstract": "A challenge towards developing NLP systems for the world’s languages is understanding how they generalize to typological differences relevant for real-world applications. To this end, we propose M2C, a morphologically-aware framework for behavioral testing of NLP models. We use M2C to generate tests that probe models’ behavior in light of specific linguistic features in 12 typologically diverse languages. We evaluate state-of-the-art language models on the generated tests. While models excel at most tests in English, we highlight generalization failures to specific typological characteristics such as temporal expressions in Swahili and compounding possessives in Finish. Our findings motivate the development of models that address these blind spots."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 398,
      "original_url": "https://aclanthology.org/2023.acl-long.397/",
      "title": "Local Byte Fusion for Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.397.pdf",
      "authors": [
        "Makesh Narsimhan Sreedhar",
        "Xiangpeng Wan",
        "Yu Cheng",
        "Junjie Hu"
      ],
      "abstract": "Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus may not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes oversegment low-resource languages, leading to a drop in translation performance. An alternative to subword tokenizers is byte-based tokenization, i.e., tokenization into byte sequences using the UTF-8 encoding scheme. Byte tokens often represent inputs at a sub-character granularity, i.e., one character can be represented by a span of byte tokens. This results in much longer byte sequences that are hard to interpret without aggregating local information from multiple byte tokens. In this paper, we propose a Local Byte Fusion (LOBEF) method for byte-based machine translation—utilizing byte n-gram and word boundaries—to aggregate local semantic information. Extensive experiments on multilingual translation, zero-shot cross-lingual transfer, and domain adaptation reveal a consistent improvement over vanilla byte-based models. Further analysis also indicates that our byte-based models are parameter-efficient and perform competitive to subword models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 399,
      "original_url": "https://aclanthology.org/2023.acl-long.398/",
      "title": "Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.398.pdf",
      "authors": [
        "Benjamin Minixhofer",
        "Jonas Pfeiffer",
        "Ivan Vulić"
      ],
      "abstract": "Many NLP pipelines split text into sentences as one of the crucial preprocessing steps. Prior sentence segmentation tools either rely on punctuation or require a considerable amount of sentence-segmented training data: both central assumptions might fail when porting sentence segmenters to diverse languages on a massive scale. In this work, we thus introduce a multilingual punctuation-agnostic sentence segmentation method, currently covering 85 languages, trained in a self-supervised fashion on unsegmented text, by making use of newline characters which implicitly perform segmentation into paragraphs. We further propose an approach that adapts our method to the segmentation in a given corpus by using only a small number (64-256) of sentence-segmented examples. The main results indicate that our method outperforms all the prior best sentence-segmentation tools by an average of 6.1% F1 points. Furthermore, we demonstrate that proper sentence segmentation has a point: the use of a (powerful) sentence segmenter makes a considerable difference for a downstream application such as machine translation (MT). By using our method to match sentence segmentation to the segmentation used during training of MT models, we achieve an average improvement of 2.3 BLEU points over the best prior segmentation tool, as well as massive gains over a trivial segmenter that splits text into equally-sized blocks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 400,
      "original_url": "https://aclanthology.org/2023.acl-long.399/",
      "title": "Multi-target Backdoor Attacks for Code Pre-trained Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.399.pdf",
      "authors": [
        "Yanzhou Li",
        "Shangqing Liu",
        "Kangjie Chen",
        "Xiaofei Xie",
        "Tianwei Zhang",
        "Yang Liu"
      ],
      "abstract": "Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experimental results demonstrate that our approach effectively and stealthily attacks code-related downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 401,
      "original_url": "https://aclanthology.org/2023.acl-long.400/",
      "title": "Learning Better Masking for Better Language Model Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.400.pdf",
      "authors": [
        "Dongjie Yang",
        "Zhuosheng Zhang",
        "Hai Zhao"
      ],
      "abstract": "Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 402,
      "original_url": "https://aclanthology.org/2023.acl-long.401/",
      "title": "VisText: A Benchmark for Semantically Rich Chart Captioning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.401.pdf",
      "authors": [
        "Benny Tang",
        "Angie Boggust",
        "Arvind Satyanarayan"
      ],
      "abstract": "Captions that describe or explain charts help improve recall and comprehension of the depicted data and provide a more accessible medium for people with visual disabilities. However, current approaches for automatically generating such captions struggle to articulate the perceptual or cognitive features that are the hallmark of charts (e.g., complex trends and patterns). In response, we introduce VisText: a dataset of 12,441 pairs of charts and captions that describe the charts’ construction, report key statistics, and identify perceptual and cognitive phenomena. In VisText, a chart is available as three representations: a rasterized image, a backing data table, and a scene graph—a hierarchical representation of a chart’s visual elements akin to a web page’s Document Object Model (DOM). To evaluate the impact of VisText, we fine-tune state-of-the-art language models on our chart captioning task and apply prefix-tuning to produce captions that vary the semantic content they convey. Our models generate coherent, semantically rich captions and perform on par with state-of-the-art chart captioning models across machine translation and text generation metrics. Through qualitative analysis, we identify six broad categories of errors that our models make that can inform future work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 403,
      "original_url": "https://aclanthology.org/2023.acl-long.402/",
      "title": "Byte-Level Grammatical Error Correction Using Synthetic and Curated Corpora",
      "pdf_url": "https://aclanthology.org/2023.acl-long.402.pdf",
      "authors": [
        "Svanhvít Lilja Ingólfsdóttir",
        "Petur Ragnarsson",
        "Haukur Jónsson",
        "Haukur Simonarson",
        "Vilhjalmur Thorsteinsson",
        "Vésteinn Snæbjarnarson"
      ],
      "abstract": "Grammatical error correction (GEC) is the task of correcting typos, spelling, punctuation and grammatical issues in text. Approaching the problem as a sequence-to-sequence task, we compare the use of a common subword unit vocabulary and byte-level encoding. Initial synthetic training data is created using an error-generating pipeline, and used for finetuning two subword-level models and one byte-level model. Models are then finetuned further on hand-corrected error corpora, including texts written by children, university students, dyslexic and second-language writers, and evaluated over different error types and error origins. We show that a byte-level model enables higher correction quality than a subword approach, not only for simple spelling errors, but also for more complex semantic, stylistic and grammatical issues. In particular, initial training on synthetic corpora followed by finetuning on a relatively small parallel corpus of real-world errors helps the byte-level model correct a wide range of commonly occurring errors. Our experiments are run for the Icelandic language but should hold for other similar languages, and in particular to morphologically rich ones."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 404,
      "original_url": "https://aclanthology.org/2023.acl-long.403/",
      "title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.403.pdf",
      "authors": [
        "Qianhui Wu",
        "Huiqiang Jiang",
        "Haonan Yin",
        "Börje Karlsson",
        "Chin-Yew Lin"
      ],
      "abstract": "Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 405,
      "original_url": "https://aclanthology.org/2023.acl-long.404/",
      "title": "Peeking inside the black box: A Commonsense-aware Generative Framework for Explainable Complaint Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.404.pdf",
      "authors": [
        "Apoorva Singh",
        "Raghav Jain",
        "Prince Jha",
        "Sriparna Saha"
      ],
      "abstract": "Complaining is an illocutionary act in which the speaker communicates his/her dissatisfaction with a set of circumstances and holds the hearer (the complainee) answerable, directly or indirectly. Considering breakthroughs in machine learning approaches, the complaint detection task has piqued the interest of the natural language processing (NLP) community. Most of the earlier studies failed to justify their findings, necessitating the adoption of interpretable models that can explain the model’s output in real time. We introduce an explainable complaint dataset, X-CI, the first benchmark dataset for explainable complaint detection. Each instance in the X-CI dataset is annotated with five labels: complaint label, emotion label, polarity label, complaint severity level, and rationale (explainability), i.e., the causal span explaining the reason for the complaint/non-complaint label. We address the task of explainable complaint detection and propose a commonsense-aware unified generative framework by reframing the multitask problem as a text-to-text generation task. Our framework can predict the complaint cause, severity level, emotion, and polarity of the text in addition to detecting whether it is a complaint or not. We further establish the advantages of our proposed model on various evaluation metrics over the state-of-the-art models and other baselines when applied to the X-CI dataset in both full and few-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 406,
      "original_url": "https://aclanthology.org/2023.acl-long.405/",
      "title": "MMDialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.405.pdf",
      "authors": [
        "Jiazhan Feng",
        "Qingfeng Sun",
        "Can Xu",
        "Pu Zhao",
        "Yaming Yang",
        "Chongyang Tao",
        "Dongyan Zhao",
        "Qingwei Lin"
      ],
      "abstract": "Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in https://github.com/victorsungo/MMDialog."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 407,
      "original_url": "https://aclanthology.org/2023.acl-long.406/",
      "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.406.pdf",
      "authors": [
        "Jonas Belouadi",
        "Steffen Eger"
      ],
      "abstract": "State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 408,
      "original_url": "https://aclanthology.org/2023.acl-long.407/",
      "title": "Envisioning Future from the Past: Hierarchical Duality Learning for Multi-Turn Dialogue Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.407.pdf",
      "authors": [
        "Ang Lv",
        "Jinpeng Li",
        "Shufang Xie",
        "Rui Yan"
      ],
      "abstract": "In this paper, we define a widely neglected property in dialogue text, duality, which is a hierarchical property that is reflected in human behaviours in daily conversations: Based on the logic in a conversation (or a sentence), people can infer follow-up utterances (or tokens) based on the previous text, and vice versa. We propose a hierarchical duality learning for dialogue (HDLD) to simulate this human cognitive ability, for generating high quality responses that connect both previous and follow-up dialogues. HDLD utilizes hierarchical dualities at token hierarchy and utterance hierarchy. HDLD maximizes the mutual information between past and future utterances. Thus, even if future text is invisible during inference, HDLD is capable of estimating future information implicitly based on dialogue history and generates both coherent and informative responses. In contrast to previous approaches that solely utilize future text as auxiliary information to encode during training, HDLD leverages duality to enable interaction between dialogue history and the future. This enhances the utilization of dialogue data, leading to the improvement in both automatic and human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 409,
      "original_url": "https://aclanthology.org/2023.acl-long.408/",
      "title": "DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.408.pdf",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "abstract": "Capturing complex contextual dependencies plays a vital role in Emotion Recognition in Conversations (ERC). Previous studies have predominantly focused on speaker-aware context modeling, overlooking the discourse structure of the conversation. In this paper, we introduce Dual Graph ATtention networks (DualGATs) to concurrently consider the complementary aspects of discourse structure and speaker-aware context, aiming for more precise ERC. Specifically, we devise a Discourse-aware GAT (DisGAT) module to incorporate discourse structural information by analyzing the discourse dependencies between utterances. Additionally, we develop a Speaker-aware GAT (SpkGAT) module to incorporate speaker-aware contextual information by considering the speaker dependencies between utterances. Furthermore, we design an interaction module that facilitates the integration of the DisGAT and SpkGAT modules, enabling the effective interchange of relevant information between the two modules. We extensively evaluate our method on four datasets, and experimental results demonstrate that our proposed DualGATs surpass state-of-the-art baselines on the majority of the datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 410,
      "original_url": "https://aclanthology.org/2023.acl-long.409/",
      "title": "Consistent Prototype Learning for Few-Shot Continual Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.409.pdf",
      "authors": [
        "Xiudi Chen",
        "Hui Wu",
        "Xiaodong Shi"
      ],
      "abstract": "Few-shot continual relation extraction aims to continually train a model on incrementally few-shot data to learn new relations while avoiding forgetting old ones. However, current memory-based methods are prone to overfitting memory samples, resulting in insufficient activation of old relations and limited ability to handle the confusion of similar classes. In this paper, we design a new N-way-K-shot Continual Relation Extraction (NK-CRE) task and propose a novel few-shot continual relation extraction method with Consistent Prototype Learning (ConPL) to address the aforementioned issues. Our proposed ConPL is mainly composed of three modules: 1) a prototype-based classification module that provides primary relation predictions under few-shot continual learning; 2) a memory-enhanced module designed to select vital samples and refined prototypical representations as a novel multi-information episodic memory; 3) a consistent learning module to reduce catastrophic forgetting by enforcing distribution consistency. To effectively mitigate catastrophic forgetting, ConPL ensures that the samples and prototypes in the episodic memory remain consistent in terms of classification and distribution. Additionally, ConPL uses prompt learning to extract better representations and adopts a focal loss to alleviate the confusion of similar classes. Experimental results on two commonly-used datasets show that our model consistently outperforms other competitive baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 411,
      "original_url": "https://aclanthology.org/2023.acl-long.410/",
      "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.410.pdf",
      "authors": [
        "Myles Foley",
        "Ambrish Rawat",
        "Taesung Lee",
        "Yufang Hou",
        "Gabriele Picco",
        "Giulio Zizzo"
      ],
      "abstract": "The wide applicability and adaptability of generative large language models (LLMs) has enabled their rapid adoption. While the pre-trained models can perform many tasks, such models are often fine-tuned to improve their performance on various downstream applications. However, this leads to issues over violation of model licenses, model theft, and copyright infringement. Moreover, recent advances show that generative technology is capable of producing harmful content which exacerbates the problems of accountability within model supply chains. Thus, we need a method to investigate how a model was trained or a piece of text was generated and what their pre-trained base model was. In this paper we take the first step to address this open problem by tracing back the origin of a given fine-tuned LLM to its corresponding pre-trained base model. We consider different knowledge levels and attribution strategies, and find that we can correctly trace back 8 out of the 10 fine tuned models with our best method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 412,
      "original_url": "https://aclanthology.org/2023.acl-long.411/",
      "title": "Large Language Models Meet NL2Code: A Survey",
      "pdf_url": "https://aclanthology.org/2023.acl-long.411.pdf",
      "authors": [
        "Daoguang Zan",
        "Bei Chen",
        "Fengji Zhang",
        "Dianjie Lu",
        "Bingchao Wu",
        "Bei Guan",
        "Wang Yongji",
        "Jian-Guang Lou"
      ],
      "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are “Large Size, Premium Data, Expert Tuning”. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 413,
      "original_url": "https://aclanthology.org/2023.acl-long.412/",
      "title": "When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.412.pdf",
      "authors": [
        "Jingwei Ni",
        "Zhijing Jin",
        "Qian Wang",
        "Mrinmaya Sachan",
        "Markus Leippold"
      ],
      "abstract": "Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work – sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works. Previous studies show that MTL performance can be improved by algorithmic tricks. However, what tasks and skills should be included is less well explored. In this work, we conduct a case study in Financial NLP where multiple datasets exist for skills relevant to the domain, such as numeric reasoning and sentiment analysis. Due to the task difficulty and data scarcity in the Financial NLP domain, we explore when aggregating such diverse skills from multiple datasets with MTL can work. Our findings suggest that the key to MTL success lies in skill diversity, relatedness between tasks, and choice of aggregation size and shared capacity. Specifically, MTL works well when tasks are diverse but related, and when the size of the task aggregation and the shared capacity of the model are balanced to avoid overwhelming certain tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 414,
      "original_url": "https://aclanthology.org/2023.acl-long.413/",
      "title": "Enhancing Grammatical Error Correction Systems with Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.413.pdf",
      "authors": [
        "Yuejiao Fei",
        "Leyang Cui",
        "Sen Yang",
        "Wai Lam",
        "Zhenzhong Lan",
        "Shuming Shi"
      ],
      "abstract": "Grammatical error correction systems improve written communication by detecting and correcting language mistakes. To help language learners better understand why the GEC system makes a certain correction, the causes of errors (evidence words) and the corresponding error types are two key factors. To enhance GEC systems with explanations, we introduce EXPECT, a large dataset annotated with evidence words and grammatical error types. We propose several baselines and anlysis to understand this task. Furthermore, human evaluation verifies our explainable GEC system’s explanations can assist second-language learners in determining whether to accept a correction suggestion and in understanding the associated grammar rule."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 415,
      "original_url": "https://aclanthology.org/2023.acl-long.414/",
      "title": "Linguistic representations for fewer-shot relation extraction across domains",
      "pdf_url": "https://aclanthology.org/2023.acl-long.414.pdf",
      "authors": [
        "Sireesh Gururaja",
        "Ritam Dutt",
        "Tinglong Liao",
        "Carolyn Rosé"
      ],
      "abstract": "Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 416,
      "original_url": "https://aclanthology.org/2023.acl-long.415/",
      "title": "DarkBERT: A Language Model for the Dark Side of the Internet",
      "pdf_url": "https://aclanthology.org/2023.acl-long.415.pdf",
      "authors": [
        "Youngjin Jin",
        "Eugene Jang",
        "Jian Cui",
        "Jin-Woo Chung",
        "Yongjae Lee",
        "Seungwon Shin"
      ],
      "abstract": "Recent research has suggested that there are clear differences in the language used in the Dark Web compared to that of the Surface Web. As studies on the Dark Web commonly require textual analysis of the domain, language models specific to the Dark Web may provide valuable insights to researchers. In this work, we introduce DarkBERT, a language model pretrained on Dark Web data. We describe the steps taken to filter and compile the text data used to train DarkBERT to combat the extreme lexical and structural diversity of the Dark Web that may be detrimental to building a proper representation of the domain. We evaluate DarkBERT and its vanilla counterpart along with other widely used language models to validate the benefits that a Dark Web domain specific model offers in various use cases. Our evaluations show that DarkBERT outperforms current language models and may serve as a valuable resource for future research on the Dark Web."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 417,
      "original_url": "https://aclanthology.org/2023.acl-long.416/",
      "title": "MDACE: MIMIC Documents Annotated with Code Evidence",
      "pdf_url": "https://aclanthology.org/2023.acl-long.416.pdf",
      "authors": [
        "Hua Cheng",
        "Rana Jafari",
        "April Russell",
        "Russell Klopfer",
        "Edmond Lu",
        "Benjamin Striner",
        "Matthew Gormley"
      ],
      "abstract": "We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply predicting a set of final codes for a patient encounter is insufficient as CAC systems are required to provide supporting textual evidence to justify the billing codes. A model able to produce accurate and reliable supporting evidence for each code would be a tremendous benefit. However, a human annotated code evidence corpus is extremely difficult to create because it requires specialized knowledge. In this paper, we introduce MDACE, the first publicly available code evidence dataset, which is built on a subset of the MIMIC-III clinical records. The dataset – annotated by professional medical coders – consists of 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evidence spans. We implemented several evidence extraction methods based on the EffectiveCAN model (Liu et al., 2021) to establish baseline performance on this dataset. MDACE can be used to evaluate code evidence extraction methods for CAC systems, as well as the accuracy and interpretability of deep learning models for multi-label classification. We believe that the release of MDACE will greatly improve the understanding and application of deep learning technologies for medical coding and document classification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 418,
      "original_url": "https://aclanthology.org/2023.acl-long.417/",
      "title": "Towards Zero-Shot Multilingual Transfer for Code-Switched Responses",
      "pdf_url": "https://aclanthology.org/2023.acl-long.417.pdf",
      "authors": [
        "Ting-Wei Wu",
        "Changsheng Zhao",
        "Ernie Chang",
        "Yangyang Shi",
        "Pierce Chuang",
        "Vikas Chandra",
        "Biing Juang"
      ],
      "abstract": "Recent task-oriented dialog systems have had great success in building English-based personal assistants, but extending these systems to a global audience is challenging due to the need for annotated data in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of code-switched responses. We propose a new adapter-based framework that allows for efficient transfer by learning task-specific representations and encapsulating source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 419,
      "original_url": "https://aclanthology.org/2023.acl-long.418/",
      "title": "One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.418.pdf",
      "authors": [
        "Guangtao Zeng",
        "Peiyuan Zhang",
        "Wei Lu"
      ],
      "abstract": "Fine-tuning pre-trained language models for multiple tasks can be expensive in terms of storage. Parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose ProPETL, a novel method that enables efficient sharing of a single prototype PETL network (e.g. adapter, LoRA, and prefix-tuning) across layers and tasks. We learn binary masks to select different sub-networks from the prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial structural information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate ProPETL on various downstream tasks and show that it can outperform other PETL methods with around 10% parameters required by the latter."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 420,
      "original_url": "https://aclanthology.org/2023.acl-long.419/",
      "title": "Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk",
      "pdf_url": "https://aclanthology.org/2023.acl-long.419.pdf",
      "authors": [
        "Jianquan Li",
        "XiangBo Wu",
        "Xiaokang Liu",
        "Qianqian Xie",
        "Prayag Tiwari",
        "Benyou Wang"
      ],
      "abstract": "Language is the principal tool for human communication, in which humor is one of the most attractive parts. Producing natural language like humans using computers, a.k.a, Natural Language Generation (NLG), has been widely used for dialogue systems, chatbots, machine translation, as well as computer-aid creation e.g., idea generations, scriptwriting. However, the humor aspect of natural language is relatively under-investigated, especially in the age of pre-trained language models. In this work, we aim to preliminarily test *whether NLG can generate humor as humans do*. We build a largest dataset consisting of numerous **C**hinese **C**omical **C**rosstalk scripts (called **C**3 in short), which is for a popular Chinese performing art called ‘Xiangsheng’ or ‘相声’ since 1800s.We benchmark various generation approaches including training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and large-scale PLMs (with and without fine-tuning). Moreover, we also conduct a human assessment, showing that 1) *large-scale pretraining largely improves crosstalk generation quality*; and 2) *even the scripts generated from the best PLM is far from what we expect*. We conclude humor generation could be largely improved using large-scaled PLMs, but it is still in its infancy. The data and benchmarking code are publicly available in [https://github.com/anonNo2/crosstalk-generation](https://github.com/anonNo2/crosstalk-generation)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 421,
      "original_url": "https://aclanthology.org/2023.acl-long.420/",
      "title": "Convergence and Diversity in the Control Hierarchy",
      "pdf_url": "https://aclanthology.org/2023.acl-long.420.pdf",
      "authors": [
        "Alexandra Butoi",
        "Ryan Cotterell",
        "David Chiang"
      ],
      "abstract": "Weir has defined a hierarchy of language classes whose second member (L2) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and L2 is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir’s definition of a controllable CFG (called a labeled distinguished CFG) to give a definition of controllable pushdown automata (PDAs), called labeled distinguished PDAs. This yields three new characterizations of L2 as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any kind of automaton we know of, so we invent one and call it a Pushdown Adjoining Automaton (PAA)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 422,
      "original_url": "https://aclanthology.org/2023.acl-long.421/",
      "title": "ConFEDE: Contrastive Feature Decomposition for Multimodal Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.421.pdf",
      "authors": [
        "Jiuding Yang",
        "Yakun Yu",
        "Di Niu",
        "Weidong Guo",
        "Yu Xu"
      ],
      "abstract": "Multimodal Sentiment Analysis aims to predict the sentiment of video content. Recent research suggests that multimodal sentiment analysis critically depends on learning a good representation of multimodal information, which should contain both modality-invariant representations that are consistent across modalities as well as modality-specific representations. In this paper, we propose ConFEDE, a unified learning framework that jointly performs contrastive representation learning and contrastive feature decomposition to enhance the representation of multimodal information. It decomposes each of the three modalities of a video sample, including text, video frames, and audio, into a similarity feature and a dissimilarity feature, which are learned by a contrastive relation centered around the text. We conducted extensive experiments on CH-SIMS, MOSI and MOSEI to evaluate various state-of-the-art multimodal sentiment analysis methods. Experimental results show that ConFEDE outperforms all baselines on these datasets on a range of metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 423,
      "original_url": "https://aclanthology.org/2023.acl-long.422/",
      "title": "Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic",
      "pdf_url": "https://aclanthology.org/2023.acl-long.422.pdf",
      "authors": [
        "Connor Pryor",
        "Quan Yuan",
        "Jeremiah Liu",
        "Mehran Kazemi",
        "Deepak Ramachandran",
        "Tania Bedrax-Weiss",
        "Lise Getoor"
      ],
      "abstract": "Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot learning, and out-of-domain generalization performance. Over three dialog structure induction datasets and across unsupervised and semi-supervised settings for standard and cross-domain generalization, the injection of symbolic knowledge using NEUPSL DSI provides a consistent boost in performance over the canonical baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 424,
      "original_url": "https://aclanthology.org/2023.acl-long.423/",
      "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
      "pdf_url": "https://aclanthology.org/2023.acl-long.423.pdf",
      "authors": [
        "Wenjun Peng",
        "Jingwei Yi",
        "Fangzhao Wu",
        "Shangxi Wu",
        "Bin Bin Zhu",
        "Lingjuan Lyu",
        "Binxing Jiao",
        "Tong Xu",
        "Guangzhong Sun",
        "Xing Xie"
      ],
      "abstract": "Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called {pasted macro ‘METHOD’} that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer’s model for copyright verification while minimizing the adverse impact on the original embeddings’ utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality. Our code is available at https://github.com/yjw1029/EmbMarker."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 425,
      "original_url": "https://aclanthology.org/2023.acl-long.424/",
      "title": "Answering Ambiguous Questions via Iterative Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-long.424.pdf",
      "authors": [
        "Weiwei Sun",
        "Hengyi Cai",
        "Hongshen Chen",
        "Pengjie Ren",
        "Zhumin Chen",
        "Maarten de Rijke",
        "Zhaochun Ren"
      ],
      "abstract": "In open-domain question answering, due to the ambiguity of questions, multiple plausible answers may exist. To provide feasible answers to an ambiguous question,one approach is to directly predict all valid answers, but this can struggle with balancing relevance and diversity. An alternative is to gather candidate answers and aggregate them, but this method can be computationally costly and may neglect dependencies among answers. In this paper, we present AmbigPrompt to address the imperfections of existing approaches to answering ambiguous questions. Specifically, we integrate an answering model with a prompting model in an iterative manner. The prompting model adaptively tracks the reading process and progressively triggers the answering model to compose distinct and relevant answers. Additionally, we develop a task-specific post-pretraining approach for both the answering model and the prompting model, which greatly improves the performance of our framework. Empirical studies on two commonly-used open benchmarks show that AmbigPrompt achieves state-of-the-art or competitive results while using less memory and having a lower inference latency than competing approaches. Additionally, AmbigPrompt also performs well in low-resource settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 426,
      "original_url": "https://aclanthology.org/2023.acl-long.425/",
      "title": "A Dataset of Argumentative Dialogues on Scientific Papers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.425.pdf",
      "authors": [
        "Federico Ruggeri",
        "Mohsen Mesgar",
        "Iryna Gurevych"
      ],
      "abstract": "With recent advances in question-answering models, various datasets have been collected to improve and study the effectiveness of these models on scientific texts. Questions and answers in these datasets explore a scientific paper by seeking factual information from the paper’s content. However, these datasets do not tackle the argumentative content of scientific papers, which is of huge importance in persuasiveness of a scientific discussion. We introduce ArgSciChat, a dataset of 41 argumentative dialogues between scientists on 20 NLP papers. The unique property of our dataset is that it includes both exploratory and argumentative questions and answers in a dialogue discourse on a scientific paper. Moreover, the size of ArgSciChat demonstrates the difficulties in collecting dialogues for specialized domains. Thus, our dataset is a challenging resource to evaluate dialogue agents in low-resource domains, in which collecting training data is costly. We annotate all sentences of dialogues in ArgSciChat and analyze them extensively. The results confirm that dialogues in ArgSciChat include exploratory and argumentative interactions. Furthermore, we use our dataset to fine-tune and evaluate a pre-trained document-grounded dialogue agent. The agent achieves a low performance on our dataset, motivating a need for dialogue agents with a capability to reason and argue about their answers. We publicly release ArgSciChat."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 427,
      "original_url": "https://aclanthology.org/2023.acl-long.426/",
      "title": "Massively Multilingual Lexical Specialization of Multilingual Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.426.pdf",
      "authors": [
        "Tommaso Green",
        "Simone Paolo Ponzetto",
        "Goran Glavaš"
      ],
      "abstract": "While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet’s multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multilingual lexical specialization brings substantial gains in two standard cross-lingual lexical tasks, bilingual lexicon induction and cross-lingual word similarity, as well as in cross-lingual sentence retrieval. Crucially, we observe gains for languages unseen in specialization, indicating that multilingual lexical specialization enables generalization to languages with no lexical constraints. In a series of subsequent controlled experiments, we show that the number of specialization constraints plays a much greater role than the set of languages from which they originate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 428,
      "original_url": "https://aclanthology.org/2023.acl-long.427/",
      "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.427.pdf",
      "authors": [
        "Afra Feyza Akyurek",
        "Ekin Akyurek",
        "Ashwin Kalyan",
        "Peter Clark",
        "Derry Tanti Wijaya",
        "Niket Tandon"
      ],
      "abstract": "Despite their unprecedented success, even the largest language models make mistakes. Similar to how humans learn and improve using feedback, previous work proposed providing language models with natural language feedback to guide them in repairing their outputs. Because human-generated critiques are expensive to obtain, researchers have devised learned critique generators in lieu of human critics while assuming one can train downstream models to utilize generated feedback. However, this approach does not apply to black-box or limited access models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of large general-purpose language agents, fine-tuning is neither computationally nor spatially efficient as it results in multiple copies of the network. In this work, we introduce RL4F (Reinforcement Learning for Feedback), a multi-agent collaborative framework where the critique generator is trained to maximize end-task performance of GPT-3, a fixed model more than 200 times its size. RL4F produces critiques that help GPT-3 revise its outputs. We study three datasets for action planning, summarization and alphabetization and show relative improvements up to 10% in multiple text similarity metrics over other learned, retrieval-augmented or prompting-based critique generators."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 429,
      "original_url": "https://aclanthology.org/2023.acl-long.428/",
      "title": "WebIE: Faithful and Robust Information Extraction on the Web",
      "pdf_url": "https://aclanthology.org/2023.acl-long.428.pdf",
      "authors": [
        "Chenxi Whitehouse",
        "Clara Vania",
        "Alham Fikri Aji",
        "Christos Christodoulopoulos",
        "Andrea Pierleoni"
      ],
      "abstract": "Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate ~25K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 430,
      "original_url": "https://aclanthology.org/2023.acl-long.429/",
      "title": "NormBank: A Knowledge Bank of Situational Social Norms",
      "pdf_url": "https://aclanthology.org/2023.acl-long.429.pdf",
      "authors": [
        "Caleb Ziems",
        "Jane Dwivedi-Yu",
        "Yi-Chia Wang",
        "Alon Halevy",
        "Diyi Yang"
      ],
      "abstract": "We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents’ contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NormBank contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic — one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NormBank. We further demonstrate the utility of this resource with a series of transfer experiments. For data and code, see https://github.com/SALT-NLP/normbank"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 431,
      "original_url": "https://aclanthology.org/2023.acl-long.430/",
      "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.430.pdf",
      "authors": [
        "CheolWon Na",
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "Automatic processing of source code, such as code clone detection and software vulnerability detection, is very helpful to software engineers. Large pre-trained Programming Language (PL) models (such as CodeBERT, GraphCodeBERT, CodeT5, etc.), show very powerful performance on these tasks. However, these PL models are vulnerable to adversarial examples that are generated with slight perturbation. Unlike natural language, an adversarial example of code must be semantic-preserving and compilable. Due to the requirements, it is hard to directly apply the existing attack methods for natural language models. In this paper, we propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model), a high-performance and effective black-box attack method to generate adversarial examples using dead code insertion. We evaluate our proposed method on 9 victim downstream-task large code models. Our method outperforms the state-of-the-art black-box attack in both attack efficiency and attack quality, while generated adversarial examples are compiled preserving semantic functionality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 432,
      "original_url": "https://aclanthology.org/2023.acl-long.431/",
      "title": "Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.431.pdf",
      "authors": [
        "Wei Liu",
        "Xiyan Fu",
        "Michael Strube"
      ],
      "abstract": "Coherence is an important aspect of text quality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document’s coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where we mine different subgraph patterns. We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs. Finally, a GCN is applied to the heterogeneous graph to model the connectivity relationships. We evaluate our method on two tasks, assessing discourse coherence and automated essay scoring. Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 433,
      "original_url": "https://aclanthology.org/2023.acl-long.432/",
      "title": "HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.432.pdf",
      "authors": [
        "He Zhu",
        "Chong Zhang",
        "Junjie Huang",
        "Junran Wu",
        "Ke Xu"
      ],
      "abstract": "Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowledge. Under such observation, we tend to investigate the feasibility of a memory-friendly model with strong generalization capability that could boost the performance of HTC without prior statistics or label semantics. In this paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance the text representations with only syntactic information of the label hierarchy. Specifically, we convert the label hierarchy into an unweighted tree structure, termed coding tree, with the guidance of structural entropy. Then we design a structure encoder to incorporate hierarchy-aware information in the coding tree into text representations. Besides the text encoder, HiTIN only contains a few multi-layer perceptions and linear transformations, which greatly saves memory. We conduct experiments on three commonly used datasets and the results demonstrate that HiTIN could achieve better test performance and less memory consumption than state-of-the-art (SOTA) methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 434,
      "original_url": "https://aclanthology.org/2023.acl-long.433/",
      "title": "Contextual Knowledge Learning for Dialogue Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.433.pdf",
      "authors": [
        "Wen Zheng",
        "Natasa Milic-Frayling",
        "Ke Zhou"
      ],
      "abstract": "Incorporating conversational context and knowledge into dialogue generation models has been essential for improving the quality of the generated responses. The context, comprising utterances from previous dialogue exchanges, is used as a source of content for response generation and as a means of selecting external knowledge. However, to avoid introducing irrelevant content, it is key to enable fine-grained scoring of context and knowledge. In this paper, we present a novel approach to context and knowledge weighting as an integral part of model training. We guide the model training through a Contextual Knowledge Learning (CKL) process which involves Latent Vectors for context and knowledge, respectively. CKL Latent Vectors capture the relationship between context, knowledge, and responses through weak supervision and enable differential weighting of context utterances and knowledge sentences during the training process. Experiments with two standard datasets and human evaluation demonstrate that CKL leads to a significant improvement compared with the performance of six strong baseline models and shows robustness with regard to reduced sizes of training sets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 435,
      "original_url": "https://aclanthology.org/2023.acl-long.434/",
      "title": "Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.434.pdf",
      "authors": [
        "Ke Wang",
        "Xin Ge",
        "Jiayi Wang",
        "Yuqi Zhang",
        "Yu Zhao"
      ],
      "abstract": "Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human’s edits. Translation Suggestion (TS), as an interactive mode to assist human translators, requires machines to generate alternatives for specific incorrect words or phrases selected by human translators. In this paper, we utilize the parameterized objective function of neural machine translation (NMT) and propose a novel constrained decoding algorithm, namely Prefix-Suffix Guided Decoding (PSGD), to deal with the TS problem without additional training. Compared to state-of-the-art lexical-constrained decoding method, PSGD improves translation quality by an average of 10.6 BLEU and reduces time overhead by an average of 63.4% on benchmark datasets. Furthermore, on both the WeTS and the WMT 2022 Translation Suggestion datasets, it is superior over other supervised learning systems trained with TS annotated data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 436,
      "original_url": "https://aclanthology.org/2023.acl-long.435/",
      "title": "Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated Parallel Corpus of Novels",
      "pdf_url": "https://aclanthology.org/2023.acl-long.435.pdf",
      "authors": [
        "Yuchen Eleanor Jiang",
        "Tianyu Liu",
        "Shuming Ma",
        "Dongdong Zhang",
        "Mrinmaya Sachan",
        "Ryan Cotterell"
      ],
      "abstract": "Several recent papers claim to have achieved human parity at sentence-level machine translation (MT)—especially between high-resource language pairs. In response, the MT community has, in part, shifted its focus to document-level translation. Translating documents requires a deeper understanding of the structure and meaning of text, which is often captured by various kinds of discourse phenomena such as consistency, coherence, and cohesion. However, this renders conventional sentence-level MT evaluation benchmarks inadequate for evaluating the performance of context-aware MT systems. This paperpresents a new dataset with rich discourse annotations, built upon the large-scale parallel corpus BWB introduced in Jiang et al. (2022a). The new BWB annotation introduces four extra evaluation aspects, i.e., entity, terminology, coreference, and quotation, covering 15,095 entity mentions in both languages. Using these annotations, we systematically investigate the similarities and differences between the discourse structures of source and target languages, and the challenges they pose to MT. We discover that MT outputs differ fundamentally from human translations in terms of their latent discourse structures. This gives us a new perspective on the challenges and opportunities in document-level MT. We make our resource publicly available to spur future research in document-level MT and its generalization to other language translation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 437,
      "original_url": "https://aclanthology.org/2023.acl-long.436/",
      "title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.436.pdf",
      "authors": [
        "Yan Zhou",
        "Qingkai Fang",
        "Yang Feng"
      ],
      "abstract": "End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport (CMOT) to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 438,
      "original_url": "https://aclanthology.org/2023.acl-long.437/",
      "title": "On the Evaluation of Neural Selective Prediction Methods for Natural Language Processing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.437.pdf",
      "authors": [
        "Zhengyao Gu",
        "Mark Hopkins"
      ],
      "abstract": "We provide a survey and empirical comparison of the state-of-the-art in neural selective classification for NLP tasks. We also provide a methodological blueprint, including a novel metric called refinement that provides a calibrated evaluation of confidence functions for selective prediction. Finally, we supply documented, open-source code to support the future development of selective prediction techniques."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 439,
      "original_url": "https://aclanthology.org/2023.acl-long.438/",
      "title": "Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.438.pdf",
      "authors": [
        "Tianshu Yu",
        "Haoyu Gao",
        "Ting-En Lin",
        "Min Yang",
        "Yuchuan Wu",
        "Wentao Ma",
        "Chao Wang",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task from textual dialog pre-training to speech-text dialog pre-training scenarios. Experimental results on four different downstream speech-text tasks demonstrate the superiority of SPECTRA in learning speech-text alignment and multi-turn dialog context."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 440,
      "original_url": "https://aclanthology.org/2023.acl-long.439/",
      "title": "Text Style Transfer with Contrastive Transfer Pattern Mining",
      "pdf_url": "https://aclanthology.org/2023.acl-long.439.pdf",
      "authors": [
        "Jingxuan Han",
        "Quan Wang",
        "Licheng Zhang",
        "Weidong Chen",
        "Yan Song",
        "Zhendong Mao"
      ],
      "abstract": "Text style transfer (TST) is an important task in natural language generation, which aims to alter the stylistic attributes (e.g., sentiment) of a sentence and keep its semantic meaning unchanged. Most existing studies mainly focus on the transformation between styles, yet ignore that this transformation can be actually carried out via different hidden transfer patterns. To address this problem, we propose a novel approach, contrastive transfer pattern mining (CTPM), which automatically mines and utilizes inherent latent transfer patterns to improve the performance of TST. Specifically, we design an adaptive clustering module to automatically discover hidden transfer patterns from the data, and introduce contrastive learning based on the discovered patterns to obtain more accurate sentence representations, and thereby benefit the TST task. To the best of our knowledge, this is the first work that proposes the concept of transfer patterns in TST, and our approach can be applied in a plug-and-play manner to enhance other TST methods to further improve their performance. Extensive experiments on benchmark datasets verify the effectiveness and generality of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 441,
      "original_url": "https://aclanthology.org/2023.acl-long.440/",
      "title": "Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.440.pdf",
      "authors": [
        "Zhenrui Yue",
        "Huimin Zeng",
        "Mengfei Lan",
        "Heng Ji",
        "Dong Wang"
      ],
      "abstract": "With emerging online topics as a source for numerous new events, detecting unseen / rare event types presents an elusive challenge for existing event detection methods, where only limited data access is provided for training. To address the data scarcity problem in event detection, we propose MetaEvent, a meta learning-based framework for zero- and few-shot event detection. Specifically, we sample training tasks from existing event types and perform meta training to search for optimal parameters that quickly adapt to unseen tasks. In our framework, we propose to use the cloze-based prompt and a trigger-aware soft verbalizer to efficiently project output to unseen event types. Moreover, we design a contrastive meta objective based on maximum mean discrepancy (MMD) to learn class-separating features. As such, the proposed MetaEvent can perform zero-shot event detection by mapping features to event types without any prior knowledge. In our experiments, we demonstrate the effectiveness of MetaEvent in both zero-shot and few-shot scenarios, where the proposed method achieves state-of-the-art performance in extensive experiments on benchmark datasets FewEvent and MAVEN."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 442,
      "original_url": "https://aclanthology.org/2023.acl-long.441/",
      "title": "Text Style Transfer Back-Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.441.pdf",
      "authors": [
        "Daimeng Wei",
        "Zhanglin Wu",
        "Hengchao Shang",
        "Zongyao Li",
        "Minghan Wang",
        "Jiaxin Guo",
        "Xiaoyu Chen",
        "Zhengzhe Yu",
        "Hao Yang"
      ],
      "abstract": "Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a generalized data augmentation method. Our training code and text style transfer model are open-sourced."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 443,
      "original_url": "https://aclanthology.org/2023.acl-long.442/",
      "title": "Generating Visual Spatial Description via Holistic 3D Scene Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.442.pdf",
      "authors": [
        "Yu Zhao",
        "Hao Fei",
        "Wei Ji",
        "Jianguo Wei",
        "Meishan Zhang",
        "Min Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 444,
      "original_url": "https://aclanthology.org/2023.acl-long.443/",
      "title": "Continual Knowledge Distillation for Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.443.pdf",
      "authors": [
        "Yuanchi Zhang",
        "Peng Li",
        "Maosong Sun",
        "Yang Liu"
      ],
      "abstract": "While many parallel corpora are not publicly accessible for data copyright, data privacy and competitive differentiation reasons, trained translation models are increasingly available on open platforms. In this work, we propose a method called continual knowledge distillation to take advantage of existing translation models to improve one model of interest. The basic idea is to sequentially transfer knowledge from each trained model to the distilled model. Extensive experiments on Chinese-English and German-English datasets show that our method achieves significant and consistent improvements over strong baselines under both homogeneous and heterogeneous trained model settings and is robust to malicious models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 445,
      "original_url": "https://aclanthology.org/2023.acl-long.444/",
      "title": "Query Refinement Prompts for Closed-Book Long-Form QA",
      "pdf_url": "https://aclanthology.org/2023.acl-long.444.pdf",
      "authors": [
        "Reinald Kim Amplayo",
        "Kellie Webster",
        "Michael Collins",
        "Dipanjan Das",
        "Shashi Narayan"
      ],
      "abstract": "Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once – to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 446,
      "original_url": "https://aclanthology.org/2023.acl-long.445/",
      "title": "CONE: An Efficient COarse-to-fiNE Alignment Framework for Long Video Temporal Grounding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.445.pdf",
      "authors": [
        "Zhijian Hou",
        "Wanjun Zhong",
        "Lei Ji",
        "Difei Gao",
        "Kun Yan",
        "W.k. Chan",
        "Chong-Wah Ngo",
        "Mike Zheng Shou",
        "Nan Duan"
      ],
      "abstract": "This paper tackles an emerging and challenging problem of long video temporal grounding (VTG) that localizes video moments related to a natural language (NL) query. Compared with short videos, long videos are also highly demanded but less explored, which brings new challenges in higher inference computation cost and weaker multi-modal alignment. To address these challenges, we propose CONE, an efficient COarse-to-fiNE alignment framework. CONE is a plug-and-play framework on top of existing VTG models to handle long videos through a sliding window mechanism. Specifically, CONE (1) introduces a query-guided window selection strategy to speed up inference, and (2) proposes a coarse-to-fine mechanism via a novel incorporation of contrastive learning to enhance multi-modal alignment for long videos. Extensive experiments on two large-scale long VTG benchmarks consistently show both substantial performance gains (e.g., from 3.13 to 6.87% on MAD) and state-of-the-art results. Analyses also reveal higher efficiency as the query-guided window selection mechanism accelerates inference time by 2x on Ego4D-NLQ and 15x on MAD while keeping SOTA results. Codes have been released at https://github.com/houzhijian/CONE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 447,
      "original_url": "https://aclanthology.org/2023.acl-long.446/",
      "title": "Few-Shot Document-Level Event Argument Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.446.pdf",
      "authors": [
        "Xianjun Yang",
        "Yujie Lu",
        "Linda Petzold"
      ],
      "abstract": "Event argument extraction (EAE) has been well studied at the sentence level but under-explored at the document level. In this paper, we study to capture event arguments that actually spread across sentences in documents. Prior works usually assume full access to rich document supervision, ignoring the fact that the available argument annotation is limited in production. To fill this gap, we present FewDocAE, a Few-Shot Document-Level Event Argument Extraction benchmark, based on the existing document-level event extraction dataset. We first define the new problem and reconstruct the corpus by a novel N-Way-D-Doc sampling instead of the traditional N-Way-K-Shot strategy. Then we adjust the current document-level neural models into the few-shot setting to provide baseline results under in- and cross-domain settings. Since the argument extraction depends on the context from multiple sentences and the learning process is limited to very few examples, we find this novel task to be very challenging with substantively low performance. Considering FewDocAE is closely related to practical use under low-resource regimes, we hope this benchmark encourages more research in this direction. Our data and codes will be available online."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 448,
      "original_url": "https://aclanthology.org/2023.acl-long.447/",
      "title": "ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.447.pdf",
      "authors": [
        "Kuan-Hao Huang",
        "Varun Iyer",
        "I-Hung Hsu",
        "Anoop Kumar",
        "Kai-Wei Chang",
        "Aram Galstyan"
      ],
      "abstract": "Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity – the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of ParaAMR for improving various NLP applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 449,
      "original_url": "https://aclanthology.org/2023.acl-long.448/",
      "title": "Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.448.pdf",
      "authors": [
        "Songming Zhang",
        "Yunlong Liang",
        "Shuaibo Wang",
        "Yufeng Chen",
        "Wenjuan Han",
        "Jian Liu",
        "Jinan Xu"
      ],
      "abstract": "Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a new method named Top-1 Information Enhanced Knowledge Distillation (TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the learning of the top-1 information from the teacher. Additionally, we develop an iterative KD procedure to infuse more additional knowledge by distilling on the data without ground-truth targets. Experiments on WMT’14 English-German, WMT’14 English-French and WMT’16 English-Romanian demonstrate that our method can respectively boost Transformerbase students by +1.04, +0.60 and +1.11 BLEU scores and significantly outperforms the vanilla word-level KD baseline. Besides, our method shows higher generalizability on different teacher-student capacity gaps than existing KD techniques."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 450,
      "original_url": "https://aclanthology.org/2023.acl-long.449/",
      "title": "Multi-Row, Multi-Span Distant Supervision For Table+Text Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.449.pdf",
      "authors": [
        "Vishwajeet Kumar",
        "Yash Gupta",
        "Saneem Chemmengath",
        "Jaydeep Sen",
        "Soumen Chakrabarti",
        "Samarth Bharadwaj",
        "Feifei Pan"
      ],
      "abstract": "Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table cells across table rows but also multiple text spans within the scope of a table row and its associated text. This leads to a noisy multi-instance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi-instance distant supervision approach helps MITQA get sate-of-the-art results beating the existing baselines for both HybridQA and OTT-QA, putting MITQA at the top of HybridQA leaderboard with best EM and F1 scores on a held out test set."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 451,
      "original_url": "https://aclanthology.org/2023.acl-long.450/",
      "title": "HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level",
      "pdf_url": "https://aclanthology.org/2023.acl-long.450.pdf",
      "authors": [
        "Haoran Luo",
        "Haihong E",
        "Yuhao Yang",
        "Yikai Guo",
        "Mingzhi Sun",
        "Tianyu Yao",
        "Zichen Tang",
        "Kaiyang Wan",
        "Meina Song",
        "Wei Lin"
      ],
      "abstract": "Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs’ representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 452,
      "original_url": "https://aclanthology.org/2023.acl-long.451/",
      "title": "ORGAN: Observation-Guided Radiology Report Generation via Tree Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.451.pdf",
      "authors": [
        "Wenjun Hou",
        "Kaishuai Xu",
        "Yi Cheng",
        "Wenjie Li",
        "Jiang Liu"
      ],
      "abstract": "This paper explores the task of radiology report generation, which aims at generating free-text descriptions for a set of radiographs. One significant challenge of this task is how to correctly maintain the consistency between the images and the lengthy report. Previous research explored solving this issue through planning-based methods, which generate reports only based on high-level plans. However, these plans usually only contain the major observations from the radiographs (e.g., lung opacity), lacking much necessary information, such as the observation characteristics and preliminary clinical diagnoses. To address this problem, the system should also take the image information into account together with the textual plan and perform stronger reasoning during the generation process. In this paper, we propose an Observation-guided radiology Report Generation framework (ORGan). It first produces an observation plan and then feeds both the plan and radiographs for report generation, where an observation graph and a tree reasoning mechanism are adopted to precisely enrich the plan information by capturing the multi-formats of each observation. Experimental results demonstrate that our framework outperforms previous state-of-the-art methods regarding text quality and clinical efficacy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 453,
      "original_url": "https://aclanthology.org/2023.acl-long.452/",
      "title": "Data Curation Alone Can Stabilize In-context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.452.pdf",
      "authors": [
        "Ting-Yun Chang",
        "Robin Jia"
      ],
      "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets—both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7% and 6.3%, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 454,
      "original_url": "https://aclanthology.org/2023.acl-long.453/",
      "title": "MidMed: Towards Mixed-Type Dialogues for Medical Consultation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.453.pdf",
      "authors": [
        "Xiaoming Shi",
        "Zeming Liu",
        "Chuan Wang",
        "Haitao Leng",
        "Kui Xue",
        "Xiaofan Zhang",
        "Shaoting Zhang"
      ],
      "abstract": "Most medical dialogue systems assume that patients have clear goals (seeking a diagnosis, medicine querying, etc.) before medical consultation. However, in many real situations, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. For further study, we create a novel human-to-human mixed-type medical consultation dialogue corpus, termed MidMed, covering four dialogue types: task-oriented dialogue for diagnosis, recommendation, QA, and chitchat. MidMed covers four departments (otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,309 dialogues. Furthermore, we build benchmarking baselines on MidMed and propose an instruction-guiding medical dialogue generation framework, termed InsMed, to handle mixed-type dialogues. Experimental results show the effectiveness of InsMed."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 455,
      "original_url": "https://aclanthology.org/2023.acl-long.454/",
      "title": "FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.454.pdf",
      "authors": [
        "Qinyuan Ye",
        "Iz Beltagy",
        "Matthew Peters",
        "Xiang Ren",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Large pre-trained models are capable of few-shot in-context learning (ICL), i.e., performing a new task by prepending a few demonstrations before the test input. However, the concatenated demonstrations are often excessively long and induce additional computation. Inspired by fusion-in-decoder (FiD) models which efficiently aggregate more passages and thus outperforms concatenation-based models in open-domain QA, we hypothesize that similar techniques can be applied to improve the efficiency and end-task performance of ICL. To verify this, we present a comprehensive study on applying three fusion methods—concatenation-based (early fusion), FiD (intermediate), and ensemble-based (late)—to ICL. We adopt a meta-learning setup where a model is first trained to perform ICL on a mixture of tasks using one selected fusion method, then evaluated on held-out tasks for ICL. Results on 11 held-out tasks show that FiD-ICL matches or outperforms the other two fusion methods. Additionally, we show that FiD-ICL (1) is 10x faster at inference time compared to concat-based and ensemble-based ICL, as we can easily pre-compute the representations of in-context examples and reuse them; (2) enables scaling up to meta-training 3B-sized models, which would fail for concat-based ICL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 456,
      "original_url": "https://aclanthology.org/2023.acl-long.455/",
      "title": "S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.455.pdf",
      "authors": [
        "Benfeng Xu",
        "Quan Wang",
        "Yajuan Lyu",
        "Dai Dai",
        "Yongdong Zhang",
        "Zhendong Mao"
      ],
      "abstract": "Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 457,
      "original_url": "https://aclanthology.org/2023.acl-long.456/",
      "title": "DSEE: Dually Sparsity-embedded Efficient Tuning of Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.456.pdf",
      "authors": [
        "Xuxi Chen",
        "Tianlong Chen",
        "Weizhu Chen",
        "Ahmed Hassan Awadallah",
        "Zhangyang Wang",
        "Yu Cheng"
      ],
      "abstract": "Gigantic pre-trained models have become central to natural language processing (NLP), serving as the starting point for fine-tuning towards a range of downstream tasks. However, two pain points persist for this paradigm: (a) as the pre-trained models grow bigger (e.g., 175B parameters for GPT-3), even the fine-tuning process can be time-consuming and computationally expensive; (b) the fine-tuned model has the same size as its starting point by default, which is neither sensible due to its more specialized functionality, nor practical since many fine-tuned models will be deployed in resource-constrained environments. To address these pain points, we propose a framework for resource- and parameter-efficient fine-tuning by leveraging the sparsity prior in both weight updates and the final model weights. Our proposed framework, dubbed Dually Sparsity-Embedded Efficient Tuning (DSEE), aims to achieve two key objectives: (i) parameter efficient fine-tuning - by enforcing sparsity-aware low-rank updates on top of the pre-trained weights; and (ii) resource-efficient inference - by encouraging a sparse weight structure towards the final fine-tuned model. We leverage sparsity in these two directions by exploiting both unstructured and structured sparse patterns in pre-trained language models viaa unified approach. Extensive experiments and in-depth investigations, with diverse network backbones (i.e., BERT, RoBERTa, and GPT-2) on dozens of datasets, consistently demonstrate impressive parameter-/inference-efficiency, while maintaining competitive downstream performance. For instance, DSEE saves about 25% inference FLOPs while achieving comparable performance, with 0.5% trainable parameters on BERT. Codes are available at https://github.com/VITA-Group/DSEE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 458,
      "original_url": "https://aclanthology.org/2023.acl-long.457/",
      "title": "CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.457.pdf",
      "authors": [
        "Jinfeng Zhou",
        "Chujie Zheng",
        "Bo Wang",
        "Zheng Zhang",
        "Minlie Huang"
      ],
      "abstract": "Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user’s cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 459,
      "original_url": "https://aclanthology.org/2023.acl-long.458/",
      "title": "Comparative evaluation of boundary-relaxed annotation for Entity Linking performance",
      "pdf_url": "https://aclanthology.org/2023.acl-long.458.pdf",
      "authors": [
        "Gabriel Herman Bernardim Andrade",
        "Shuntaro Yada",
        "Eiji Aramaki"
      ],
      "abstract": "Entity Linking performance has a strong reliance on having a large quantity of high-quality annotated training data available. Yet, manual annotation of named entities, especially their boundaries, is ambiguous, error-prone, and raises many inconsistencies between annotators. While imprecise boundary annotation can degrade a model’s performance, there are applications where accurate extraction of entities’ surface form is not necessary. For those cases, a lenient annotation guideline could relieve the annotators’ workload and speed up the process. This paper presents a case study designed to verify the feasibility of such annotation process and evaluate the impact of boundary-relaxed annotation in an Entity Linking pipeline. We first generate a set of noisy versions of the widely used AIDA CoNLL-YAGO dataset by expanding the boundaries subsets of annotated entity mentions and then train three Entity Linking models on this data and evaluate the relative impact of imprecise annotation on entity recognition and disambiguation performances. We demonstrate that the magnitude of effects caused by noise in the Named Entity Recognition phase is dependent on both model complexity and noise ratio, while Entity Disambiguation components are susceptible to entity boundary imprecision due to strong vocabulary dependency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 460,
      "original_url": "https://aclanthology.org/2023.acl-long.459/",
      "title": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.459.pdf",
      "authors": [
        "Shuheng Liu",
        "Alan Ritter"
      ],
      "abstract": "The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good generalization: model architecture, number of parameters, time period of the pre-training corpus, in addition to the amount of fine-tuning data. We suggest current evaluation methods have, in some sense, underestimated progress on NER over the past 20 years, as NER models have not only improved on the original CoNLL-2003 test set, but improved even more on modern data. Our datasets can be found at https://github.com/ShuhengL/acl2023_conllpp."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 461,
      "original_url": "https://aclanthology.org/2023.acl-long.460/",
      "title": "READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises",
      "pdf_url": "https://aclanthology.org/2023.acl-long.460.pdf",
      "authors": [
        "Chenglei Si",
        "Zhengyan Zhang",
        "Yingfa Chen",
        "Xiaozhi Wang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We experiment with a series of strong pretrained language models as well as robust training methods, we find that these models often suffer significant performance drops on READIN even with robustness methods like data augmentation. As the first large-scale attempt in creating a benchmark with noises geared towards user-generated inputs, we believe that READIN serves as an important complement to existing Chinese NLP benchmarks. The source code and dataset can be obtained from https://github.com/thunlp/READIN."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 462,
      "original_url": "https://aclanthology.org/2023.acl-long.461/",
      "title": "MAD-TSC: A Multilingual Aligned News Dataset for Target-dependent Sentiment Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.461.pdf",
      "authors": [
        "Evan Dufraisse",
        "Adrian Popescu",
        "Julien Tourille",
        "Armelle Brun",
        "Jerome Deshayes"
      ],
      "abstract": "Target-dependent sentiment classification (TSC) enables a fine-grained automatic analysis of sentiments expressed in texts. Sentiment expression varies depending on the domain, and it is necessary to create domain-specific datasets. While socially important, TSC in the news domain remains relatively understudied. We introduce MAD-TSC, a new dataset which differs substantially from existing resources. First, it includes aligned examples in eight languages to facilitate a comparison of performance for individual languages, and a direct comparison of human and machine translation. Second, the dataset is sampled from a diversified parallel news corpus, and is diversified in terms of news sources and geographic spread of entities. Finally, MAD-TSC is more challenging than existing datasets because its examples are more complex. We exemplify the use of MAD-TSC with comprehensive monolingual and multilingual experiments. The latter show that machine translations can successfully replace manual ones, and that performance for all included languages can match that of English by automatically translating test examples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 463,
      "original_url": "https://aclanthology.org/2023.acl-long.462/",
      "title": "A New Dataset and Empirical Study for Sentence Simplification in Chinese",
      "pdf_url": "https://aclanthology.org/2023.acl-long.462.pdf",
      "authors": [
        "Shiping Yang",
        "Renliang Sun",
        "Xiaojun Wan"
      ],
      "abstract": "Sentence Simplification is a valuable technique that can benefit language learners and children a lot. However, current research focuses more on English sentence simplification. The development of Chinese sentence simplification is relatively slow due to the lack of data. To alleviate this limitation, this paper introduces CSS, a new dataset for assessing sentence simplification in Chinese. We collect manual simplifications from human annotators and perform data analysis to show the difference between English and Chinese sentence simplifications. Furthermore, we test several unsupervised and zero/few-shot learning methods on CSS and analyze the automatic evaluation and human evaluation results. In the end, we explore whether Large Language Models can serve as high-quality Chinese sentence simplification systems by evaluating them on CSS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 464,
      "original_url": "https://aclanthology.org/2023.acl-long.463/",
      "title": "Factual or Contextual? Disentangling Error Types in Entity Description Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.463.pdf",
      "authors": [
        "Navita Goyal",
        "Ani Nenkova",
        "Hal Daumé III"
      ],
      "abstract": "In the task of entity description generation, given a context and a specified entity, a model must describe that entity correctly and in a contextually-relevant way. In this task, as well as broader language generation tasks, the generation of a nonfactual description (factual error) versus an incongruous description (contextual error) is fundamentally different, yet often conflated. We develop an evaluation paradigm that enables us to disentangle these two types of errors in naturally occurring textual contexts. We find that factuality and congruity are often at odds, and that models specifically struggle with accurate descriptions of entities that are less familiar to people. This shortcoming of language models raises concerns around the trustworthiness of such models, since factual errors on less well-known entities are exactly those that a human reader will not recognize."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 465,
      "original_url": "https://aclanthology.org/2023.acl-long.464/",
      "title": "Weakly Supervised Vision-and-Language Pre-training with Relative Representations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.464.pdf",
      "authors": [
        "Chi Chen",
        "Peng Li",
        "Maosong Sun",
        "Yang Liu"
      ],
      "abstract": "Weakly supervised vision-and-language pre-training (WVLP), which learns cross-modal representations with limited cross-modal supervision, has been shown to effectively reduce the data cost of pre-training while maintaining decent performance on downstream tasks. However, current WVLP methods use only local descriptions of images, i.e., object tags, as cross-modal anchors to construct weakly-aligned image-text pairs for pre-training. This affects the data quality and thus the effectiveness of pre-training. In this paper, we propose to directly take a small number of aligned image-text pairs as anchors, and represent each unaligned image and text by its similarities to these anchors, i.e., relative representations. We build a WVLP framework based on the relative representations, namely RELIT, which collects high-quality weakly-aligned image-text pairs from large-scale image-only and text-only data for pre-training through relative representation-based retrieval and generation. Experiments on four downstream tasks show that RELIT achieves new state-of-the-art results under the weakly supervised setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 466,
      "original_url": "https://aclanthology.org/2023.acl-long.465/",
      "title": "HermEs: Interactive Spreadsheet Formula Prediction via Hierarchical Formulet Expansion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.465.pdf",
      "authors": [
        "Wanrong He",
        "Haoyu Dong",
        "Yihuai Gao",
        "Zhichao Fan",
        "Xingzhuo Guo",
        "Zhitao Hou",
        "Xiao Lv",
        "Ran Jia",
        "Shi Han",
        "Dongmei Zhang"
      ],
      "abstract": "We propose HermEs, the first approach for spreadsheet formula prediction via HiEraRchical forMulet ExpanSion, where hierarchical expansion means generating formulas following the underlying parse tree structure, and Formulet refers to commonly-used multi-level patterns mined from real formula parse trees. HermEs improves the formula prediction accuracy by (1) guaranteeing correct grammar by hierarchical generation rather than left-to-right generation and (2) significantly streamlining the token-level decoding with high-level Formulet. Notably, instead of generating formulas in a pre-defined fixed order, we propose a novel sampling strategy to systematically exploit a variety of hierarchical and multi-level expansion orders and provided solid mathematical proof, with the aim of meeting diverse human needs of the formula writing order in real applications. We further develop an interactive formula completion interface based on HermEs, which shows a new user experience in https://github.com/formulet/HERMES."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 467,
      "original_url": "https://aclanthology.org/2023.acl-long.466/",
      "title": "ArgU: A Controllable Factual Argument Generator",
      "pdf_url": "https://aclanthology.org/2023.acl-long.466.pdf",
      "authors": [
        "Sougata Saha",
        "Rohini Srihari"
      ],
      "abstract": "Effective argumentation is essential towards a purposeful conversation with a satisfactory outcome. For example, persuading someone to reconsider smoking might involve empathetic, well founded arguments based on facts and expert opinions about its ill-effects and the consequences on one’s family. However, the automatic generation of high-quality factual arguments can be challenging. Addressing existing controllability issues can make the recent advances in computational models for argument generation a potential solution. In this paper, we introduce ArgU: a neural argument generator capable of producing factual arguments from input facts and real-world concepts that can be explicitly controlled for stance and argument structure using Walton’s argument scheme-based control codes. Unfortunately, computational argument generation is a relatively new field and lacks datasets conducive to training. Hence, we have compiled and released an annotated corpora of 69,428 arguments spanning six topics and six argument schemes, making it the largest publicly available corpus for identifying argument schemes; the paper details our annotation and dataset creation framework. We further experiment with an argument generation strategy that establishes an inference strategy by generating an “argument template” before actual argument generation. Our results demonstrate that it is possible to automatically generate diverse arguments exhibiting different inference patterns for the same set of facts by using control codes based on argument schemes and stance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 468,
      "original_url": "https://aclanthology.org/2023.acl-long.467/",
      "title": "Learning Answer Generation using Supervision from Automatic Question Answering Evaluators",
      "pdf_url": "https://aclanthology.org/2023.acl-long.467.pdf",
      "authors": [
        "Matteo Gabburo",
        "Siddhant Garg",
        "Rik Koncel-Kedziorski",
        "Alessandro Moschitti"
      ],
      "abstract": "Recent studies show that sentence-level extractive QA, i.e., based on Answer Sentence Selection (AS2), is outperformed by Generation-based QA (GenQA) models, which generate answers using the top-k answer sentences ranked by AS2 models (a la retrieval-augmented generation style). In this paper, we propose a novel training paradigm for GenQA using supervision from automatic QA evaluation models (GAVA). Specifically, we propose three strategies to transfer knowledge from these QA evaluation models to a GenQA model: (i) augmenting training data with answers generated by the GenQA model and labelled by GAVA (either statically, before training, or (ii) dynamically, at every training epoch); and (iii) using the GAVA score for weighting the generator loss during the learning of the GenQA model. We evaluate our proposed methods on two academic and one industrial dataset, obtaining a significant improvement in answering accuracy over the previous state of the art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 469,
      "original_url": "https://aclanthology.org/2023.acl-long.468/",
      "title": "RECAP: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.468.pdf",
      "authors": [
        "Shuai Liu",
        "Hyundong Cho",
        "Marjorie Freedman",
        "Xuezhe Ma",
        "Jonathan May"
      ],
      "abstract": "Endowing chatbots with a consistent persona is essential to an engaging conversation, yet it remains an unresolved challenge. In this work, we propose a new retrieval-enhanced approach for personalized response generation. Specifically, we design a hierarchical transformer retriever trained on dialogue domain data to perform personalized retrieval and a context-aware prefix encoder that fuses the retrieved information to the decoder more effectively. Extensive experiments on a real-world dataset demonstrate the effectiveness of our model at generating more fluent and personalized responses. We quantitatively evaluate our model’s performance under a suite of human and automatic metrics and find it to be superior compared to state-of-the-art baselines on English Reddit conversations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 470,
      "original_url": "https://aclanthology.org/2023.acl-long.469/",
      "title": "Don’t Parse, Choose Spans! Continuous and Discontinuous Constituency Parsing via Autoregressive Span Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.469.pdf",
      "authors": [
        "Songlin Yang",
        "Kewei Tu"
      ],
      "abstract": "We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection. Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and leverage a pointer network to autoregressively select spans by that order. To deal with discontinuous spans, we consecutively select their subspans from left to right, label all but last subspans with special discontinuous labels and the last subspan as the whole discontinuous spans’ labels. We use simple heuristic to output valid trees so that our approach is able to predict all possible continuous and discontinuous constituency trees without sacrificing data coverage and without the need to use expensive chart-based parsing algorithms. Experiments on multiple continuous and discontinuous benchmarks show that our model achieves state-of-the-art or competitive performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 471,
      "original_url": "https://aclanthology.org/2023.acl-long.470/",
      "title": "Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.470.pdf",
      "authors": [
        "Maxwell Crouse",
        "Pavan Kapanipathi",
        "Subhajit Chaudhury",
        "Tahira Naseem",
        "Ramon Fernandez Astudillo",
        "Achille Fokoue",
        "Tim Klinger"
      ],
      "abstract": "Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their ability to compositionally generalize. In this work, we approach semantic parsing from, quite literally, the opposite direction; that is, we introduce a neural semantic parsing generation method that constructs logical forms from the bottom up, beginning from the logical form’s leaves. The system we introduce is lazy in that it incrementally builds up a set of potential semantic parses, but only expands and processes the most promising candidate parses at each generation step. Such a parsimonious expansion scheme allows the system to maintain an arbitrarily large set of parse hypotheses that are never realized and thus incur minimal computational overhead. We evaluate our approach on compositional generalization; specifically, on the challenging CFQ dataset and two other Text-to-SQL datasets where we show that our novel, bottom-up semantic parsing technique outperforms general-purpose semantic parsers while also being competitive with semantic parsers that have been tailored to each task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 472,
      "original_url": "https://aclanthology.org/2023.acl-long.471/",
      "title": "AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression",
      "pdf_url": "https://aclanthology.org/2023.acl-long.471.pdf",
      "authors": [
        "Siyue Wu",
        "Hongzhan Chen",
        "Xiaojun Quan",
        "Qifan Wang",
        "Rui Wang"
      ],
      "abstract": "Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher’s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 473,
      "original_url": "https://aclanthology.org/2023.acl-long.472/",
      "title": "(QA)2: Question Answering with Questionable Assumptions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.472.pdf",
      "authors": [
        "Najoung Kim",
        "Phu Mon Htut",
        "Samuel R. Bowman",
        "Jackson Petty"
      ],
      "abstract": "Naturally occurring information-seeking questions often contain questionable assumptions—assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for information-seeking questions. For instance, the question “When did Marie Curie discover Uranium?” cannot be answered as a typical “when” question without addressing the false assumption “Marie Curie discovered Uranium”. In this work, we propose (QA)2 (Question Answering with Questionable Assumptions), an open-domain evaluation dataset consisting of naturally occurring search engine queries that may or may not contain questionable assumptions. To be successful on (QA)2, systems must be able to detect questionable assumptions and also be able to produce adequate responses for both typical information-seeking questions and ones with questionable assumptions. Through human rater acceptability on end-to-end QA with (QA)2, we find that current models do struggle with handling questionable assumptions, leaving substantial headroom for progress."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 474,
      "original_url": "https://aclanthology.org/2023.acl-long.473/",
      "title": "Attributable and Scalable Opinion Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.473.pdf",
      "authors": [
        "Tom Hosking",
        "Hao Tang",
        "Mirella Lapata"
      ],
      "abstract": "We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 475,
      "original_url": "https://aclanthology.org/2023.acl-long.474/",
      "title": "Targeted Data Generation: Finding and Fixing Model Weaknesses",
      "pdf_url": "https://aclanthology.org/2023.acl-long.474.pdf",
      "authors": [
        "Zexue He",
        "Marco Tulio Ribeiro",
        "Fereshte Khani"
      ],
      "abstract": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail systematically on specific subgroups of data, resulting in unfair outcomes and eroding user trust. Additional data collection may not help in addressing these weaknesses, as such challenging subgroups may be unknown to users, and underrepresented in the existing and new data. We propose Targeted Data Generation (TDG), a framework that automatically identifies challenging subgroups, and generates new data for those subgroups using large language models (LLMs) with a human in the loop. TDG estimates the expected benefit and potential harm of data augmentation for each subgroup, and selects the ones most likely to improve within-group performance without hurting overall performance. In our experiments, TDG significantly improves the accuracy on challenging subgroups for state-of-the-art sentiment analysis and natural language inference models, while also improving overall test accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 476,
      "original_url": "https://aclanthology.org/2023.acl-long.475/",
      "title": "HiFi: High-Information Attention Heads Hold for Parameter-Efficient Model Adaptation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.475.pdf",
      "authors": [
        "Anchun Gui",
        "Han Xiao"
      ],
      "abstract": "To fully leverage the advantages of large-scale pre-trained language models (PLMs) on downstream tasks, it has become a ubiquitous adaptation paradigm to fine-tune the entire parameters of PLMs. However, this paradigm poses issues of inefficient updating and resource over-consuming for fine-tuning in data-scarce and resource-limited scenarios, because of the large scale of parameters in PLMs. To alleviate these concerns, in this paper, we propose a parameter-efficient fine-tuning method HiFi, that is, only the highly informative and strongly correlated attention heads for the specific task are fine-tuned. To search for those significant attention heads, we develop a novel framework to analyze the effectiveness of heads. Specifically, we first model the relationship between heads into a graph from two perspectives of information richness and correlation, and then apply PageRank algorithm to determine the relative importance of each head. Extensive experiments on the GLUE benchmark demonstrate the effectiveness of our method, and show that HiFi obtains state-of-the-art performance over the prior baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 477,
      "original_url": "https://aclanthology.org/2023.acl-long.476/",
      "title": "CFSum Coarse-to-Fine Contribution Network for Multimodal Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.476.pdf",
      "authors": [
        "Min Xiao",
        "Junnan Zhu",
        "Haitao Lin",
        "Yu Zhou",
        "Chengqing Zong"
      ],
      "abstract": "Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies that useful images can even help generate non-visual words which are implicitly represented in the image."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 478,
      "original_url": "https://aclanthology.org/2023.acl-long.477/",
      "title": "On “Scientific Debt” in NLP: A Case for More Rigour in Language Model Pre-Training Research",
      "pdf_url": "https://aclanthology.org/2023.acl-long.477.pdf",
      "authors": [
        "Made Nindyatama Nityasya",
        "Haryo Wibowo",
        "Alham Fikri Aji",
        "Genta Winata",
        "Radityo Eko Prasojo",
        "Phil Blunsom",
        "Adhiguna Kuncoro"
      ],
      "abstract": "This evidence-based position paper critiques current research practices within the language model pre-training literature. Despite rapid recent progress afforded by increasingly better pre-trained language models (PLMs), current PLM research practices often conflate different possible sources of model improvement, without conducting proper ablation studies and principled comparisons between different models under comparable conditions. These practices (i) leave us ill-equipped to understand which pre-training approaches should be used under what circumstances; (ii) impede reproducibility and credit assignment; and (iii) render it difficult to understand: “How exactly does each factor contribute to the progress that we have today?” We provide a case in point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and demonstrate how — under comparable conditions where the baselines are tuned to a similar extent — these baselines (and even-simpler variants thereof) can, in fact, achieve competitive or better performance than BERT. These findings demonstrate how disentangling different factors of model improvements can lead to valuable new insights. We conclude with recommendations for how to encourage and incentivize this line of work, and accelerate progress towards a better and more systematic understanding of what factors drive the progress of our foundation models today."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 479,
      "original_url": "https://aclanthology.org/2023.acl-long.478/",
      "title": "End-to-end Knowledge Retrieval with Multi-modal Queries",
      "pdf_url": "https://aclanthology.org/2023.acl-long.478.pdf",
      "authors": [
        "Man Luo",
        "Zhiyuan Fang",
        "Tejas Gokhale",
        "Yezhou Yang",
        "Chitta Baral"
      ],
      "abstract": "We investigate knowledge retrieval with multi-modal queries, i.e. queries containing information split across image and text inputs, a challenging task that differs from previous work on cross-modal retrieval. We curate a new dataset called ReMuQ for benchmarking progress on this task. ReMuQ requires a system to retrieve knowledge from a large corpus by integrating contents from both text and image queries. We introduce a retriever model “ReViz” that can directly process input text and images to retrieve relevant knowledge in an end-to-end fashion without being dependent on intermediate modules such as object detectors or caption generators. We introduce a new pretraining task that is effective for learning knowledge retrieval with multimodal queries and also improves performance on downstream tasks. We demonstrate superior performance in retrieval on two datasets (ReMuQ and OK-VQA) under zero-shot settings as well as further improvements when finetuned on these datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 480,
      "original_url": "https://aclanthology.org/2023.acl-long.479/",
      "title": "AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.479.pdf",
      "authors": [
        "Rongjie Huang",
        "Huadai Liu",
        "Xize Cheng",
        "Yi Ren",
        "Linjun Li",
        "Zhenhui Ye",
        "Jinzheng He",
        "Lichao Zhang",
        "Jinglin Liu",
        "Xiang Yin",
        "Zhou Zhao"
      ],
      "abstract": "Direct speech-to-speech translation (S2ST) aims to convert speech from one language into another, and has demonstrated significant progress to date. Despite the recent success, current S2ST models still suffer from distinct degradation in noisy environments and fail to translate visual speech (i.e., the movement of lips and teeth). In this work, we present AV-TranSpeech, the first audio-visual speech-to-speech (AV-S2ST) translation model without relying on intermediate text. AV-TranSpeech complements the audio stream with visual information to promote system robustness and opens up a host of practical applications: dictation or dubbing archival films. To mitigate the data scarcity with limited parallel AV-S2ST data, we 1) explore self-supervised pre-training with unlabeled audio-visual data to learn contextual representation, and 2) introduce cross-modal distillation with S2ST models trained on the audio-only corpus to further reduce the requirements of visual data. Experimental results on two language pairs demonstrate that AV-TranSpeech outperforms audio-only models under all settings regardless of the type of noise. With low-resource audio-visual data (10h, 30h), cross-modal distillation yields an improvement of 7.6 BLEU on average compared with baselines. Audio samples are available at https://AV-TranSpeech.github.io/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 481,
      "original_url": "https://aclanthology.org/2023.acl-long.480/",
      "title": "Dual Class Knowledge Propagation Network for Multi-label Few-shot Intent Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.480.pdf",
      "authors": [
        "Feng Zhang",
        "Wei Chen",
        "Fei Ding",
        "Tengjiao Wang"
      ],
      "abstract": "Multi-label intent detection aims to assign multiple labels to utterances and attracts increasing attention as a practical task in task-oriented dialogue systems. As dialogue domains change rapidly and new intents emerge fast, the lack of annotated data motivates multi-label few-shot intent detection. However, previous studies are confused by the identical representation of the utterance with multiple labels and overlook the intrinsic intra-class and inter-class interactions. To address these two limitations, we propose a novel dual class knowledge propagation network in this paper. In order to learn well-separated representations for utterances with multiple intents, we first introduce a label-semantic augmentation module incorporating class name information. For better consideration of the inherent intra-class and inter-class relations, an instance-level and a class-level graph neural network are constructed, which not only propagate label information but also propagate feature structure. And we use a simple yet effective method to predict the intent count of each utterance. Extensive experimental results on two multi-label intent datasets have demonstrated that our proposed method outperforms strong baselines by a large margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 482,
      "original_url": "https://aclanthology.org/2023.acl-long.481/",
      "title": "VendorLink: An NLP approach for Identifying & Linking Vendor Migrants & Potential Aliases on Darknet Markets",
      "pdf_url": "https://aclanthology.org/2023.acl-long.481.pdf",
      "authors": [
        "Vageesh Saxena",
        "Nils Rethmeier",
        "Gijs van Dijck",
        "Gerasimos Spanakis"
      ],
      "abstract": "The anonymity on the Darknet allows vendors to stay undetected by using multiple vendor aliases or frequently migrating between markets. Consequently, illegal markets and their connections are challenging to uncover on the Darknet. To identify relationships between illegal markets and their vendors, we propose VendorLink, an NLP-based approach that examines writing patterns to verify, identify, and link unique vendor accounts across text advertisements (ads) on seven public Darknet markets. In contrast to existing literature, VendorLink utilizes the strength of supervised pre-training to perform closed-set vendor verification, open-set vendor identification, and low-resource market adaption tasks. Through VendorLink, we uncover (i) 15 migrants and 71 potential aliases in the Alphabay-Dreams-Silk dataset, (ii) 17 migrants and 3 potential aliases in the Valhalla-Berlusconi dataset, and (iii) 75 migrants and 10 potential aliases in the Traderoute-Agora dataset. Altogether, our approach can help Law Enforcement Agencies (LEA) make more informed decisions by verifying and identifying migrating vendors and their potential aliases on existing and Low-Resource (LR) emerging Darknet markets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 483,
      "original_url": "https://aclanthology.org/2023.acl-long.482/",
      "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
      "pdf_url": "https://aclanthology.org/2023.acl-long.482.pdf",
      "authors": [
        "Yiming Wang",
        "Zhuosheng Zhang",
        "Rui Wang"
      ],
      "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the “Lasswell Communication Model” proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs’ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 484,
      "original_url": "https://aclanthology.org/2023.acl-long.483/",
      "title": "Efficient Shapley Values Estimation by Amortization for Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.483.pdf",
      "authors": [
        "Chenghao Yang",
        "Fan Yin",
        "He He",
        "Kai-Wei Chang",
        "Xiaofei Ma",
        "Bing Xiang"
      ],
      "abstract": "Despite the popularity of Shapley Values in explaining neural text classification models, computing them is prohibitive for large pretrained models due to a large number of model evaluations. In practice, Shapley Values are often estimated with a small number of stochastic model evaluations. However, we show that the estimated Shapley Values are sensitive to random seed choices – the top-ranked features often have little overlap across different seeds, especially on examples with longer input texts. This can only be mitigated by aggregating thousands of model evaluations, which on the other hand, induces substantial computational overheads. To mitigate the trade-off between stability and efficiency, we develop an amortized model that directly predicts each input feature’s Shapley Value without additional model evaluations. It is trained on a set of examples whose Shapley Values are estimated from a large number of model evaluations to ensure stability. Experimental results on two text classification datasets demonstrate that our amortized model estimates Shapley Values accurately with up to 60 times speedup compared to traditional methods. Further, our model does not suffer from stability issues as inference is deterministic. We release our code at https://github.com/yangalan123/Amortized-Interpretability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 485,
      "original_url": "https://aclanthology.org/2023.acl-long.484/",
      "title": "PeerDA: Data Augmentation via Modeling Peer Relation for Span Identification Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.484.pdf",
      "authors": [
        "Weiwen Xu",
        "Xin Li",
        "Yang Deng",
        "Wai Lam",
        "Lidong Bing"
      ],
      "abstract": "Span identification aims at identifying specific text spans from text input and classifying them into pre-defined categories. Different from previous works that merely leverage the Subordinate (SUB) relation (i.e. if a span is an instance of a certain category) to train models, this paper for the first time explores the Peer (PR) relation, which indicates that two spans are instances of the same category and share similar features. Specifically, a novel Peer Data Augmentation (PeerDA) approach is proposed which employs span pairs with the PR relation as the augmentation data for training. PeerDA has two unique advantages: (1) There are a large number of PR span pairs for augmenting the training data. (2) The augmented data can prevent the trained model from over-fitting the superficial span-category mapping by pushing the model to leverage the span semantics. Experimental results on ten datasets over four diverse tasks across seven domains demonstrate the effectiveness of PeerDA. Notably, PeerDA achieves state-of-the-art results on six of them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 486,
      "original_url": "https://aclanthology.org/2023.acl-long.485/",
      "title": "Dynamic Regularization in UDA for Transformers in Multimodal Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.485.pdf",
      "authors": [
        "Ivonne Monter-Aldana",
        "Adrian Pastor Lopez Monroy",
        "Fernando Sanchez-Vega"
      ],
      "abstract": "Multimodal machine learning is a cutting-edge field that explores ways to incorporate information from multiple sources into models. As more multimodal data becomes available, this field has become increasingly relevant. This work focuses on two key challenges in multimodal machine learning. The first is finding efficient ways to combine information from different data types. The second is that often, one modality (e.g., text) is stronger and more relevant, making it difficult to identify meaningful patterns in the weaker modality (e.g., image). Our approach focuses on more effectively exploiting the weaker modality while dynamically regularizing the loss function. First, we introduce a new two-stream model called Multimodal BERT-ViT, which features a novel intra-CLS token fusion. Second, we utilize a dynamic adjustment that maintains a balance between specialization and generalization during the training to avoid overfitting, which we devised. We add this dynamic adjustment to the Unsupervised Data Augmentation (UDA) framework. We evaluate the effectiveness of these proposals on the task of multi-label movie genre classification using the Moviescope and MM-IMDb datasets. The evaluation revealed that our proposal offers substantial benefits, while simultaneously enabling us to harness the weaker modality without compromising the information provided by the stronger."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 487,
      "original_url": "https://aclanthology.org/2023.acl-long.486/",
      "title": "Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.486.pdf",
      "authors": [
        "Lea Frermann",
        "Jiatong Li",
        "Shima Khanehzar",
        "Gosia Mikolajczak"
      ],
      "abstract": "Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization of framing from the communication sciences which explicitly captures elements of narratives, including conflict and its resolution, and integrate it with the narrative framing of key entities in the story as heroes, victims or villains. We adapt an effective annotation paradigm that breaks a complex annotation task into a series of simpler binary questions, and present an annotated data set of English news articles, and a case study on the framing of climate change in articles from news outlets across the political spectrum. Finally, we explore automatic multi-label prediction of our frames with supervised and semi-supervised approaches, and present a novel retrieval-based method which is both effective and transparent in its predictions. We conclude with a discussion of opportunities and challenges for future work on document-level models of narrative framing."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 488,
      "original_url": "https://aclanthology.org/2023.acl-long.487/",
      "title": "bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark",
      "pdf_url": "https://aclanthology.org/2023.acl-long.487.pdf",
      "authors": [
        "Momchil Hardalov",
        "Pepa Atanasova",
        "Todor Mihaylov",
        "Galia Angelova",
        "Kiril Simov",
        "Petya Osenova",
        "Veselin Stoyanov",
        "Ivan Koychev",
        "Preslav Nakov",
        "Dragomir Radev"
      ],
      "abstract": "We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 489,
      "original_url": "https://aclanthology.org/2023.acl-long.488/",
      "title": "DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.488.pdf",
      "authors": [
        "Yuxi Feng",
        "Xiaoyuan Yi",
        "Xiting Wang",
        "Laks Lakshmanan, V.S.",
        "Xing Xie"
      ],
      "abstract": "Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 490,
      "original_url": "https://aclanthology.org/2023.acl-long.489/",
      "title": "What does the Failure to Reason with “Respectively” in Zero/Few-Shot Settings Tell Us about Language Models?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.489.pdf",
      "authors": [
        "Ruixiang Cui",
        "Seolhwa Lee",
        "Daniel Hershcovich",
        "Anders Søgaard"
      ],
      "abstract": "Humans can effortlessly understand the coordinate structure of sentences such as “Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, *respectively*”. In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of “respectively”. We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag behind humans in generalizing to the long tail of linguistic constructions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 491,
      "original_url": "https://aclanthology.org/2023.acl-long.490/",
      "title": "BLIND: Bias Removal With No Demographics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.490.pdf",
      "authors": [
        "Hadas Orgad",
        "Yonatan Belinkov"
      ],
      "abstract": "Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model’s success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 492,
      "original_url": "https://aclanthology.org/2023.acl-long.491/",
      "title": "How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.491.pdf",
      "authors": [
        "Salijona Dyrmishi",
        "Salah Ghamizi",
        "Maxime Cordy"
      ],
      "abstract": "Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks – malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to position human perceptibility as a first-class success criterion for text attacks, and provide guidance for research to build effective attack algorithms and, in turn, design appropriate defence mechanisms."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 493,
      "original_url": "https://aclanthology.org/2023.acl-long.492/",
      "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.492.pdf",
      "authors": [
        "Michal Štefánik",
        "Marek Kadlcik",
        "Petr Sojka"
      ],
      "abstract": "Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model’s ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference. Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens’ semantic similarity can largely mitigate catastrophic forgetting of adaptation, while (2) preserving the adaptation in-domain quality, (3) with negligible additions to compute costs. In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but naive exact-match token-level objectives and expressive but computationally- and resource-intensive sequential objectives."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 494,
      "original_url": "https://aclanthology.org/2023.acl-long.493/",
      "title": "The CRINGE Loss: Learning what language not to model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.493.pdf",
      "authors": [
        "Leonard Adolphs",
        "Tianyu Gao",
        "Jing Xu",
        "Kurt Shuster",
        "Sainbayar Sukhbaatar",
        "Jason Weston"
      ],
      "abstract": "Standard language model training employs gold human documents or human-human interaction data, and treats all training data as positive examples. Growing evidence shows that even with very large amounts of positive training data, issues remain that can be alleviated with relatively small amounts of negative data – examples of what the model should not do. In this work, we propose a novel procedure to train with such data called the “CRINGE” loss (ContRastive Iterative Negative GEneration). We show the effectiveness of this approach across three different experiments on the tasks of safe generation, contradiction avoidance, and open-domain dialogue. Our models outperform multiple strong baselines and are conceptually simple, easy to train and implement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 495,
      "original_url": "https://aclanthology.org/2023.acl-long.494/",
      "title": "Modeling User Satisfaction Dynamics in Dialogue via Hawkes Process",
      "pdf_url": "https://aclanthology.org/2023.acl-long.494.pdf",
      "authors": [
        "Fanghua Ye",
        "Zhiyuan Hu",
        "Emine Yilmaz"
      ],
      "abstract": "Dialogue systems have received increasing attention while automatically evaluating their performance remains challenging. User satisfaction estimation (USE) has been proposed as an alternative. It assumes that the performance of a dialogue system can be measured by user satisfaction and uses an estimator to simulate users. The effectiveness of USE depends heavily on the estimator. Existing estimators independently predict user satisfaction at each turn and ignore satisfaction dynamics across turns within a dialogue. In order to fully simulate users, it is crucial to take satisfaction dynamics into account. To fill this gap, we propose a new estimator ASAP (sAtisfaction eStimation via HAwkes Process) that treats user satisfaction across turns as an event sequence and employs a Hawkes process to effectively model the dynamics in this sequence. Experimental results on four benchmark dialogue datasets demonstrate that ASAP can substantially outperform state-of-the-art baseline estimators."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 496,
      "original_url": "https://aclanthology.org/2023.acl-long.495/",
      "title": "Towards Identifying Fine-Grained Depression Symptoms from Memes",
      "pdf_url": "https://aclanthology.org/2023.acl-long.495.pdf",
      "authors": [
        "Shweta Yadav",
        "Cornelia Caragea",
        "Chenye Zhao",
        "Naincy Kumari",
        "Marvin Solberg",
        "Tanmay Sharma"
      ],
      "abstract": "The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized intervention, it is crucial to identify fine-grained mental health symptoms. To this end, we conduct a focused study on depression disorder and introduce a new task of identifying fine-grained depressive symptoms from memes. Toward this, we create a high-quality dataset (RESTORE) annotated with 8 fine-grained depression symptoms based on the clinically adopted PHQ-9 questionnaire. We benchmark RESTORE on 20 strong monomodal and multimodal methods. Additionally, we show how imposing orthogonal constraints on textual and visual feature representations in a multimodal setting can enforce the model to learn non-redundant and de-correlated features leading to a better prediction of fine-grained depression symptoms. Further, we conduct an extensive human analysis and elaborate on the limitations of existing multimodal models that often overlook the implicit connection between visual and textual elements of a meme."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 497,
      "original_url": "https://aclanthology.org/2023.acl-long.496/",
      "title": "SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.496.pdf",
      "authors": [
        "Suwon Shon",
        "Siddhant Arora",
        "Chyi-Jiunn Lin",
        "Ankita Pasad",
        "Felix Wu",
        "Roshan S Sharma",
        "Wei-Lun Wu",
        "Hung-yi Lee",
        "Karen Livescu",
        "Shinji Watanabe"
      ],
      "abstract": "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models’ performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 498,
      "original_url": "https://aclanthology.org/2023.acl-long.497/",
      "title": "My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave",
      "pdf_url": "https://aclanthology.org/2023.acl-long.497.pdf",
      "authors": [
        "Pavan Holur",
        "David Chong",
        "Timothy Tangherlini",
        "Vwani Roychowdhury"
      ],
      "abstract": "News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of aligned story actors responsible for sustaining the issue-specific narratives. Discovering aligned actors, and the groups these alignments create, brings us closer to estimating the narrative that each group represents. With the help of Large Language Models (LLM), we address this task by: (i) Introducing a corpus of text segments rich in narrative content associated with six different current issues; (ii) Introducing a novel two-step graph-based framework that (a) identifies alignments between actors (INCANT) and (b) extracts aligned actor groups using the network structure (TAMPA). Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework. Across domains, alignment relationships from INCANT are accurate (macro F1 >= 0.75) and actor groups from TAMPA are preferred over 2 non-trivial baseline models (ACC >= 0.75)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 499,
      "original_url": "https://aclanthology.org/2023.acl-long.498/",
      "title": "Characterizing and Measuring Linguistic Dataset Drift",
      "pdf_url": "https://aclanthology.org/2023.acl-long.498.pdf",
      "authors": [
        "Tyler Chang",
        "Kishaloy Halder",
        "Neha Anna John",
        "Yogarshi Vyas",
        "Yassine Benajiba",
        "Miguel Ballesteros",
        "Dan Roth"
      ],
      "abstract": "NLP models often degrade in performance when real world data distributions differ markedly from training data. However, existing dataset drift metrics in NLP have generally not considered specific dimensions of linguistic drift that affect model performance, and they have not been validated in their ability to predict model performance at the individual example level, where such metrics are often used in practice. In this paper, we propose three dimensions of linguistic dataset drift: vocabulary, structural, and semantic drift. These dimensions correspond to content word frequency divergences, syntactic divergences, and meaning changes not captured by word frequencies (e.g. lexical semantic change). We propose interpretable metrics for all three drift dimensions, and we modify past performance prediction methods to predict model performance at both the example and dataset level for English sentiment classification and natural language inference. We find that our drift metrics are more effective than previous metrics at predicting out-of-domain model accuracies (mean 16.8% root mean square error decrease), particularly when compared to popular fine-tuned embedding distances (mean 47.7% error decrease). Fine-tuned embedding distances are much more effective at ranking individual examples by expected performance, but decomposing into vocabulary, structural, and semantic drift produces the best example rankings of all considered model-agnostic drift metrics (mean 6.7% ROC AUC increase)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 500,
      "original_url": "https://aclanthology.org/2023.acl-long.499/",
      "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.499.pdf",
      "authors": [
        "Yujia Qin",
        "Zihan Cai",
        "Dian Jin",
        "Lan Yan",
        "Shihao Liang",
        "Kunlun Zhu",
        "Yankai Lin",
        "Xu Han",
        "Ning Ding",
        "Huadong Wang",
        "Ruobing Xie",
        "Fanchao Qi",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
      ],
      "abstract": "Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 15,372 supporting facts and 125,954 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively. The interface, dataset, and codes are publicly available at https://github.com/thunlp/WebCPM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 501,
      "original_url": "https://aclanthology.org/2023.acl-long.500/",
      "title": "Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.500.pdf",
      "authors": [
        "Hongwei Zeng",
        "Bifan Wei",
        "Jun Liu",
        "Weiping Fu"
      ],
      "abstract": "Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation (ZeroCQG), which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage knowledge transfer framework, Synthesize, Prompt, and trAnsfer with pRe-Trained lAnguage model (SPARTA) to effectively leverage knowledge from single-turn question generation instances. To validate the zero-shot performance of SPARTA, we conduct extensive experiments on three conversational datasets: CoQA, QuAC, and DoQA by transferring knowledge from three single-turn datasets: MS MARCO, NewsQA, and SQuAD. The experimental results demonstrate the superior performance of our method. Specifically, SPARTA has achieved 14.81 BLEU-4 (88.2% absolute improvement compared to T5) in CoQA with knowledge transferred from SQuAD."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 502,
      "original_url": "https://aclanthology.org/2023.acl-long.501/",
      "title": "FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.501.pdf",
      "authors": [
        "Chen-Yu Lee",
        "Chun-Liang Li",
        "Hao Zhang",
        "Timothy Dozat",
        "Vincent Perot",
        "Guolong Su",
        "Xiang Zhang",
        "Kihyuk Sohn",
        "Nikolay Glushnev",
        "Renshen Wang",
        "Joshua Ainslie",
        "Shangbang Long",
        "Siyang Qin",
        "Yasuhisa Fujii",
        "Nan Hua",
        "Tomas Pfister"
      ],
      "abstract": "The recent advent of self-supervised pre-training techniques has led to a surge in the use of multimodal learning in form document understanding. However, existing approaches that extend the mask language modeling to other modalities require careful multi-task tuning, complex reconstruction target designs, or additional pre-training data. In FormNetV2, we introduce a centralized multimodal graph contrastive learning strategy to unify self-supervised pre-training for all modalities in one loss. The graph contrastive objective maximizes the agreement of multimodal representations, providing a natural interplay for all modalities without special customization. In addition, we extract image features within the bounding box that joins a pair of tokens connected by a graph edge, capturing more targeted visual cues without loading a sophisticated and separately pre-trained image embedder. FormNetV2 establishes new state-of-the-art performance on FUNSD, CORD, SROIE and Payment benchmarks with a more compact model size."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 503,
      "original_url": "https://aclanthology.org/2023.acl-long.502/",
      "title": "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies",
      "pdf_url": "https://aclanthology.org/2023.acl-long.502.pdf",
      "authors": [
        "Shiyue Zhang",
        "Shijie Wu",
        "Ozan Irsoy",
        "Steven Lu",
        "Mohit Bansal",
        "Mark Dredze",
        "David Rosenberg"
      ],
      "abstract": "Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P – that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may “over-generalize”, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 504,
      "original_url": "https://aclanthology.org/2023.acl-long.503/",
      "title": "Knowledgeable Parameter Efficient Tuning Network for Commonsense Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.503.pdf",
      "authors": [
        "Ziwang Zhao",
        "Linmei Hu",
        "Hanyu Zhao",
        "Yingxia Shao",
        "Yequan Wang"
      ],
      "abstract": "Commonsense question answering is important for making decisions about everyday matters. Although existing commonsense question answering works based on fully fine-tuned PLMs have achieved promising results, they suffer from prohibitive computation costs as well as poor interpretability. Some works improve the PLMs by incorporating knowledge to provide certain evidence, via elaborately designed GNN modules which require expertise. In this paper, we propose a simple knowledgeable parameter efficient tuning network to couple PLMs with external knowledge for commonsense question answering. Specifically, we design a trainable parameter-sharing adapter attached to a parameter-freezing PLM to incorporate knowledge at a small cost. The adapter is equipped with both entity- and query-related knowledge via two auxiliary knowledge-related tasks (i.e., span masking and relation discrimination). To make the adapter focus on the relevant knowledge, we design gating and attention mechanisms to respectively filter and fuse the query information from the PLM. Extensive experiments on two benchmark datasets show that KPE is parameter-efficient and can effectively incorporate knowledge for improving commonsense question answering."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 505,
      "original_url": "https://aclanthology.org/2023.acl-long.504/",
      "title": "BLASER: A Text-Free Speech-to-Speech Translation Evaluation Metric",
      "pdf_url": "https://aclanthology.org/2023.acl-long.504.pdf",
      "authors": [
        "Mingda Chen",
        "Paul-Ambroise Duquenne",
        "Pierre Andrews",
        "Justine Kao",
        "Alexandre Mourachko",
        "Holger Schwenk",
        "Marta R. Costa-jussà"
      ],
      "abstract": "End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we propose a text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the dependency on ASR systems. BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation. To evaluate our approach, we construct training and evaluation sets from more than 40k human annotations covering seven language directions. The best results of BLASER are achieved by training with supervision from human rating scores. We show that when evaluated at the sentence level, BLASER correlates significantly better with human judgment compared to ASR dependent metrics including ASR-SENTBLEU in all translation directions and ASR-COMET in five of them. Our analysis shows combining speech and text as inputs to BLASER does not increase the correlation with human scores, but best correlations are achieved when using speech, which motivates the goal of our research. Moreover, we show that using ASR for references is detrimental for text-based metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 506,
      "original_url": "https://aclanthology.org/2023.acl-long.505/",
      "title": "NLPositionality: Characterizing Design Biases of Datasets and Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.505.pdf",
      "authors": [
        "Sebastin Santy",
        "Jenny Liang",
        "Ronan Le Bras",
        "Katharina Reinecke",
        "Maarten Sap"
      ],
      "abstract": "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 507,
      "original_url": "https://aclanthology.org/2023.acl-long.506/",
      "title": "Backpack Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.506.pdf",
      "authors": [
        "John Hewitt",
        "John Thickstun",
        "Christopher Manning",
        "Percy Liang"
      ],
      "abstract": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model’s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM’s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 508,
      "original_url": "https://aclanthology.org/2023.acl-long.507/",
      "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.507.pdf",
      "authors": [
        "Virginia Felkner",
        "Ho-Chun Herbert Chang",
        "Eugene Jang",
        "Jonathan May"
      ],
      "abstract": "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 509,
      "original_url": "https://aclanthology.org/2023.acl-long.508/",
      "title": "Grounded Multimodal Named Entity Recognition on Social Media",
      "pdf_url": "https://aclanthology.org/2023.acl-long.508.pdf",
      "authors": [
        "Jianfei Yu",
        "Ziyan Li",
        "Jieming Wang",
        "Rui Xia"
      ],
      "abstract": "In recent years, Multimodal Named Entity Recognition (MNER) on social media has attracted considerable attention. However, existing MNER studies only extract entity-type pairs in text, which is useless for multimodal knowledge graph construction and insufficient for entity disambiguation. To solve these issues, in this work, we introduce a Grounded Multimodal Named Entity Recognition (GMNER) task. Given a text-image social post, GMNER aims to identify the named entities in text, their entity types, and their bounding box groundings in image (i.e. visual regions). To tackle the GMNER task, we construct a Twitter dataset based on two existing MNER datasets. Moreover, we extend four well-known MNER methods to establish a number of baseline systems and further propose a Hierarchical Index generation framework named H-Index, which generates the entity-type-region triples in a hierarchical manner with a sequence-to-sequence model. Experiment results on our annotated dataset demonstrate the superiority of our H-Index framework over baseline systems on the GMNER task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 510,
      "original_url": "https://aclanthology.org/2023.acl-long.509/",
      "title": "Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference",
      "pdf_url": "https://aclanthology.org/2023.acl-long.509.pdf",
      "authors": [
        "Junhao Zheng",
        "Qianli Ma",
        "Shengjie Qiu",
        "Yue Wu",
        "Peitian Ma",
        "Junlong Liu",
        "Huawen Feng",
        "Xichen Shang",
        "Haibin Chen"
      ],
      "abstract": "Fine-tuning has been proven to be a simple and effective technique to transfer the learned knowledge of Pre-trained Language Models (PLMs) to downstream tasks. However, vanilla fine-tuning easily overfits the target data and degrades the generalization ability. Most existing studies attribute it to catastrophic forgetting, and they retain the pre-trained knowledge indiscriminately without identifying what knowledge is transferable. Motivated by this, we frame fine-tuning into a causal graph and discover that the crux of catastrophic forgetting lies in the missing causal effects from the pre-trained data. Based on the causal view, we propose a unified objective for fine-tuning to retrieve the causality back. Intriguingly, the unified objective can be seen as the sum of the vanilla fine-tuning objective, which learns new knowledge from target data, and the causal objective, which preserves old knowledge from PLMs. Therefore, our method is flexible and can mitigate negative transfer while preserving knowledge. Since endowing models with commonsense is a long-standing challenge, we implement our method on commonsense QA with a proposed heuristic estimation to verify its effectiveness. In the experiments, our method outperforms state-of-the-art fine-tuning methods on all six commonsense QA datasets and can be implemented as a plug-in module to inflate the performance of existing QA models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 511,
      "original_url": "https://aclanthology.org/2023.acl-long.510/",
      "title": "Translation-Enhanced Multilingual Text-to-Image Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.510.pdf",
      "authors": [
        "Yaoyiran Li",
        "Ching-Yun Chang",
        "Stephen Rawls",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "abstract": "Research on text-to-image generation (TTI) still predominantly focuses on the English language due to the lack of annotated image-caption data in other languages; in the long run, this might widen inequitable access to TTI technology. In this work, we thus investigate multilingual TTI (termed mTTI) and the current potential of neural machine translation (NMT) to bootstrap mTTI systems. We provide two key contributions. 1) Relying on a multilingual multi-modal encoder, we provide a systematic empirical study of standard methods used in cross-lingual NLP when applied to mTTI: Translate Train, Translate Test, and Zero-Shot Transfer. 2) We propose Ensemble Adapter (EnsAd), a novel parameter-efficient approach that learns to weigh and consolidate the multilingual text knowledge within the mTTI framework, mitigating the language gap and thus improving mTTI performance. Our evaluations on standard mTTI datasets COCO-CN, Multi30K Task2, and LAION-5B demonstrate the potential of translation-enhanced mTTI systems and also validate the benefits of the proposed EnsAd which derives consistent gains across all datasets. Further investigations on model variants, ablation studies, and qualitative analyses provide additional insights on the inner workings of the proposed mTTI approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 512,
      "original_url": "https://aclanthology.org/2023.acl-long.511/",
      "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.511.pdf",
      "authors": [
        "Joshua Maynez",
        "Priyanka Agrawal",
        "Sebastian Gehrmann"
      ],
      "abstract": "Pre-trained large language models (PLMs) underly most new developments in natural language processing. They have shifted the field from application-specific model pipelines to a single model that is adapted to a wide range of tasks. Autoregressive PLMs like GPT-3 or PaLM and associated techniques like fewshot learning, have additionally shifted the output modality to generation instead of classification or regression. Despite their ubiquitous use, the generation quality of language models is rarely evaluated when these models are introduced. Additionally, it is unclear how existing generation tasks–while they can be used to compare systems at a high level–relate to the real world use cases for which people have been adopting them. In this work, we discuss how to adapt existing application-specific generation benchmarks to PLMs and provide an in-depth, empirical study of the limitations and capabilities of PLMs in natural language generation tasks along dimensions such as scale, architecture, input and output language. Our results show that PLMs differ in their applicability to different data regimes and their generalization to multiple languages. They further inform practitioners as to which PLMs to use for a given generation task setup. We share best practices to be taken into consideration when benchmarking generation capabilities during the development of upcoming PLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 513,
      "original_url": "https://aclanthology.org/2023.acl-long.512/",
      "title": "lilGym: Natural Language Visual Reasoning with Reinforcement Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.512.pdf",
      "authors": [
        "Anne Wu",
        "Kiante Brantley",
        "Noriyuki Kojima",
        "Yoav Artzi"
      ],
      "abstract": "We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 514,
      "original_url": "https://aclanthology.org/2023.acl-long.513/",
      "title": "Unsupervised Melody-to-Lyrics Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.513.pdf",
      "authors": [
        "Yufei Tian",
        "Anjali Narayan-Chen",
        "Shereen Oraby",
        "Alessandra Cervone",
        "Gunnar Sigurdsson",
        "Chenyang Tao",
        "Wenbo Zhao",
        "Yiwen Chen",
        "Tagyoung Chung",
        "Jing Huang",
        "Nanyun Peng"
      ],
      "abstract": "Automatic melody-to-lyric generation is a task in which song lyrics are generated to go with a given melody. It is of significant practical interest and more challenging than unconstrained lyric generation as the music imposes additional constraints onto the lyrics. The training data is limited as most songs are copyrighted, resulting in models that underfit the complicated cross-modal relationship between melody and lyrics. In this work, we propose a method for generating high-quality lyrics without training on any aligned melody-lyric data. Specifically, we design a hierarchical lyric generation framework that first generates a song outline and second the complete lyrics. The framework enables disentanglement of training (based purely on text) from inference (melody-guided text generation) to circumvent the shortage of parallel data. We leverage the segmentation and rhythm alignment between melody and lyrics to compile the given melody into decoding constraints as guidance during inference. The two-step hierarchical design also enables content control via the lyric outline, a much-desired feature for democratizing collaborative song creation. Experimental results show that our model can generate high-quality lyrics that are more on-topic, singable, intelligible, and coherent than strong baselines, for example SongMASS, a SOTA model trained on a parallel dataset, with a 24% relative overall quality improvement based on human ratings. Our code is available at https://github.com/amazon-science/unsupervised-melody-to-lyrics-generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 515,
      "original_url": "https://aclanthology.org/2023.acl-long.514/",
      "title": "Causality-aware Concept Extraction based on Knowledge-guided Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-long.514.pdf",
      "authors": [
        "Siyu Yuan",
        "Deqing Yang",
        "Jinxi Liu",
        "Shuyu Tian",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Rui Xie"
      ],
      "abstract": "Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 516,
      "original_url": "https://aclanthology.org/2023.acl-long.515/",
      "title": "Span-level Aspect-based Sentiment Analysis via Table Filling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.515.pdf",
      "authors": [
        "Mao Zhang",
        "Yongxin Zhu",
        "Zhen Liu",
        "Zhimin Bao",
        "Yunfei Wu",
        "Xing Sun",
        "Linli Xu"
      ],
      "abstract": "In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding opinion expressions, in this paper, we propose Table Filling BERT (TF-BERT), which considers the consistency of multi-word opinion expressions at the span-level. Specially, we learn the span representations with a table filling method, by constructing an upper triangular table for each sentiment polarity, of which the elements represent the sentiment intensity of the specific sentiment polarity for all spans in the sentence. Two methods are then proposed, including table-decoding and table-aggregation, to filter out target spans or aggregate each table for sentiment polarity classification. In addition, we design a sentiment consistency regularizer to guarantee the sentiment consistency of each span for different sentiment polarities. Experimental results on three benchmarks demonstrate the effectiveness of our proposed model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 517,
      "original_url": "https://aclanthology.org/2023.acl-long.516/",
      "title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.516.pdf",
      "authors": [
        "Jing Qian",
        "Hong Wang",
        "Zekun Li",
        "Shiyang Li",
        "Xifeng Yan"
      ],
      "abstract": "Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 518,
      "original_url": "https://aclanthology.org/2023.acl-long.517/",
      "title": "EEL: Efficiently Encoding Lattices for Reranking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.517.pdf",
      "authors": [
        "Prasann Singhal",
        "Jiacheng Xu",
        "Xi Ye",
        "Greg Durrett"
      ],
      "abstract": "Standard decoding approaches for conditional text generation tasks typically search for an output hypothesis with high model probability, but this may not yield the best hypothesis according to human judgments of quality. Reranking to optimize for “downstream” metrics can more closely optimize for quality, but many metrics of interest are computed with pre-trained language models, which are slow to apply to large numbers of hypotheses. We explore an approach for reranking hypotheses by using Transformers to efficiently encode lattices of generated outputs, a method we call EEL. With a single Transformer pass over the entire lattice, we can approximately compute a contextualized representation of each token as if it were only part of a single hypothesis in isolation. We combine this approach with a new class of token-factored rerankers (TFRs) that allow for efficient extraction of high reranker-scoring hypotheses from the lattice. Empirically, our approach incurs minimal degradation error compared to the exponentially slower approach of encoding each hypothesis individually. When applying EEL with TFRs across three text generation tasks, our results show both substantial speedup compared to naive reranking and often better performance on downstream metrics than comparable approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 519,
      "original_url": "https://aclanthology.org/2023.acl-long.518/",
      "title": "CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.518.pdf",
      "authors": [
        "Zhenhui Ye",
        "Rongjie Huang",
        "Yi Ren",
        "Ziyue Jiang",
        "Jinglin Liu",
        "Jinzheng He",
        "Xiang Yin",
        "Zhou Zhao"
      ],
      "abstract": "Improving text representation has attracted much attention to achieve expressive text-to-speech (TTS). However, existing works only implicitly learn the prosody with masked token reconstruction tasks, which leads to low training efficiency and difficulty in prosody modeling. We propose CLAPSpeech, a cross-modal contrastive pre-training framework that learns from the prosody variance of the same text token under different contexts. Specifically, 1) with the design of a text encoder and a prosody encoder, we encourage the model to connect the text context with its corresponding prosody pattern in the joint multi-modal space; 2) we introduce a multi-scale pre-training pipeline to capture prosody patterns in multiple levels. 3) we show how to incorporate CLAPSpeech into existing TTS models for better prosody. Experiments on three datasets not only show that CLAPSpeech could improve the prosody prediction for existing TTS methods, but also demonstrate its generalization ability to adapt to multiple languages and multi-speaker text-to-speech. We also deeply analyze the principle behind the performance of CLAPSpeech. Ablation studies demonstrate the necessity of each component in CLAPSpeech. Source code and audio samples are available at https://clapspeech.github.io."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 520,
      "original_url": "https://aclanthology.org/2023.acl-long.519/",
      "title": "Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.519.pdf",
      "authors": [
        "Yulong Chen",
        "Huajian Zhang",
        "Yijie Zhou",
        "Xuefeng Bai",
        "Yueguan Wang",
        "Ming Zhong",
        "Jianhao Yan",
        "Yafu Li",
        "Judy Li",
        "Xianchao Zhu",
        "Yue Zhang"
      ],
      "abstract": "Most existing cross-lingual summarization (CLS) work constructs CLS corpora by simply and directly translating pre-annotated summaries from one language to another, which can contain errors from both summarization and translation processes. To address this issue, we propose ConvSumX, a cross-lingual conversation summarization benchmark, through a new annotation schema that explicitly considers source input context. ConvSumX consists of 2 sub-tasks under different real-world scenarios, with each covering 3 language directions. We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated CLS corpora and empirically find that ConvSumX is more faithful towards input text. Additionally, based on the same intuition, we propose a 2-Step method, which takes both conversation and summary as input to simulate human annotation process. Experimental results show that 2-Step method surpasses strong baselines on ConvSumX under both automatic and human evaluation. Analysis shows that both source input text and summary are crucial for modeling cross-lingual summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 521,
      "original_url": "https://aclanthology.org/2023.acl-long.520/",
      "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.520.pdf",
      "authors": [
        "Xiaohang Tang",
        "Yi Zhou",
        "Danushka Bollegala"
      ],
      "abstract": "Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots C1 and C2 of a corpus taken respectively at two distinct timestamps T1 and T2, we first propose an unsupervised method to select (a) pivot terms related to both C1 and C2, and (b) anchor terms that are associated with a specific pivot term in each individual snapshot.We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms.Moreover, we propose an automatic method to learn time-sensitive templates from C1 and C2, without requiring any human supervision.Next, we use the generated prompts to adapt a pretrained MLM to T2 by fine-tuning using those prompts.Multiple experiments show that our proposed method significantly reduces the perplexity of test sentences in C2, outperforming the current state-of-the-art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 522,
      "original_url": "https://aclanthology.org/2023.acl-long.521/",
      "title": "How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech",
      "pdf_url": "https://aclanthology.org/2023.acl-long.521.pdf",
      "authors": [
        "Aditya Yedetore",
        "Tal Linzen",
        "Robert Frank",
        "R. Thomas McCoy"
      ],
      "abstract": "When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children’s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children’s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 523,
      "original_url": "https://aclanthology.org/2023.acl-long.522/",
      "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator",
      "pdf_url": "https://aclanthology.org/2023.acl-long.522.pdf",
      "authors": [
        "Jian Yang",
        "Shuming Ma",
        "Li Dong",
        "Shaohan Huang",
        "Haoyang Huang",
        "Yuwei Yin",
        "Dongdong Zhang",
        "Liqun Yang",
        "Furu Wei",
        "Zhoujun Li"
      ],
      "abstract": "Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GanLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 524,
      "original_url": "https://aclanthology.org/2023.acl-long.523/",
      "title": "Log-linear Guardedness and its Implications",
      "pdf_url": "https://aclanthology.org/2023.acl-long.523.pdf",
      "authors": [
        "Shauli Ravfogel",
        "Yoav Goldberg",
        "Ryan Cotterell"
      ],
      "abstract": "Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we constructively demonstrate that a multiclass log-linear model can be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of linear guardedness as a downstream bias mitigation technique.These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between intrinsic and extrinsic bias in neural models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 525,
      "original_url": "https://aclanthology.org/2023.acl-long.524/",
      "title": "Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM’s Translation Capability",
      "pdf_url": "https://aclanthology.org/2023.acl-long.524.pdf",
      "authors": [
        "Eleftheria Briakou",
        "Colin Cherry",
        "George Foster"
      ],
      "abstract": "Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism—the unintentional consumption of bilingual signals, including translation examples—in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM’s out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 526,
      "original_url": "https://aclanthology.org/2023.acl-long.525/",
      "title": "Open Set Relation Extraction via Unknown-Aware Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.525.pdf",
      "authors": [
        "Jun Zhao",
        "Xin Zhao",
        "WenYu Zhan",
        "Qi Zhang",
        "Tao Gui",
        "Zhongyu Wei",
        "Yun Wen Chen",
        "Xiang Gao",
        "Xuanjing Huang"
      ],
      "abstract": "The existing supervised relation extraction methods have achieved impressive performance in a closed-set setting, in which the relations remain the same during both training and testing. In a more realistic open-set setting, unknown relations may appear in the test set. Due to the lack of supervision signals from unknown relations, a well-performing closed-set relation extractor can still confidently misclassify them into known relations. In this paper, we propose an unknown-aware training method, regularizing the model by dynamically synthesizing negative instances that can provide the missing supervision signals. Inspired by text adversarial attack, We adaptively apply small but critical perturbations to original training data,synthesizing difficult enough negative instances that are mistaken by the model as known relations, thus facilitating a compact decision boundary. Experimental results show that our method achieves SOTA unknown relation detection without compromising the classification of known relations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 527,
      "original_url": "https://aclanthology.org/2023.acl-long.526/",
      "title": "Learning to Imagine: Visually-Augmented Natural Language Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.526.pdf",
      "authors": [
        "Tianyi Tang",
        "Yushuo Chen",
        "Yifan Du",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "People often imagine relevant scenes to aid in the writing process. In this work, we aim to utilize visual information for composition in the same manner as humans. We propose a method, LIVE, that makes pre-trained language models (PLMs) Learn to Imagine for Visually-augmented natural language gEneration. First, we imagine the scene based on the text: we use a diffusion model to synthesize high-quality images conditioned on the input texts. Second, we use CLIP to determine whether the text can evoke the imagination in a posterior way. Finally, our imagination is dynamic, and we conduct synthesis for each sentence rather than generate only one image for an entire paragraph. Technically, we propose a novel plug-and-play fusion layer to obtain visually-augmented representations for each text. Our vision-text fusion layer is compatible with Transformer-based architecture. We have conducted extensive experiments on four generation tasks using BART and T5, and the automatic results and human evaluation demonstrate the effectiveness of our proposed method. We will release the code, model, and data at the link: https://github.com/RUCAIBox/LIVE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 528,
      "original_url": "https://aclanthology.org/2023.acl-long.527/",
      "title": "Generating Hashtags for Short-form Videos with Guided Signals",
      "pdf_url": "https://aclanthology.org/2023.acl-long.527.pdf",
      "authors": [
        "Tiezheng Yu",
        "Hanchao Yu",
        "Davis Liang",
        "Yuning Mao",
        "Shaoliang Nie",
        "Po-Yao Huang",
        "Madian Khabsa",
        "Pascale Fung",
        "Yi-Chia Wang"
      ],
      "abstract": "Short-form video hashtag recommendation (SVHR) aims to recommend hashtags to content creators from videos and corresponding descriptions. Most prior studies regard SVHR as a classification or ranking problem and select hashtags from a set of limited candidates. However, in reality, users can create new hashtags, and trending hashtags change rapidly over time on social media. Both of these properties cannot be easily modeled with classification approaches. To bridge this gap, we formulate SVHR as a generation task that better represents how hashtags are created naturally. Additionally, we propose the Guided Generative Model (GGM) where we augment the input features by retrieving relevant hashtags from a large-scale hashtag pool as extra guidance signals. Experimental results on two short-form video datasets show that our generative models outperform strong classification baselines, and the guidance signals further boost the performance by 8.11 and 2.17 absolute ROUGE-1 scores on average, respectively. We also perform extensive analyses including human evaluation, demonstrating that our generative model can create meaningful and relevant novel hashtags while achieving state-of-the-art performance on known hashtags"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 529,
      "original_url": "https://aclanthology.org/2023.acl-long.528/",
      "title": "NEUROSTRUCTURAL DECODING: Neural Text Generation with Structural Constraints",
      "pdf_url": "https://aclanthology.org/2023.acl-long.528.pdf",
      "authors": [
        "Mohaddeseh Bastan",
        "Mihai Surdeanu",
        "Niranjan Balasubramanian"
      ],
      "abstract": "Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints. We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build NeuroStructural Decoding on the NeuroLogic Decoding (Lu etal. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object)during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences. Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 530,
      "original_url": "https://aclanthology.org/2023.acl-long.529/",
      "title": "The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.529.pdf",
      "authors": [
        "Zhuang Li",
        "Lizhen Qu",
        "Philip Cohen",
        "Raj Tumuluri",
        "Gholamreza Haffari"
      ],
      "abstract": "Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues. However, human translations are expensive, while machine translations are cheap but prone to error and bias. In this work, we propose an active learning approach that exploits the strengths of both human and machine translations by iteratively adding small batches of human translations into the machine-translated training set. Besides, we propose novel aggregated acquisition criteria that help our active learning method select utterances to be manually translated. Our experiments demonstrate that an ideal utterance selection can significantly reduce the error and bias in the translated data, resulting in higher parser accuracies than the parsers merely trained on the machine-translated data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 531,
      "original_url": "https://aclanthology.org/2023.acl-long.530/",
      "title": "Ideology Prediction from Scarce and Biased Supervision: Learn to Disregard the “What” and Focus on the “How”!",
      "pdf_url": "https://aclanthology.org/2023.acl-long.530.pdf",
      "authors": [
        "Chen Chen",
        "Dylan Walker",
        "Venkatesh Saligrama"
      ],
      "abstract": "We propose a novel supervised learning approach for political ideology prediction (PIP) that is capable of predicting out-of-distribution inputs. This problem is motivated by the fact that manual data-labeling is expensive, while self-reported labels are often scarce and exhibit significant selection bias. We propose a novel statistical model that decomposes the document embeddings into a linear superposition of two vectors; a latent neutral context vector independent of ideology, and a latent position vector aligned with ideology. We train an end-to-end model that has intermediate contextual and positional vectors as outputs. At deployment time, our model predicts labels for input documents by exclusively leveraging the predicted positional vectors. On two benchmark datasets we show that our model is capable of outputting predictions even when trained with as little as 5% biased data, and is significantly more accurate than the state-of-the-art. Through crowd-sourcing we validate the neutrality of contextual vectors, and show that context filtering results in ideological concentration, allowing for prediction on out-of-distribution examples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 532,
      "original_url": "https://aclanthology.org/2023.acl-long.531/",
      "title": "Unsupervised Extractive Summarization of Emotion Triggers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.531.pdf",
      "authors": [
        "Tiberiu Sosea",
        "Hongli Zhan",
        "Junyi Jessy Li",
        "Cornelia Caragea"
      ],
      "abstract": "Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting (Zhan et al., 2022)’s abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from external sources combined with a language understanding module, and outperforms strong baselines. We release our data and code at https://github.com/tsosea2/CovidET-EXT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 533,
      "original_url": "https://aclanthology.org/2023.acl-long.532/",
      "title": "Document-Level Event Argument Extraction With a Chain Reasoning Paradigm",
      "pdf_url": "https://aclanthology.org/2023.acl-long.532.pdf",
      "authors": [
        "Jian Liu",
        "Chen Liang",
        "Jinan Xu",
        "Haoyan Liu",
        "Zhe Zhao"
      ],
      "abstract": "Document-level event argument extraction aims to identify event arguments beyond sentence level, where a significant challenge is to model long-range dependencies. Focusing on this challenge, we present a new chain reasoning paradigm for the task, which can generate decomposable first-order logic rules for reasoning. This paradigm naturally captures long-range interdependence due to the chains’ compositional nature, which also improves interpretability by explicitly modeling the reasoning process. We introduce T-norm fuzzy logic for optimization, which permits end-to-end learning and shows promise for integrating the expressiveness of logical reasoning with the generalization of neural networks. In experiments, we show that our approach outperforms previous methods by a significant margin on two standard benchmarks (over 6 points in F1).Moreover, it is data-efficient in low-resource scenarios and robust enough to defend against adversarial attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 534,
      "original_url": "https://aclanthology.org/2023.acl-long.533/",
      "title": "Pre-training Multi-party Dialogue Models with Latent Discourse Inference",
      "pdf_url": "https://aclanthology.org/2023.acl-long.533.pdf",
      "authors": [
        "Yiyang Li",
        "Xinting Huang",
        "Wei Bi",
        "Hai Zhao"
      ],
      "abstract": "Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that understands the discourse structure of multi-party dialogues, namely, to whom each utterance is replying. However, due to the lack of explicitly annotated discourse labels in multi-party dialogue corpora, previous works fail to scale up the pre-training process by putting aside the unlabeled multi-party conversational data for nothing. To fully utilize the unlabeled data, we propose to treat the discourse structures as latent variables, then jointly infer them and pre-train the discourse-aware model by unsupervised latent variable inference methods. Experiments on multiple downstream tasks show that our pre-trained model outperforms strong baselines by large margins and achieves state-of-the-art (SOTA) results, justifying the effectiveness of our method. The official implementation of this paper is available at https://github.com/EricLee8/MPD_EMVI."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 535,
      "original_url": "https://aclanthology.org/2023.acl-long.534/",
      "title": "Interpreting Positional Information in Perspective of Word Order",
      "pdf_url": "https://aclanthology.org/2023.acl-long.534.pdf",
      "authors": [
        "Zhang Xilong",
        "Liu Ruochen",
        "Liu Jin",
        "Liang Xuefeng"
      ],
      "abstract": "The attention mechanism is a powerful and effective method utilized in natural language processing. However, it has been observed that this method is insensitive to positional information. Although several studies have attempted to improve positional encoding and investigate the influence of word order perturbation, it remains unclear how positional encoding impacts NLP models from the perspective of word order. In this paper, we aim to shed light on this problem by analyzing the working mechanism of the attention module and investigating the root cause of its inability to encode positional information. Our hypothesis is that the insensitivity can be attributed to the weight sum operation utilized in the attention module. To verify this hypothesis, we propose a novel weight concatenation operation and evaluate its efficacy in neural machine translation tasks. Our enhanced experimental results not only reveal that the proposed operation can effectively encode positional information but also confirm our hypothesis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 536,
      "original_url": "https://aclanthology.org/2023.acl-long.535/",
      "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.535.pdf",
      "authors": [
        "Chandra Bhagavatula",
        "Jena D. Hwang",
        "Doug Downey",
        "Ronan Le Bras",
        "Ximing Lu",
        "Lianhui Qin",
        "Keisuke Sakaguchi",
        "Swabha Swayamdipta",
        "Peter West",
        "Yejin Choi"
      ],
      "abstract": "Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly. We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model’s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 537,
      "original_url": "https://aclanthology.org/2023.acl-long.536/",
      "title": "More than Classification: A Unified Framework for Event Temporal Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.536.pdf",
      "authors": [
        "Quzhe Huang",
        "Yutong Hu",
        "Shengqi Zhu",
        "Yansong Feng",
        "Chang Liu",
        "Dongyan Zhao"
      ],
      "abstract": "Event temporal relation extraction (ETRE) is usually formulated as a multi-label classification task, where each type of relation is simply treated as a one-hot label. This formulation ignores the meaning of relations and wipes out their intrinsic dependency. After examining the relation definitions in various ETRE tasks, we observe that all relations can be interpreted using the start and end time points of events. For example, relation Includes could be interpreted as event 1 starting no later than event 2 and ending no earlier than event 2. In this paper, we propose a unified event temporal relation extraction framework, which transforms temporal relations into logical expressions of time points and completes the ETRE by predicting the relations between certain time point pairs. Experiments on TB-Dense and MATRES show significant improvements over a strong baseline and outperform the state-of-the-art model by 0.3% on both datasets. By representing all relations in a unified framework, we can leverage the relations with sufficient data to assist the learning of other relations, thus achieving stable improvement in low-data scenarios. When the relation definitions are changed, our method can quickly adapt to the new ones by simply modifying the logic expressions that map time points to new event relations. The code is released at https://github.com/AndrewZhe/A-Unified-Framework-for-ETRE"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 538,
      "original_url": "https://aclanthology.org/2023.acl-long.537/",
      "title": "Multi-Source Test-Time Adaptation as Dueling Bandits for Extractive Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.537.pdf",
      "authors": [
        "Hai Ye",
        "Qizhe Xie",
        "Hwee Tou Ng"
      ],
      "abstract": "In this work, we study multi-source test-time model adaptation from user feedback, where K distinct models are established for adaptation. To allow efficient adaptation, we cast the problem as a stochastic decision-making process, aiming to determine the best adapted model after adaptation. We discuss two frameworks: multi-armed bandit learning and multi-armed dueling bandits. Compared to multi-armed bandit learning, the dueling framework allows pairwise collaboration among K models, which is solved by a novel method named Co-UCB proposed in this work. Experiments on six datasets of extractive question answering (QA) show that the dueling framework using Co-UCB is more effective than other strong baselines for our studied problem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 539,
      "original_url": "https://aclanthology.org/2023.acl-long.538/",
      "title": "Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery",
      "pdf_url": "https://aclanthology.org/2023.acl-long.538.pdf",
      "authors": [
        "Yutao Mou",
        "Xiaoshuai Song",
        "Keqing He",
        "Chen Zeng",
        "Pei Wang",
        "Jingang Wang",
        "Yunsen Xian",
        "Weiran Xu"
      ],
      "abstract": "Generalized intent discovery aims to extend a closed-set in-domain intent classifier to an open-world intent set including in-domain and out-of-domain intents. The key challenges lie in pseudo label disambiguation and representation learning. Previous methods suffer from a coupling of pseudo label disambiguation and representation learning, that is, the reliability of pseudo labels relies on representation learning, and representation learning is restricted by pseudo labels in turn. In this paper, we propose a decoupled prototype learning framework (DPL) to decouple pseudo label disambiguation and representation learning. Specifically, we firstly introduce prototypical contrastive representation learning (PCL) to get discriminative representations. And then we adopt a prototype-based label disambiguation method (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD work in a collaborative fashion and facilitate pseudo label disambiguation. Experiments and analysis on three benchmark datasets show the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 540,
      "original_url": "https://aclanthology.org/2023.acl-long.539/",
      "title": "DecompEval: Evaluating Generated Texts as Unsupervised Decomposed Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.539.pdf",
      "authors": [
        "Pei Ke",
        "Fei Huang",
        "Fei Mi",
        "Yasheng Wang",
        "Qun Liu",
        "Xiaoyan Zhu",
        "Minlie Huang"
      ],
      "abstract": "Existing evaluation metrics for natural language generation (NLG) tasks face the challenges on generalization ability and interpretability. Specifically, most of the well-performed metrics are required to train on evaluation datasets of specific NLG tasks and evaluation dimensions, which may cause over-fitting to task-specific datasets. Furthermore, existing metrics only provide an evaluation score for each dimension without revealing the evidence to interpret how this score is obtained. To deal with these challenges, we propose a simple yet effective metric called DecompEval. This metric formulates NLG evaluation as an instruction-style question answering task and utilizes instruction-tuned pre-trained language models (PLMs) without training on evaluation datasets, aiming to enhance the generalization ability. To make the evaluation process more interpretable, we decompose our devised instruction-style question about the quality of generated texts into the subquestions that measure the quality of each sentence. The subquestions with their answers generated by PLMs are then recomposed as evidence to obtain the evaluation result. Experimental results show that DecompEval achieves state-of-the-art performance in untrained metrics for evaluating text summarization and dialogue generation, which also exhibits strong dimension-level / task-level generalization ability and interpretability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 541,
      "original_url": "https://aclanthology.org/2023.acl-long.540/",
      "title": "Backdooring Neural Code Search",
      "pdf_url": "https://aclanthology.org/2023.acl-long.540.pdf",
      "authors": [
        "Weisong Sun",
        "Yuchen Chen",
        "Guanhong Tao",
        "Chunrong Fang",
        "Xiangyu Zhang",
        "Quanjun Zhang",
        "Bin Luo"
      ],
      "abstract": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 542,
      "original_url": "https://aclanthology.org/2023.acl-long.541/",
      "title": "Concise Answers to Complex Questions: Summarization of Long-form Answers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.541.pdf",
      "authors": [
        "Abhilash Potluri",
        "Fangyuan Xu",
        "Eunsol Choi"
      ],
      "abstract": "Long-form question answering systems provide rich information by presenting paragraph-level answers, often containing optional background or auxiliary information. While such comprehensive answers are helpful, not all information is required to answer the question (e.g. users with domain knowledge do not need an explanation of background). Can we provide a concise version of the answer by summarizing it, while still addressing the question? We conduct a user study on summarized answers generated from state-of-the-art models and our newly proposed extract-and-decontextualize approach. We find a large proportion of long-form answers (over 90%) in the ELI5 domain can be adequately summarized by at least one system, while complex and implicit answers are challenging to compress. We observe that decontextualization improves the quality of the extractive summary, exemplifying its potential in the summarization task. To promote future work, we provide an extractive summarization dataset covering 1K long-form answers and our user study annotations. Together, we present the first study on summarizing long-form answers, taking a step forward for QA agents that can provide answers at multiple granularities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 543,
      "original_url": "https://aclanthology.org/2023.acl-long.542/",
      "title": "Towards Better Entity Linking with Multi-View Enhanced Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.542.pdf",
      "authors": [
        "Yi Liu",
        "Yuan Tian",
        "Jianxun Lian",
        "Xinlong Wang",
        "Yanan Cao",
        "Fang Fang",
        "Wen Zhang",
        "Haizhen Huang",
        "Weiwei Deng",
        "Qi Zhang"
      ],
      "abstract": "Dense retrieval is widely used for entity linking to retrieve entities from large-scale knowledge bases. Mainstream techniques are based on a dual-encoder framework, which encodes mentions and entities independently and calculates their relevances via rough interaction metrics, resulting in difficulty in explicitly modeling multiple mention-relevant parts within entities to match divergent mentions. Aiming at learning entity representations that can match divergent mentions, this paper proposes a Multi-View Enhanced Distillation (MVD) framework, which can effectively transfer knowledge of multiple fine-grained and mention-relevant parts within entities from cross-encoders to dual-encoders. Each entity is split into multiple views to avoid irrelevant information being over-squashed into the mention-relevant view. We further design cross-alignment and self-alignment mechanisms for this framework to facilitate fine-grained knowledge distillation from the teacher model to the student model. Meanwhile, we reserve a global-view that embeds the entity as a whole to prevent dispersal of uniform information. Experiments show our method achieves state-of-the-art performance on several entity linking benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 544,
      "original_url": "https://aclanthology.org/2023.acl-long.543/",
      "title": "A Measure-Theoretic Characterization of Tight Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.543.pdf",
      "authors": [
        "Li Du",
        "Lucas Torroba Hennigen",
        "Tiago Pimentel",
        "Clara Meister",
        "Jason Eisner",
        "Ryan Cotterell"
      ],
      "abstract": "Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 545,
      "original_url": "https://aclanthology.org/2023.acl-long.544/",
      "title": "PAED: Zero-Shot Persona Attribute Extraction in Dialogues",
      "pdf_url": "https://aclanthology.org/2023.acl-long.544.pdf",
      "authors": [
        "Luyao Zhu",
        "Wei Li",
        "Rui Mao",
        "Vlad Pandelea",
        "Erik Cambria"
      ],
      "abstract": "Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for persona attribute extraction. We also propose a contrastive learning- and generation-based model with a novel hard negative sampling strategy for generalized zero-shot persona attribute extraction. We benchmark our model with state-of-the-art baselines on our dataset and a public dataset, showing outstanding accuracy gains. Our sampling strategy also exceeds others by a large margin in persona attribute extraction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 546,
      "original_url": "https://aclanthology.org/2023.acl-long.545/",
      "title": "PromptRank: Unsupervised Keyphrase Extraction Using Prompt",
      "pdf_url": "https://aclanthology.org/2023.acl-long.545.pdf",
      "authors": [
        "Aobo Kong",
        "Shiwan Zhao",
        "Hao Chen",
        "Qicheng Li",
        "Yong Qin",
        "Ruiqi Sun",
        "Xiaoyan Bai"
      ],
      "abstract": "The keyphrase extraction task refers to the automatic selection of phrases from a given document to summarize its core content. State-of-the-art (SOTA) performance has recently been achieved by embedding-based algorithms, which rank candidates according to how similar their embeddings are to document embeddings. However, such solutions either struggle with the document and candidate length discrepancies or fail to fully utilize the pre-trained language model (PLM) without further fine-tuning. To this end, in this paper, we propose a simple yet effective unsupervised approach, PromptRank, based on the PLM with an encoder-decoder architecture. Specifically, PromptRank feeds the document into the encoder and calculates the probability of generating the candidate with a designed prompt by the decoder. We extensively evaluate the proposed PromptRank on six widely used benchmarks. PromptRank outperforms the SOTA approach MDERank, improving the F1 score relatively by 34.18%, 24.87%, and 17.57% for 5, 10, and 15 returned results, respectively. This demonstrates the great potential of using prompt for unsupervised keyphrase extraction. We release our code at https://github.com/HLT-NLP/PromptRank."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 547,
      "original_url": "https://aclanthology.org/2023.acl-long.546/",
      "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
      "pdf_url": "https://aclanthology.org/2023.acl-long.546.pdf",
      "authors": [
        "Alex Mallen",
        "Akari Asai",
        "Victor Zhong",
        "Rajarshi Das",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 548,
      "original_url": "https://aclanthology.org/2023.acl-long.547/",
      "title": "infoVerse: A Universal Framework for Dataset Characterization with Multidimensional Meta-information",
      "pdf_url": "https://aclanthology.org/2023.acl-long.547.pdf",
      "authors": [
        "Jaehyung Kim",
        "Yekyung Kim",
        "Karin de Langis",
        "Jinwoo Shin",
        "Dongyeop Kang"
      ],
      "abstract": "The success of NLP systems often relies on the availability of large, high-quality datasets. However, not all samples in these datasets are equally valuable for learning, as some may be redundant or noisy. Several methods for characterizing datasets based on model-driven meta-information (e.g., model’s confidence) have been developed, but the relationship and complementary effects of these methods have received less attention. In this paper, we introduce infoVerse, a universal framework for dataset characterization, which provides a new feature space that effectively captures multidimensional characteristics of datasets by incorporating various model-driven meta-information. infoVerse reveals distinctive regions of the dataset that are not apparent in the original semantic space, hence guiding users (or models) in identifying which samples to focus on for exploration, assessment, or annotation. Additionally, we propose a novel sampling method on infoVerse to select a set of data points that maximizes informativeness. In three real-world applications (data pruning, active learning, and data annotation), the samples chosen on infoVerse space consistently outperform strong baselines in all applications. Our code and demo are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 549,
      "original_url": "https://aclanthology.org/2023.acl-long.548/",
      "title": "SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.548.pdf",
      "authors": [
        "Akshita Jha",
        "Aida Mostafazadeh Davani",
        "Chandan K Reddy",
        "Shachi Dave",
        "Vinodkumar Prabhakaran",
        "Sunipa Dev"
      ],
      "abstract": "Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 550,
      "original_url": "https://aclanthology.org/2023.acl-long.549/",
      "title": "Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.549.pdf",
      "authors": [
        "Lucy Lu Wang",
        "Yulia Otmakhova",
        "Jay DeYoung",
        "Thinh Hung Truong",
        "Bailey Kuehl",
        "Erin Bransom",
        "Byron Wallace"
      ],
      "abstract": "Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical literature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using standard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better automated evaluation methods for literature review MDS. We take advantage of community submissions to the Multi-document Summarization for Literature Review (MSLR) shared task to compile a diverse and representative sample of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated summaries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to capture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 551,
      "original_url": "https://aclanthology.org/2023.acl-long.550/",
      "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
      "pdf_url": "https://aclanthology.org/2023.acl-long.550.pdf",
      "authors": [
        "Jiangjie Chen",
        "Wei Shi",
        "Ziquan Fu",
        "Sijie Cheng",
        "Lei Li",
        "Yanghua Xiao"
      ],
      "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as “lions don’t live in the ocean”, is also ubiquitous in the world but rarely mentioned explicitly in text. What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 552,
      "original_url": "https://aclanthology.org/2023.acl-long.551/",
      "title": "An Inner Table Retriever for Robust Table Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.551.pdf",
      "authors": [
        "Weizhe Lin",
        "Rexhina Blloshmi",
        "Bill Byrne",
        "Adria de Gispert",
        "Gonzalo Iglesias"
      ],
      "abstract": "Recent years have witnessed the thriving of pretrained Transformer-based language models for understanding semi-structured tables, with several applications, such as Table Question Answering (TableQA).These models are typically trained on joint tables and surrounding natural language text, by linearizing table content into sequences comprising special tokens and cell information. This yields very long sequences which increase system inefficiency, and moreover, simply truncating long sequences results in information loss for downstream tasks. We propose Inner Table Retriever (ITR), a general-purpose approach for handling long tables in TableQA that extracts sub-tables to preserve the most relevant information for a question. We show that ITR can be easily integrated into existing systems to improve their accuracy with up to 1.3-4.8% and achieve state-of-the-art results in two benchmarks, i.e., 63.4% in WikiTableQuestions and 92.1% in WikiSQL. Additionally, we show that ITR makes TableQA systems more robust to reduced model capacity and to different ordering of columns and rows. We make our code available at: https://github.com/amazon-science/robust-tableqa."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 553,
      "original_url": "https://aclanthology.org/2023.acl-long.552/",
      "title": "SIMSUM: Document-level Text Simplification via Simultaneous Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.552.pdf",
      "authors": [
        "Sofia Blinova",
        "Xinyu Zhou",
        "Martin Jaggi",
        "Carsten Eickhoff",
        "Seyed Ali Bahrainian"
      ],
      "abstract": "Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Our model is designed with explicit summarization and simplification models and guides the generation using the main keywords of a source text. In order to evaluate our new model, we use two existing benchmark datasets for simplification, namely D-Wikipedia and Wiki-Doc. We compare our model’s performance with state of the art and show that SIMSUM achieves top results on the D-Wikipedia dataset SARI (+1.20), D-SARI (+1.64), and FKGL (-0.35) scores, improving over the best baseline models. In order to evaluate the quality of the generated text, we analyze the outputs from different models qualitatively and demonstrate the merit of our new model. Our code and datasets are available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 554,
      "original_url": "https://aclanthology.org/2023.acl-long.553/",
      "title": "SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.553.pdf",
      "authors": [
        "Junkai Zhou",
        "Liang Pang",
        "Huawei Shen",
        "Xueqi Cheng"
      ],
      "abstract": "Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 555,
      "original_url": "https://aclanthology.org/2023.acl-long.554/",
      "title": "NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic",
      "pdf_url": "https://aclanthology.org/2023.acl-long.554.pdf",
      "authors": [
        "Zi’ou Zheng",
        "Xiaodan Zhu"
      ],
      "abstract": "Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles’ heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle’s syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial examples with fewer visits to the victim models. The victim models are found to be more vulnerable under the label-flipping setting. NatLogAttack provides a tool to probe the existing and future NLI models’ capacity from a key viewpoint and we hope more logic-based attacks will be further explored for understanding the desired property of reasoning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 556,
      "original_url": "https://aclanthology.org/2023.acl-long.555/",
      "title": "Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.555.pdf",
      "authors": [
        "Ashish Sharma",
        "Kevin Rushton",
        "Inna Lin",
        "David Wadden",
        "Khendra Lucas",
        "Adam Miner",
        "Theresa Nguyen",
        "Tim Althoff"
      ],
      "abstract": "A proven therapeutic technique to overcome negative thoughts is to replace them with a more hopeful “reframed thought.” Although therapy can help people practice and learn this Cognitive Reframing of Negative Thoughts, clinician shortages and mental health stigma commonly limit people’s access to therapy. In this paper, we conduct a human-centered study of how language models may assist people in reframing negative thoughts. Based on psychology literature, we define a framework of seven linguistic attributes that can be used to reframe a thought. We develop automated metrics to measure these attributes and validate them with expert judgements from mental health practitioners. We collect a dataset of 600 situations, thoughts and reframes from practitioners and use it to train a retrieval-enhanced in-context learning model that effectively generates reframed thoughts and controls their linguistic attributes. To investigate what constitutes a “high-quality” reframe, we conduct an IRB-approved randomized field study on a large mental health website with over 2,000 participants. Amongst other findings, we show that people prefer highly empathic or specific reframes, as opposed to reframes that are overly positive. Our findings provide key implications for the use of LMs to assist people in overcoming negative thoughts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 557,
      "original_url": "https://aclanthology.org/2023.acl-long.556/",
      "title": "Dating Greek Papyri with Text Regression",
      "pdf_url": "https://aclanthology.org/2023.acl-long.556.pdf",
      "authors": [
        "John Pavlopoulos",
        "Maria Konstantinidou",
        "Isabelle Marthot-Santaniello",
        "Holger Essler",
        "Asimina Paparigopoulou"
      ],
      "abstract": "Dating Greek papyri accurately is crucial not only to edit their texts but also to understand numerous other aspects of ancient writing, document and book production and circulation, as well as various other aspects of administration, everyday life and intellectual history of antiquity. Although a substantial number of Greek papyri documents bear a date or other conclusive data as to their chronological placement, an even larger number can only be dated tentatively or in approximation, due to the lack of decisive evidence. By creating a dataset of 389 transcriptions of documentary Greek papyri, we train 389 regression models and we predict a date for the papyri with an average MAE of 54 years and an MSE of 1.17, outperforming image classifiers and other baselines. Last, we release date estimations for 159 manuscripts, for which only the upper limit is known."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 558,
      "original_url": "https://aclanthology.org/2023.acl-long.557/",
      "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.557.pdf",
      "authors": [
        "Harsh Trivedi",
        "Niranjan Balasubramanian",
        "Tushar Khot",
        "Ashish Sabharwal"
      ],
      "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 559,
      "original_url": "https://aclanthology.org/2023.acl-long.558/",
      "title": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking",
      "pdf_url": "https://aclanthology.org/2023.acl-long.558.pdf",
      "authors": [
        "Jinheon Baek",
        "Alham Fikri Aji",
        "Jens Lehmann",
        "Sung Ju Hwang"
      ],
      "abstract": "There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 560,
      "original_url": "https://aclanthology.org/2023.acl-long.559/",
      "title": "DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.559.pdf",
      "authors": [
        "Ella Neeman",
        "Roee Aharoni",
        "Or Honovich",
        "Leshem Choshen",
        "Idan Szpektor",
        "Omri Abend"
      ],
      "abstract": "Question answering models commonly have access to two sources of “knowledge” during inference time: (1) parametric knowledge - the factual knowledge encoded in the model weights, and (2) contextual knowledge - external knowledge (e.g., a Wikipedia passage) given to the model to generate a grounded answer. Having these two sources of knowledge entangled together is a core issue for generative QA models as it is unclear whether the answer stems from the given non-parametric knowledge or not. This unclarity has implications on issues of trust, interpretability and factuality. In this work, we propose a new paradigm in which QA models are trained to disentangle the two sources of knowledge. Using counterfactual data augmentation, we introduce a model that predicts two answers for a given question: one based on given contextual knowledge and one based on parametric knowledge. Our experiments on the Natural Questions dataset show that this approach improves the performance of QA models by making them more robust to knowledge conflicts between the two knowledge sources, while generating useful disentangled answers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 561,
      "original_url": "https://aclanthology.org/2023.acl-long.560/",
      "title": "A New Direction in Stance Detection: Target-Stance Extraction in the Wild",
      "pdf_url": "https://aclanthology.org/2023.acl-long.560.pdf",
      "authors": [
        "Yingjie Li",
        "Krishna Garg",
        "Cornelia Caragea"
      ],
      "abstract": "Stance detection aims to detect the stance toward a corresponding target. Existing works use the assumption that the target is known in advance, which is often not the case in the wild. Given a text from social media platforms, the target information is often unknown due to implicit mentions in the source text and it is infeasible to have manual target annotations at a large scale. Therefore, in this paper, we propose a new task Target-Stance Extraction (TSE) that aims to extract the (target, stance) pair from the text. We benchmark the task by proposing a two-stage framework that first identifies the relevant target in the text and then detects the stance given the predicted target and text. Specifically, we first propose two different settings: Target Classification and Target Generation, to identify the potential target from a given text. Then we propose a multi-task approach that takes target prediction as the auxiliary task to detect the stance toward the predicted target. We evaluate the proposed framework on both in-target stance detection in which the test target is always seen in the training stage and zero-shot stance detection that needs to detect the stance for the targets that are unseen during the training phase. The new TSE task can facilitate future research in the field of stance detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 562,
      "original_url": "https://aclanthology.org/2023.acl-long.561/",
      "title": "Improved Instruction Ordering in Recipe-Grounded Conversation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.561.pdf",
      "authors": [
        "Duong Le",
        "Ruohao Guo",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "In this paper, we study the task of instructional dialogue and focus on the cooking domain. Analyzing the generated output of the GPT-J model, we reveal that the primary challenge for a recipe-grounded dialog system is how to provide the instructions in the correct order. We hypothesize that this is due to the model’s lack of understanding of user intent and inability to track the instruction state (i.e., which step was last instructed). Therefore, we propose to explore two auxiliary subtasks, namely User Intent Detection and Instruction State Tracking, to support Response Generation with improved instruction grounding. Experimenting with our newly collected dataset, ChattyChef, shows that incorporating user intent and instruction state information helps the response generation model mitigate the incorrect order issue. Furthermore, to investigate whether ChatGPT has completely solved this task, we analyze its outputs and find that it also makes mistakes (10.7% of the responses), about half of which are out-of-order instructions. We will release ChattyChef to facilitate further research in this area at: https://github.com/octaviaguo/ChattyChef."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 563,
      "original_url": "https://aclanthology.org/2023.acl-long.562/",
      "title": "Token-wise Decomposition of Autoregressive Language Model Hidden States for Analyzing Model Predictions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.562.pdf",
      "authors": [
        "Byung-Doh Oh",
        "William Schuler"
      ],
      "abstract": "While there is much recent interest in studying why Transformer-based large language models make predictions the way they do, the complex computations performed within each layer have made their behavior somewhat opaque. To mitigate this opacity, this work presents a linear decomposition of final hidden states from autoregressive language models based on each initial input token, which is exact for virtually all contemporary Transformer architectures. This decomposition allows the definition of probability distributions that ablate the contribution of specific input tokens, which can be used to analyze their influence on model probabilities over a sequence of upcoming words with only one forward pass from the model. Using the change in next-word probability as a measure of importance, this work first examines which context words make the biggest contribution to language model predictions. Regression experiments suggest that Transformer-based language models rely primarily on collocational associations, followed by linguistic factors such as syntactic dependencies and coreference relationships in making next-word predictions. Additionally, analyses using these measures to predict syntactic dependencies and coreferent mention spans show that collocational association and repetitions of the same token largely explain the language models’ predictions on these tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 564,
      "original_url": "https://aclanthology.org/2023.acl-long.563/",
      "title": "Document-Level Multi-Event Extraction with Event Proxy Nodes and Hausdorff Distance Minimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.563.pdf",
      "authors": [
        "Xinyu Wang",
        "Lin Gui",
        "Yulan He"
      ],
      "abstract": "Document-level multi-event extraction aims to extract the structural information from a given document automatically. Most recent approaches usually involve two steps: (1) modeling entity interactions; (2) decoding entity interactions into events. However, such approaches ignore a global view of inter-dependency of multiple events. Moreover, an event is decoded by iteratively merging its related entities as arguments, which might suffer from error propagation and is computationally inefficient. In this paper, we propose an alternative approach for document-level multi-event extraction with event proxy nodes and Hausdorff distance minimization. The event proxy nodes, representing pseudo-events, are able to build connections with other event proxy nodes, essentially capturing global information. The Hausdorff distance makes it possible to compare the similarity between the set of predicted events and the set of ground-truth events. By directly minimizing Hausdorff distance, the model is trained towards the global optimum directly, which improves performance and reduces training time. Experimental results show that our model outperforms previous state-of-the-art method in F1-score on two datasets with only a fraction of training time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 565,
      "original_url": "https://aclanthology.org/2023.acl-long.564/",
      "title": "Dialog-Post: Multi-Level Self-Supervised Objectives and Hierarchical Model for Dialogue Post-Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.564.pdf",
      "authors": [
        "Zhenyu Zhang",
        "Lei Shen",
        "Yuming Zhao",
        "Meng Chen",
        "Xiaodong He"
      ],
      "abstract": "Dialogue representation and understanding aim to convert conversational inputs into embeddings and fulfill discriminative tasks. Compared with free-form text, dialogue has two important characteristics, hierarchical semantic structure and multi-facet attributes. Therefore, directly applying the pretrained language models (PLMs) might result in unsatisfactory performance. Recently, several work focused on the dialogue-adaptive post-training (DialPost) that further trains PLMs to fit dialogues. To model dialogues more comprehensively, we propose a DialPost method, Dialog-Post, with multi-level self-supervised objectives and a hierarchical model. These objectives leverage dialogue-specific attributes and use self-supervised signals to fully facilitate the representation and understanding of dialogues. The novel model is a hierarchical segment-wise self-attention network, which contains inner-segment and inter-segment self-attention sub-layers followed by an aggregation and updating module. To evaluate the effectiveness of our methods, we first apply two public datasets for the verification of representation ability. Then we conduct experiments on a newly-labelled dataset that is annotated with 4 dialogue understanding tasks. Experimental results show that our method outperforms existing SOTA models and achieves a 3.3% improvement on average."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 566,
      "original_url": "https://aclanthology.org/2023.acl-long.565/",
      "title": "Language Detoxification with Attribute-Discriminative Latent Space",
      "pdf_url": "https://aclanthology.org/2023.acl-long.565.pdf",
      "authors": [
        "Jin Myung Kwak",
        "Minseon Kim",
        "Sung Ju Hwang"
      ],
      "abstract": "Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-Discriminative Language Model (ADLM) on detoxified language and dialogue generation tasks, on which our method significantly outperforms baselines both in performance and efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 567,
      "original_url": "https://aclanthology.org/2023.acl-long.566/",
      "title": "Just Like a Human Would, Direct Access to Sarcasm Augmented with Potential Result and Reaction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.566.pdf",
      "authors": [
        "Changrong Min",
        "Ximing Li",
        "Liang Yang",
        "Zhilin Wang",
        "Bo Xu",
        "Hongfei Lin"
      ],
      "abstract": "Sarcasm, as a form of irony conveying mockery and contempt, has been widespread in social media such as Twitter and Weibo, where the sarcastic text is commonly characterized as an incongruity between the surface positive and negative situation. Naturally, it has an urgent demand to automatically identify sarcasm from social media, so as to illustrate people’s real views toward specific targets. In this paper, we develop a novel sarcasm detection method, namely Sarcasm Detector with Augmentation of Potential Result and Reaction (SD-APRR). Inspired by the direct access view, we treat each sarcastic text as an incomplete version without latent content associated with implied negative situations, including the result and human reaction caused by its observable content. To fill the latent content, we estimate the potential result and human reaction for each given training sample by [xEffect] and [xReact] relations inferred by the pre-trained commonsense reasoning tool COMET, and integrate the sample with them as an augmented one. We can then employ those augmented samples to train the sarcasm detector, whose encoder is a graph neural network with a denoising module. We conduct extensive empirical experiments to evaluate the effectiveness of SD-APRR. The results demonstrate that SD-APRR can outperform strong baselines on benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 568,
      "original_url": "https://aclanthology.org/2023.acl-long.567/",
      "title": "Adaptive and Personalized Exercise Generation for Online Language Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.567.pdf",
      "authors": [
        "Peng Cui",
        "Mrinmaya Sachan"
      ],
      "abstract": "Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise generation for online language learning. To this end, we combine a knowledge tracing model that estimates each student’s evolving knowledge states from their learning history and a controlled text generation model that generates exercise sentences based on the student’s current estimated knowledge state and instructor requirements of desired properties (e.g., domain knowledge and difficulty). We train and evaluate our model on real-world learner interaction data from Duolingo and demonstrate that LMs guided by student states can generate superior exercises. Then, we discuss the potential use of our model in educational applications using various simulations. These simulations show that our model can adapt to students’ individual abilities and can facilitate their learning efficiency by personalizing learning sequences."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 569,
      "original_url": "https://aclanthology.org/2023.acl-long.568/",
      "title": "NLP Reproducibility For All: Understanding Experiences of Beginners",
      "pdf_url": "https://aclanthology.org/2023.acl-long.568.pdf",
      "authors": [
        "Shane Storks",
        "Keunwoo Yu",
        "Ziqiao Ma",
        "Joyce Chai"
      ],
      "abstract": "As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners’ feedback to provide actionable ideas on how to better support them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 570,
      "original_url": "https://aclanthology.org/2023.acl-long.569/",
      "title": "Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA",
      "pdf_url": "https://aclanthology.org/2023.acl-long.569.pdf",
      "authors": [
        "Elias Stengel-Eskin",
        "Jimena Guallar-Blasco",
        "Yi Zhou",
        "Benjamin Van Durme"
      ],
      "abstract": "Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. Our analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. We then develop an English question-generation model which we demonstrate via automatic and human evaluation produces less ambiguous questions. We further show that the question generation objective we use allows the model to integrate answer group information without any direct supervision."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 571,
      "original_url": "https://aclanthology.org/2023.acl-long.570/",
      "title": "UMRSpell: Unifying the Detection and Correction Parts of Pre-trained Models towards Chinese Missing, Redundant, and Spelling Correction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.570.pdf",
      "authors": [
        "Zheyu He",
        "Yujin Zhu",
        "Linlin Wang",
        "Liang Xu"
      ],
      "abstract": "Chinese Spelling Correction (CSC) is the task of detecting and correcting misspelled charac- ters in Chinese texts. As an important step for various downstream tasks, CSC confronts two challenges: 1) Character-level errors consist not only of spelling errors but also of missing and redundant ones that cause variable length between input and output texts, for which most CSC methods could not handle well because of the consistence length of texts required by their inherent detection-correction framework. Con- sequently, the two errors are considered out- side the scope and left to future work, despite the fact that they are widely found and bound to CSC task in Chinese industrial scenario, such as Automatic Speech Recognition (ASR) and Optical Character Recognition (OCR). 2) Most existing CSC methods focus on either detector or corrector and train different mod- els for each one, respectively, leading to in- sufficiency of parameters sharing. To address these issues, we propose a novel model UMR- Spell to learn detection and correction parts together at the same time from a multi-task learning perspective by using a detection trans- mission self-attention matrix, and flexibly deal with both missing, redundant, and spelling er- rors through re-tagging rules. Furthermore, we build a new dataset ECMR-2023 containing five kinds of character-level errors to enrich the CSC task closer to real-world applications. Ex- periments on both SIGHAN benchmarks and ECMR-2023 demonstrate the significant effec- tiveness of UMRSpell over previous represen- tative baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 572,
      "original_url": "https://aclanthology.org/2023.acl-long.571/",
      "title": "LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.571.pdf",
      "authors": [
        "Jeremiah Milbauer",
        "Annie Louis",
        "Mohammad Javad Hosseini",
        "Alex Fabrikant",
        "Donald Metzler",
        "Tal Schuster"
      ],
      "abstract": "Transformer encoders contextualize token representations by attending to all other tokens at each layer, leading to quadratic increase in compute effort with the input length. In practice, however, the input text of many NLP tasks can be seen as a sequence of related segments (e.g., the sequence of sentences within a passage, or the hypothesis and premise in NLI). While attending across these segments is highly beneficial for many tasks, we hypothesize that this interaction can be delayed until later encoding stages. To this end, we introduce Layer-Adjustable Interactions in Transformers (LAIT). Within LAIT, segmented inputs are first encoded independently, and then jointly. This partial two-tower architecture bridges the gap between a Dual Encoder’s ability to pre-compute representations for segments and a fully self-attentive Transformer’s capacity to model cross-segment attention. The LAIT framework effectively leverages existing pretrained Transformers and converts them into the hybrid of the two aforementioned architectures, allowing for easy and intuitive control over the performance-efficiency tradeoff. Experimenting on a wide range of NLP tasks, we find LAIT able to reduce 30-50% of the attention FLOPs on many tasks, while preserving high accuracy; in some practical settings, LAIT could reduce actual latency by orders of magnitude."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 573,
      "original_url": "https://aclanthology.org/2023.acl-long.572/",
      "title": "Local Interpretation of Transformer Based on Linear Decomposition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.572.pdf",
      "authors": [
        "Sen Yang",
        "Shujian Huang",
        "Wei Zou",
        "Jianbing Zhang",
        "Xinyu Dai",
        "Jiajun Chen"
      ],
      "abstract": "In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 574,
      "original_url": "https://aclanthology.org/2023.acl-long.573/",
      "title": "DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.573.pdf",
      "authors": [
        "Vijay Viswanathan",
        "Luyu Gao",
        "Tongshuang Wu",
        "Pengfei Liu",
        "Graham Neubig"
      ],
      "abstract": "Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 575,
      "original_url": "https://aclanthology.org/2023.acl-long.574/",
      "title": "Multilingual Event Extraction from Historical Newspaper Adverts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.574.pdf",
      "authors": [
        "Nadav Borenstein",
        "Natália da Silva Perez",
        "Isabelle Augenstein"
      ],
      "abstract": "NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well studied tasks, and for languages other than English. This paper addresses these challenges while focusing on the under-explored task of event extraction from a novel domain of historical texts. We introduce a new multilingual dataset in English, French, and Dutch composed of newspaper ads from the early modern colonial period reporting on enslaved people who liberated themselves from enslavement. We find that: 1) even with scarce annotated data, it is possible to achieve surprisingly good results by formulating the problem as an extractive QA task and leveraging existing datasets and models for modern languages; and 2) cross-lingual low-resource learning for historical languages is highly challenging, and machine translation of the historical datasets to the considered target languages is, in practice, often the best-performing solution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 576,
      "original_url": "https://aclanthology.org/2023.acl-long.575/",
      "title": "BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency",
      "pdf_url": "https://aclanthology.org/2023.acl-long.575.pdf",
      "authors": [
        "Zhenyu Lei",
        "Herun Wan",
        "Wenqian Zhang",
        "Shangbin Feng",
        "Zilong Chen",
        "Jundong Li",
        "Qinghua Zheng",
        "Minnan Luo"
      ],
      "abstract": "Twitter bots are automatic programs operated by malicious actors to manipulate public opinion and spread misinformation. Research efforts have been made to automatically identify bots based on texts and networks on social media. Existing methods only leverage texts or networks alone, and while few works explored the shallow combination of the two modalities, we hypothesize that the interaction and information exchange between texts and graphs could be crucial for holistically evaluating bot activities on social media. In addition, according to a recent survey (Cresci, 2020), Twitter bots are constantly evolving while advanced bots steal genuine users’ tweets and dilute their malicious content to evade detection. This results in greater inconsistency across the timeline of novel Twitter bots, which warrants more attention. In light of these challenges, we propose BIC, a Twitter Bot detection framework with text-graph Interaction and semantic Consistency. Specifically, in addition to separately modeling the two modalities on social media, BIC employs a text-graph interaction module to enable information exchange across modalities in the learning process. In addition, given the stealing behavior of novel Twitter bots, BIC proposes to model semantic consistency in tweets based on attention weights while using it to augment the decision process. Extensive experiments demonstrate that BIC consistently outperforms state-of-the-art baselines on two widely adopted datasets. Further analyses reveal that text-graph interactions and modeling semantic consistency are essential improvements and help combat bot evolution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 577,
      "original_url": "https://aclanthology.org/2023.acl-long.576/",
      "title": "Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.576.pdf",
      "authors": [
        "Mayur Patidar",
        "Prayushi Faldu",
        "Avinash Singh",
        "Lovekesh Vig",
        "Indrajit Bhattacharya",
        "Mausam"
      ],
      "abstract": "When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 578,
      "original_url": "https://aclanthology.org/2023.acl-long.577/",
      "title": "Understanding Client Reactions in Online Mental Health Counseling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.577.pdf",
      "authors": [
        "Anqi Li",
        "Lizhi Ma",
        "Yaling Mei",
        "Hongliang He",
        "Shuai Zhang",
        "Huachuan Qiu",
        "Zhenzhong Lan"
      ],
      "abstract": "Communication success relies heavily on reading participants’ reactions. Such feedback is especially important for mental health counselors, who must carefully consider the client’s progress and adjust their approach accordingly. However, previous NLP research on counseling has mainly focused on studying counselors’ intervention strategies rather than their clients’ reactions to the intervention. This work aims to fill this gap by developing a theoretically grounded annotation framework that encompasses counselors’ strategies and client reaction behaviors. The framework has been tested against a large-scale, high-quality text-based counseling dataset we collected over the past two years from an online welfare counseling platform. Our study show how clients react to counselors’ strategies, how such reactions affect the final counseling outcomes, and how counselors can adjust their strategies in response to these reactions. We also demonstrate that this study can help counselors automatically predict their clients’ states."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 579,
      "original_url": "https://aclanthology.org/2023.acl-long.578/",
      "title": "Nonlinear Structural Equation Model Guided Gaussian Mixture Hierarchical Topic Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.578.pdf",
      "authors": [
        "HeGang Chen",
        "Pengbo Mao",
        "Yuyin Lu",
        "Yanghui Rao"
      ],
      "abstract": "Hierarchical topic models, which can extract semantically meaningful topics from a textcorpus in an unsupervised manner and automatically organise them into a topic hierarchy, have been widely used to discover the underlying semantic structure of documents. However, the existing models often assume in the prior that the topic hierarchy is a tree structure, ignoring symmetrical dependenciesbetween topics at the same level. Moreover, the sparsity of text data often complicate the analysis. To address these issues, we propose NSEM-GMHTM as a deep topic model, witha Gaussian mixture prior distribution to improve the model’s ability to adapt to sparse data, which explicitly models hierarchical and symmetric relations between topics through the dependency matrices and nonlinear structural equations. Experiments on widely used datasets show that our NSEM-GMHTM generates more coherent topics and a more rational topic structure when compared to state-of-theart baselines. Our code is available at https: //github.com/nbnbhwyy/NSEM-GMHTM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 580,
      "original_url": "https://aclanthology.org/2023.acl-long.579/",
      "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
      "pdf_url": "https://aclanthology.org/2023.acl-long.579.pdf",
      "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Xuebo Liu",
        "Min Zhang",
        "Bo Du",
        "Dacheng Tao"
      ],
      "abstract": "Token dropping is a recently-proposed strategy to speed up the pretraining of masked language models, such as BERT, by skipping the computation of a subset of the input tokens at several middle layers. It can effectively reduce the training time without degrading much performance on downstream tasks. However, we empirically find that token dropping is prone to a semantic loss problem and falls short in handling semantic-intense tasks. Motivated by this, we propose a simple yet effective semantic-consistent learning method (ScTD) to improve the token dropping. ScTD aims to encourage the model to learn how to preserve the semantic information in the representation space. Extensive experiments on 12 tasks show that, with the help of our ScTD, token dropping can achieve consistent and significant performance gains across all task types and model sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings up to +1.56% average improvement over the vanilla token dropping."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 581,
      "original_url": "https://aclanthology.org/2023.acl-long.580/",
      "title": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.580.pdf",
      "authors": [
        "Ariel Gera",
        "Roni Friedman",
        "Ofir Arviv",
        "Chulaka Gunasekara",
        "Benjamin Sznajder",
        "Noam Slonim",
        "Eyal Shnarch"
      ],
      "abstract": "Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 582,
      "original_url": "https://aclanthology.org/2023.acl-long.581/",
      "title": "FACTIFY-5WQA: 5W Aspect-based Fact Verification through Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.581.pdf",
      "authors": [
        "Anku Rani",
        "S.M Towhidul Islam Tonmoy",
        "Dwip Dalal",
        "Shreya Gautam",
        "Megha Chakraborty",
        "Aman Chadha",
        "Amit Sheth",
        "Amitava Das"
      ],
      "abstract": "Automatic fact verification has received significant attention recently. Contemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores which are not human-interpretable. A human fact-checker generally follows several logical steps to verify a verisimilitude claim and conclude whether it’s truthful or a mere masquerade. Popular fact-checking websites follow a common structure for fact categorization such as half true, half false, false, pants on fire, etc. Therefore, it is necessary to have an aspect-based (delineating which part(s) are true and which are false) explainable system that can assist human fact-checkers in asking relevant questions related to a fact, which can then be validated separately to reach a final verdict. In this paper, we propose a 5W framework (who, what, when, where, and why) for question-answer-based fact explainability. To that end, we present a semi-automatically generated dataset called FACTIFY-5WQA, which consists of 391, 041 facts along with relevant 5W QAs – underscoring our major contribution to this paper. A semantic role labeling system has been utilized to locate 5Ws, which generates QA pairs for claims using a masked language model. Finally, we report a baseline QA system to automatically locate those answers from evidence documents, which can serve as a baseline for future research in the field. Lastly, we propose a robust fact verification system that takes paraphrased claims and automatically validates them. The dataset and the baseline model are available at https: //github.com/ankuranii/acl-5W-QA"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 583,
      "original_url": "https://aclanthology.org/2023.acl-long.582/",
      "title": "Naamapadam: A Large-Scale Named Entity Annotated Data for Indic Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.582.pdf",
      "authors": [
        "Arnav Mhaske",
        "Harshit Kedia",
        "Sumanth Doddapaneni",
        "Mitesh M. Khapra",
        "Pratyush Kumar",
        "Rudra Murthy",
        "Anoop Kunchukuttan"
      ],
      "abstract": "We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families. The dataset contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person, Location, and, Organization) for 9 out of the 11 languages. The training dataset has been automatically created from the Samanantar parallel corpus by projecting automatically tagged entities from an English sentence to the corresponding Indian language translation. We also create manually annotated testsets for 9 languages. We demonstrate the utility of the obtained dataset on the Naamapadam-test dataset. We also release IndicNER, a multilingual IndicBERT model fine-tuned on Naamapadam training set. IndicNER achieves an F1 score of more than 80 for 7 out of 9 test languages. The dataset and models are available under open-source licences at https://ai4bharat.iitm.ac.in/naamapadam."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 584,
      "original_url": "https://aclanthology.org/2023.acl-long.583/",
      "title": "CREPE: Open-Domain Question Answering with False Presuppositions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.583.pdf",
      "authors": [
        "Xinyan Yu",
        "Sewon Min",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "When asking about unfamiliar topics, information seeking users often pose questions with false presuppositions. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25% of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 585,
      "original_url": "https://aclanthology.org/2023.acl-long.584/",
      "title": "Joint Document-Level Event Extraction via Token-Token Bidirectional Event Completed Graph",
      "pdf_url": "https://aclanthology.org/2023.acl-long.584.pdf",
      "authors": [
        "Qizhi Wan",
        "Changxuan Wan",
        "Keli Xiao",
        "Dexi Liu",
        "Chenliang Li",
        "Bolong Zheng",
        "Xiping Liu",
        "Rong Hu"
      ],
      "abstract": "We solve the challenging document-level event extraction problem by proposing a joint exaction methodology that can avoid inefficiency and error propagation issues in classic pipeline methods. Essentially, we address the three crucial limitations in existing studies. First, the autoregressive strategy of path expansion heavily relies on the orders of argument role. Second, the number of events in documents must be specified in advance. Last, unexpected errors usually exist when decoding events based on the entity-entity adjacency matrix. To address these issues, this paper designs a Token-Token Bidirectional Event Completed Graph (TT-BECG) in which the relation eType-Role1-Role2 serves as the edge type, precisely revealing which tokens play argument roles in an event of a specific event type. Exploiting the token-token adjacency matrix of the TT-BECG, we develop an edge-enhanced joint document-level event extraction model. Guided by the target token-token adjacency matrix, the predicted token-token adjacency matrix can be obtained during the model training. Then, extracted events and event records in a document are decoded based on the predicted matrix, including the graph structure and edge type decoding. Extensive experiments are conducted on two public datasets, and the results confirm the effectiveness of our method and its superiority over the state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 586,
      "original_url": "https://aclanthology.org/2023.acl-long.585/",
      "title": "Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.585.pdf",
      "authors": [
        "Xiaolin Zheng",
        "Mengling Hu",
        "Weiming Liu",
        "Chaochao Chen",
        "Xinting Liao"
      ],
      "abstract": "Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical studies on eight short text clustering datasets demonstrate that RSTC significantly outperforms the state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 587,
      "original_url": "https://aclanthology.org/2023.acl-long.586/",
      "title": "Multilingual Knowledge Graph Completion with Language-Sensitive Multi-Graph Attention",
      "pdf_url": "https://aclanthology.org/2023.acl-long.586.pdf",
      "authors": [
        "Rongchuan Tang",
        "Yang Zhao",
        "Chengqing Zong",
        "Yu Zhou"
      ],
      "abstract": "Multilingual Knowledge Graph Completion (KGC) aims to predict missing links with multilingual knowledge graphs. However, existing approaches suffer from two main drawbacks: (a) alignment dependency: the multilingual KGC is always realized with joint entity or relation alignment, which introduces additional alignment models and increases the complexity of the whole framework; (b) training inefficiency: the trained model will only be used for the completion of one target KG, although the data from all KGs are used simultaneously. To address these drawbacks, we propose a novel multilingual KGC framework with language-sensitive multi-graph attention such that the missing links on all given KGs can be inferred by a universal knowledge completion model. Specifically, we first build a relational graph neural network by sharing the embeddings of aligned nodes to transfer language-independent knowledge. Meanwhile, a language-sensitive multi-graph attention (LSMGA) is proposed to deal with the information inconsistency among different KGs. Experimental results show that our model achieves significant improvements on the DBP-5L and E-PKG datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 588,
      "original_url": "https://aclanthology.org/2023.acl-long.587/",
      "title": "What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.587.pdf",
      "authors": [
        "Griffin Adams",
        "Bichlien Nguyen",
        "Jake Smith",
        "Yingce Xia",
        "Shufang Xie",
        "Anna Ostropolets",
        "Budhaditya Deb",
        "Yuan-Jyue Chen",
        "Tristan Naumann",
        "Noémie Elhadad"
      ],
      "abstract": "Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise–the disagreement between model and metric defined candidate rankings–minimized."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 589,
      "original_url": "https://aclanthology.org/2023.acl-long.588/",
      "title": "Annotating Mentions Alone Enables Efficient Domain Adaptation for Coreference Resolution",
      "pdf_url": "https://aclanthology.org/2023.acl-long.588.pdf",
      "authors": [
        "Nupoor Gandhi",
        "Anjalie Field",
        "Emma Strubell"
      ],
      "abstract": "Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, it remains a challenge to successfully transfer these models to new target domains containing many out-of-vocabulary spans and requiring differing annotation schemes. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. In this work, we show that adapting mention detection is the key component to successful domain adaptation of coreference models, rather than antecedent linking. We also show annotating mentions alone is nearly twice as fast as annotating full coreference chains. Based on these insights, we propose a method for efficiently adapting coreference models, which includes a high-precision mention detection objective and requires only mention annotations in the target domain. Extensive evaluation across three English coreference datasets: CoNLL-2012 (news/conversation), i2b2/VA (medical notes), and child welfare notes, reveals that our approach facilitates annotation-efficient transfer and results in a 7-14% improvement in average F1 without increasing annotator time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 590,
      "original_url": "https://aclanthology.org/2023.acl-long.589/",
      "title": "A Universal Discriminator for Zero-Shot Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.589.pdf",
      "authors": [
        "Haike Xu",
        "Zongyu Lin",
        "Jing Zhou",
        "Yanan Zheng",
        "Zhilin Yang"
      ],
      "abstract": "Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0%, 7.8%, and 11.5% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 591,
      "original_url": "https://aclanthology.org/2023.acl-long.590/",
      "title": "Syntax and Geometry of Information",
      "pdf_url": "https://aclanthology.org/2023.acl-long.590.pdf",
      "authors": [
        "Raphaël Bailly",
        "Laurent Leblond",
        "Kata Gábor"
      ],
      "abstract": "This paper presents an information-theoretical model of syntactic generalization. We study syntactic generalization from the perspective of the capacity to disentangle semantic and structural information, emulating the human capacity to assign a grammaticality judgment to semantically nonsensical sentences. In order to isolate the structure, we propose to represent the probability distribution behind a corpus as the product of the probability of a semantic context and the probability of a structure, the latter being independent of the former. We further elaborate the notion of abstraction as a relaxation of the property of independence. It is based on the measure of structural and contextual information for a given representation. We test abstraction as an optimization objective on the task of inducing syntactic categories from natural language data and show that it significantly outperforms alternative methods. Furthermore, we find that when syntax-unaware optimization objectives succeed in the task, their success is mainly due to an implicit disentanglement process rather than to the model structure. On the other hand, syntactic categories can be deduced in a principled way from the independence between structure and context."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 592,
      "original_url": "https://aclanthology.org/2023.acl-long.591/",
      "title": "GreenKGC: A Lightweight Knowledge Graph Completion Method",
      "pdf_url": "https://aclanthology.org/2023.acl-long.591.pdf",
      "authors": [
        "Yun Cheng Wang",
        "Xiou Ge",
        "Bin Wang",
        "C.-C. Jay Kuo"
      ],
      "abstract": "Knowledge graph completion (KGC) aims to discover missing relationships between entities in knowledge graphs (KGs). Most prior KGC work focuses on learning embeddings for entities and relations through a simple score function. Yet, a higher-dimensional embedding space is usually required for a better reasoning capability, which leads to larger model size and hinders applicability to real-world problems (e.g., large-scale KGs or mobile/edge computing). A lightweight modularized KGC solution, called GreenKGC, is proposed in this work to address this issue. GreenKGC consists of three modules: representation learning, feature pruning, and decision learning, to extract discriminant KG features and make accurate predictions on missing relationships using classifiers and negative sampling. Experimental results demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in most datasets. In addition, low-dimensional GreenKGC can achieve competitive or even better performance against high-dimensional models with a much smaller model size."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 593,
      "original_url": "https://aclanthology.org/2023.acl-long.592/",
      "title": "Unsupervised Open-domain Keyphrase Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.592.pdf",
      "authors": [
        "Lam Do",
        "Pritom Saha Akash",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "In this work, we study the problem of unsupervised open-domain keyphrase generation, where the objective is a keyphrase generation model that can be built without using human-labeled data and can perform consistently across domains. To solve this problem, we propose a seq2seq model that consists of two modules, namely phraseness and informativeness module, both of which can be built in an unsupervised and open-domain fashion. The phraseness module generates phrases, while the informativeness module guides the generation towards those that represent the core concepts of the text. We thoroughly evaluate our proposed method using eight benchmark datasets from different domains. Results on in-domain datasets show that our approach achieves state-of-the-art results compared with existing unsupervised models, and overall narrows the gap between supervised and unsupervised methods down to about 16%. Furthermore, we demonstrate that our model performs consistently across domains, as it surpasses the baselines on out-of-domain datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 594,
      "original_url": "https://aclanthology.org/2023.acl-long.593/",
      "title": "A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.593.pdf",
      "authors": [
        "Jiyue Jiang",
        "Sheng Wang",
        "Qintong Li",
        "Lingpeng Kong",
        "Chuan Wu"
      ],
      "abstract": "When communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS conversation (CSConv) dataset, which contains about 2.6K groups of dialogues with therapy principles and emotional support strategy labels. Making chit chat while providing emotional support is overlooked by the majority of existing cognitive dialogue systems. In this paper, we propose a multi-source knowledge fusion method for CS dialogue (CSD), to generate open-ended responses guided by the therapy principle and emotional support strategy. We first use a progressive mask method based on external knowledge to learn encoders as effective classifiers, which is the prerequisite to predict the therapy principle and emotional support strategy of the target response. Then a decoder interacts with the perceived therapy principle and emotional support strategy to generate responses. Extensive experiments conducted on the CSConv dataset demonstrate the effectiveness of the proposed method, while there is still a large space for improvement compared to human performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 595,
      "original_url": "https://aclanthology.org/2023.acl-long.594/",
      "title": "Plug-and-Play Knowledge Injection for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.594.pdf",
      "authors": [
        "Zhengyan Zhang",
        "Zhiyuan Zeng",
        "Yankai Lin",
        "Huadong Wang",
        "Deming Ye",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Peng Li",
        "Maosong Sun",
        "Jie Zhou"
      ],
      "abstract": "Injecting external knowledge can improve the performance of pre-trained language models (PLMs) on various downstream NLP tasks. However, massive retraining is required to deploy new knowledge injection methods or knowledge bases for downstream tasks. In this work, we are the first to study how to improve the flexibility and efficiency of knowledge injection by reusing existing downstream models. To this end, we explore a new paradigm plug-and-play knowledge injection, where knowledge bases are injected into frozen existing downstream models by a knowledge plugin. Correspondingly, we propose a plug-and-play injection method map-tuning, which trains a mapping of knowledge embeddings to enrich model inputs with mapped embeddings while keeping model parameters frozen. Experimental results on three knowledge-driven NLP tasks show that existing injection methods are not suitable for the new paradigm, while map-tuning effectively improves the performance of downstream models. Moreover, we show that a frozen downstream model can be well adapted to different domains with different mapping networks of domain knowledge. Our code and models are available at https://github.com/THUNLP/Knowledge-Plugin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 596,
      "original_url": "https://aclanthology.org/2023.acl-long.595/",
      "title": "Two Birds One Stone: Dynamic Ensemble for OOD Intent Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.595.pdf",
      "authors": [
        "Yunhua Zhou",
        "Jianqiang Yang",
        "Pengyu Wang",
        "Xipeng Qiu"
      ],
      "abstract": "Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does not know. This paper investigates “overthinking” in the open-world scenario and its impact on OOD intent classification. Inspired by this, we propose a two-birds-one-stone method, which allows the model to decide whether to make a decision on OOD classification early during inference and can ensure accuracy and accelerate inference. At the same time, to adapt to the behavior of dynamic inference, we also propose a training method based on ensemble methods. In addition to bringing certain theoretical insights, we also conduct detailed experiments on three real-world intent datasets. Compared with the previous baselines, our method can not only improve inference speed, but also achieve significant performance improvements."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 597,
      "original_url": "https://aclanthology.org/2023.acl-long.596/",
      "title": "SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.596.pdf",
      "authors": [
        "Philippe Laban",
        "Jesse Vig",
        "Wojciech Kryscinski",
        "Shafiq Joty",
        "Caiming Xiong",
        "Chien-Sheng Wu"
      ],
      "abstract": "Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits - such as adding relevant background information or reordering content - may require document-level context. Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process. To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories. To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.9, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 598,
      "original_url": "https://aclanthology.org/2023.acl-long.597/",
      "title": "Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.597.pdf",
      "authors": [
        "Juanhui Li",
        "Harry Shomer",
        "Jiayuan Ding",
        "Yiqi Wang",
        "Yao Ma",
        "Neil Shah",
        "Jiliang Tang",
        "Dawei Yin"
      ],
      "abstract": "Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, with promising insights regarding the scalability of state-of-the-art KGC methods today, as well as careful attention to more suitable MP designs for KGC tasks tomorrow."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 599,
      "original_url": "https://aclanthology.org/2023.acl-long.598/",
      "title": "A dynamic programming algorithm for span-based nested named-entity recognition in O(n2)",
      "pdf_url": "https://aclanthology.org/2023.acl-long.598.pdf",
      "authors": [
        "Caio Corro"
      ],
      "abstract": "Span-based nested named-entity recognition (NER) has a cubic-time complexity using avariant of the CYK algorithm. We show that by adding a supplementary structural constraint on the search space, nested NER has a quadratic-time complexity, that is the same asymptotic complexity than the non-nested case. The proposed algorithm covers a large part of three standard English benchmarks and delivers comparable experimental results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 600,
      "original_url": "https://aclanthology.org/2023.acl-long.599/",
      "title": "Target-Side Augmentation for Document-Level Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.599.pdf",
      "authors": [
        "Guangsheng Bao",
        "Zhiyang Teng",
        "Yue Zhang"
      ],
      "abstract": "Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA) model to generate many potential translations for each source document. Learning on these wider range translations, an MT model can learn a smoothed distribution, thereby reducing the risk of data sparsity. We demonstrate that the DA model, which estimates the posterior distribution, largely improves the MT performance, outperforming the previous best system by 2.30 s-BLEU on News and achieving new state-of-the-art on News and Europarl benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 601,
      "original_url": "https://aclanthology.org/2023.acl-long.600/",
      "title": "Rethinking Masked Language Modeling for Chinese Spelling Correction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.600.pdf",
      "authors": [
        "Hongqiu Wu",
        "Shaohua Zhang",
        "Yuchen Zhang",
        "Hai Zhao"
      ],
      "abstract": "In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy – randomly masking 20% non-error tokens from the input sequence during fine-tuning – is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 602,
      "original_url": "https://aclanthology.org/2023.acl-long.601/",
      "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues",
      "pdf_url": "https://aclanthology.org/2023.acl-long.601.pdf",
      "authors": [
        "Yunxin Li",
        "Baotian Hu",
        "Chen Xinyu",
        "Yuxin Ding",
        "Lin Ma",
        "Min Zhang"
      ],
      "abstract": "Conditional inference on joint textual and visual clues is a multi-modal reasoning task that textual clues provide prior permutation or external knowledge, which are complementary with visual content and pivotal to deducing the correct option. Previous methods utilizing pretrained vision-language models (VLMs) have achieved impressive performances, yet they show a lack of multimodal context reasoning capability, especially for text-modal information. To address this issue, we propose a Multi-modal Context Reasoning approach, named ModCR. Compared to VLMs performing reasoning via cross modal semantic alignment, it regards the given textual abstract semantic and objective image information as the pre-context information and embeds them into the language model to perform context reasoning. Different from recent vision-aided language models used in natural language processing, ModCR incorporates the multi-view semantic alignment information between language and vision by introducing the learnable alignment prefix between image and text in the pretrained language model. This makes the language model well-suitable for such multi-modal reasoning scenario on joint textual and visual clues. We conduct extensive experiments on two corresponding data sets and experimental results show significantly improved performance (exact gain by 4.8% on PMR test set) compared to previous strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 603,
      "original_url": "https://aclanthology.org/2023.acl-long.602/",
      "title": "Simple and Effective Unsupervised Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.602.pdf",
      "authors": [
        "Changhan Wang",
        "Hirofumi Inaguma",
        "Peng-Jen Chen",
        "Ilia Kulikov",
        "Yun Tang",
        "Wei-Ning Hsu",
        "Michael Auli",
        "Juan Pino"
      ],
      "abstract": "The amount of labeled data to train models for speech tasks is limited for most languages, however, the data scarcity is exacerbated for speech translation which requires labeled data covering two different languages. To address this issue, we study a simple and effective approach to build speech translation systems without labeled data by leveraging recent advances in unsupervised speech recognition, machine translation and speech synthesis, either in a pipeline approach, or to generate pseudo-labels for training end-to-end speech translation models. Furthermore, we present an unsupervised domain adaptation technique for pre-trained speech models which improves the performance of downstream unsupervised speech recognition, especially for low-resource settings. Experiments show that unsupervised speech-to-text translation outperforms the previous unsupervised state of the art by 3.2 BLEU on the Libri-Trans benchmark, on CoVoST 2, our best systems outperform the best supervised end-to-end models (without pre-training) from only two years ago by an average of 5.0 BLEU over five X-En directions. We also report competitive results on MuST-C and CVSS benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 604,
      "original_url": "https://aclanthology.org/2023.acl-long.603/",
      "title": "Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.603.pdf",
      "authors": [
        "Xuan Long Do",
        "Bowei Zou",
        "Shafiq Joty",
        "Tran Tai",
        "Liangming Pan",
        "Nancy Chen",
        "Ai Ti Aw"
      ],
      "abstract": "Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 605,
      "original_url": "https://aclanthology.org/2023.acl-long.604/",
      "title": "CHEER: Centrality-aware High-order Event Reasoning Network for Document-level Event Causality Identification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.604.pdf",
      "authors": [
        "Meiqi Chen",
        "Yixin Cao",
        "Yan Zhang",
        "Zhiwei Liu"
      ],
      "abstract": "Document-level Event Causality Identification (DECI) aims to recognize causal relations between events within a document. Recent studies focus on building a document-level graph for cross-sentence reasoning, but ignore important causal structures — there are one or two “central” events that prevail throughout the document, with most other events serving as either their cause or consequence. In this paper, we manually annotate central events for a systematical investigation and propose a novel DECI model, CHEER, which performs high-order reasoning while considering event centrality. First, we summarize a general GNN-based DECI model and provide a unified view for better understanding. Second, we design an Event Interaction Graph (EIG) involving the interactions among events (e.g., coreference) and event pairs, e.g., causal transitivity, cause(A, B) AND cause(B, C) → cause(A, C). Finally, we incorporate event centrality information into the EIG reasoning network via well-designed features and multi-task learning. We have conducted extensive experiments on two benchmark datasets. The results present great improvements (5.9% F1 gains on average) and demonstrate the effectiveness of each main component."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 606,
      "original_url": "https://aclanthology.org/2023.acl-long.605/",
      "title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.605.pdf",
      "authors": [
        "Yuqiao Wen",
        "Zichao Li",
        "Wenyu Du",
        "Lili Mou"
      ],
      "abstract": "Knowledge distillation (KD) is the process of transferring knowledge from a large model to a small one. It has gained increasing attention in the natural language processing community, driven by the demands of compressing ever-growing language models. In this work, we propose an FDISTILL framework, which formulates sequence-level knowledge distillation as minimizing a generalized f-divergence function. We propose four distilling variants under our framework and show that existing SeqKD and ENGINE approaches are approximations of our FDISTILL methods. We further derive step-wise decomposition for our FDISTILL, reducing intractable sequence-level divergence to word-level losses that can be computed in a tractable manner. Experiments across four datasets show that our methods outperform existing KD approaches, and that our symmetric distilling losses can better force the student to learn from the teacher distribution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 607,
      "original_url": "https://aclanthology.org/2023.acl-long.606/",
      "title": "Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.606.pdf",
      "authors": [
        "Dou Hu",
        "Yinan Bao",
        "Lingwei Wei",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model’s context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 608,
      "original_url": "https://aclanthology.org/2023.acl-long.607/",
      "title": "A Novel Table-to-Graph Generation Approach for Document-Level Joint Entity and Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.607.pdf",
      "authors": [
        "Ruoyu Zhang",
        "Yanzeng Li",
        "Lei Zou"
      ],
      "abstract": "Document-level relation extraction (DocRE) aims to extract relations among entities within a document, which is crucial for applications like knowledge graph construction. Existing methods usually assume that entities and their mentions are identified beforehand, which falls short of real-world applications. To overcome this limitation, we propose TaG, a novel table-to-graph generation model for joint extractionof entities and relations at document-level. To enhance the learning of task dependencies, TaG induces a latent graph among mentions, with different types of edges indicating different task information, which is further broadcast with a relational graph convolutional network. To alleviate the error propagation problem, we adapt the hierarchical agglomerative clustering algorithm to back-propagate task information at decoding stage. Experiments on the benchmark dataset, DocRED, demonstrate that TaG surpasses previous methods by a large margin and achieves state-of-the-art results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 609,
      "original_url": "https://aclanthology.org/2023.acl-long.608/",
      "title": "A Synthetic Data Generation Framework for Grounded Dialogues",
      "pdf_url": "https://aclanthology.org/2023.acl-long.608.pdf",
      "authors": [
        "Jianzhu Bao",
        "Rui Wang",
        "Yasheng Wang",
        "Aixin Sun",
        "Yitong Li",
        "Fei Mi",
        "Ruifeng Xu"
      ],
      "abstract": "Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 610,
      "original_url": "https://aclanthology.org/2023.acl-long.609/",
      "title": "MasakhaPOS: Part-of-Speech Tagging for Typologically Diverse African languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.609.pdf",
      "authors": [
        "Cheikh M. Bamba Dione",
        "David Ifeoluwa Adelani",
        "Peter Nabende",
        "Jesujoba Alabi",
        "Thapelo Sindane",
        "Happy Buzaaba",
        "Shamsuddeen Hassan Muhammad",
        "Chris Chinenye Emezue",
        "Perez Ogayo",
        "Anuoluwapo Aremu",
        "Catherine Gitau",
        "Derguene Mbaye",
        "Jonathan Mukiibi",
        "Blessing Sibanda",
        "Bonaventure F. P. Dossou",
        "Andiswa Bukula",
        "Rooweither Mabuya",
        "Allahsera Auguste Tapo",
        "Edwin Munkoh-Buabeng",
        "Victoire Memdjokam Koagne",
        "Fatoumata Ouoba Kabore",
        "Amelia Taylor",
        "Godson Kalipe",
        "Tebogo Macucwa",
        "Vukosi Marivate",
        "Tajuddeen Gwadabe",
        "Mboning Tchiaze Elvis",
        "Ikechukwu Onyenwe",
        "Gratien Atindogbe",
        "Tolulope Adelani",
        "Idris Akinade",
        "Olanrewaju Samuel",
        "Marien Nahimana",
        "Théogène Musabeyezu",
        "Emile Niyomutabazi",
        "Ester Chimhenga",
        "Kudzai Gotosa",
        "Patrick Mizha",
        "Apelete Agbolo",
        "Seydou Traore",
        "Chinedu Uchechukwu",
        "Aliyu Yusuf",
        "Muhammad Abdullahi",
        "Dietrich Klakow"
      ],
      "abstract": "In this paper, we present AfricaPOS, the largest part-of-speech (POS) dataset for 20 typologically diverse African languages. We discuss the challenges in annotating POS for these languages using the universal dependencies (UD) guidelines. We conducted extensive POS baseline experiments using both conditional random field and several multilingual pre-trained language models. We applied various cross-lingual transfer models trained with data available in the UD. Evaluating on the AfricaPOS dataset, we show that choosing the best transfer language(s) in both single-source and multi-source setups greatly improves the POS tagging performance of the target languages, in particular when combined with parameter-fine-tuning methods. Crucially, transferring knowledge from a language that matches the language family and morphosyntactic properties seems to be more effective for POS tagging in unseen languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 611,
      "original_url": "https://aclanthology.org/2023.acl-long.610/",
      "title": "Semantic Structure Enhanced Event Causality Identification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.610.pdf",
      "authors": [
        "Zhilei Hu",
        "Zixuan Li",
        "Xiaolong Jin",
        "Long Bai",
        "Saiping Guan",
        "Jiafeng Guo",
        "Xueqi Cheng"
      ],
      "abstract": "Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn).It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 612,
      "original_url": "https://aclanthology.org/2023.acl-long.611/",
      "title": "Weakly-Supervised Spoken Video Grounding via Semantic Interaction Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.611.pdf",
      "authors": [
        "Ye Wang",
        "Wang Lin",
        "Shengyu Zhang",
        "Tao Jin",
        "Linjun Li",
        "Xize Cheng",
        "Zhou Zhao"
      ],
      "abstract": "The task of spoken video grounding aims to localize moments in videos that are relevant to descriptive spoken queries. However, extracting semantic information from speech and modeling the cross-modal correlation pose two critical challenges. Previous studies solve them by representing spoken queries based on the matched video frames, which require tremendous effort for frame-level labeling. In this work, we investigate weakly-supervised spoken video grounding, i.e., learning to localize moments without expensive temporal annotations. To effectively represent the cross-modal semantics, we propose Semantic Interaction Learning (SIL), a novel framework consisting of the acoustic-semantic pre-training (ASP) and acoustic-visual contrastive learning (AVCL). In ASP, we pre-train an effective encoder for the grounding task with three comprehensive tasks, where the robustness task enhances stability by explicitly capturing the invariance between time- and frequency-domain features, the conciseness task avoids over-smooth attention by compressing long sequence into segments, and the semantic task improves spoken language understanding by modeling the precise semantics. In AVCL, we mine pseudo labels with discriminative sampling strategies and directly strengthen the interaction between speech and video by maximizing their mutual information. Extensive experiments demonstrate the effectiveness and superiority of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 613,
      "original_url": "https://aclanthology.org/2023.acl-long.612/",
      "title": "Rehearsal-free Continual Language Learning via Efficient Parameter Isolation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.612.pdf",
      "authors": [
        "Zhicheng Wang",
        "Yufang Liu",
        "Tao Ji",
        "Xiaoling Wang",
        "Yuanbin Wu",
        "Congcong Jiang",
        "Ye Chao",
        "Zhencong Han",
        "Ling Wang",
        "Xu Shao",
        "Wenqiu Zeng"
      ],
      "abstract": "We study the problem of defying catastrophic forgetting when learning a series of language processing tasks. Compared with previous methods, we emphasize the importance of not caching history tasks’ data, which makes the problem more challenging. Our proposed method applies the parameter isolation strategy. For each task, it allocates a small portion of private parameters and learns them with a shared pre-trained model. To load correct parameters at testing time, we introduce a simple yet effective non-parametric method. Experiments on continual language learning benchmarks show that our method is significantly better than all existing no-data-cache methods, and is comparable (or even better) than those using historical data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 614,
      "original_url": "https://aclanthology.org/2023.acl-long.613/",
      "title": "Label-Aware Hyperbolic Embeddings for Fine-grained Emotion Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.613.pdf",
      "authors": [
        "Chih Yao Chen",
        "Tun Min Hung",
        "Yi-Li Hsu",
        "Lun-Wei Ku"
      ],
      "abstract": "Fine-grained emotion classification (FEC) is a challenging task. Specifically, FEC needs to handle subtle nuance between labels, which can be complex and confusing. Most existing models only address text classification problem in the euclidean space, which we believe may not be the optimal solution as labels of close semantic (e.g., afraid and terrified) may not be differentiated in such space, which harms the performance. In this paper, we propose HypEmo, a novel framework that can integrate hyperbolic embeddings to improve the FEC task. First, we learn label embeddings in the hyperbolic space to better capture their hierarchical structure, and then our model projects contextualized representations to the hyperbolic space to compute the distance between samples and labels. Experimental results show that incorporating such distance to weight cross entropy loss substantially improve the performance on two benchmark datasets, with around 3% improvement compared to previous state-of-the-art, and could even improve up to 8.6% when the labels are hard to distinguish. Code is available at https://github.com/dinobby/HypEmo."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 615,
      "original_url": "https://aclanthology.org/2023.acl-long.614/",
      "title": "Combo of Thinking and Observing for Outside-Knowledge VQA",
      "pdf_url": "https://aclanthology.org/2023.acl-long.614.pdf",
      "authors": [
        "Qingyi Si",
        "Yuchen Mo",
        "Zheng Lin",
        "Huishan Ji",
        "Weiping Wang"
      ],
      "abstract": "Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text which further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments validate the superiority of the proposed method which outperforms the state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablations of each component, and systematically study the roles of varying types of knowledge. Codes and knowledge data are to be released."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 616,
      "original_url": "https://aclanthology.org/2023.acl-long.615/",
      "title": "AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.615.pdf",
      "authors": [
        "I-Hung Hsu",
        "Zhiyu Xie",
        "Kuan-Hao Huang",
        "Prem Natarajan",
        "Nanyun Peng"
      ],
      "abstract": "Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation (AMR) of the input passages. Incorporating such information into generation-based models is challenging due to the heterogeneous nature of the natural language form prevalently used in generation-based models and the structured form of AMRs. In this work, we study strategies to incorporate AMR into generation-based EAE models. We propose AMPERE, which generates AMR-aware prefixes for every layer of the generation model. Thus, the prefix introduces AMR information to the generation-based EAE model and then improves the generation. We also introduce an adjusted copy mechanism to AMPERE to help overcome potential noises brought by the AMR graph. Comprehensive experiments and analyses on ACE2005 and ERE datasets show that AMPERE can get 4% - 10% absolute F1 score improvements with reduced training data and it is in general powerful across different training sizes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 617,
      "original_url": "https://aclanthology.org/2023.acl-long.616/",
      "title": "Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships",
      "pdf_url": "https://aclanthology.org/2023.acl-long.616.pdf",
      "authors": [
        "David Jurgens",
        "Agrima Seth",
        "Jackson Sargent",
        "Athena Aghighi",
        "Michael Geraci"
      ],
      "abstract": "Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextual-appropriateness judgments are predictive of other social factors expressed in language such as condescension and politeness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 618,
      "original_url": "https://aclanthology.org/2023.acl-long.617/",
      "title": "TART: Improved Few-shot Text Classification Using Task-Adaptive Reference Transformation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.617.pdf",
      "authors": [
        "Shuo Lei",
        "Xuchao Zhang",
        "Jianfeng He",
        "Fanglan Chen",
        "Chang-Tien Lu"
      ],
      "abstract": "Meta-learning has emerged as a trending technique to tackle few-shot text classification and achieve state-of-the-art performance. However, the performance of existing approaches heavily depends on the inter-class variance of the support set. As a result, it can perform well on tasks when the semantics of sampled classes are distinct while failing to differentiate classes with similar semantics. In this paper, we propose a novel Task-Adaptive Reference Transformation (TART) network, aiming to enhance the generalization by transforming the class prototypes to per-class fixed reference points in task-adaptive metric spaces. To further maximize divergence between transformed prototypes in task-adaptive metric spaces, TART introduces a discriminative reference regularization among transformed prototypes. Extensive experiments are conducted on four benchmark datasets and our method demonstrates clear superiority over the state-of-the-art models in all the datasets. In particular, our model surpasses the state-of-the-art method by 7.4% and 5.4% in 1-shot and 5-shot classification on the 20 Newsgroups dataset, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 619,
      "original_url": "https://aclanthology.org/2023.acl-long.618/",
      "title": "How Do In-Context Examples Affect Compositional Generalization?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.618.pdf",
      "authors": [
        "Shengnan An",
        "Zeqi Lin",
        "Qiang Fu",
        "Bei Chen",
        "Nanning Zheng",
        "Jian-Guang Lou",
        "Dongmei Zhang"
      ],
      "abstract": "Compositional generalization–understanding unseen combinations of seen primitives–is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning–the prevailing few-shot paradigm based on large language models–exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple. Furthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus. We hope our analysis would facilitate the understanding and utilization of in-context learning paradigm."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 620,
      "original_url": "https://aclanthology.org/2023.acl-long.619/",
      "title": "Attractive Storyteller: Stylized Visual Storytelling with Unpaired Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.619.pdf",
      "authors": [
        "Dingyi Yang",
        "Qin Jin"
      ],
      "abstract": "Most research on stylized image captioning aims to generate style-specific captions using unpaired text, and has achieved impressive performance for simple styles like positive and negative. However, unlike previous single-sentence captions whose style is mostly embodied in distinctive words or phrases, real-world styles are likely to be implied at the syntactic and discourse levels. In this work, we introduce a new task of Stylized Visual Storytelling (SVST), which aims to describe a photo stream with stylized stories that are more expressive and attractive. We propose a multitasking memory-augmented framework called StyleVSG, which is jointly trained on factual visual storytelling data and unpaired style corpus, achieving a trade-off between style accuracy and visual relevance. Particularly for unpaired stylized text, StyleVSG learns to reconstruct the stylistic story from roughly parallel visual inputs mined with the CLIP model, avoiding problems caused by random mapping in previous methods. Furthermore, a memory module is designed to preserve the consistency and coherence of generated stories. Experiments show that our method can generate attractive and coherent stories with different styles such as fairy tale, romance, and humor. The overall performance of our StyleVSG surpasses state-of-the-art methods on both automatic and human evaluation metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 621,
      "original_url": "https://aclanthology.org/2023.acl-long.620/",
      "title": "Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.620.pdf",
      "authors": [
        "Robert Giaquinto",
        "Dejiao Zhang",
        "Benjamin Kleiner",
        "Yang Li",
        "Ming Tan",
        "Parminder Bhatia",
        "Ramesh Nallapati",
        "Xiaofei Ma"
      ],
      "abstract": "Many machine learning-based low-code or no-code applications involve generating code that interacts with structured knowledge. For example, one of the most studied tasks in this area is generating SQL code from a natural language statement. Prior work shows that incorporating context information from the database schema, such as table and column names, is beneficial to model performance on this task. In this work we present a large pretraining dataset and strategy for learning representations of text, tables, and SQL code that leverages the entire context of the problem. Specifically, we build on existing encoder-decoder architecture by introducing a multitask pretraining framework that complements the unique attributes of our diverse pretraining data. Our work represents the first study on large-scale pretraining of encoder-decoder models for interacting with structured knowledge, and offers a new state-of-the-art foundation model in text-to-SQL generation. We validate our approach with experiments on two SQL tasks, showing improvement over existing methods, including a 1.7 and 2.2 percentage point improvement over prior state-of-the-arts on Spider and CoSQL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 622,
      "original_url": "https://aclanthology.org/2023.acl-long.621/",
      "title": "WSPAlign: Word Alignment Pre-training via Large-Scale Weakly Supervised Span Prediction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.621.pdf",
      "authors": [
        "Qiyu Wu",
        "Masaaki Nagata",
        "Yoshimasa Tsuruoka"
      ],
      "abstract": "Most existing word alignment methods rely on manual alignment datasets or parallel corpora, which limits their usefulness. Here, to mitigate the dependence on manual data, we broaden the source of supervision by relaxing the requirement for correct, fully-aligned, and parallel sentences. Specifically, we make noisy, partially aligned, and non-parallel paragraphs in this paper. We then use such a large-scale weakly-supervised dataset for word alignment pre-training via span prediction. Extensive experiments with various settings empirically demonstrate that our approach, which is named WSPAlign, is an effective and scalable way to pre-train word aligners without manual data. When fine-tuned on standard benchmarks, WSPAlign has set a new state of the art by improving upon the best supervised baseline by 3.3 6.1 points in F1 and 1.5 6.1 points in AER. Furthermore, WSPAlign also achieves competitive performance compared with the corresponding baselines in few-shot, zero-shot and cross-lingual tests, which demonstrates that WSPAlign is potentially more practical for low-resource languages than existing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 623,
      "original_url": "https://aclanthology.org/2023.acl-long.622/",
      "title": "Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.622.pdf",
      "authors": [
        "Junmo Kang",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "Fine-tuning large models is highly effective, however, inference can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner might instead choose to allocate the available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through extensive experiments on six diverse tasks, we show that distilling from T5-XXL (11B) to T5-Small (60M) is almost always a cost-efficient strategy compared to annotating more data to directly train a compact model (T5-Small). We further investigate how the optimal budget allocated towards computation varies across scenarios. We will make our code, datasets, annotation cost estimates, and baseline models available as a benchmark to support further work on cost-efficient training of compact models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 624,
      "original_url": "https://aclanthology.org/2023.acl-long.623/",
      "title": "OD-RTE: A One-Stage Object Detection Framework for Relational Triple Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.623.pdf",
      "authors": [
        "Jinzhong Ning",
        "Zhihao Yang",
        "Yuanyuan Sun",
        "Zhizheng Wang",
        "Hongfei Lin"
      ],
      "abstract": "The Relational Triple Extraction (RTE) task is a fundamental and essential information extraction task. Recently, the table-filling RTE methods have received lots of attention. Despite their success, they suffer from some inherent problems such as underutilizing regional information of triple. In this work, we treat the RTE task based on table-filling method as an Object Detection task and propose a one-stage Object Detection framework for Relational Triple Extraction (OD-RTE). In this framework, the vertices-based bounding box detection, coupled with auxiliary global relational triple region detection, ensuring that regional information of triple could be fully utilized. Besides, our proposed decoding scheme could extract all types of triples. In addition, the negative sampling strategy of relations in the training stage improves the training efficiency while alleviating the imbalance of positive and negative relations. The experimental results show that 1) OD-RTE achieves the state-of-the-art performance on two widely used datasets (i.e., NYT and WebNLG). 2) Compared with the best performing table-filling method, OD-RTE achieves faster training and inference speed with lower GPU memory usage. To facilitate future research in this area, the codes are publicly available at https://github.com/NingJinzhong/ODRTE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 625,
      "original_url": "https://aclanthology.org/2023.acl-long.624/",
      "title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and Theory-of-Mind in Dungeons and Dragons",
      "pdf_url": "https://aclanthology.org/2023.acl-long.624.pdf",
      "authors": [
        "Pei Zhou",
        "Andrew Zhu",
        "Jennifer Hu",
        "Jay Pujara",
        "Xiang Ren",
        "Chris Callison-Burch",
        "Yejin Choi",
        "Prithviraj Ammanabrolu"
      ],
      "abstract": "We propose a novel task, G4C, to study teacher-student natural language interactions in a goal-driven and grounded environment. Dungeons and Dragons (D&D), a role-playing game, provides an ideal setting to investigate such interactions. Here, the Dungeon Master (DM), i.e., the teacher, guides the actions of several players—students, each with their own personas and abilities—to achieve shared goals grounded in a fantasy world. Our approach is to decompose and model these interactions into (1) the DM’s intent to guide players toward a given goal; (2) the DM’s guidance utterance to the players expressing this intent; and (3) a theory-of-mind (ToM) model that anticipates the players’ reaction to the guidance one turn into the future. We develop a novel reinforcement learning (RL) method for training a DM that generates guidance for players by rewarding utterances where the intent matches the ToM-anticipated player actions. Human and automated evaluations show that a DM trained to explicitly model intents and incorporate ToM of the players using RL generates better-quality guidance that is 3x more likely to fulfill the DM’s intent than a vanilla natural language generation (NLG) approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 626,
      "original_url": "https://aclanthology.org/2023.acl-long.625/",
      "title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.625.pdf",
      "authors": [
        "Tianxiang Sun",
        "Zhengfu He",
        "Qin Zhu",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "abstract": "Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addition, we demonstrate that MP2 can achieve surprisingly fast and strong adaptation to downstream tasks by merely learning 8 parameters to combine the pre-trained modular prompts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 627,
      "original_url": "https://aclanthology.org/2023.acl-long.626/",
      "title": "Is GPT-3 a Good Data Annotator?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.626.pdf",
      "authors": [
        "Bosheng Ding",
        "Chengwei Qin",
        "Linlin Liu",
        "Yew Ken Chia",
        "Boyang Li",
        "Shafiq Joty",
        "Lidong Bing"
      ],
      "abstract": "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 628,
      "original_url": "https://aclanthology.org/2023.acl-long.627/",
      "title": "Multi-Grained Knowledge Retrieval for End-to-End Task-Oriented Dialog",
      "pdf_url": "https://aclanthology.org/2023.acl-long.627.pdf",
      "authors": [
        "Fanqi Wan",
        "Weizhou Shen",
        "Ke Yang",
        "Xiaojun Quan",
        "Wei Bi"
      ],
      "abstract": "Retrieving proper domain knowledge from an external database lies at the heart of end-to-end task-oriented dialog systems to generate informative responses. Most existing systems blend knowledge retrieval with response generation and optimize them with direct supervision from reference responses, leading to suboptimal retrieval performance when the knowledge base becomes large-scale. To address this, we propose to decouple knowledge retrieval from response generation and introduce a multi-grained knowledge retriever (MAKER) that includes an entity selector to search for relevant entities and an attribute selector to filter out irrelevant attributes. To train the retriever, we propose a novel distillation objective that derives supervision signals from the response generator. Experiments conducted on three standard benchmarks with both small and large-scale knowledge bases demonstrate that our retriever performs knowledge retrieval more effectively than existing methods. Our code has been made publicly available at https://github.com/18907305772/MAKER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 629,
      "original_url": "https://aclanthology.org/2023.acl-long.628/",
      "title": "Few-shot Event Detection: An Empirical Study and a Unified View",
      "pdf_url": "https://aclanthology.org/2023.acl-long.628.pdf",
      "authors": [
        "Yubo Ma",
        "Zehao Wang",
        "Yixin Cao",
        "Aixin Sun"
      ],
      "abstract": "Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we compare 12 representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. Experiments consistently demonstrate that prompt-based methods, including ChatGPT, still significantly trail prototype-based methods in terms of overall performance. To investigate their superior performance, we break down their design elements along several dimensions and build a unified framework on prototype-based methods. Under such unified view, each prototype-method can be viewed a combination of different modules from these design elements. We further combine all advantageous modules and propose a simple yet effective baseline, which outperforms existing methods by a large margin (e.g., 2.7% F1 gains under low-resource setting)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 630,
      "original_url": "https://aclanthology.org/2023.acl-long.629/",
      "title": "How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases",
      "pdf_url": "https://aclanthology.org/2023.acl-long.629.pdf",
      "authors": [
        "Aaron Mueller",
        "Tal Linzen"
      ],
      "abstract": "Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 631,
      "original_url": "https://aclanthology.org/2023.acl-long.630/",
      "title": "ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.630.pdf",
      "authors": [
        "Valentina Pyatkin",
        "Jena D. Hwang",
        "Vivek Srikumar",
        "Ximing Lu",
        "Liwei Jiang",
        "Yejin Choi",
        "Chandra Bhagavatula"
      ],
      "abstract": "Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life. We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e., the diverse contexts in which moral rules can be bent), and we hope that research in this direction can assist both cognitive and computational investigations of moral judgments."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 632,
      "original_url": "https://aclanthology.org/2023.acl-long.631/",
      "title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero- and Few-Shot Generalisation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.631.pdf",
      "authors": [
        "Hamish Ivison",
        "Akshita Bhagia",
        "Yizhong Wang",
        "Hannaneh Hajishirzi",
        "Matthew Peters"
      ],
      "abstract": "Recent NLP models have shown the remarkable ability to effectively generalise ‘zero-shot’ to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard the length of instructions and few-shot example inputs in terms of compute usage. As a result, HINT can enhance its performance by up to 25% by incorporating additional few-shot data, while utilizing only up to 5% more compute. This combines the strengths of parameter-efficient fine-tuning and in-context learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 633,
      "original_url": "https://aclanthology.org/2023.acl-long.632/",
      "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.632.pdf",
      "authors": [
        "Chenglei Si",
        "Dan Friedman",
        "Nitish Joshi",
        "Shi Feng",
        "Danqi Chen",
        "He He"
      ],
      "abstract": "In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases—for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 634,
      "original_url": "https://aclanthology.org/2023.acl-long.633/",
      "title": "An Inclusive Notion of Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.633.pdf",
      "authors": [
        "Ilia Kuznetsov",
        "Iryna Gurevych"
      ],
      "abstract": "Natural language processing (NLP) researchers develop models of grammar, meaning and communication based on written text. Due to task and data differences, what is considered text can vary substantially across studies. A conceptual framework for systematically capturing these differences is lacking. We argue that clarity on the notion of text is crucial for reproducible and generalizable NLP. Towards that goal, we propose common terminology to discuss the production and transformation of textual data, and introduce a two-tier taxonomy of linguistic and non-linguistic elements that are available in textual sources and can be used in NLP modeling. We apply this taxonomy to survey existing work that extends the notion of text beyond the conservative language-centered view. We outline key desiderata and challenges of the emerging inclusive approach to text in NLP, and suggest community-level reporting as a crucial next step to consolidate the discussion."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 635,
      "original_url": "https://aclanthology.org/2023.acl-long.634/",
      "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
      "pdf_url": "https://aclanthology.org/2023.acl-long.634.pdf",
      "authors": [
        "Yuheng Zha",
        "Yichi Yang",
        "Ruichen Li",
        "Zhiting Hu"
      ],
      "abstract": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 636,
      "original_url": "https://aclanthology.org/2023.acl-long.635/",
      "title": "Multi-source Semantic Graph-based Multimodal Sarcasm Explanation Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.635.pdf",
      "authors": [
        "Liqiang Jing",
        "Xuemeng Song",
        "Kun Ouyang",
        "Mengzhao Jia",
        "Liqiang Nie"
      ],
      "abstract": "Multimodal Sarcasm Explanation (MuSE) is a new yet challenging task, which aims to generate a natural language sentence for a multimodal social post (an image as well as its caption) to explain why it contains sarcasm. Although the existing pioneer study has achieved great success with the BART backbone, it overlooks the gap between the visual feature space and the decoder semantic space, the object-level metadata of the image, as well as the potential external knowledge. To solve these limitations, in this work, we propose a novel mulTi-source sEmantic grAph-based Multimodal sarcasm explanation scheme, named TEAM. In particular, TEAM extracts the object-level semantic meta-data instead of the traditional global visual features from the input image. Meanwhile, TEAM resorts to ConceptNet to obtain the external related knowledge concepts for the input text and the extracted object meta-data. Thereafter, TEAM introduces a multi-source semantic graph that comprehensively characterize the multi-source (i.e., caption, object meta-data, external knowledge) semantic relations to facilitate the sarcasm reasoning. Extensive experiments on a public released dataset MORE verify the superiority of our model over cutting-edge methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 637,
      "original_url": "https://aclanthology.org/2023.acl-long.636/",
      "title": "Counterfactual Active Learning for Out-of-Distribution Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.636.pdf",
      "authors": [
        "Xun Deng",
        "Wenjie Wang",
        "Fuli Feng",
        "Hanwang Zhang",
        "Xiangnan He",
        "Yong Liao"
      ],
      "abstract": "We study the out-of-distribution generalization of active learning that adaptively selects samples for annotation in learning the decision boundary of classification. Our empirical study finds that increasingly annotating seen samples may hardly benefit the generalization. To address the problem, we propose Counterfactual Active Learning (CounterAL) that empowers active learning with counterfactual thinking to bridge the seen samples with unseen cases. In addition to annotating factual samples, CounterAL requires annotators to answer counterfactual questions to construct counterfactual samples for training. To achieve CounterAL, we design a new acquisition strategy that selects the informative factual-counterfactual pairs for annotation; and a new training strategy that pushes the model update to focus on the discrepancy between factual and counterfactual samples. We evaluate CounterAL on multiple public datasets of sentiment analysis and natural language inference. The experiment results show that CounterAL requires fewer acquisition rounds and outperforms existing active learning methods by a large margin in OOD tests with comparable IID performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 638,
      "original_url": "https://aclanthology.org/2023.acl-long.637/",
      "title": "Multi-granularity Temporal Question Answering over Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.637.pdf",
      "authors": [
        "Ziyang Chen",
        "Jinzhi Liao",
        "Xiang Zhao"
      ],
      "abstract": "Recently, question answering over temporal knowledge graphs (i.e., TKGQA) has been introduced and investigated, in quest of reasoning about dynamic factual knowledge. To foster research on TKGQA, a few datasets have been curated (e.g., CronQuestions and Complex-CronQuestions), and various models have been proposed based on these datasets. Nevertheless, existing efforts overlook the fact that real-life applications of TKGQA also tend to be complex in temporal granularity, i.e., the questions may concern mixed temporal granularities (e.g., both day and month). To overcome the limitation, in this paper, we motivate the notion of multi-granularity temporal question answering over knowledge graphs and present a large scale dataset for multi-granularity TKGQA, namely MultiTQ. To the best of our knowledge, MultiTQis among the first of its kind, and compared with existing datasets on TKGQA, MultiTQfeatures at least two desirable aspects—ample relevant facts and multiple temporal granularities. It is expected to better reflect real-world challenges, and serve as a test bed for TKGQA models. In addition, we propose a competing baseline MultiQA over MultiTQ, which is experimentally demonstrated to be effective in dealing with TKGQA. The data and code are released at https://github.com/czy1999/MultiTQ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 639,
      "original_url": "https://aclanthology.org/2023.acl-long.638/",
      "title": "A New Aligned Simple German Corpus",
      "pdf_url": "https://aclanthology.org/2023.acl-long.638.pdf",
      "authors": [
        "Vanessa Toborek",
        "Moritz Busch",
        "Malte Boßert",
        "Christian Bauckhage",
        "Pascal Welke"
      ],
      "abstract": "“Leichte Sprache”, the German counterpart to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German – German. It contains multiple document-aligned sources which we have aligned using automatic sentence-alignment methods. We evaluate our alignments based on a manually labelled subset of aligned documents. The quality of our sentence alignments, as measured by the F1-score, surpasses previous work. We publish the dataset under CC BY-SA and the accompanying code under MIT license."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 640,
      "original_url": "https://aclanthology.org/2023.acl-long.639/",
      "title": "Introducing Semantics into Speech Encoders",
      "pdf_url": "https://aclanthology.org/2023.acl-long.639.pdf",
      "authors": [
        "Derek Xu",
        "Shuyan Dong",
        "Changhan Wang",
        "Suyoun Kim",
        "Zhaojiang Lin",
        "Bing Liu",
        "Akshat Shrivastava",
        "Shang-Wen Li",
        "Liang-Hsuan Tseng",
        "Guan-Ting Lin",
        "Alexei Baevski",
        "Hung-yi Lee",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "abstract": "Recent studies find existing self-supervised speech encoders contain primarily acoustic rather than semantic information. As a result, pipelined supervised automatic speech recognition (ASR) to large language model (LLM) systems achieve state-of-the-art results on semantic spoken language tasks by utilizing rich semantic representations from the LLM. These systems come at the cost of labeled audio transcriptions, which is expensive and time-consuming to obtain. We propose a task-agnostic unsupervised way of incorporating semantic information from LLMs into self-supervised speech encoders without labeled audio transcriptions. By introducing semantics, we improve existing speech encoder spoken language understanding (SLU) performance by over 5% on intent classification (IC), with modest gains in named entity resolution (NER) and slot filling (SF), and spoken question answering (SQA) FF1 score by over 2%. Our approach, which uses no ASR data, achieves similar performance as methods trained on over 100 hours of labeled audio transcripts, demonstrating the feasibility of unsupervised semantic augmentations to existing speech encoders."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 641,
      "original_url": "https://aclanthology.org/2023.acl-long.640/",
      "title": "Constrained Tuple Extraction with Interaction-Aware Network",
      "pdf_url": "https://aclanthology.org/2023.acl-long.640.pdf",
      "authors": [
        "Xiaojun Xue",
        "Chunxia Zhang",
        "Tianxiang Xu",
        "Zhendong Niu"
      ],
      "abstract": "Tuples extraction is a fundamental task for information extraction and knowledge graph construction. The extracted tuples are usually represented as knowledge triples consisting of subject, relation, and object. In practice, however, the validity of knowledge triples is associated with and changes with the spatial, temporal, or other kinds of constraints. Motivated by this observation, this paper proposes a constrained tuple extraction (CTE) task to guarantee the validity of knowledge tuples. Formally, the CTE task is to extract constrained tuples from unstructured text, which adds constraints to conventional triples. To this end, we propose an interaction-aware network. Combinatorial interactions among context-specific external features and distinct-granularity internal features are exploited to effectively mine the potential constraints. Moreover, we have built a new dataset containing totally 1,748,826 constrained tuples for training and 3656 ones for evaluation. Experiments on our dataset and the public CaRB dataset demonstrate the superiority of the proposed model. The constructed dataset and the codes are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 642,
      "original_url": "https://aclanthology.org/2023.acl-long.641/",
      "title": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.641.pdf",
      "authors": [
        "Zhiyang Xu",
        "Ying Shen",
        "Lifu Huang"
      ],
      "abstract": "Instruction tuning, a new learning paradigm that fine-tunes pre-trained language models on tasks specified through instructions, has shown promising zero-shot performance on various natural language processing tasks. However, it has yet to be explored for vision and multimodal tasks. In this work, we introduce MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq format covering 10 broad categories. The tasks are derived from 21 existing open-source datasets and each task is equipped with 5 expert-written instructions. We take OFA as the base pre-trained model for multimodal instruction tuning, and to further improve its zero-shot performance, we explore multiple transfer learning strategies to leverage the large-scale Natural Instructions dataset. Experimental results demonstrate strong zero-shot performance on various unseen multimodal tasks and the benefit of transfer learning from a text-only instruction dataset. We also design a new evaluation metric – Sensitivity, to evaluate how sensitive the model is to the variety of instructions. Our results indicate that fine-tuning the model on a diverse set of tasks and instructions leads to a reduced sensitivity to variations in instructions for each task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 643,
      "original_url": "https://aclanthology.org/2023.acl-long.642/",
      "title": "Single Sequence Prediction over Reasoning Graphs for Multi-hop QA",
      "pdf_url": "https://aclanthology.org/2023.acl-long.642.pdf",
      "authors": [
        "Gowtham Ramesh",
        "Makesh Narsimhan Sreedhar",
        "Junjie Hu"
      ],
      "abstract": "Recent generative approaches for multi-hop question answering (QA) utilize the fusion-in-decoder method to generate a single sequence output which includes both a final answer and a reasoning path taken to arrive at that answer, such as passage titles and key facts from those passages. While such models can lead to better interpretability and high quantitative scores, they often have difficulty accurately identifying the passages corresponding to key entities in the context, resulting in incorrect passage hops and a lack of faithfulness in the reasoning path. To address this, we propose a single-sequence prediction method over a local reasoning graph that integrates a graph structure connecting key entities in each context passage to relevant subsequent passages for each question. We use a graph neural network to encode this graph structure and fuse the resulting representations into the entity representations of the model. Our experiments show significant improvements in answer exact-match/F1 scores and faithfulness of grounding in the reasoning path on the HotpotQA dataset and achieve state-of-the-art numbers on the Musique dataset with only up to a 4% increase in model parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 644,
      "original_url": "https://aclanthology.org/2023.acl-long.643/",
      "title": "Contrastive Error Attribution for Finetuned Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.643.pdf",
      "authors": [
        "Faisal Ladhak",
        "Esin Durmus",
        "Tatsunori Hashimoto"
      ],
      "abstract": "Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing, such as gradient-based influence measures, do not perform reliably for detecting faithfulness errors in NLG datasets. We overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs. Our proposed method can achieve a mean average precision of 0.93 at detecting known data errors across synthetic tasks with known ground truth, substantially outperforming existing approaches. Using this approach and re-training models on cleaned data leads to a 70% reduction in entity hallucinations on the NYT dataset and a 55% reduction in semantic errors on the E2E dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 645,
      "original_url": "https://aclanthology.org/2023.acl-long.644/",
      "title": "DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications",
      "pdf_url": "https://aclanthology.org/2023.acl-long.644.pdf",
      "authors": [
        "Adam Ivankay",
        "Mattia Rigotti",
        "Pascal Frossard"
      ],
      "abstract": "Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show that this phenomenon extends to specific and important high stakes domains like biomedical datasets. In particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model’s inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. This is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. To this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. This results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. Next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 646,
      "original_url": "https://aclanthology.org/2023.acl-long.645/",
      "title": "Neural Machine Translation for Mathematical Formulae",
      "pdf_url": "https://aclanthology.org/2023.acl-long.645.pdf",
      "authors": [
        "Felix Petersen",
        "Moritz Schubotz",
        "Andre Greiner-Petter",
        "Bela Gipp"
      ],
      "abstract": "We tackle the problem of neural machine translation of mathematical formulae between ambiguous presentation languages and unambiguous content languages. Compared to neural machine translation on natural language, mathematical formulae have a much smaller vocabulary and much longer sequences of symbols, while their translation requires extreme precision to satisfy mathematical information needs. In this work, we perform the tasks of translating from LaTeX to Mathematica as well as from LaTeX to semantic LaTeX. While recurrent, recursive, and transformer networks struggle with preserving all contained information, we find that convolutional sequence-to-sequence networks achieve 95.1% and 90.7% exact matches, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 647,
      "original_url": "https://aclanthology.org/2023.acl-long.646/",
      "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.646.pdf",
      "authors": [
        "Deokjae Lee",
        "JunYeong Lee",
        "Jung-Woo Ha",
        "Jin-Hwa Kim",
        "Sang-Woo Lee",
        "Hwaran Lee",
        "Hyun Oh Song"
      ],
      "abstract": "The deployment of large-scale generative models is often restricted by their potential risk of causing harm to users in unpredictable ways. We focus on the problem of black-box red teaming, where a red team generates test cases and interacts with the victim model to discover a diverse set of failures with limited query access. Existing red teaming methods construct test cases based on human supervision or language model (LM) and query all test cases in a brute-force manner without incorporating any information from past evaluations, resulting in a prohibitively large number of queries. To this end, we propose Bayesian red teaming (BRT), novel query-efficient black-box red teaming methods based on Bayesian optimization, which iteratively identify diverse positive test cases leading to model failures by utilizing the pre-defined user input pool and the past evaluations. Experimental results on various user input pools demonstrate that our method consistently finds a significantly larger number of diverse positive test cases under the limited query budget than the baseline methods.The source code is available at https://github.com/snu-mllab/Bayesian-Red-Teaming."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 648,
      "original_url": "https://aclanthology.org/2023.acl-long.647/",
      "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
      "pdf_url": "https://aclanthology.org/2023.acl-long.647.pdf",
      "authors": [
        "Xiaochuang Han",
        "Sachin Kumar",
        "Yulia Tsvetkov"
      ],
      "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 649,
      "original_url": "https://aclanthology.org/2023.acl-long.648/",
      "title": "Recall, Expand, and Multi-Candidate Cross-Encode: Fast and Accurate Ultra-Fine Entity Typing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.648.pdf",
      "authors": [
        "Chengyue Jiang",
        "Wenyang Hui",
        "Yong Jiang",
        "Xiaobin Wang",
        "Pengjun Xie",
        "Kewei Tu"
      ],
      "abstract": "Ultra-fine entity typing (UFET) predicts extremely free-formed types (e.g., president, politician) of a given entity mention (e.g., Joe Biden) in context. State-of-the-art (SOTA) methods use the cross-encoder (CE) based architecture. CE concatenates a mention (and its context) with each type and feeds the pair into a pretrained language model (PLM) to score their relevance. It brings deeper interaction between the mention and the type to reach better performance but has to perform N (the type set size) forward passes to infer all the types of a single mention. CE is therefore very slow in inference when the type set is large (e.g., N=10k for UFET). % Cross-encoder also ignores the correlation between different types.To this end, we propose to perform entity typing in a recall-expand-filter manner. The recall and expansion stages prune the large type set and generate K (typically much smaller than N) most relevant type candidates for each mention. At the filter stage, we use a novel model called {pasted macro ‘NAME’} to concurrently encode and score all these K candidates in only one forward pass to obtain the final type prediction. We investigate different model options for each stage and conduct extensive experiments to compare each option, experiments show that our method reaches SOTA performance on UFET and is thousands of times faster than the CE-based architecture. We also found our method is very effective in fine-grained (130 types) and coarse-grained (9 types) entity typing. Our code is available at {pasted macro ‘CODE’}."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 650,
      "original_url": "https://aclanthology.org/2023.acl-long.649/",
      "title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with Adversarial Network for Audio-Visual Speech Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.649.pdf",
      "authors": [
        "Yuchen Hu",
        "Chen Chen",
        "Ruizhe Li",
        "Heqing Zou",
        "Eng Siong Chng"
      ],
      "abstract": "Audio-visual speech recognition (AVSR) attracts a surge of research interest recently by leveraging multimodal signals to understand human speech. Mainstream approaches addressing this task have developed sophisticated architectures and techniques for multi-modality fusion and representation learning. However, the natural heterogeneity of different modalities causes distribution gap between their representations, making it challenging to fuse them. In this paper, we aim to learn the shared representations across modalities to bridge their gap. Different from existing similar methods on other multimodal tasks like sentiment analysis, we focus on the temporal contextual dependencies considering the sequence-to-sequence task setting of AVSR. In particular, we propose an adversarial network to refine frame-level modality-invariant representations (MIR-GAN), which captures the commonality across modalities to ease the subsequent multimodal fusion process. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach outperforms the state-of-the-arts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 651,
      "original_url": "https://aclanthology.org/2023.acl-long.650/",
      "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors",
      "pdf_url": "https://aclanthology.org/2023.acl-long.650.pdf",
      "authors": [
        "Liyan Tang",
        "Tanya Goyal",
        "Alex Fabbri",
        "Philippe Laban",
        "Jiacheng Xu",
        "Semih Yavuz",
        "Wojciech Kryscinski",
        "Justin Rousseau",
        "Greg Durrett"
      ],
      "abstract": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems’ outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant recent summarization models. We further perform a finer-grained analysis per error-type and find similar performance variance across error types for different factuality metrics. Our results show that no one metric is superior in all settings or for all error types, and we provide recommendations for best practices given these insights."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 652,
      "original_url": "https://aclanthology.org/2023.acl-long.651/",
      "title": "GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.651.pdf",
      "authors": [
        "Jia-Chen Gu",
        "Zhenhua Ling",
        "Quan Liu",
        "Cong Liu",
        "Guoping Hu"
      ],
      "abstract": "Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that GIFT can significantly improve the performance of three PLMs on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-the-art performance on MPC understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 653,
      "original_url": "https://aclanthology.org/2023.acl-long.652/",
      "title": "Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.652.pdf",
      "authors": [
        "Artem Vazhentsev",
        "Gleb Kuzmin",
        "Akim Tsvigun",
        "Alexander Panchenko",
        "Maxim Panov",
        "Mikhail Burtsev",
        "Artem Shelmanov"
      ],
      "abstract": "Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that state-of-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 654,
      "original_url": "https://aclanthology.org/2023.acl-long.653/",
      "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-long.653.pdf",
      "authors": [
        "Zheng Xin Yong",
        "Hailey Schoelkopf",
        "Niklas Muennighoff",
        "Alham Fikri Aji",
        "David Ifeoluwa Adelani",
        "Khalid Almubarak",
        "M Saiful Bari",
        "Lintang Sutawika",
        "Jungo Kasai",
        "Ahmed Baruwa",
        "Genta Winata",
        "Stella Biderman",
        "Edward Raff",
        "Dragomir Radev",
        "Vassilina Nikoulina"
      ],
      "abstract": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 655,
      "original_url": "https://aclanthology.org/2023.acl-long.654/",
      "title": "Logic-driven Indirect Supervision: An Application to Crisis Counseling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.654.pdf",
      "authors": [
        "Mattia Medina Grespan",
        "Meghan Broadbent",
        "Xinyao Zhang",
        "Katherine Axford",
        "Brent Kious",
        "Zac Imel",
        "Vivek Srikumar"
      ],
      "abstract": "Ensuring the effectiveness of text-based crisis counseling requires observing ongoing conversations and providing feedback, both labor-intensive tasks. Automatic analysis of conversations—at the full chat and utterance levels—may help support counselors and provide better care. While some session-level training data (e.g., rating of patient risk) is often available from counselors, labeling utterances requires expensive post hoc annotation. But the latter can not only provide insights about conversation dynamics, but can also serve to support quality assurance efforts for counselors. In this paper, we examine if inexpensive—and potentially noisy—session-level annotation can help improve label utterances. To this end, we propose a logic-based indirect supervision approach that exploits declaratively stated structural dependencies between both levels of annotation to improve utterance modeling. We show that adding these rules gives an improvement of 3.5% f-score over a strong multi-task baseline for utterance-level predictions. We demonstrate via ablation studies how indirect supervision via logic rules also improves the consistency and robustness of the system."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 656,
      "original_url": "https://aclanthology.org/2023.acl-long.655/",
      "title": "Grounding Characters and Places in Narrative Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.655.pdf",
      "authors": [
        "Sandeep Soni",
        "Amanpreet Sihra",
        "Elizabeth Evans",
        "Matthew Wilkens",
        "David Bamman"
      ],
      "abstract": "Tracking characters and locations throughout a story can help improve the understanding of its plot structure. Prior research has analyzed characters and locations from text independently without grounding characters to their locations in narrative time. Here, we address this gap by proposing a new spatial relationship categorization task. The objective of the task is to assign a spatial relationship category for every character and location co-mention within a window of text, taking into consideration linguistic context, narrative tense, and temporal scope. To this end, we annotate spatial relationships in approximately 2500 book excerpts and train a model using contextual embeddings as features to predict these relationships. When applied to a set of books, this model allows us to test several hypotheses on mobility and domestic space, revealing that protagonists are more mobile than non-central characters and that women as characters tend to occupy more interior space than men. Overall, our work is the first step towards joint modeling and analysis of characters and places in narrative text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 657,
      "original_url": "https://aclanthology.org/2023.acl-long.656/",
      "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.656.pdf",
      "authors": [
        "Shangbin Feng",
        "Chan Young Park",
        "Yuhan Liu",
        "Yulia Tsvetkov"
      ],
      "abstract": "Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 658,
      "original_url": "https://aclanthology.org/2023.acl-long.657/",
      "title": "SLABERT Talk Pretty One Day: Modeling Second Language Acquisition with BERT",
      "pdf_url": "https://aclanthology.org/2023.acl-long.657.pdf",
      "authors": [
        "Aditya Yadavalli",
        "Alekhya Yadavalli",
        "Vera Tobin"
      ],
      "abstract": "Second language acquisition (SLA) research has extensively studied cross-linguistic transfer, the influence of linguistic structure of a speaker’s native language [L1] on the successful acquisition of a foreign language [L2]. Effects of such transfer can be positive (facilitating acquisition) or negative (impeding acquisition). We find that NLP literature has not given enough attention to the phenomenon of negative transfer. To understand patterns of both positive and negative transfer between L1 and L2, we model sequential second language acquisition in LMs. Further, we build a Mutlilingual Age Ordered CHILDES (MAO-CHILDES)—a dataset consisting of 5 typologically diverse languages, i.e., German, French, Polish, Indonesian, and Japanese—to understand the degree to which native Child-Directed Speech (CDS) [L1] can help or conflict with English language acquisition [L2]. To examine the impact of native CDS, we use the TILT-based cross lingual transfer learning approach established by Papadimitriou and Jurafsky (2020) and find that, as in human SLA, language family distance predicts more negative transfer. Additionally, we find that conversational speech data shows greater facilitation for language acquisition than scripted speech data. Our findings call for further research using our novel Transformer-based SLA models and we would like to encourage it by releasing our code, data, and models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 659,
      "original_url": "https://aclanthology.org/2023.acl-long.658/",
      "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.658.pdf",
      "authors": [
        "Albert Xu",
        "Xiang Ren",
        "Robin Jia"
      ],
      "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 660,
      "original_url": "https://aclanthology.org/2023.acl-long.659/",
      "title": "Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.659.pdf",
      "authors": [
        "Chengwei Qin",
        "Shafiq Joty",
        "Qian Li",
        "Ruochen Zhao"
      ],
      "abstract": "Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most cases, it does not always outperform multi-task learning. We further provide an in-depth analysis from the perspective of task similarity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 661,
      "original_url": "https://aclanthology.org/2023.acl-long.660/",
      "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale",
      "pdf_url": "https://aclanthology.org/2023.acl-long.660.pdf",
      "authors": [
        "Hritik Bansal",
        "Karthik Gopalakrishnan",
        "Saket Dingliwal",
        "Sravan Bodapati",
        "Katrin Kirchhoff",
        "Dan Roth"
      ],
      "abstract": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: ~70% of the attention heads and ~20% of the feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These induction heads overlap with task-specific important heads, reinforcing arguments by Olsson et al. (2022) regarding induction head generality to more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained for in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 662,
      "original_url": "https://aclanthology.org/2023.acl-long.661/",
      "title": "Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya",
      "pdf_url": "https://aclanthology.org/2023.acl-long.661.pdf",
      "authors": [
        "Fitsum Gaim",
        "Wonsuk Yang",
        "Hancheol Park",
        "Jong Park"
      ],
      "abstract": "Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on various topics. The dataset construction method is discussed, which is applicable to constructing similar resources for related languages. We present comprehensive experiments and analyses of several resource-efficient approaches to QA, including monolingual, cross-lingual, and multilingual setups, along with comparisons against machine-translated silver data. Our strong baseline models reach 76% in the F1 score, while the estimated human performance is 92%, indicating that the benchmark presents a good challenge for future work. We make the dataset, models, and leaderboard publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 663,
      "original_url": "https://aclanthology.org/2023.acl-long.662/",
      "title": "ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain",
      "pdf_url": "https://aclanthology.org/2023.acl-long.662.pdf",
      "authors": [
        "Mike Zhang",
        "Rob van der Goot",
        "Barbara Plank"
      ],
      "abstract": "The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R-large, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves state-of-the-art results on 6 out of 9 datasets. Our analysis reveals that ESCOXLM-R performs better on short spans and outperforms XLM-R-large on entity-level and surface-level span-F1, likely due to ESCO containing short skill and occupation titles, and encoding information on the entity-level."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 664,
      "original_url": "https://aclanthology.org/2023.acl-long.663/",
      "title": "CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.663.pdf",
      "authors": [
        "Minghan Li",
        "Sheng-Chieh Lin",
        "Barlas Oguz",
        "Asish Ghoshal",
        "Jimmy Lin",
        "Yashar Mehdad",
        "Wen-tau Yih",
        "Xilun Chen"
      ],
      "abstract": "Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on various retrieval tasks. These methods, however, are orders of magnitude slower and need much more space to store their indices compared to their single-vector counterparts. In this paper, we unify different multi-vector retrieval models from a token routing viewpoint and propose conditional token interaction via dynamic lexical routing, namely CITADEL, for efficient and effective multi-vector retrieval.CITADEL learns to route different token vectors to the predicted lexical keys such that a query token vector only interacts with document token vectors routed to the same key. This design significantly reduces the computation cost while maintaining high accuracy. Notably, CITADEL achieves the same or slightly better performance than the previous state of the art, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR) evaluations, while being nearly 40 times faster. Source code and data are available at https://github.com/facebookresearch/dpr-scale/tree/citadel."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 665,
      "original_url": "https://aclanthology.org/2023.acl-long.664/",
      "title": "MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.664.pdf",
      "authors": [
        "Bang Yang",
        "Fenglin Liu",
        "Xian Wu",
        "Yaowei Wang",
        "Xu Sun",
        "Yuexian Zou"
      ],
      "abstract": "Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8% and 21.5% absolute improvements in terms of BLEU@4 and CIDEr metrics. Our code is available at https://github.com/yangbang18/MultiCapCLIP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 666,
      "original_url": "https://aclanthology.org/2023.acl-long.665/",
      "title": "Transfer and Active Learning for Dissonance Detection: Addressing the Rare-Class Challenge",
      "pdf_url": "https://aclanthology.org/2023.acl-long.665.pdf",
      "authors": [
        "Vasudha Varadarajan",
        "Swanie Juhng",
        "Syeda Mahwish",
        "Xiaoran Liu",
        "Jonah Luby",
        "Christian Luhmann",
        "H. Andrew Schwartz"
      ],
      "abstract": "While transformer-based systems have enabled greater accuracies with fewer training examples, data acquisition obstacles still persist for rare-class tasks – when the class label is very infrequent (e.g. < 5% of samples). Active learning has in general been proposed to alleviate such challenges, but choice of selection strategy, the criteria by which rare-class examples are chosen, has not been systematically evaluated. Further, transformers enable iterative transfer-learning approaches. We propose and investigate transfer- and active learning solutions to the rare class problem of dissonance detection through utilizing models trained on closely related tasks and the evaluation of acquisition strategies, including a proposed probability-of-rare-class (PRC) approach. We perform these experiments for a specific rare-class problem: collecting language samples of cognitive dissonance from social media. We find that PRC is a simple and effective strategy to guide annotations and ultimately improve model accuracy while transfer-learning in a specific order can improve the cold-start performance of the learner but does not benefit iterations of active learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 667,
      "original_url": "https://aclanthology.org/2023.acl-long.666/",
      "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.666.pdf",
      "authors": [
        "Qi Jia",
        "Yizhu Liu",
        "Haifeng Tang",
        "Kenny Zhu"
      ],
      "abstract": "Curriculum learning has shown promising improvements in multiple domains by training machine learning models from easy samples to hard ones. Previous works which either design rules or train models for scoring the difficulty highly rely on task-specific expertise, and cannot generalize. Inspired by the “easy-to-hard” intuition, we propose to do in-sample curriculum learning for natural language generation tasks. Our learning strategy starts training the model to generate the last few words, i.e., do sequence completion, and gradually extends to generate the whole output sequence. Comprehensive experiments show that it generalizes well to different tasks and achieves significant improvements over strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 668,
      "original_url": "https://aclanthology.org/2023.acl-long.667/",
      "title": "Product Question Answering in E-Commerce: A Survey",
      "pdf_url": "https://aclanthology.org/2023.acl-long.667.pdf",
      "authors": [
        "Yang Deng",
        "Wenxuan Zhang",
        "Qian Yu",
        "Wai Lam"
      ],
      "abstract": "Product question answering (PQA), aiming to automatically provide instant responses to customer’s questions in E-Commerce platforms, has drawn increasing attention in recent years. Compared with typical QA problems, PQA exhibits unique challenges such as the subjectivity and reliability of user-generated contents in E-commerce platforms. Therefore, various problem settings and novel methods have been proposed to capture these special characteristics. In this paper, we aim to systematically review existing research efforts on PQA. Specifically, we categorize PQA studies into four problem settings in terms of the form of provided answers. We analyze the pros and cons, as well as present existing datasets and evaluation protocols for each setting. We further summarize the most significant challenges that characterize PQA from general QA applications and discuss their corresponding solutions. Finally, we conclude this paper by providing the prospect on several future directions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 669,
      "original_url": "https://aclanthology.org/2023.acl-long.668/",
      "title": "Towards Domain-Agnostic and Domain-Adaptive Dementia Detection from Spoken Language",
      "pdf_url": "https://aclanthology.org/2023.acl-long.668.pdf",
      "authors": [
        "Shahla Farzana",
        "Natalie Parde"
      ],
      "abstract": "Health-related speech datasets are often small and varied in focus. This makes it difficult to leverage them to effectively support healthcare goals. Robust transfer of linguistic features across different datasets orbiting the same goal carries potential to address this concern. To test this hypothesis, we experiment with domain adaptation (DA) techniques on heterogeneous spoken language data to evaluate generalizability across diverse datasets for a common task: dementia detection. We find that adapted models exhibit better performance across conversational and task-oriented datasets. The feature-augmented DA method achieves a 22% increase in accuracy adapting from a conversational to task-specific dataset compared to a jointly trained baseline. This suggests promising capacity of these techniques to allow for productive use of disparate data for a complex spoken language healthcare task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 670,
      "original_url": "https://aclanthology.org/2023.acl-long.669/",
      "title": "Generalizing Backpropagation for Gradient-Based Interpretability",
      "pdf_url": "https://aclanthology.org/2023.acl-long.669.pdf",
      "authors": [
        "Kevin Du",
        "Lucas Torroba Hennigen",
        "Niklas Stoehr",
        "Alex Warstadt",
        "Ryan Cotterell"
      ],
      "abstract": "Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model’s output with respect to its inputs. While these methods can indicate which input features may be important for the model’s prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT’s behavior on the subject–verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 671,
      "original_url": "https://aclanthology.org/2023.acl-long.670/",
      "title": "UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language",
      "pdf_url": "https://aclanthology.org/2023.acl-long.670.pdf",
      "authors": [
        "Xinyi Mou",
        "Zhongyu Wei",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structure-aware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 672,
      "original_url": "https://aclanthology.org/2023.acl-long.671/",
      "title": "Generic Temporal Reasoning with Differential Analysis and Explanation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.671.pdf",
      "authors": [
        "Yu Feng",
        "Ben Zhou",
        "Haoyu Wang",
        "Helen Jin",
        "Dan Roth"
      ],
      "abstract": "Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems’ generalizability due to existing datasets’ limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY’s supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 673,
      "original_url": "https://aclanthology.org/2023.acl-long.672/",
      "title": "Model-Based Simulation for Optimising Smart Reply",
      "pdf_url": "https://aclanthology.org/2023.acl-long.672.pdf",
      "authors": [
        "Benjamin Towle",
        "Ke Zhou"
      ],
      "abstract": "Smart Reply (SR) systems present a user with a set of replies, of which one can be selected in place of having to type out a response. To perform well at this task, a system should be able to effectively present the user with a diverse set of options, to maximise the chance that at least one of them conveys the user’s desired response. This is a significant challenge, due to the lack of datasets containing sets of responses to learn from. Resultantly, previous work has focused largely on post-hoc diversification, rather than explicitly learning to predict sets of responses. Motivated by this problem, we present a novel method SimSR, that employs model-based simulation to discover high-value response sets, through simulating possible user responses with a learned world model. Unlike previous approaches, this allows our method to directly optimise the end-goal of SR–maximising the relevance of at least one of the predicted replies. Empirically on two public datasets, when compared to SoTA baselines, our method achieves up to 21% and 18% improvement in ROUGE score and Self-ROUGE score respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 674,
      "original_url": "https://aclanthology.org/2023.acl-long.673/",
      "title": "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.673.pdf",
      "authors": [
        "John Wieting",
        "Jonathan Clark",
        "William Cohen",
        "Graham Neubig",
        "Taylor Berg-Kirkpatrick"
      ],
      "abstract": "Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval - the last of which we introduce in this paper. Overall, our model outperforms both a strong contrastive and generative baseline on these tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 675,
      "original_url": "https://aclanthology.org/2023.acl-long.674/",
      "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.674.pdf",
      "authors": [
        "Tianxing He",
        "Jingyu Zhang",
        "Tianle Wang",
        "Sachin Kumar",
        "Kyunghyun Cho",
        "James Glass",
        "Yulia Tsvetkov"
      ],
      "abstract": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 676,
      "original_url": "https://aclanthology.org/2023.acl-long.675/",
      "title": "Dealing with Semantic Underspecification in Multimodal NLP",
      "pdf_url": "https://aclanthology.org/2023.acl-long.675.pdf",
      "authors": [
        "Sandro Pezzelle"
      ],
      "abstract": "Intelligent systems that aim at mastering language as humans do must deal with its semantic underspecification, namely, the possibility for a linguistic signal to convey only part of the information needed for communication to succeed. Consider the usages of the pronoun they, which can leave the gender and number of its referent(s) underspecified. Semantic underspecification is not a bug but a crucial language feature that boosts its storage and processing efficiency. Indeed, human speakers can quickly and effortlessly integrate semantically-underspecified linguistic signals with a wide range of non-linguistic information, e.g., the multimodal context, social or cultural conventions, and shared knowledge. Standard NLP models have, in principle, no or limited access to such extra information, while multimodal systems grounding language into other modalities, such as vision, are naturally equipped to account for this phenomenon. However, we show that they struggle with it, which could negatively affect their performance and lead to harmful consequences when used for applications. In this position paper, we argue that our community should be aware of semantic underspecification if it aims to develop language technology that can successfully interact with human users. We discuss some applications where mastering it is crucial and outline a few directions toward achieving this goal."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 677,
      "original_url": "https://aclanthology.org/2023.acl-long.676/",
      "title": "Trigger Warning Assignment as a Multi-Label Document Classification Problem",
      "pdf_url": "https://aclanthology.org/2023.acl-long.676.pdf",
      "authors": [
        "Matti Wiegmann",
        "Magdalena Wolska",
        "Christopher Schröder",
        "Ole Borchardt",
        "Benno Stein",
        "Martin Potthast"
      ],
      "abstract": "A trigger warning is used to warn people about potentially disturbing content. We introduce trigger warning assignment as a multi-label classification task, create the Webis Trigger Warning Corpus 2022, and with it the first dataset of 1 million fanfiction works from Archive of our Own with up to 36 different warnings per document. To provide a reliable catalog of trigger warnings, we organized 41 million of free-form tags assigned by fanfiction authors into the first comprehensive taxonomy of trigger warnings by mapping them to the 36 institutionally recommended warnings. To determine the best operationalization of trigger warnings, we explore state-of-the-art multi-label models, examining the trade-off between assigning coarse- and fine-grained warnings, open- and closed-set classification, document length, and label confidence. Our models achieve micro-F1 scores of about 0.5, which reveals the difficulty of the task. Tailored representations, long input sequences, and a higher recall on rare warnings would help."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 678,
      "original_url": "https://aclanthology.org/2023.acl-long.677/",
      "title": "WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-long.677.pdf",
      "authors": [
        "Wenjie Zhuo",
        "Yifan Sun",
        "Xiaohan Wang",
        "Linchao Zhu",
        "Yi Yang"
      ],
      "abstract": "This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly facilitating the alignment and uniformity in the feature space. A popular alternative to the “pushing” operation is whitening the feature space, which scatters all the samples for uniformity. Since the whitening and the contrastive learning have large redundancy w.r.t. the uniformity, they are usually used separately and do not easily work together. For the first time, this paper integrates whitening into the contrastive learning scheme and facilitates two benefits. 1) Better uniformity. We find that these two approaches are not totally redundant but actually have some complementarity due to different uniformity mechanism. 2) Better alignment. We randomly divide the feature into multiple groups along the channel axis and perform whitening independently within each group. By shuffling the group division, we derive multiple distortions of a single sample and thus increase the positive sample diversity. Consequently, using multiple positive samples with enhanced diversity further improves contrastive learning due to better alignment. Extensive experiments on seven semantic textual similarity tasks show our method achieves consistent improvement over the contrastive learning baseline and sets new states of the art, e.g., 78.78% (+2.53% based on BERT{pasted macro ‘BA’}) Spearman correlation on STS tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 679,
      "original_url": "https://aclanthology.org/2023.acl-long.678/",
      "title": "Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms",
      "pdf_url": "https://aclanthology.org/2023.acl-long.678.pdf",
      "authors": [
        "Tianshu Zhang",
        "Changchang Liu",
        "Wei-Han Lee",
        "Yu Su",
        "Huan Sun"
      ],
      "abstract": "This paper studies a new task of federated learning (FL) for semantic parsing, where multiple clients collaboratively train one global model without sharing their semantic parsing data. By leveraging data from multiple clients, the FL paradigm can be especially beneficial for clients that have little training data to develop a data-hungry neural semantic parser on their own. We propose an evaluation setup to study this task, where we re-purpose widely-used single-domain text-to-SQL datasets as clients to form a realistic heterogeneous FL setting and collaboratively train a global model. As standard FL algorithms suffer from the high client heterogeneity in our realistic setup, we further propose a novel LOss Reduction Adjusted Re-weighting (Lorar) mechanism, which adjusts each client’s contribution to the global model update based on its training loss reduction during each round. Our intuition is that the larger the loss reduction, the further away the current global model is from the client’s local optimum, and the larger weight the client should get. By applying Lorar to three widely adopted FL algorithms (FedAvg, FedOPT and FedProx), we observe that their performance can be improved substantially on average (4%-20% absolute gain under MacroAvg) and that clients with smaller datasets enjoy larger performance gains. In addition, the global model converges faster for almost all the clients."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 680,
      "original_url": "https://aclanthology.org/2023.acl-long.679/",
      "title": "Causality-Guided Multi-Memory Interaction Network for Multivariate Stock Price Movement Prediction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.679.pdf",
      "authors": [
        "Di Luo",
        "Weiheng Liao",
        "Shuqi Li",
        "Xin Cheng",
        "Rui Yan"
      ],
      "abstract": "Over the past few years, we’ve witnessed an enormous interest in stock price movement prediction using AI techniques. In recent literature, auxiliary data has been used to improve prediction accuracy, such as textual news. When predicting a particular stock, we assume that information from other stocks should also be utilized as auxiliary data to enhance performance. In this paper, we propose the Causality-guided Multi-memory Interaction Network (CMIN), a novel end-to-end deep neural network for stock movement prediction which, for the first time, models the multi-modality between financial text data and causality-enhanced stock correlations to achieve higher prediction accuracy. CMIN transforms the basic attention mechanism into Causal Attention by calculating transfer entropy between multivariate stocks in order to avoid attention on spurious correlations. Furthermore, we introduce a fusion mechanism to model the multi-directional interactions through which CMIN learns not only the self-influence but also the interactive influence in information flows representing the interrelationship between text and stock correlations. The effectiveness of the proposed approach is demonstrated by experiments on three real-world datasets collected from the U.S. and Chinese markets, where CMIN outperforms existing models to establish a new state-of-the-art prediction accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 681,
      "original_url": "https://aclanthology.org/2023.acl-long.680/",
      "title": "DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.680.pdf",
      "authors": [
        "SongYang Gao",
        "Shihan Dou",
        "Yan Liu",
        "Xiao Wang",
        "Qi Zhang",
        "Zhongyu Wei",
        "Jin Ma",
        "Ying Shan"
      ],
      "abstract": "Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data’s probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70% compared to current best-performing adversarial training methods. Experiments demonstrate that DSRM considerably improves BERT’s resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 682,
      "original_url": "https://aclanthology.org/2023.acl-long.681/",
      "title": "A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires",
      "pdf_url": "https://aclanthology.org/2023.acl-long.681.pdf",
      "authors": [
        "Hoyun Song",
        "Jisu Shin",
        "Huije Lee",
        "Jong Park"
      ],
      "abstract": "Social media is one of the most highly sought resources for analyzing characteristics of the language by its users. In particular, many researchers utilized various linguistic features of mental health problems from social media. However, existing approaches to detecting mental disorders face critical challenges, such as the scarcity of high-quality data or the trade-off between addressing the complexity of models and presenting interpretable results grounded in expert domain knowledge. To address these challenges, we design a simple but flexible model that preserves domain-based interpretability. We propose a novel approach that captures the semantic meanings directly from the text and compares them to symptom-related descriptions. Experimental results demonstrate that our model outperforms relevant baselines on various mental disorder detection tasks. Our detailed analysis shows that the proposed model is effective at leveraging domain knowledge, transferable to other mental disorders, and providing interpretable detection results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 683,
      "original_url": "https://aclanthology.org/2023.acl-long.682/",
      "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
      "pdf_url": "https://aclanthology.org/2023.acl-long.682.pdf",
      "authors": [
        "Kundan Krishna",
        "Saurabh Garg",
        "Jeffrey Bigham",
        "Zachary Lipton"
      ],
      "abstract": "For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gainsare attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning.In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around 10x–500x less data), outperforming the latter on 7 and 5 datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks,including the GLUE benchmark. Besides classification tasks, self-pretraining also provides benefits on structured output prediction tasks such as span based question answering and commonsense inference, often providing more than 50% of the performance boosts provided by pretraining on the BookWiki corpus. Our results hint that in many scenarios, performance gains attributable to pretraining are driven primarily by the pretraining objective itself and are not always attributable to the use of external pretraining data in massive amounts. These findings are especially relevant in light of concerns about intellectual property and offensive content in web-scale pretraining data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 684,
      "original_url": "https://aclanthology.org/2023.acl-long.683/",
      "title": "Towards Open-World Product Attribute Mining: A Lightly-Supervised Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-long.683.pdf",
      "authors": [
        "Liyan Xu",
        "Chenwei Zhang",
        "Xian Li",
        "Jingbo Shang",
        "Jinho D. Choi"
      ],
      "abstract": "We present a new task setting for attribute mining on e-commerce products, serving as a practical solution to extract open-world attributes without extensive human intervention. Our supervision comes from a high-quality seed attribute set bootstrapped from existing resources, and we aim to expand the attribute vocabulary of existing seed types, and also to discover any new attribute types automatically. A new dataset is created to support our setting, and our approach Amacer is proposed specifically to tackle the limited supervision. Especially, given that no direct supervision is available for those unseen new attributes, our novel formulation exploits self-supervised heuristic and unsupervised latent attributes, which attains implicit semantic signals as additional supervision by leveraging product context. Experiments suggest that our approach surpasses various baselines by 12 F1, expanding attributes of existing types significantly by up to 12 times, and discovering values from 39% new types."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 685,
      "original_url": "https://aclanthology.org/2023.acl-long.684/",
      "title": "XDailyDialog: A Multilingual Parallel Dialogue Corpus",
      "pdf_url": "https://aclanthology.org/2023.acl-long.684.pdf",
      "authors": [
        "Zeming Liu",
        "Ping Nie",
        "Jie Cai",
        "Haifeng Wang",
        "Zheng-Yu Niu",
        "Peng Zhang",
        "Mrinmaya Sachan",
        "Kaiping Peng"
      ],
      "abstract": "High-quality datasets are significant to the development of dialogue models. However, most existing datasets for open-domain dialogue modeling are limited to a single language. The absence of multilingual open-domain dialog datasets not only limits the research on multilingual or cross-lingual transfer learning, but also hinders the development of robust open-domain dialog systems that can be deployed in other parts of the world. In this paper, we provide a multilingual parallel open-domain dialog dataset, XDailyDialog, to enable researchers to explore the challenging task of multilingual and cross-lingual open-domain dialog. XDailyDialog includes 13K dialogues aligned across 4 languages (52K dialogues and 410K utterances in total). We then propose a dialog generation model, kNN-Chat, which has a novel kNN-search mechanism to support unified response retrieval for monolingual, multilingual, and cross-lingual dialogue. Experiment results show the effectiveness of this framework. We will make XDailyDialog and kNN-Chat publicly available soon."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 686,
      "original_url": "https://aclanthology.org/2023.acl-long.685/",
      "title": "PAL to Lend a Helping Hand: Towards Building an Emotion Adaptive Polite and Empathetic Counseling Conversational Agent",
      "pdf_url": "https://aclanthology.org/2023.acl-long.685.pdf",
      "authors": [
        "Kshitij Mishra",
        "Priyanshu Priya",
        "Asif Ekbal"
      ],
      "abstract": "The World Health Organization (WHO) has significantly emphasized the need for mental health care. The social stigma associated with mental illness prevents individuals from addressing their issues and getting assistance. In such a scenario, the relevance of online counseling has increased dramatically. The feelings and attitudes that a client and a counselor express towards each other result in a higher or lower counseling experience. A counselor should be friendly and gain clients’ trust to make them share their problems comfortably. Thus, it is essential for the counselor to adequately comprehend the client’s emotions and ensure client’s welfare, i.e. s/he should adapt and deal with the clients politely and empathetically to provide a pleasant, cordial and personalized experience. Motivated by this, in this work, we attempt to build a novel Polite and empAthetic counseLing conversational agent PAL to lay down the counseling support to substance addict and crime victims. To have client’s emotion-based polite and empathetic responses, two counseling datasets laying down the counseling support to substance addicts and crime victims are annotated. These annotated datasets are used to build PAL in a reinforcement learning framework. A novel reward function is formulated to ensure correct politeness and empathy preferences as per client’s emotions with naturalness and non-repetitiveness in responses. Thorough automatic and human evaluation showcase the usefulness and strength of the designed novel reward function. Our proposed system is scalable and can be easily modified with different modules of preference models as per need."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 687,
      "original_url": "https://aclanthology.org/2023.acl-long.686/",
      "title": "Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.686.pdf",
      "authors": [
        "Yue Deng",
        "Wenxuan Zhang",
        "Sinno Jialin Pan",
        "Lidong Bing"
      ],
      "abstract": "Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a more general solution, we propose a unified bidirectional generative framework to tackle various cross-domain ABSA tasks. Specifically, our framework trains a generative model in both text-to-label and label-to-text directions. The former transforms each task into a unified format to learn domain-agnostic features, and the latter generates natural sentences from noisy labels for data augmentation, with which a more accurate model can be trained. To investigate the effectiveness and generality of our framework, we conduct extensive experiments on four cross-domain ABSA tasks and present new state-of-the-art results on all tasks. Our data and code are publicly available at https://github.com/DAMO-NLP-SG/BGCA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 688,
      "original_url": "https://aclanthology.org/2023.acl-long.687/",
      "title": "Contrastive Decoding: Open-ended Text Generation as Optimization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.687.pdf",
      "authors": [
        "Xiang Lisa Li",
        "Ari Holtzman",
        "Daniel Fried",
        "Percy Liang",
        "Jason Eisner",
        "Tatsunori Hashimoto",
        "Luke Zettlemoyer",
        "Mike Lewis"
      ],
      "abstract": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 689,
      "original_url": "https://aclanthology.org/2023.acl-long.688/",
      "title": "Resolving Indirect Referring Expressions for Entity Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.688.pdf",
      "authors": [
        "Mohammad Javad Hosseini",
        "Filip Radlinski",
        "Silvia Pareti",
        "Annie Louis"
      ],
      "abstract": "Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice ‘Should we make a Simnel cake or a Pandan cake¿ a natural response from a non-expert may be indirect: ‘let’s make the green one‘. Reference resolution has been little studied with natural expressions, thus robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of entity pairs and utterances, and develop models for the disambiguation problem. Consisting of 42K indirect referring expressions across three domains, it enables for the first time the study of how large language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 690,
      "original_url": "https://aclanthology.org/2023.acl-long.689/",
      "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.689.pdf",
      "authors": [
        "Andrea Santilli",
        "Silvio Severino",
        "Emilian Postolache",
        "Valentino Maiorca",
        "Michele Mancusi",
        "Riccardo Marin",
        "Emanuele Rodola"
      ],
      "abstract": "Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 691,
      "original_url": "https://aclanthology.org/2023.acl-long.690/",
      "title": "Hard Sample Aware Prompt-Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.690.pdf",
      "authors": [
        "Yuanjian Xu",
        "Qi An",
        "Jiahuan Zhang",
        "Peng Li",
        "Zaiqing Nie"
      ],
      "abstract": "Prompt-tuning based few-shot learning has garnered increasing attention in recent years due to its efficiency and promising capability. To achieve the best performance for NLP tasks with just a few samples, it is vital to include as many informative samples as possible and to avoid misleading ones. However, there is no work in prompt-tuning literature addressing the problem of differentiating informative hard samples from misleading ones in model training, which is challenging due to the lack of supervision signals about the quality of the samples to train a well-performed model. We propose a Hard Sample Aware Prompt-Tuning framework (i.e. HardPT) to solve the non-differentiable problem in hard sample identification with reinforcement learning, and to strengthen the discrimination of the feature space without changing the original data distribution via an adaptive contrastive learning method. An extensive empirical study on a series of NLP tasks demonstrates the capability of HardPT in few-shot scenarios. HardPT obtains new SOTA results on all evaluated NLP tasks, including pushing the SST-5 accuracy to 49.5% (1.1% point absolute improvement), QNLI accuracy to 74.6% (1.9% absolute improvement), NMLI accuracy to 71.5 (0.7% absolute improvement), TACREV F1-score to 28.2 (1.0 absolute improvement), and i2b2/VA F1-score to 41.2 (1.3 absolute improvement)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 692,
      "original_url": "https://aclanthology.org/2023.acl-long.691/",
      "title": "WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events",
      "pdf_url": "https://aclanthology.org/2023.acl-long.691.pdf",
      "authors": [
        "Marco Antonio Stranisci",
        "Rossana Damiano",
        "Enrico Mensa",
        "Viviana Patti",
        "Daniele Radicioni",
        "Tommaso Caselli"
      ],
      "abstract": "Biographical event detection is a relevant task that allows for the exploration and comparison of the ways in which people’s lives are told and represented. This may support several real-life applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was aligned with 5 existing corpora in order to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 693,
      "original_url": "https://aclanthology.org/2023.acl-long.692/",
      "title": "Best-k Search Algorithm for Neural Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.692.pdf",
      "authors": [
        "Jiacheng Xu",
        "Caiming Xiong",
        "Silvio Savarese",
        "Yingbo Zhou"
      ],
      "abstract": "Modern natural language generation paradigms require a decoding strategy to obtain quality sequences out of the model. Beam search yields high-quality but low diversity outputs; stochastic approaches suffer from high variance and sometimes low quality. In this work, we propose a deterministic search algorithm balancing both quality and diversity. We first investigate the vanilla best-first search (BFS) algorithm and then propose the best-k search algorithm. Inspired by BFS, we greedily expand the top k nodes, instead of the first node, to boost efficiency and diversity. Upweighting recently discovered nodes accompanied by heap pruning ensures the completeness of the search procedure. Experiments on four NLG tasks show that best-k search yields more diverse and natural outputs compared to strong baselines, while our approach maintains high text quality. The proposed algorithm is parameter-free, lightweight, efficient, and easy-to-use."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 694,
      "original_url": "https://aclanthology.org/2023.acl-long.693/",
      "title": "Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.693.pdf",
      "authors": [
        "Sumanth Doddapaneni",
        "Rahul Aralikatte",
        "Gowtham Ramesh",
        "Shreya Goyal",
        "Mitesh M. Khapra",
        "Anoop Kunchukuttan",
        "Pratyush Kumar"
      ],
      "abstract": "Building Natural Language Understanding (NLU) capabilities for Indic languages, which have a collective speaker base of more than one billion speakers is absolutely crucial. In this work, we aim to improve the NLU capabilities of Indic languages by making contributions along 3 important axes (i) monolingual corpora (ii) NLU testsets (iii) multilingual LLMs focusing on Indic languages. Specifically, we curate the largest monolingual corpora, IndicCorp, with 20.9B tokens covering 24 languages from 4 language families - a 2.3x increase over prior work, while supporting 12 additional languages. Next, we create a human-supervised benchmark, IndicXTREME, consisting of nine diverse NLU tasks covering 20 languages. Across languages and tasks, IndicXTREME contains a total of 105 evaluation sets, of which 52 are new contributions to the literature. To the best of our knowledge, this is the first effort towards creating a standard benchmark for Indic languages that aims to test the multilingual zero-shot capabilities of pretrained language models. Finally, we train IndicBERT v2, a state-of-the-art model supporting all the languages. Averaged across languages and tasks, the model achieves an absolute improvement of 2 points over a strong baseline. The data and models are available at https://github.com/AI4Bharat/IndicBERT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 695,
      "original_url": "https://aclanthology.org/2023.acl-long.694/",
      "title": "Transforming Visual Scene Graphs to Image Captions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.694.pdf",
      "authors": [
        "Xu Yang",
        "Jiawei Peng",
        "Zihua Wang",
        "Haiyang Xu",
        "Qinghao Ye",
        "Chenliang Li",
        "Songfang Huang",
        "Fei Huang",
        "Zhangzikang Li",
        "Yu Zhang"
      ],
      "abstract": "We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. After embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. Since both the encoder and decoder are built based on the MHA, as a result, we construct a simple and homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TFSGC. The code is in: https://anonymous.4open.science/r/ACL23_TFSGC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 696,
      "original_url": "https://aclanthology.org/2023.acl-long.695/",
      "title": "Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.695.pdf",
      "authors": [
        "Yun Tang",
        "Anna Sun",
        "Hirofumi Inaguma",
        "Xinyue Chen",
        "Ning Dong",
        "Xutai Ma",
        "Paden Tomasello",
        "Juan Pino"
      ],
      "abstract": "Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED’s strength in non-monotonic sequence to sequence learning while retaining Transducer’s streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the MuST-C dataset and the findings demonstrate that TAED performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 697,
      "original_url": "https://aclanthology.org/2023.acl-long.696/",
      "title": "Improving Domain Generalization for Prompt-Aware Essay Scoring via Disentangled Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.696.pdf",
      "authors": [
        "Zhiwei Jiang",
        "Tianyi Gao",
        "Yafeng Yin",
        "Meng Liu",
        "Hua Yu",
        "Zifeng Cheng",
        "Qing Gu"
      ],
      "abstract": "Automated Essay Scoring (AES) aims to score essays written in response to specific prompts. Many AES models have been proposed, but most of them are either prompt-specific or prompt-adaptive and cannot generalize well on “unseen” prompts. This work focuses on improving the generalization ability of AES models from the perspective of domain generalization, where the data of target prompts cannot be accessed during training. Specifically, we propose a prompt-aware neural AES model to extract comprehensive representation for essay scoring, including both prompt-invariant and prompt-specific features. To improve the generalization of representation, we further propose a novel disentangled representation learning framework. In this framework, a contrastive norm-angular alignment strategy and a counterfactual self-training strategy are designed to disentangle the prompt-invariant information and prompt-specific information in representation. Extensive experimental results on datasets of both ASAP and TOEFL11 demonstrate the effectiveness of our method under the domain generalization setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 698,
      "original_url": "https://aclanthology.org/2023.acl-long.697/",
      "title": "What’s the Meaning of Superhuman Performance in Today’s NLU?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.697.pdf",
      "authors": [
        "Simone Tedeschi",
        "Johan Bos",
        "Thierry Declerck",
        "Jan Hajič",
        "Daniel Hershcovich",
        "Eduard Hovy",
        "Alexander Koller",
        "Simon Krek",
        "Steven Schockaert",
        "Rico Sennrich",
        "Ekaterina Shutova",
        "Roberto Navigli"
      ],
      "abstract": "In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 699,
      "original_url": "https://aclanthology.org/2023.acl-long.698/",
      "title": "PromptNER: Prompt Locating and Typing for Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.698.pdf",
      "authors": [
        "Yongliang Shen",
        "Zeqi Tan",
        "Shuhui Wu",
        "Wenqi Zhang",
        "Rongsheng Zhang",
        "Yadong Xi",
        "Weiming Lu",
        "Yueting Zhuang"
      ],
      "abstract": "Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7% on average."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 700,
      "original_url": "https://aclanthology.org/2023.acl-long.699/",
      "title": "Hints on the data for language modeling of synthetic languages with transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.699.pdf",
      "authors": [
        "Rodolfo Zevallos",
        "Nuria Bel"
      ],
      "abstract": "Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However, there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The question we have addressed in this paper is to what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference. These details can be important for low-resourced languages that need to plan the production of datasets. We evaluated intrinsically and extrinsically the differences of five different languages with different pretraining dataset sizes and three different tokenization methods for each. The results confirm that the size of the vocabulary due to morphological characteristics is directly correlated with both the LM perplexity and the performance of two typical downstream tasks such as NER identification and POS labeling. The experiments also provide new evidence that a canonical tokenizer can reduce perplexity by more than a half for a polysynthetic language like Quechua as well as raising F1 from 0.8 to more than 0.9 in both downstream tasks with a LM trained with only 6M tokens."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 701,
      "original_url": "https://aclanthology.org/2023.acl-long.700/",
      "title": "Neural Machine Translation Methods for Translating Text to Sign Language Glosses",
      "pdf_url": "https://aclanthology.org/2023.acl-long.700.pdf",
      "authors": [
        "Dele Zhu",
        "Vera Czehmann",
        "Eleftherios Avramidis"
      ],
      "abstract": "State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 702,
      "original_url": "https://aclanthology.org/2023.acl-long.701/",
      "title": "Revisiting Event Argument Extraction: Can EAE Models Learn Better When Being Aware of Event Co-occurrences?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.701.pdf",
      "authors": [
        "Yuxin He",
        "Jingyue Hu",
        "Buzhou Tang"
      ],
      "abstract": "Event co-occurrences have been proved effective for event extraction (EE) in previous studies, but have not been considered for event argument extraction (EAE) recently. In this paper, we try to fill this gap between EE research and EAE research, by highlighting the question that “Can EAE models learn better when being aware of event co-occurrences?”. To answer this question, we reformulate EAE as a problem of table generation and extend a SOTA prompt-based EAE model into a non-autoregressive generation framework, called TabEAE, which is able to extract the arguments of multiple events in parallel. Under this framework, we experiment with 3 different training-inference schemes on 4 datasets (ACE05, RAMS, WikiEvents and MLEE) and discover that via training the model to extract all events in parallel, it can better distinguish the semantic boundary of each event and its ability to extract single event gets substantially improved. Experimental results show that our method achieves new state-of-the-art performance on the 4 datasets. Our code is avilable at https://github.com/Stardust-hyx/TabEAE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 703,
      "original_url": "https://aclanthology.org/2023.acl-long.702/",
      "title": "HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.702.pdf",
      "authors": [
        "Qianyu He",
        "Yikai Zhang",
        "Jiaqing Liang",
        "Yuncheng Huang",
        "Yanghua Xiao",
        "Yunwen Chen"
      ],
      "abstract": "Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics. Resources of HAUSER are publicly available at https://github.com/Abbey4799/HAUSER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 704,
      "original_url": "https://aclanthology.org/2023.acl-long.703/",
      "title": "Large-scale Lifelong Learning of In-context Instructions and How to Tackle It",
      "pdf_url": "https://aclanthology.org/2023.acl-long.703.pdf",
      "authors": [
        "Jisoo Mok",
        "Jaeyoung Do",
        "Sungjin Lee",
        "Tara Taghavi",
        "Seunghak Yu",
        "Sungroh Yoon"
      ],
      "abstract": "Jointly fine-tuning a Pre-trained Language Model (PLM) on a pre-defined set of tasks with in-context instructions has been proven to improve its generalization performance, allowing us to build a universal language model that can be deployed across task boundaries. In this work, we explore for the first time whether this attractive property of in-context instruction learning can be extended to a scenario in which tasks are fed to the target PLM in a sequential manner. The primary objective of so-called lifelong in-context instruction learning is to improve the target PLM’s instance- and task-level generalization performance as it observes more tasks. DynaInst, the proposed method to lifelong in-context instruction learning, achieves noticeable improvements in both types of generalization, nearly reaching the upper bound performance obtained through joint training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 705,
      "original_url": "https://aclanthology.org/2023.acl-long.704/",
      "title": "Controllable Text Generation via Probability Density Estimation in the Latent Space",
      "pdf_url": "https://aclanthology.org/2023.acl-long.704.pdf",
      "authors": [
        "Yuxuan Gu",
        "Xiaocheng Feng",
        "Sicheng Ma",
        "Lingyuan Zhang",
        "Heng Gong",
        "Weihong Zhong",
        "Bing Qin"
      ],
      "abstract": "Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-specific classifiers or sampling one from relevant discrete samples. However, they cannot effectively model a complex space with diverse attributes, high dimensionality, and asymmetric structure, leaving subsequent controls unsatisfying. In this work, we propose a novel control framework using probability density estimation in the latent space. Our method utilizes an invertible transformation function, the Normalizing Flow, that maps the complex distributions in the latent space to simple Gaussian distributions in the prior space. Thus, we can perform sophisticated and flexible controls in the prior space and feed the control effects back into the latent space owing to the bijection property of invertible transformations. Experiments on single-attribute and multi-attribute control reveal that our method outperforms several strong baselines on attribute relevance and text quality, achieving a new SOTA. Further analysis of control strength adjustment demonstrates the flexibility of our control strategy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 706,
      "original_url": "https://aclanthology.org/2023.acl-long.705/",
      "title": "Learning Latent Relations for Temporal Knowledge Graph Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.705.pdf",
      "authors": [
        "Mengqi Zhang",
        "Yuwei Xia",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
      ],
      "abstract": "Temporal Knowledge Graph (TKG) reasoning aims to predict future facts based on historical data. However, due to the limitations in construction tools and data sources, many important associations between entities may be omitted in TKG. We refer to these missing associations as latent relations. Most existing methods have some drawbacks in explicitly capturing intra-time latent relations between co-occurring entities and inter-time latent relations between entities that appear at different times. To tackle these problems, we propose a novel Latent relations Learning method for TKG reasoning, namely L2TKG. Specifically, we first utilize a Structural Encoder (SE) to obtain representations of entities at each timestamp. We then design a Latent Relations Learning (LRL) module to mine and exploit the intra- and inter-time latent relations. Finally, we extract the temporal representations from the output of SE and LRL for entity prediction. Extensive experiments on four datasets demonstrate the effectiveness of L2TKG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 707,
      "original_url": "https://aclanthology.org/2023.acl-long.706/",
      "title": "DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function",
      "pdf_url": "https://aclanthology.org/2023.acl-long.706.pdf",
      "authors": [
        "Haiming Wang",
        "Ye Yuan",
        "Zhengying Liu",
        "Jianhao Shen",
        "Yichun Yin",
        "Jing Xiong",
        "Enze Xie",
        "Han Shi",
        "Yujun Li",
        "Lin Li",
        "Jian Yin",
        "Zhenguo Li",
        "Xiaodan Liang"
      ],
      "abstract": "Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration. Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 708,
      "original_url": "https://aclanthology.org/2023.acl-long.707/",
      "title": "Unsupervised Selective Rationalization with Noise Injection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.707.pdf",
      "authors": [
        "Adam Storek",
        "Melanie Subbiah",
        "Kathleen McKeown"
      ],
      "abstract": "A major issue with using deep learning models in sensitive applications is that they provide no explanation for their output. To address this problem, unsupervised selective rationalization produces rationales alongside predictions by chaining two jointly-trained components, a rationale generator and a predictor. Although this architecture guarantees that the prediction relies solely on the rationale, it does not ensure that the rationale contains a plausible explanation for the prediction. We introduce a novel training technique that effectively limits generation of implausible rationales by injecting noise between the generator and the predictor. Furthermore, we propose a new benchmark for evaluating unsupervised selective rationalization models using movie reviews from existing datasets. We achieve sizeable improvements in rationale plausibility and task accuracy over the state-of-the-art across a variety of tasks, including our new benchmark, while maintaining or improving model faithfulness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 709,
      "original_url": "https://aclanthology.org/2023.acl-long.708/",
      "title": "Understanding In-Context Learning via Supportive Pretraining Data",
      "pdf_url": "https://aclanthology.org/2023.acl-long.708.pdf",
      "authors": [
        "Xiaochuang Han",
        "Daniel Simig",
        "Todor Mihaylov",
        "Yulia Tsvetkov",
        "Asli Celikyilmaz",
        "Tianlu Wang"
      ],
      "abstract": "In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model’s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 710,
      "original_url": "https://aclanthology.org/2023.acl-long.709/",
      "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.709.pdf",
      "authors": [
        "Zhexin Zhang",
        "Jiaxin Wen",
        "Minlie Huang"
      ],
      "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 711,
      "original_url": "https://aclanthology.org/2023.acl-long.710/",
      "title": "Effective Contrastive Weighting for Dense Query Expansion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.710.pdf",
      "authors": [
        "Xiao Wang",
        "Sean MacAvaney",
        "Craig Macdonald",
        "Iadh Ounis"
      ],
      "abstract": "Verbatim queries submitted to search engines often do not sufficiently describe the user’s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query’srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 712,
      "original_url": "https://aclanthology.org/2023.acl-long.711/",
      "title": "Improving the Detection of Multilingual Online Attacks with Rich Social Media Data from Singapore",
      "pdf_url": "https://aclanthology.org/2023.acl-long.711.pdf",
      "authors": [
        "Janosch Haber",
        "Bertie Vidgen",
        "Matthew Chapman",
        "Vibhor Agarwal",
        "Roy Ka-Wei Lee",
        "Yong Keong Yap",
        "Paul Röttger"
      ],
      "abstract": "Toxic content is a global problem, but most resources for detecting toxic content are in English. When datasets are created in other languages, they often focus exclusively on one language or dialect. In many cultural and geographical settings, however, it is common to code-mix languages, combining and interchanging them throughout conversations. To shine a light on this practice, and enable more research into code-mixed toxic content, we introduce SOA, a new multilingual dataset of online attacks. Using the multilingual city-state of Singapore as a starting point, we collect a large corpus of Reddit comments in Indonesian, Malay, Singlish, and other languages, and provide fine-grained hierarchical labels for online attacks. We publish the corpus with rich metadata, as well as additional unlabelled data for domain adaptation. We share comprehensive baseline results, show how the metadata can be used for granular error analysis, and demonstrate the benefits of domain adaptation for detecting multilingual online attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 713,
      "original_url": "https://aclanthology.org/2023.acl-long.712/",
      "title": "Reanalyzing L2 Preposition Learning with Bayesian Mixed Effects and a Pretrained Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-long.712.pdf",
      "authors": [
        "Jakob Prange",
        "Man Ho Ivy Wong"
      ],
      "abstract": "We use both Bayesian and neural models to dissect a data set of Chinese learners’ pre- and post-interventional responses to two tests measuring their understanding of English prepositions. The results mostly replicate previous findings from frequentist analyses and newly reveal crucial interactions between student ability, task type, and stimulus sentence. Given the sparsity of the data as well as high diversity among learners, the Bayesian method proves most useful; but we also see potential in using language model probabilities as predictors of grammaticality and learnability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 714,
      "original_url": "https://aclanthology.org/2023.acl-long.713/",
      "title": "Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.713.pdf",
      "authors": [
        "Artidoro Pagnoni",
        "Alex Fabbri",
        "Wojciech Kryscinski",
        "Chien-Sheng Wu"
      ],
      "abstract": "In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 715,
      "original_url": "https://aclanthology.org/2023.acl-long.714/",
      "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.714.pdf",
      "authors": [
        "Fangyu Liu",
        "Francesco Piccinno",
        "Syrine Krichene",
        "Chenxi Pang",
        "Kenton Lee",
        "Mandar Joshi",
        "Yasemin Altun",
        "Nigel Collier",
        "Julian Eisenschlos"
      ],
      "abstract": "Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 716,
      "original_url": "https://aclanthology.org/2023.acl-long.715/",
      "title": "MGR: Multi-generator Based Rationalization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.715.pdf",
      "authors": [
        "Wei Liu",
        "Haozhao Wang",
        "Jun Wang",
        "Ruixuan Li",
        "Xinyang Li",
        "YuanKai Zhang",
        "Yang Qiu"
      ],
      "abstract": "Rationalization is to employ a generator and a predictor to construct a self-explaining NLP model in which the generator selects a subset of human-intelligible pieces of the input text to the following predictor. However, rationalization suffers from two key challenges, i.e., spurious correlation and degeneration, where the predictor overfits the spurious or meaningless pieces solely selected by the not-yet well-trained generator and in turn deteriorates the generator. Although many studies have been proposed to address the two challenges, they are usually designed separately and do not take both of them into account. In this paper, we propose a simple yet effective method named MGR to simultaneously solve the two problems. The key idea of MGR is to employ multiple generators such that the occurrence stability of real pieces is improved and more meaningful pieces are delivered to the predictor. Empirically, we show that MGR improves the F1 score by up to 20.9% as compared to state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 717,
      "original_url": "https://aclanthology.org/2023.acl-long.716/",
      "title": "BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.716.pdf",
      "authors": [
        "Liang Ma",
        "Shuyang Cao",
        "Robert L Logan IV",
        "Di Lu",
        "Shihao Ran",
        "Ke Zhang",
        "Joel Tetreault",
        "Alejandro Jaimes"
      ],
      "abstract": "The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced into a summary, 2) effective on human-written texts, and 3) sensitive to different error types (as summaries can contain multiple errors). To address these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a dataset of 889 human-written, minimally different summary pairs, where a single error is introduced to a summary from the CNN/DailyMail dataset to produce an unfaithful summary. We find BUMP complements existing benchmarks in a number of ways: 1) the summaries in BUMP are harder to discriminate and less probable under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be used to measure the consistency of metrics, and reveals that the most discriminative metrics tend not to be the most consistent, and 3) unlike datasets containing generated summaries with multiple errors, BUMP enables the measurement of metrics’ performance on individual error types."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 718,
      "original_url": "https://aclanthology.org/2023.acl-long.717/",
      "title": "Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.717.pdf",
      "authors": [
        "Rheeya Uppaal",
        "Junjie Hu",
        "Yixuan Li"
      ],
      "abstract": "Out-of-distribution (OOD) detection is a critical task for reliable predictions over text. Fine-tuning with pre-trained language models has been a de facto procedure to derive OOD detectors with respect to in-distribution (ID) data. Despite its common use, the understanding of the role of fine-tuning and its necessity for OOD detection is largely unexplored. In this paper, we raise the question: is fine-tuning necessary for OOD detection? We present a study investigating the efficacy of directly leveraging pre-trained language models for OOD detection, without any model fine-tuning on the ID data. We compare the approach with several competitive fine-tuning objectives, and offer new insights under various types of distributional shifts. Extensive experiments demonstrate near-perfect OOD detection performance (with 0% FPR95 in many cases), strongly outperforming the fine-tuned counterpart."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 719,
      "original_url": "https://aclanthology.org/2023.acl-long.718/",
      "title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.718.pdf",
      "authors": [
        "Yulong Chen",
        "Yang Liu",
        "Ruochen Xu",
        "Ziyi Yang",
        "Chenguang Zhu",
        "Michael Zeng",
        "Yue Zhang"
      ],
      "abstract": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose UniSumm, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark SummZoo. It consists of 8 summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that UniSumm outperforms strong baselines by a large margin across all sub-tasks in SummZoo under both automatic and human evaluations and achieves comparable results in human evaluation compared with a GPT-3.5 model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 720,
      "original_url": "https://aclanthology.org/2023.acl-long.719/",
      "title": "RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-long.719.pdf",
      "authors": [
        "Zhengliang Shi",
        "Weiwei Sun",
        "Shuo Zhang",
        "Zhen Zhang",
        "Pengjie Ren",
        "Zhaochun Ren"
      ],
      "abstract": "Evaluating open-domain dialogue systems is challenging for reasons such as the one-to-many problem, i.e., many appropriate responses other than just the golden response. As of now, automatic evaluation methods need better consistency with humans, while reliable human evaluation can be time- and cost-intensive. To this end, we propose the Reference-Assisted Dialogue Evaluation (RADE) approach under the multi-task learning framework, which leverages the pre-created utterance as reference other than the gold response to relief the one-to-many problem. Specifically, RADE explicitly compares reference and the candidate response to predict their overall scores. Moreover, an auxiliary response generation task enhances prediction via a shared encoder. To support RADE, we extend three datasets with additional rated responses other than just a golden response by human annotation. Experiments on our three datasets and two existing benchmarks demonstrate the effectiveness of our method, where Pearson, Spearman, and Kendall correlations with human evaluation outperform state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 721,
      "original_url": "https://aclanthology.org/2023.acl-long.720/",
      "title": "An AMR-based Link Prediction Approach for Document-level Event Argument Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.720.pdf",
      "authors": [
        "Yuqing Yang",
        "Qipeng Guo",
        "Xiangkun Hu",
        "Yue Zhang",
        "Xipeng Qiu",
        "Zheng Zhang"
      ],
      "abstract": "Recent works have introduced Abstract Meaning Representation (AMR) for Document-level Event Argument Extraction (Doc-level EAE), since AMR provides a useful interpretation of complex semantic structures and helps to capture long-distance dependency. However, in these works AMR is used only implicitly, for instance, as additional features or training signals. Motivated by the fact that all event structures can be inferred from AMR, this work reformulates EAE as a link prediction problem on AMR graphs. Since AMR is a generic structure and does not perfectly suit EAE, we propose a novel graph structure, Tailored AMR Graph (TAG), which compresses less informative subgraphs and edge types, integrates span information, and highlights surrounding events in the same document. With TAG, we further propose a novel method using graph neural networks as a link prediction model to find event arguments. Our extensive experiments on WikiEvents and RAMS show that this simpler approach outperforms the state-of-the-art models by 3.63pt and 2.33pt F1, respectively, and do so with reduced 56% inference time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 722,
      "original_url": "https://aclanthology.org/2023.acl-long.721/",
      "title": "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.721.pdf",
      "authors": [
        "Qingqing Cao",
        "Bhargavi Paranjape",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Large-scale vision language (VL) models use Transformers to perform cross-modal interactions between the input text and image. These cross-modal interactions are computationally expensive and memory-intensive due to the quadratic complexity of processing the input image and text. We present PuMer: a token reduction framework that uses text-informed Pruning and modality-aware Merging strategies to progressively reduce the tokens of input image and text, improving model inference speed and reducing memory footprint. PuMer learns to keep salient image tokens related to the input text and merges similar textual and visual tokens by adding lightweight token reducer modules at several cross-modal layers in the VL model. Training PuMer is mostly the same as finetuning the original VL model but faster. Our evaluation for two vision language models on four downstream VL tasks shows PuMer increases inference throughput by up to 2x and reduces memory footprint by over 50% while incurring less than a 1% accuracy drop."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 723,
      "original_url": "https://aclanthology.org/2023.acl-long.722/",
      "title": "Gloss-Free End-to-End Sign Language Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.722.pdf",
      "authors": [
        "Kezhou Lin",
        "Xiaohan Wang",
        "Linchao Zhu",
        "Ke Sun",
        "Bang Zhang",
        "Yi Yang"
      ],
      "abstract": "In this paper, we tackle the problem of sign language translation (SLT) without gloss annotations. Although intermediate representation like gloss has been proven effective, gloss annotations are hard to acquire, especially in large quantities. This limits the domain coverage of translation datasets, thus handicapping real-world applications. To mitigate this problem, we design the Gloss-Free End-to-end sign language translation framework (GloFE). Our method improves the performance of SLT in the gloss-free setting by exploiting the shared underlying semantics of signs and the corresponding spoken translation. Common concepts are extracted from the text and used as a weak form of intermediate representation. The global embedding of these concepts is used as a query for cross-attention to find the corresponding information within the learned visual features. In a contrastive manner, we encourage the similarity of query results between samples containing such concepts and decrease those that do not. We obtained state-of-the-art results on large-scale datasets, including OpenASL and How2Sign."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 724,
      "original_url": "https://aclanthology.org/2023.acl-long.723/",
      "title": "TAGPRIME: A Unified Framework for Relational Structure Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.723.pdf",
      "authors": [
        "I-Hung Hsu",
        "Kuan-Hao Huang",
        "Shuning Zhang",
        "Wenxin Cheng",
        "Prem Natarajan",
        "Kai-Wei Chang",
        "Nanyun Peng"
      ],
      "abstract": "Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems. TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on three different tasks that cover ten datasets across five different languages demonstrate the generality and effectiveness of TAGPRIME."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 725,
      "original_url": "https://aclanthology.org/2023.acl-long.724/",
      "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.724.pdf",
      "authors": [
        "Linyuan Gong",
        "Chenyan Xiong",
        "Xiaodong Liu",
        "Payal Bajaj",
        "Yiqing Xie",
        "Alvin Cheung",
        "Jianfeng Gao",
        "Xia Song"
      ],
      "abstract": "This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model’s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 726,
      "original_url": "https://aclanthology.org/2023.acl-long.725/",
      "title": "BITE: Textual Backdoor Attacks with Iterative Trigger Injection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.725.pdf",
      "authors": [
        "Jun Yan",
        "Vansh Gupta",
        "Xiang Ren"
      ],
      "abstract": "Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a “backdoor” into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary’s choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of “trigger words”. These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack is significantly more effective than baseline methods while maintaining decent stealthiness, raising alarm on the usage of untrusted training data. We further propose a defense method named DeBITE based on potential trigger word removal, which outperforms existing methods in defending against BITE and generalizes well to handling other backdoor attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 727,
      "original_url": "https://aclanthology.org/2023.acl-long.726/",
      "title": "A Crosslingual Investigation of Conceptualization in 1335 Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.726.pdf",
      "authors": [
        "Yihong Liu",
        "Haotian Ye",
        "Leonie Weissweiler",
        "Philipp Wicke",
        "Renhao Pei",
        "Robert Zangenfeind",
        "Hinrich Schütze"
      ],
      "abstract": "Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for ‘belly’ and ‘womb’. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept (‘bird’) and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations. The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracies between 54% and 87%"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 728,
      "original_url": "https://aclanthology.org/2023.acl-long.727/",
      "title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
      "pdf_url": "https://aclanthology.org/2023.acl-long.727.pdf",
      "authors": [
        "Yi Xu",
        "Shuqian Sheng",
        "Bo Xue",
        "Luoyi Fu",
        "Xinbing Wang",
        "Chenghu Zhou"
      ],
      "abstract": "Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the emergence of a new idea can be regarded as the fusion of two concepts that co-occur in an academic paper. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 729,
      "original_url": "https://aclanthology.org/2023.acl-long.728/",
      "title": "mCLIP: Multilingual CLIP via Cross-lingual Transfer",
      "pdf_url": "https://aclanthology.org/2023.acl-long.728.pdf",
      "authors": [
        "Guanhua Chen",
        "Lu Hou",
        "Yun Chen",
        "Wenliang Dai",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Jia Pan",
        "Wenping Wang"
      ],
      "abstract": "Large-scale vision-language pretrained (VLP) models like CLIP have shown remarkable performance on various downstream cross-modal tasks. However, they are usually biased towards English due to the lack of sufficient non-English image-text pairs. Existing multilingual VLP methods often learn retrieval-inefficient single-stream models by translation-augmented non-English image-text pairs. In this paper, we introduce mCLIP, a retrieval-efficient dual-stream multilingual VLP model, trained by aligning the CLIP model and a Multilingual Text Encoder (MTE) through a novel Triangle Cross-modal Knowledge Distillation (TriKD) method. It is parameter-efficient as only two light projectors on the top of them are updated during distillation. Furthermore, to enhance the token- and sentence-level multilingual representation of the MTE, we propose to train it with machine translation and contrastive learning jointly before the TriKD to provide a better initialization. Empirical results show that mCLIP achieves new state-of-the-art performance for both zero-shot and finetuned multilingual image-text retrieval task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 730,
      "original_url": "https://aclanthology.org/2023.acl-long.729/",
      "title": "Distantly Supervised Course Concept Extraction in MOOCs with Academic Discipline",
      "pdf_url": "https://aclanthology.org/2023.acl-long.729.pdf",
      "authors": [
        "Mengying Lu",
        "Yuquan Wang",
        "Jifan Yu",
        "Yexing Du",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "With the rapid growth of Massive Open Online Courses (MOOCs), it is expensive and time-consuming to extract high-quality knowledgeable concepts taught in the course by human effort to help learners grasp the essence of the course. In this paper, we propose to automatically extract course concepts using distant supervision to eliminate the heavy work of human annotations, which generates labels by matching them with an easily accessed dictionary. However, this matching process suffers from severe noisy and incomplete annotations because of the limited dictionary and diverse MOOCs. To tackle these challenges, we present a novel three-stage framework DS-MOCE, which leverages the power of pre-trained language models explicitly and implicitly and employs discipline-embedding models with a self-train strategy based on label generation refinement across different domains. We also provide an expert-labeled dataset spanning 20 academic disciplines. Experimental results demonstrate the superiority of DS-MOCE over the state-of-the-art distantly supervised methods (with 7% absolute F1 score improvement). Code and data are now available at https://github.com/THU-KEG/MOOC-NER."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 731,
      "original_url": "https://aclanthology.org/2023.acl-long.730/",
      "title": "Extrinsic Evaluation of Machine Translation Metrics",
      "pdf_url": "https://aclanthology.org/2023.acl-long.730.pdf",
      "authors": [
        "Nikita Moghe",
        "Tom Sherborne",
        "Mark Steedman",
        "Alexandra Birch"
      ],
      "abstract": "Automatic machine translation (MT) metrics are widely used to distinguish the quality of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the segment-level quality by correlating metrics with how useful the translations are for downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model and a translation model. We calculate the correlation between the metric’s ability to predict a good/bad translation with the success/failure on the final task for the machine translated test sentences. Our experiments demonstrate that all metrics exhibit negligible correlation with the extrinsic evaluation of the downstream outcomes. We also find that the scores provided by neural metrics are not interpretable, in large part due to having undefined ranges. We synthesise our analysis into recommendations for future MT metrics to produce labels rather than scores for more informative interaction between machine translation and multilingual language understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 732,
      "original_url": "https://aclanthology.org/2023.acl-long.731/",
      "title": "ExplainMeetSum: A Dataset for Explainable Meeting Summarization Aligned with Human Intent",
      "pdf_url": "https://aclanthology.org/2023.acl-long.731.pdf",
      "authors": [
        "Hyun Kim",
        "Minsoo Cho",
        "Seung-Hoon Na"
      ],
      "abstract": "To enhance the explainability of meeting summarization, we construct a new dataset called “ExplainMeetSum,” an augmented version of QMSum, by newly annotating evidence sentences that faithfully “explain” a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracles. We further present an explainability-aware task, named “Explainable Evidence Extraction” (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 733,
      "original_url": "https://aclanthology.org/2023.acl-long.732/",
      "title": "A Cross-Modality Context Fusion and Semantic Refinement Network for Emotion Recognition in Conversation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.732.pdf",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "abstract": "Emotion recognition in conversation (ERC) has attracted enormous attention for its applications in empathetic dialogue systems. However, most previous researches simply concatenate multimodal representations, leading to an accumulation of redundant information and a limited context interaction between modalities. Furthermore, they only consider simple contextual features ignoring semantic clues, resulting in an insufficient capture of the semantic coherence and consistency in conversations. To address these limitations, we propose a cross-modality context fusion and semantic refinement network (CMCF-SRNet). Specifically, we first design a cross-modal locality-constrained transformer to explore the multimodal interaction. Second, we investigate a graph-based semantic refinement transformer, which solves the limitation of insufficient semantic relationship information between utterances. Extensive experiments on two public benchmark datasets show the effectiveness of our proposed method compared with other state-of-the-art methods, indicating its potential application in emotion recognition. Our model will be available at https://github.com/zxiaohen/CMCF-SRNet."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 734,
      "original_url": "https://aclanthology.org/2023.acl-long.733/",
      "title": "CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.733.pdf",
      "authors": [
        "Weiqi Wang",
        "Tianqing Fang",
        "Baixuan Xu",
        "Chun Yi Louis Bo",
        "Yangqiu Song",
        "Lei Chen"
      ],
      "abstract": "Commonsense reasoning, aiming at endowing machines with a human-like ability to make situational presumptions, is extremely challenging to generalize. For someone who barely knows about “meditation,” while is knowledgeable about “singing,” he can still infer that “meditation makes people relaxed” from the existing knowledge that “singing makes people relaxed” by first conceptualizing “singing” as a “relaxing event” and then instantiating that event to “meditation.”This process, known as conceptual induction and deduction, is fundamental to commonsense reasoning while lacking both labeled data and methodologies to enhance commonsense modeling. To fill such a research gap, we propose CAT (Contextualized ConceptuAlization and InsTantiation),a semi-supervised learning framework that integrates event conceptualization and instantiation to conceptualize commonsense knowledge bases at scale. Extensive experiments show that our framework achieves state-of-the-art performances on two conceptualization tasks, and the acquired abstract commonsense knowledge can significantly improve commonsense inference modeling. Our code, data, and fine-tuned models are publicly available at [https://github.com/HKUST-KnowComp/CAT](https://github.com/HKUST-KnowComp/CAT)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 735,
      "original_url": "https://aclanthology.org/2023.acl-long.734/",
      "title": "The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research",
      "pdf_url": "https://aclanthology.org/2023.acl-long.734.pdf",
      "authors": [
        "Mohamed Abdalla",
        "Jan Philip Wahle",
        "Terry Ruas",
        "Aurélie Névéol",
        "Fanny Ducel",
        "Saif Mohammad",
        "Karen Fort"
      ],
      "abstract": "Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are significant and fast-growing. This work calls for increased transparency of industry influence in the field."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 736,
      "original_url": "https://aclanthology.org/2023.acl-long.735/",
      "title": "Language of Bargaining",
      "pdf_url": "https://aclanthology.org/2023.acl-long.735.pdf",
      "authors": [
        "Mourad Heddaya",
        "Solomon Dworkin",
        "Chenhao Tan",
        "Rob Voigt",
        "Alexander Zentefis"
      ],
      "abstract": "Leveraging an established exercise in negotiation education, we build a novel dataset for studying how the use of language shapes bilateral bargaining. Our dataset extends existing work in two ways: 1) we recruit participants via behavioral labs instead of crowdsourcing platforms and allow participants to negotiate through audio, enabling more naturalistic interactions; 2) we add a control setting where participants negotiate only through alternating, written numeric offers. Despite the two contrasting forms of communication, we find that the average agreed prices of the two treatments are identical. But when subjects can talk, fewer offers are exchanged, negotiations finish faster, the likelihood of reaching agreement rises, and the variance of prices at which subjects agree drops substantially. We further propose a taxonomy of speech acts in negotiation and enrich the dataset with annotated speech acts. We set up prediction tasks to predict negotiation success and find that being reactive to the arguments of the other party is advantageous over driving the negotiation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 737,
      "original_url": "https://aclanthology.org/2023.acl-long.736/",
      "title": "Do Question Answering Modeling Improvements Hold Across Benchmarks?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.736.pdf",
      "authors": [
        "Nelson F. Liu",
        "Tony Lee",
        "Robin Jia",
        "Percy Liang"
      ],
      "abstract": "Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence—two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 738,
      "original_url": "https://aclanthology.org/2023.acl-long.737/",
      "title": "VLN-Trans: Translator for the Vision and Language Navigation Agent",
      "pdf_url": "https://aclanthology.org/2023.acl-long.737.pdf",
      "authors": [
        "Yue Zhang",
        "Parisa Kordjamshidi"
      ],
      "abstract": "Language understanding is essential for the navigation agent to follow instructions. We observe two kinds of issues in the instructions that can make the navigation task challenging: 1. The mentioned landmarks are not recognizable by the navigation agent due to the different vision abilities of the instructor and the modeled agent. 2. The mentioned landmarks are applicable to multiple targets, thus not distinctive for selecting the target among the candidate viewpoints. To deal with these issues, we design a translator module for the navigation agent to convert the original instructions into easy-to-follow sub-instruction representations at each step. The translator needs to focus on the recognizable and distinctive landmarks based on the agent’s visual abilities and the observed visual environment. To achieve this goal, we create a new synthetic sub-instruction dataset and design specific tasks to train the translator and the navigation agent. We evaluate our approach on Room2Room (R2R), Room4room (R4R), and Room2Room Last (R2R-Last) datasets and achieve state-of-the-art results on multiple benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 739,
      "original_url": "https://aclanthology.org/2023.acl-long.738/",
      "title": "Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.738.pdf",
      "authors": [
        "Qinhong Zhou",
        "Zonghan Yang",
        "Peng Li",
        "Yang Liu"
      ],
      "abstract": "Conventional knowledge distillation (KD) methods require access to the internal information of teachers, e.g., logits. However, such information may not always be accessible for large pre-trained language models (PLMs). In this work, we focus on decision-based KD for PLMs, where only teacher decisions (i.e., top-1 labels) are accessible. Considering the information gap between logits and decisions, we propose a novel method to estimate logits from the decision distributions. Specifically, decision distributions can be both derived as a function of logits theoretically and estimated with test-time data augmentation empirically. By combining the theoretical and empirical estimations of the decision distributions together, the estimation of logits can be successfully reduced to a simple root-finding problem. Extensive experiments show that our method significantly outperforms strong baselines on both natural language understanding and machine reading comprehension datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 740,
      "original_url": "https://aclanthology.org/2023.acl-long.739/",
      "title": "Continual Contrastive Finetuning Improves Low-Resource Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.739.pdf",
      "authors": [
        "Wenxuan Zhou",
        "Sheng Zhang",
        "Tristan Naumann",
        "Muhao Chen",
        "Hoifung Poon"
      ],
      "abstract": "Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity pair embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness of our method. Particularly, when using 1% end-task training data, our method outperforms PLM-based RE classifier by 10.5% and 6.1% on the two datasets, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 741,
      "original_url": "https://aclanthology.org/2023.acl-long.740/",
      "title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-long.740.pdf",
      "authors": [
        "Lingzhi Wang",
        "Tong Chen",
        "Wei Yuan",
        "Xingshan Zeng",
        "Kam-Fai Wong",
        "Hongzhi Yin"
      ],
      "abstract": "Recent legislation of the “right to be forgotten” has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 742,
      "original_url": "https://aclanthology.org/2023.acl-long.741/",
      "title": "UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language",
      "pdf_url": "https://aclanthology.org/2023.acl-long.741.pdf",
      "authors": [
        "Nuwa Xi",
        "Sendong Zhao",
        "Haochun Wang",
        "Chi Liu",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Decoding text stimuli from cognitive signals (e.g. fMRI) enhances our understanding of the human language system, paving the way for building versatile Brain-Computer Interface. However, existing studies largely focus on decoding individual word-level fMRI volumes from a restricted vocabulary, which is far too idealized for real-world application. In this paper, we propose fMRI2text, the first open-vocabulary task aiming to bridge fMRI time series and human language. Furthermore, to explore the potential of this new task, we present a baseline solution, UniCoRN: the Unified Cognitive Signal ReconstructioN for Brain Decoding. By reconstructing both individual time points and time series, UniCoRN establishes a robust encoder for cognitive signals (fMRI & EEG). Leveraging a pre-trained language model as decoder, UniCoRN proves its efficacy in decoding coherent text from fMRI series across various split settings. Our model achieves a 34.77% BLEU score on fMRI2text, and a 37.04% BLEU when generalized to EEG-to-text decoding, thereby surpassing the former baseline. Experimental results indicate the feasibility of decoding consecutive fMRI volumes, and the effectiveness of decoding different cognitive signals using a unified structure."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 743,
      "original_url": "https://aclanthology.org/2023.acl-long.742/",
      "title": "Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths",
      "pdf_url": "https://aclanthology.org/2023.acl-long.742.pdf",
      "authors": [
        "Xiangqing Shen",
        "Siwei Wu",
        "Rui Xia"
      ],
      "abstract": "ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., head event, relation, tail event. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC’s advantages in Knowledge Coverage and Multi-hop Paths. Both the source code of Rel-CSKGC and Dense-ATOMIC are publicly available on https://github.com/NUSTM/Dense-ATOMIC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 744,
      "original_url": "https://aclanthology.org/2023.acl-long.743/",
      "title": "Shrinking Embeddings for Hyper-Relational Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.743.pdf",
      "authors": [
        "Bo Xiong",
        "Mojtaba Nayyeri",
        "Shirui Pan",
        "Steffen Staab"
      ],
      "abstract": "Link prediction on knowledge graphs (KGs) has been extensively studied on binary relational KGs, wherein each fact is represented by a triple. A significant amount of important knowledge, however, is represented by hyper-relational facts where each fact is composed of a primal triple and a set of qualifiers comprising a key-value pair that allows for expressing more complicated semantics. Although some recent works have proposed to embed hyper-relational KGs, these methods fail to capture essential inference patterns of hyper-relational facts such as qualifier monotonicity, qualifier implication, and qualifier mutual exclusion, limiting their generalization capability. To unlock this, we present ShrinkE, a geometric hyper-relational KG embedding method aiming to explicitly model these patterns. ShrinkE models the primal triple as a spatial-functional transformation from the head into a relation-specific box. Each qualifier “shrinks” the box to narrow down the possible answer set and, thus, realizes qualifier monotonicity. The spatial relationships between the qualifier boxes allow for modeling core inference patterns of qualifiers such as implication and mutual exclusion. Experimental results demonstrate ShrinkE’s superiority on three benchmarks of hyper-relational KGs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 745,
      "original_url": "https://aclanthology.org/2023.acl-long.744/",
      "title": "CTC-based Non-autoregressive Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.744.pdf",
      "authors": [
        "Chen Xu",
        "Xiaoqian Liu",
        "Xiaowen Liu",
        "Qingxuan Sun",
        "Yuhao Zhang",
        "Murun Yang",
        "Qianqian Dong",
        "Tom Ko",
        "Mingxuan Wang",
        "Tong Xiao",
        "Anxiang Ma",
        "Jingbo Zhu"
      ],
      "abstract": "Combining end-to-end speech translation (ST) and non-autoregressive (NAR) generation is promising in language and speech processing for their advantages of less error propagation and low latency. In this paper, we investigate the potential of connectionist temporal classification (CTC) for non-autoregressive speech translation (NAST).In particular, we develop a model consisting of two encoders that are guided by CTC to predict the source and target texts, respectively. Introducing CTC into NAST on both language sides has obvious challenges: 1) the conditional independent generation somewhat breaks the interdependency among tokens, and 2) the monotonic alignment assumption in standard CTC does not hold in translation tasks. In response, we develop a prediction-aware encoding approach and a cross-layer attention approach to address these issues. We also use curriculum learning to improve convergence of training. Experiments on the MuST-C ST benchmarks show that our NAST model achieves an average BLEU score of 29.5 with a speed-up of 5.67×, which is comparable to the autoregressive counterpart and even outperforms the previous best result of 0.9 BLEU points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 746,
      "original_url": "https://aclanthology.org/2023.acl-long.745/",
      "title": "Attention as a Guide for Simultaneous Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.745.pdf",
      "authors": [
        "Sara Papi",
        "Matteo Negri",
        "Marco Turchi"
      ],
      "abstract": "In simultaneous speech translation (SimulST), effective policies that determine when to write partial translations are crucial to reach high output quality with low latency. Towards this objective, we propose EDAtt (Encoder-Decoder Attention), an adaptive policy that exploits the attention patterns between audio source and target textual translation to guide an offline-trained ST model during simultaneous inference. EDAtt exploits the attention scores modeling the audio-translation relation to decide whether to emit a partial hypothesis or wait for more audio input. This is done under the assumption that, if attention is focused towards the most recently received speech segments, the information they provide can be insufficient to generate the hypothesis (indicating that the system has to wait for additional audio input). Results on en->de, es show that EDAtt yields better results compared to the SimulST state of the art, with gains respectively up to 7 and 4 BLEU points for the two languages, and with a reduction in computational-aware latency up to 1.4s and 0.7s compared to existing SimulST policies applied to offline-trained models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 747,
      "original_url": "https://aclanthology.org/2023.acl-long.746/",
      "title": "On Complementarity Objectives for Hybrid Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-long.746.pdf",
      "authors": [
        "Dohyeon Lee",
        "Seung-won Hwang",
        "Kyungjae Lee",
        "Seungtaek Choi",
        "Sunghyun Park"
      ],
      "abstract": "Dense retrieval has shown promising results in various information retrieval tasks, and hybrid retrieval, combined with the strength of sparse retrieval, has also been actively studied. A key challenge in hybrid retrieval is to make sparse and dense complementary to each other. Existing models have focused on dense models to capture “residual” features neglected in the sparse models. Our key distinction is to show how this notion of residual complementarity is limited, and propose a new objective, denoted as RoC (Ratio of Complementarity), which captures a fuller notion of complementarity. We propose a two-level orthogonality designed to improve RoC, then show that the improved RoC of our model, in turn, improves the performance of hybrid retrieval. Our method outperforms all state-of-the-art methods on three representative IR benchmarks: MSMARCO-Passage, Natural Questions, and TREC Robust04, with statistical significance. Our finding is also consistent in various adversarial settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 748,
      "original_url": "https://aclanthology.org/2023.acl-long.747/",
      "title": "C-STANCE: A Large Dataset for Chinese Zero-Shot Stance Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.747.pdf",
      "authors": [
        "Chenye Zhao",
        "Yingjie Li",
        "Cornelia Caragea"
      ],
      "abstract": "Zero-shot stance detection (ZSSD) aims to determine whether the author of a text is in favor of, against, or neutral toward a target that is unseen during training. Despite the growing attention on ZSSD, most recent advances in this task are limited to English and do not pay much attention to other languages such as Chinese. To support ZSSD research, in this paper, we present C-STANCE that, to our knowledge, is the first Chinese dataset for zero-shot stance detection. We introduce two challenging subtasks for ZSSD: target-based ZSSD and domain-based ZSSD. Our dataset includes both noun-phrase targets and claim targets, covering a wide range of domains. We provide a detailed description and analysis of our dataset. To establish results on C-STANCE, we report performance scores using state-of-the-art deep learning models. We publicly release our dataset and code to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 749,
      "original_url": "https://aclanthology.org/2023.acl-long.748/",
      "title": "Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.748.pdf",
      "authors": [
        "Haoli Bai",
        "Zhiguang Liu",
        "Xiaojun Meng",
        "Li Wentao",
        "Shuang Liu",
        "Yifeng Luo",
        "Nian Xie",
        "Rongfu Zheng",
        "Liangwei Wang",
        "Lu Hou",
        "Jiansheng Wei",
        "Xin Jiang",
        "Qun Liu"
      ],
      "abstract": "Unsupervised pre-training on millions of digital-born or scanned documents has shown promising advances in visual document understanding (VDU). While various vision-language pre-training objectives are studied in existing solutions, the document textline, as an intrinsic granularity in VDU, has seldom been explored so far. A document textline usually contains words that are spatially and semantically correlated, which can be easily obtained from OCR engines. In this paper, we propose Wukong-Reader, trained with new pre-training objectives to leverage the structural knowledge nested in document textlines. We introduce textline-region contrastive learning to achieve fine-grained alignment between the visual regions and texts of document textlines. Furthermore, masked region modeling and textline-grid matching are also designed to enhance the visual and layout representations of textlines. Experiments show that Wukong-Reader brings superior performance on various VDU tasks in both English and Chinese. The fine-grained alignment over textlines also empowers Wukong-Reader with promising localization ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 750,
      "original_url": "https://aclanthology.org/2023.acl-long.749/",
      "title": "PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.749.pdf",
      "authors": [
        "Yunshui Li",
        "Binyuan Hui",
        "ZhiChao Yin",
        "Min Yang",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes PaCE, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data. Furthermore, we propose a progressive training method where old experts from the past can assist new experts, facilitating the expansion of their capabilities. Experimental results demonstrate that PaCE achieves state-of-the-art results on eight multi-modal dialog benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 751,
      "original_url": "https://aclanthology.org/2023.acl-long.750/",
      "title": "MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.750.pdf",
      "authors": [
        "Yongfeng Huang",
        "Yanyang Li",
        "Yichong Xu",
        "Lin Zhang",
        "Ruyi Gan",
        "Jiaxing Zhang",
        "Liwei Wang"
      ],
      "abstract": "Recent advances in pre-trained language models (PLMs) have facilitated the development ofcommonsense reasoning tasks. However, existing methods rely on multi-hop knowledgeretrieval and thus suffer low accuracy due toembedded noise in the acquired knowledge. In addition, these methods often attain highcomputational costs and nontrivial knowledgeloss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poorlocal optimum. In this work, we propose MultiView Knowledge Retrieval with Prompt Tuning (MVP-Tuning). MVP-Tuning leveragessimilar question-answer pairs in the training setto improve knowledge retrieval and employsa single prompt-tuned PLM to model knowledge and input text jointly. We conduct our experiments on five commonsense reasoning QAbenchmarks to show that MVP-Tuning outperforms all other baselines in 4 out of 5 datasetswith less than 2% trainable parameters. MVPTuning even gets a new state-of-the-art resulton OpenBookQA and is number one on theleaderboard."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 752,
      "original_url": "https://aclanthology.org/2023.acl-long.751/",
      "title": "PEIT: Bridging the Modality Gap with Pre-trained Models for End-to-End Image Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.751.pdf",
      "authors": [
        "Shaolin Zhu",
        "Shangjie Li",
        "Yikun Lei",
        "Deyi Xiong"
      ],
      "abstract": "Image translation is a task that translates an image containing text in the source language to the target language. One major challenge with image translation is the modality gap between visual text inputs and textual inputs/outputs of machine translation (MT). In this paper, we propose PEIT, an end-to-end image translation framework that bridges the modality gap with pre-trained models. It is composed of four essential components: a visual encoder, a shared encoder-decoder backbone network, a vision-text representation aligner equipped with the shared encoder and a cross-modal regularizer stacked over the shared decoder. Both the aligner and regularizer aim at reducing the modality gap. To train PEIT, we employ a two-stage pre-training strategy with an auxiliary MT task: (1) pre-training the MT model on the MT training data to initialize the shared encoder-decoder backbone network; and (2) pre-training PEIT with the aligner and regularizer on a synthesized dataset with rendered images containing text from the MT training data. In order to facilitate the evaluation of PEIT and promote research on image translation, we create a large-scale image translation corpus ECOIT containing 480K image-translation pairs via crowd-sourcing and manual post-editing from real-world images in the e-commerce domain. Experiments on the curated ECOIT benchmark dataset demonstrate that PEIT substantially outperforms both cascaded image translation systems (OCR+MT) and previous strong end-to-end image translation model, with fewer parameters and faster decoding speed."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 753,
      "original_url": "https://aclanthology.org/2023.acl-long.752/",
      "title": "Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-long.752.pdf",
      "authors": [
        "Erik Arakelyan",
        "Arnav Arora",
        "Isabelle Augenstein"
      ],
      "abstract": "The task of Stance Detection is concerned with identifying the attitudes expressed by an author towards a target of interest. This task spans a variety of domains ranging from social media opinion identification to detecting the stance for a legal claim. However, the framing of the task varies within these domains in terms of the data collection protocol, the label dictionary and the number of available annotations. Furthermore, these stance annotations are significantly imbalanced on a per-topic and inter-topic basis. These make multi-domain stance detection challenging, requiring standardization and domain adaptation. To overcome this challenge, we propose Topic Efficient StancE Detection (TESTED), consisting of a topic-guided diversity sampling technique used for creating a multi-domain data efficient training set and a contrastive objective that is used for fine-tuning a stance classifier using the produced set. We evaluate the method on an existing benchmark of 16 datasets with in-domain, i.e. all topics seen and out-of-domain, i.e. unseen topics, experiments. The results show that the method outperforms the state-of-the-art with an average of 3.5 F1 points increase in-domain and is more generalizable with an averaged 10.2 F1 on out-of-domain evaluation while using <10% of the training data. We show that our sampling technique mitigates both inter- and per-topic class imbalances. Finally, our analysis demonstrates that the contrastive learning objective allows the model for a more pronounced segmentation of samples with varying labels."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 754,
      "original_url": "https://aclanthology.org/2023.acl-long.753/",
      "title": "DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles",
      "pdf_url": "https://aclanthology.org/2023.acl-long.753.pdf",
      "authors": [
        "Tanishq Gupta",
        "Mohd Zaki",
        "Devanshi Khatsuriya",
        "Kausik Hira",
        "N M Anoop Krishnan",
        "Mausam"
      ],
      "abstract": "A crucial component in the curation of KB for a scientific domain (e.g., materials science, food & nutrition, fuels) is information extraction from tables in the domain’s published research articles. To facilitate research in this direction, we define a novel NLP task of extracting compositions of materials (e.g., glasses) from tables in materials science papers. The task involves solving several challenges in concert, such as tables that mention compositions have highly varying structures; text in captions and full paper needs to be incorporated along with data in tables; and regular languages for numbers, chemical compounds, and composition expressions must be integrated into the model. We release a training dataset comprising 4,408 distantly supervised tables, along with 1,475 manually annotated dev and test tables. We also present DiSCoMaT, a strong baseline that combines multiple graph neural networks with several task-specific regular expressions, features, and constraints. We show that DiSCoMaT outperforms recent table processing architectures by significant margins. We release our code and data for further research on this challenging IE task from scientific tables."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 755,
      "original_url": "https://aclanthology.org/2023.acl-long.754/",
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.754.pdf",
      "authors": [
        "Yizhong Wang",
        "Yeganeh Kordi",
        "Swaroop Mishra",
        "Alisa Liu",
        "Noah A. Smith",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 756,
      "original_url": "https://aclanthology.org/2023.acl-long.755/",
      "title": "Disentangled Phonetic Representation for Chinese Spelling Correction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.755.pdf",
      "authors": [
        "Zihong Liang",
        "Xiaojun Quan",
        "Qifan Wang"
      ],
      "abstract": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous characters in Chinese texts. Although efforts have been made to introduce phonetic information (Hanyu Pinyin) in this task, they typically merge phonetic representations with character representations, which tends to weaken the representation effect of normal texts. In this work, we propose to disentangle the two types of features to allow for direct interaction between textual and phonetic information. To learn useful phonetic representations, we introduce a pinyin-to-character objective to ask the model to predict the correct characters based solely on phonetic information, where a separation mask is imposed to disable attention from phonetic input to text. To avoid overfitting the phonetics, we further design a self-distillation module to ensure that semantic information plays a major role in the prediction. Extensive experiments on three CSC benchmarks demonstrate the superiority of our method in using phonetic information."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 757,
      "original_url": "https://aclanthology.org/2023.acl-long.756/",
      "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.756.pdf",
      "authors": [
        "Ta-Chung Chi",
        "Ting-Han Fan",
        "Alexander Rudnicky",
        "Peter Ramadge"
      ],
      "abstract": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences.A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 758,
      "original_url": "https://aclanthology.org/2023.acl-long.757/",
      "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.757.pdf",
      "authors": [
        "Jiaxu Zhao",
        "Meng Fang",
        "Zijing Shi",
        "Yitong Li",
        "Ling Chen",
        "Mykola Pechenizkiy"
      ],
      "abstract": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models’ conversational capabilities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 759,
      "original_url": "https://aclanthology.org/2023.acl-long.758/",
      "title": "Learning New Skills after Deployment: Improving open-domain internet-driven dialogue with human feedback",
      "pdf_url": "https://aclanthology.org/2023.acl-long.758.pdf",
      "authors": [
        "Jing Xu",
        "Megan Ung",
        "Mojtaba Komeili",
        "Kushal Arora",
        "Y-Lan Boureau",
        "Jason Weston"
      ],
      "abstract": "Frozen models trained to mimic static datasets can never improve their performance. Models that can employ internet-retrieval for up-to-date information and obtain feedback from humans during deployment provide the promise of both adapting to new information, and improving their performance. In this work we study how to improve internet-driven conversational skills in such a learning framework. We collect deployment data, which we make publicly available, of human interactions, and collect various types of human feedback – including binary quality measurements, free-form text feedback, and fine-grained reasons for failure. We then study various algorithms for improving from such feedback, including standard supervised learning, rejection sampling, model-guiding and reward-based learning, in order to make recommendations on which type of feed- back and algorithms work best. We find the recently introduced DIRECTOR model (Arora et al., 2022) shows significant improvements over other existing approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 760,
      "original_url": "https://aclanthology.org/2023.acl-long.759/",
      "title": "Uncovering and Categorizing Social Biases in Text-to-SQL",
      "pdf_url": "https://aclanthology.org/2023.acl-long.759.pdf",
      "authors": [
        "Yan Liu",
        "Yan Gao",
        "Zhe Su",
        "Xiaokang Chen",
        "Elliott Ash",
        "Jian-Guang Lou"
      ],
      "abstract": "Large pre-trained language models are acknowledged to carry social bias towards different demographics, which can further amplify existing stereotypes in our society and cause even more harm. Text-to-SQL is an important task, models of which are mainly adopted by administrative industries, where unfair decisions may lead to catastrophic consequences. However, existing Text-to-SQL models are trained on clean, neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up social bias in models under ideal conditions, which nevertheless may emerge in real application scenarios. In this work, we aim to uncover and mitigate social bias in Text-to-SQL models. We summarize the categories of social bias that may occur in structural data for Text-to-SQL models. We build test benchmarks and reveal that models with similar task accuracy can contain social bias at very different rates. We show how to take advantage of our methodology to assess and mitigate social bias in the downstream Text-to-SQL task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 761,
      "original_url": "https://aclanthology.org/2023.acl-long.760/",
      "title": "On the Compositional Generalization in Versatile Open-domain Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-long.760.pdf",
      "authors": [
        "Tingchen Fu",
        "Xueliang Zhao",
        "Lemao Liu",
        "Rui Yan"
      ],
      "abstract": "Previous research has demonstrated the potential of multi-task learning to foster a conversational agent’s ability to acquire a variety of skills. However, these approaches either suffer from interference among different datasets (also known as negative transfer), or fail to effectively reuse knowledge and skills learned from other datasets. In contrast to previous works, we develop a sparsely activated modular network: (1) We propose a well-rounded set of operators and instantiate each operator with an independent module; (2) We formulate dialogue generation as the execution of a generated programme which recursively composes and assembles modules. Extensive experiments on 9 datasets verify the efficacy of our methods through automatic evaluation and human evaluation. Notably, our model outperforms state-of-the-art supervised approaches on 4 datasets with only 10% training data thanks to the modular architecture and multi-task learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 762,
      "original_url": "https://aclanthology.org/2023.acl-long.761/",
      "title": "What is the Real Intention behind this Question? Dataset Collection and Intention Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.761.pdf",
      "authors": [
        "Maryam Sadat Mirzaei",
        "Kourosh Meshgi",
        "Satoshi Sekine"
      ],
      "abstract": "Asking and answering questions are inseparable parts of human social life. The primary purposes of asking questions are to gain knowledge or request help which has been the subject of question-answering studies. However, questions can also reflect negative intentions and include implicit offenses, such as highlighting one’s lack of knowledge or bolstering an alleged superior knowledge, which can lead to conflict in conversations; yet has been scarcely researched. This paper is the first study to introduce a dataset (Question Intention Dataset) that includes questions with positive/neutral and negative intentions and the underlying intention categories within each group. We further conduct a meta-analysis to highlight tacit and apparent intents. We also propose a classification method using Transformers augmented by TF-IDF-based features and report the results of several models for classifying the main intention categories. We aim to highlight the importance of taking intentions into account, especially implicit and negative ones, to gain insight into conflict-evoking questions and better understand human-human communication on the web for NLP applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 763,
      "original_url": "https://aclanthology.org/2023.acl-long.762/",
      "title": "Conjunct Resolution in the Face of Verbal Omissions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.762.pdf",
      "authors": [
        "Royi Rassin",
        "Yoav Goldberg",
        "Reut Tsarfaty"
      ],
      "abstract": "Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a conjunct resolution task that operates directly on the text and makes use of a split-and-rephrase paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 764,
      "original_url": "https://aclanthology.org/2023.acl-long.763/",
      "title": "Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.763.pdf",
      "authors": [
        "Mounica Maddela",
        "Megan Ung",
        "Jing Xu",
        "Andrea Madotto",
        "Heather Foran",
        "Y-Lan Boureau"
      ],
      "abstract": "Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 765,
      "original_url": "https://aclanthology.org/2023.acl-long.764/",
      "title": "Learning In-context Learning for Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.764.pdf",
      "authors": [
        "Jiawei Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Jie Lou",
        "Wei Jia",
        "Dai Dai",
        "Hua Wu",
        "Boxi Cao",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Named entity recognition in real-world applications suffers from the diversity of entity types, the emergence of new entity types, and the lack of high-quality annotations. To address the above problems, this paper proposes an in-context learning-based NER approach, which can effectively inject in-context NER ability into PLMs and recognize entities of novel types on-the-fly using only a few demonstrative instances. Specifically, we model PLMs as a meta-function Lambda_instruction, demonstrations, text.M, and a new entity extractor can be implicitly constructed by applying new instruction and demonstrations to PLMs, i.e., (Lambda . M) (instruction, demonstrations) ->F where F will be a new entity extractor F: text -> entities. To inject the above in-context NER ability into PLMs, we propose a meta-function pre-training algorithm, which pre-trains PLMs by comparing the (instruction, demonstration)-initialized extractor with a surrogate golden extractor. Experimental results on 4 few-shot NER datasets show that our method can effectively inject in-context NER ability into PLMs and significantly outperforms the PLMs+fine-tuning counterparts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 766,
      "original_url": "https://aclanthology.org/2023.acl-long.765/",
      "title": "Holistic Prediction on a Time-Evolving Attributed Graph",
      "pdf_url": "https://aclanthology.org/2023.acl-long.765.pdf",
      "authors": [
        "Shohei Yamasaki",
        "Yuya Sasaki",
        "Panagiotis Karras",
        "Makoto Onizuka"
      ],
      "abstract": "Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribute prediction is independent, and fall short of predicting the appearance of new nodes that were not observed in the past. In this paper, we address two interrelated questions; (1) can we exploit task interdependence to improve prediction accuracy? and (2) can we predict new nodes with their attributes? We propose a unified framework that predicts node attributes and topology changes such as the appearance and disappearance of links and the emergence and loss of nodes. This frame-work comprises components for independent and interactive prediction and for predicting new nodes. Our experimental study using real-world data confirms that our interdependent prediction framework achieves higher accuracy than methods based on independent prediction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 767,
      "original_url": "https://aclanthology.org/2023.acl-long.766/",
      "title": "Modeling Instance Interactions for Joint Information Extraction with Neural High-Order Conditional Random Field",
      "pdf_url": "https://aclanthology.org/2023.acl-long.766.pdf",
      "authors": [
        "Zixia Jia",
        "Zhaohui Yan",
        "Wenjuan Han",
        "Zilong Zheng",
        "Kewei Tu"
      ],
      "abstract": "Prior works on joint Information Extraction (IE) typically model instance (e.g., event triggers, entities, roles, relations) interactions by representation enhancement, type dependencies scoring, or global decoding. We find that the previous models generally consider binary type dependency scoring of a pair of instances, and leverage local search such as beam search to approximate global solutions. To better integrate cross-instance interactions, in this work, we introduce a joint IE framework (CRFIE) that formulates joint IE as a high-order Conditional Random Field. Specifically, we design binary factors and ternary factors to directly model interactions between not only a pair of instances but also triplets. Then, these factors are utilized to jointly predict labels of all instances. To address the intractability problem of exact high-order inference, we incorporate a high-order neural decoder that is unfolded from a mean-field variational inference method, which achieves consistent learning and inference. The experimental results show that our approach achieves consistent improvements on three IE tasks compared with our baseline and prior work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 768,
      "original_url": "https://aclanthology.org/2023.acl-long.767/",
      "title": "Training Trajectories of Language Models Across Scales",
      "pdf_url": "https://aclanthology.org/2023.acl-long.767.pdf",
      "authors": [
        "Mengzhou Xia",
        "Mikel Artetxe",
        "Chunting Zhou",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Danqi Chen",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125M to 175B parameters—on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 769,
      "original_url": "https://aclanthology.org/2023.acl-long.768/",
      "title": "A Diverse Set of Freely Available Linguistic Resources for Turkish",
      "pdf_url": "https://aclanthology.org/2023.acl-long.768.pdf",
      "authors": [
        "Duygu Altinok"
      ],
      "abstract": "This study presents a diverse set of freely available linguistic resources for Turkish natural language processing, including corpora, pretrained models and education material. Although Turkish is spoken by a sizeable population of over 80 million people, Turkish linguistic resources for natural language processing remain scarce. In this study, we provide corpora to allow practitioners to build their own applications and pretrained models that would assist industry researchers in creating quick prototypes. The provided corpora include named entity recognition datasets of diverse genres, including Wikipedia articles and supplement products customer reviews. In addition, crawling e-commerce and movie reviews websites, we compiled several sentiment analysis datasets of different genres. Our linguistic resources for Turkish also include pretrained spaCy language models. To the best of our knowledge, our models are the first spaCy models trained for the Turkish language. Finally, we provide various types of education material, such as video tutorials and code examples, that can support the interested audience on practicing Turkish NLP. The advantages of our linguistic resources are three-fold: they are freely available, they are first of their kind, and they are easy to use in a broad range of implementations. Along with a thorough description of the resource creation process, we also explain the position of our resources in the Turkish NLP world."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 770,
      "original_url": "https://aclanthology.org/2023.acl-long.769/",
      "title": "Measuring Consistency in Text-based Financial Forecasting Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.769.pdf",
      "authors": [
        "Linyi Yang",
        "Yingpeng Ma",
        "Yue Zhang"
      ],
      "abstract": "Financial forecasting has been an important and active area of machine learning research, as even the most modest advantages in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models – their invariance under meaning-preserving alternations in input – is a crucial property for building user trust. Despite this, current methods for financial forecasting do not take consistency into consideration. To address this issue, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that current text-based methods are not suitable for robustly predicting market information."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 771,
      "original_url": "https://aclanthology.org/2023.acl-long.770/",
      "title": "Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.770.pdf",
      "authors": [
        "Nuno M. Guerreiro",
        "Pierre Colombo",
        "Pablo Piantanida",
        "André Martins"
      ],
      "abstract": "Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning. In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are detached from the source content, they exhibit encoder-decoder attention patterns that are statistically different from those of good quality translations. We frame this problem with an optimal transport formulation and propose a fully unsupervised, plug-in detector that can be used with any attention-based NMT model. Experimental results show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that employ external models trained on millions of samples for related tasks such as quality estimation and cross-lingual sentence similarity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 772,
      "original_url": "https://aclanthology.org/2023.acl-long.771/",
      "title": "RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank",
      "pdf_url": "https://aclanthology.org/2023.acl-long.771.pdf",
      "authors": [
        "Jiduan Liu",
        "Jiahao Liu",
        "Qifan Wang",
        "Jingang Wang",
        "Wei Wu",
        "Yunsen Xian",
        "Dongyan Zhao",
        "Kai Chen",
        "Rui Yan"
      ],
      "abstract": "Unsupervised sentence representation learning is one of the fundamental problems in natural language processing with various downstream applications. Recently, contrastive learning has been widely adopted which derives high-quality sentence representations by pulling similar semantics closer and pushing dissimilar ones away. However, these methods fail to capture the fine-grained ranking information among the sentences, where each sentence is only treated as either positive or negative. In many real-world scenarios, one needs to distinguish and rank the sentences based on their similarities to a query sentence, e.g., very relevant, moderate relevant, less relevant, irrelevant, etc. In this paper, we propose a novel approach, RankCSE, for unsupervised sentence representation learning, which incorporates ranking consistency and ranking distillation with contrastive learning into a unified framework. In particular, we learn semantically discriminative sentence representations by simultaneously ensuring ranking consistency between two representations with different dropout masks, and distilling listwise ranking knowledge from the teacher. An extensive set of experiments are conducted on both semantic textual similarity (STS) and transfer (TR) tasks. Experimental results demonstrate the superior performance of our approach over several state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 773,
      "original_url": "https://aclanthology.org/2023.acl-long.772/",
      "title": "Entailment as Robust Self-Learner",
      "pdf_url": "https://aclanthology.org/2023.acl-long.772.pdf",
      "authors": [
        "Jiaxin Ge",
        "Hongyin Luo",
        "Yoon Kim",
        "James Glass"
      ],
      "abstract": "Entailment has been recognized as an important metric for evaluating natural language understanding (NLU) models, and recent studies have found that entailment pretraining benefits weakly supervised fine-tuning. In this work, we design a prompting strategy that formulates a number of different NLU tasks as contextual entailment. This approach improves the zero-shot adaptation of pretrained entailment models. Secondly, we notice that self-training entailment-based models with unlabeled data can significantly improve the adaptation performance on downstream tasks. To achieve more stable improvement, we propose the Simple Pseudo-Label Editing (SimPLE) algorithm for better pseudo-labeling quality in self-training. We also found that both pretrained entailment-based models and the self-trained models are robust against adversarial evaluation data. Experiments on binary and multi-class classification tasks show that SimPLE leads to more robust self-training results, indicating that the self-trained entailment models are more efficient and trustworthy than large language models on language understanding tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 774,
      "original_url": "https://aclanthology.org/2023.acl-long.773/",
      "title": "ReCode: Robustness Evaluation of Code Generation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.773.pdf",
      "authors": [
        "Shiqi Wang",
        "Zheng Li",
        "Haifeng Qian",
        "Chenghao Yang",
        "Zijian Wang",
        "Mingyue Shang",
        "Varun Kumar",
        "Samson Tan",
        "Baishakhi Ray",
        "Parminder Bhatia",
        "Ramesh Nallapati",
        "Murali Krishna Ramanathan",
        "Dan Roth",
        "Bing Xiang"
      ],
      "abstract": "Code generation models have achieved impressive performance. However, they tend to be brittle as slight edits to a prompt could lead to very different generations; these robustness properties, critical for user experience when deployed in real-life applications, are not well understood. Most existing works on robustness in text or code tasks have focused on classification, while robustness in generation tasks is an uncharted area and to date there is no comprehensive benchmark for robustness in code generation. In this paper, we propose ReCode, a comprehensive robustness evaluation benchmark for code generation models. We customize over 30 transformations specifically for code on docstrings, function and variable names, code syntax, and code format. They are carefully designed to be natural in real-life coding practice, preserve the original semantic meaning, and thus provide multifaceted assessments of a model’s robustness performance. With human annotators, we verified that over 90% of the perturbed prompts do not alter the semantic meaning of the original prompt. In addition, we define robustness metrics for code generation models considering the worst-case behavior under each type of perturbation, taking advantage of the fact that executing the generated code can serve as objective evaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well as function completion tasks derived from them. Interesting observations include: better robustness for CodeGen over InCoder and GPT-J; models are most sensitive to syntax perturbations; more challenging robustness evaluation on MBPP over HumanEval."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 775,
      "original_url": "https://aclanthology.org/2023.acl-long.774/",
      "title": "EPIC: Multi-Perspective Annotation of a Corpus of Irony",
      "pdf_url": "https://aclanthology.org/2023.acl-long.774.pdf",
      "authors": [
        "Simona Frenda",
        "Alessandro Pedrani",
        "Valerio Basile",
        "Soda Marem Lo",
        "Alessandra Teresa Cignarella",
        "Raffaella Panizzon",
        "Cristina Marco",
        "Bianca Scarlini",
        "Viviana Patti",
        "Cristina Bosco",
        "Davide Bernardi"
      ],
      "abstract": "We present EPIC (English Perspectivist Irony Corpus), the first annotated corpus for irony analysis based on the principles of data perspectivism. The corpus contains short conversations from social media in five regional varieties of English, and it is annotated by contributors from five countries corresponding to those varieties. We analyse the resource along the perspectives induced by the diversity of the annotators, in terms of origin, age, and gender, and the relationship between these dimensions, irony, and the topics of conversation. We validate EPIC by creating perspective-aware models that encode the perspectives of annotators grouped according to their demographic characteristics. Firstly, the performance of perspectivist models confirms that different annotators induce very different models. Secondly, in the classification of ironic and non-ironic texts, perspectivist models prove to be generally more confident than the non-perspectivist ones. Furthermore, comparing the performance on a perspective-based test set with those achieved on a gold standard test set, we can observe how perspectivist models tend to detect more precisely the positive class, showing their ability to capture the different perceptions of irony. Thanks to these models, we are moreover able to show interesting insights about the variation in the perception of irony by the different groups of annotators, such as among different generations and nationalities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 776,
      "original_url": "https://aclanthology.org/2023.acl-long.775/",
      "title": "Dialogue Summarization with Static-Dynamic Structure Fusion Graph",
      "pdf_url": "https://aclanthology.org/2023.acl-long.775.pdf",
      "authors": [
        "Shen Gao",
        "Xin Cheng",
        "Mingzhe Li",
        "Xiuying Chen",
        "Jinpeng Li",
        "Dongyan Zhao",
        "Rui Yan"
      ],
      "abstract": "Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years. Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web applications such as email thread summarization and meeting minutes draft. Dialogue summarization is a challenging task in that dialogue has dynamic interaction nature and presumably inconsistent information flow among various speakers. Many researchers address this task by modeling dialogue with pre-computed static graph structure using external linguistic toolkits. However, such methods heavily depend on the reliability of external tools and the static graph construction is disjoint with the graph representation learning phase, which makes the graph can’t be dynamically adapted for the downstream summarization task. In this paper, we propose a Static-Dynamic graph-based Dialogue Summarization model (SDDS), which fuses prior knowledge from human expertise and adaptively learns the graph structure in an end-to-end learning fashion. To verify the effectiveness of SDDS, we conduct experiments on three benchmark datasets (SAMSum, MediaSum, and DialogSum) and the results verify the superiority of SDDS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 777,
      "original_url": "https://aclanthology.org/2023.acl-long.776/",
      "title": "Large-Scale Correlation Analysis of Automated Metrics for Topic Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.776.pdf",
      "authors": [
        "Jia Peng Lim",
        "Hady Lauw"
      ],
      "abstract": "Automated coherence metrics constitute an important and popular way to evaluate topic models. Previous works present a mixed picture of their presumed correlation with human judgement. In this paper, we conduct a large-scale correlation analysis of coherence metrics. We propose a novel sampling approach to mine topics for the purpose of metric evaluation, and conduct the analysis via three large corpora showing that certain automated coherence metrics are correlated. Moreover, we extend the analysis to measure topical differences between corpora. Lastly, we examine the reliability of human judgement by conducting an extensive user study, which is designed as an amalgamation of different proxy tasks to derive a finer insight into the human decision-making processes. Our findings reveal some correlation between automated coherence metrics and human judgement, especially for generic corpora."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 778,
      "original_url": "https://aclanthology.org/2023.acl-long.777/",
      "title": "U-CREAT: Unsupervised Case Retrieval using Events extrAcTion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.777.pdf",
      "authors": [
        "Abhinav Joshi",
        "Akshat Sharma",
        "Sai Kiran Tanikella",
        "Ashutosh Modi"
      ],
      "abstract": "The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 779,
      "original_url": "https://aclanthology.org/2023.acl-long.778/",
      "title": "ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.778.pdf",
      "authors": [
        "Omkar Joshi",
        "Priya Pitre",
        "Yashodhara Haribhakta"
      ],
      "abstract": "Argument Quality Detection is an emerging field in NLP which has seen significant recent development. However, existing datasets in this field suffer from a lack of quality, quantity and diversity of topics and arguments, specifically the presence of vague arguments that are not persuasive in nature. In this paper, we leverage a combined experience of 10+ years of Parliamentary Debating to create a dataset that covers significantly more topics and has a wide range of sources to capture more diversity of opinion. With 34,890 high-quality argument-analysis pairs (a term we introduce in this paper), this is also the largest dataset of its kind to our knowledge. In addition to this contribution, we introduce an innovative argument scoring system based on instance-level annotator reliability and propose a quantitative model of scoring the relevance of arguments to a range of topics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 780,
      "original_url": "https://aclanthology.org/2023.acl-long.779/",
      "title": "Reference Matters: Benchmarking Factual Error Correction for Dialogue Summarization with Fine-grained Evaluation Framework",
      "pdf_url": "https://aclanthology.org/2023.acl-long.779.pdf",
      "authors": [
        "Mingqi Gao",
        "Xiaojun Wan",
        "Jia Su",
        "Zhefeng Wang",
        "Baoxing Huai"
      ],
      "abstract": "Factuality is important to dialogue summarization. Factual error correction (FEC) of model-generated summaries is one way to improve factuality. Current FEC evaluation that relies on factuality metrics is not reliable and detailed enough. To address this problem, we are the first to manually annotate a FEC dataset for dialogue summarization containing 4000 items and propose FERRANTI, a fine-grained evaluation framework based on reference correction that automatically evaluates the performance of FEC models on different error categories. Using this evaluation framework, we conduct sufficient experiments with FEC approaches under a variety of settings and find the best training modes and significant differences in the performance of the existing approaches on different factual error categories."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 781,
      "original_url": "https://aclanthology.org/2023.acl-long.780/",
      "title": "Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker",
      "pdf_url": "https://aclanthology.org/2023.acl-long.780.pdf",
      "authors": [
        "Melanie Sclar",
        "Sachin Kumar",
        "Peter West",
        "Alane Suhr",
        "Yejin Choi",
        "Yulia Tsvetkov"
      ],
      "abstract": "Theory of Mind (ToM)—the ability to reason about the mental states of other people—is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity’s beliefs, their estimation of other entities’ beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks’ theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 782,
      "original_url": "https://aclanthology.org/2023.acl-long.781/",
      "title": "Don’t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.781.pdf",
      "authors": [
        "Ashim Gupta",
        "Carter Blum",
        "Temma Choji",
        "Yingjie Fei",
        "Shalin Shah",
        "Alakananda Vempala",
        "Vivek Srikumar"
      ],
      "abstract": "Can language models transform inputs to protect text classifiers against adversarial attacks? In this work, we present ATINTER, a model that intercepts and learns to rewrite adversarial inputs to make them non-adversarial for a downstream text classifier. Our experiments on four datasets and five attack mechanisms reveal that ATINTER is effective at providing better adversarial robustness than existing defense approaches, without compromising task accuracy. For example, on sentiment classification using the SST-2 dataset, our method improves the adversarial accuracy over the best existing defense approach by more than 4% with a smaller decrease in task accuracy (0.5 % vs 2.5%). Moreover, we show that ATINTER generalizes across multiple downstream tasks and classifiers without having to explicitly retrain it for those settings. For example, we find that when ATINTER is trained to remove adversarial perturbations for the sentiment classification task on the SST-2 dataset, it even transfers to a semantically different task of news classification (on AGNews) and improves the adversarial robustness by more than 10%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 783,
      "original_url": "https://aclanthology.org/2023.acl-long.782/",
      "title": "Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring",
      "pdf_url": "https://aclanthology.org/2023.acl-long.782.pdf",
      "authors": [
        "Cong Wang",
        "Zhiwei Jiang",
        "Yafeng Yin",
        "Zifeng Cheng",
        "Shiping Ge",
        "Qing Gu"
      ],
      "abstract": "Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the pseudo-groundtruth, and then train a neural AES model by learning from the aggregation of these quality signals. To aggregate these inconsistent quality signals into a unified supervision, we view the AES task as a ranking problem, and design a special Deep Pairwise Rank Aggregation (DPRA) loss for training. In the DPRA loss, we set a learnable confidence weight for each signal to address the conflicts among signals, and train the neural AES model in a pairwise way to disentangle the cascade effect among partial-order pairs. Experiments on eight prompts of ASPA dataset show that ULRA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both transductive and inductive settings. Further, our approach achieves comparable performance with many existing domain-adapted supervised models, showing the effectiveness of ULRA. The code is available at https://github.com/tenvence/ulra."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 784,
      "original_url": "https://aclanthology.org/2023.acl-long.783/",
      "title": "Mitigating Label Biases for In-context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.783.pdf",
      "authors": [
        "Yu Fei",
        "Yifan Hou",
        "Zeming Chen",
        "Antoine Bosselut"
      ],
      "abstract": "Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias the model’s predictions. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time). Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model’s label bias using random in-domain words from the task corpus. After controlling for this estimated bias when making predictions, our novel domain-context calibration significantly improves the ICL performance of GPT-J and GPT-3 on a wide range of tasks. The gain is substantial on tasks with large domain-label bias (up to 37% in Macro-F1). Furthermore, our results generalize to models with different scales, pretraining methods, and manually-designed task instructions, showing the prevalence of label biases in ICL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 785,
      "original_url": "https://aclanthology.org/2023.acl-long.784/",
      "title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.784.pdf",
      "authors": [
        "Chaitanya Malaviya",
        "Peter Shaw",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "abstract": "Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for “shorebirds that are not sandpipers” or “science-fiction films shot in England”. To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 786,
      "original_url": "https://aclanthology.org/2023.acl-long.785/",
      "title": "Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.785.pdf",
      "authors": [
        "Yujie Wang",
        "Hu Zhang",
        "Jiye Liang",
        "Ru Li"
      ],
      "abstract": "Recently, knowledge graphs (KGs) have won noteworthy success in commonsense question answering. Existing methods retrieve relevant subgraphs in the KGs through key entities and reason about the answer with language models (LMs) and graph neural networks. However, they ignore (i) optimizing the knowledge representation and structure of subgraphs and (ii) deeply fusing heterogeneous QA context with subgraphs. In this paper, we propose a dynamic heterogeneous-graph reasoning method with LMs and knowledge representation learning (DHLK), which constructs a heterogeneous knowledge graph (HKG) based on multiple knowledge sources and optimizes the structure and knowledge representation of the HKG using a two-stage pruning strategy and knowledge representation learning (KRL). It then performs joint reasoning by LMs and Relation Mask Self-Attention (RMSA). Specifically, DHLK filters key entities based on the dictionary vocabulary to achieve the first-stage pruning while incorporating the paraphrases in the dictionary into the subgraph to construct the HKG. Then, DHLK encodes and fuses the QA context and HKG using LM, and dynamically removes irrelevant KG entities based on the attention weights of LM for the second-stage pruning. Finally, DHLK introduces KRL to optimize the knowledge representation and perform answer reasoning on the HKG by RMSA.We evaluate DHLK at CommonsenseQA and OpenBookQA, and show its improvement on existing LM and LM+KG methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 787,
      "original_url": "https://aclanthology.org/2023.acl-long.786/",
      "title": "Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.786.pdf",
      "authors": [
        "Hao Li",
        "Viktor Schlegel",
        "Riza Batista-Navarro",
        "Goran Nenadic"
      ],
      "abstract": "Argument summarisation is a promising but currently under-explored field. Recent work has aimed to provide textual summaries in the form of concise and salient short texts, i.e., key points (KPs), in a task known as Key Point Analysis (KPA). One of the main challenges in KPA is finding high-quality key point candidates from dozens of arguments even in a small corpus. Furthermore, evaluating key points is crucial in ensuring that the automatically generated summaries are useful. Although automatic methods for evaluating summarisation have considerably advanced over the years, they mainly focus on sentence-level comparison, making it difficult to measure the quality of a summary (a set of KPs) as a whole. Aggravating this problem is the fact that human evaluation is costly and unreproducible. To address the above issues, we propose a two-step abstractive summarisation framework based on neural topic modelling with an iterative clustering procedure, to generate key points which are aligned with how humans identify key points. Our experiments show that our framework advances the state of the art in KPA, with performance improvement of up to 14 (absolute) percentage points, in terms of both ROUGE and our own proposed evaluation metrics. Furthermore, we evaluate the generated summaries using a novel set-based evaluation toolkit. Our quantitative analysis demonstrates the effectiveness of our proposed evaluation metrics in assessing the quality of generated KPs. Human evaluation further demonstrates the advantages of our approach and validates that our proposed evaluation metric is more consistent with human judgment than ROUGE scores."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 788,
      "original_url": "https://aclanthology.org/2023.acl-long.787/",
      "title": "Ambiguous Learning from Retrieval: Towards Zero-shot Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.787.pdf",
      "authors": [
        "Shan Wu",
        "Chunlei Xin",
        "Hongyu Lin",
        "Xianpei Han",
        "Cao Liu",
        "Jiansong Chen",
        "Fan Yang",
        "Guanglu Wan",
        "Le Sun"
      ],
      "abstract": "Current neural semantic parsers take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. Thus, minimizing the supervision effort is one of the key challenges in semantic parsing. In this paper, we propose the Retrieval as Ambiguous Supervision framework, in which we construct a retrieval system based on pretrained language models to collect high-coverage candidates. Assuming candidates always contain the correct ones, we convert zero-shot task into ambiguously supervised task. To improve the precision and coverage of such ambiguous supervision, we propose a confidence-driven self-training algorithm, in which a semantic parser is learned and exploited to disambiguate the candidates iteratively. Experimental results show that our approach significantly outperforms the state-of-the-art zero-shot semantic parsing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 789,
      "original_url": "https://aclanthology.org/2023.acl-long.788/",
      "title": "Explicit Syntactic Guidance for Neural Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.788.pdf",
      "authors": [
        "Yafu Li",
        "Leyang Cui",
        "Jianhao Yan",
        "Yongjing Yin",
        "Wei Bi",
        "Shuming Shi",
        "Yue Zhang"
      ],
      "abstract": "Most existing text generation models follow the sequence-to-sequence paradigm. Generative Grammar suggests that humans generate natural language texts by learning language grammar. We propose a syntax-guided generation schema, which generates the sequence guided by a constituency parse tree in a top-down direction. The decoding process can be decomposed into two parts: (1) predicting the infilling texts for each constituent in the lexicalized syntax context given the source sentence; (2) mapping and expanding each constituent to construct the next-level syntax context. Accordingly, we propose a structural beam search method to find possible syntax structures hierarchically. Experiments on paraphrase generation and machine translation show that the proposed method outperforms autoregressive baselines, while also demonstrating effectiveness in terms of interpretability, controllability, and diversity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 790,
      "original_url": "https://aclanthology.org/2023.acl-long.789/",
      "title": "What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric",
      "pdf_url": "https://aclanthology.org/2023.acl-long.789.pdf",
      "authors": [
        "Enrico Liscio",
        "Oscar Araque",
        "Lorenzo Gatti",
        "Ionut Constantinescu",
        "Catholijn Jonker",
        "Kyriaki Kalimeri",
        "Pradeep Kumar Murukannaiah"
      ],
      "abstract": "Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier’s representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and differences across moral concepts and domains. We apply Tomea on moral narratives in thirty-five thousand tweets from seven domains. We extensively evaluate the method via a crowd study, a series of cross-domain moral classification comparisons, and a qualitative analysis of cross-domain moral expression."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 791,
      "original_url": "https://aclanthology.org/2023.acl-long.790/",
      "title": "Graph-based Relation Mining for Context-free Out-of-vocabulary Word Embedding Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.790.pdf",
      "authors": [
        "Ziran Liang",
        "Yuyin Lu",
        "HeGang Chen",
        "Yanghui Rao"
      ],
      "abstract": "The out-of-vocabulary (OOV) words are difficult to represent while critical to the performance of embedding-based downstream models. Prior OOV word embedding learning methods failed to model complex word formation well. In this paper, we propose a novel graph-based relation mining method, namely GRM, for OOV word embedding learning. We first build a Word Relationship Graph (WRG) based on word formation and associate OOV words with their semantically relevant words, which can mine the relational information inside word structures. Subsequently, our GRM can infer high-quality embeddings for OOV words through passing and aggregating semantic attributes and relational information in the WRG, regardless of contextual richness. Extensive experiments demonstrate that our model significantly outperforms state-of-the-art baselines on both intrinsic and downstream tasks when faced with OOV words."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 792,
      "original_url": "https://aclanthology.org/2023.acl-long.791/",
      "title": "Multimodal Persona Based Generation of Comic Dialogs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.791.pdf",
      "authors": [
        "Harsh Agrawal",
        "Aditya Mishra",
        "Manish Gupta",
        "Mausam"
      ],
      "abstract": "We focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous works like DialoGPT, PersonaGPT and other dialog generation models encode two-party dialogues and do not account for the visual information. To the best of our knowledge we are the first to propose the paradigm of multimodal persona based dialogue generation. We contribute a novel dataset, ComSet, consisting of 54K strips, harvested from 13 popular comics available online. Further, we propose a multimodal persona-based architecture, MPDialog, to generate dialogues for the next panel in the strip which decreases the perplexity score by ~10 points over strong dialogue generation baseline models. We demonstrate that there is still ample opportunity for improvement, highlighting the importance of building stronger dialogue systems that are able to generate persona-consistent dialogues and understand the context through various modalities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 793,
      "original_url": "https://aclanthology.org/2023.acl-long.792/",
      "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.792.pdf",
      "authors": [
        "Dongfu Jiang",
        "Xiang Ren",
        "Bill Yuchen Lin"
      ],
      "abstract": "We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 794,
      "original_url": "https://aclanthology.org/2023.acl-long.793/",
      "title": "Seen to Unseen: Exploring Compositional Generalization of Multi-Attribute Controllable Dialogue Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.793.pdf",
      "authors": [
        "Weihao Zeng",
        "Lulu Zhao",
        "Keqing He",
        "Ruotong Geng",
        "Jingang Wang",
        "Wei Wu",
        "Weiran Xu"
      ],
      "abstract": "Existing controllable dialogue generation work focuses on the single-attribute control and lacks generalization capability to out-of-distribution multiple attribute combinations. In this paper, we explore the compositional generalization for multi-attribute controllable dialogue generation where a model can learn from seen attribute values and generalize to unseen combinations. We propose a prompt-based disentangled controllable dialogue generation model, DCG. It learns attribute concept composition by generating attribute-oriented prompt vectors and uses a disentanglement loss to disentangle different attributes for better generalization. Besides, we design a unified reference-free evaluation framework for multiple attributes with different levels of granularities. Experiment results on two benchmarks prove the effectiveness of our method and the evaluation metric."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 795,
      "original_url": "https://aclanthology.org/2023.acl-long.794/",
      "title": "Generating Structured Pseudo Labels for Noise-resistant Zero-shot Video Sentence Localization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.794.pdf",
      "authors": [
        "Minghang Zheng",
        "Shaogang Gong",
        "Hailin Jin",
        "Yuxin Peng",
        "Yang Liu"
      ],
      "abstract": "Video sentence localization aims to locate moments in an unstructured video according to a given natural language query. A main challenge is the expensive annotation costs and the annotation bias. In this work, we study video sentence localization in a zero-shot setting, which learns with only video data without any annotation. Existing zero-shot pipelines usually generate event proposals and then generate a pseudo query for each event proposal. However, their event proposals are obtained via visual feature clustering, which is query-independent and inaccurate; and the pseudo-queries are short or less interpretable. Moreover, existing approaches ignores the risk of pseudo-label noise when leveraging them in training. To address the above problems, we propose a Structure-based Pseudo Label generation (SPL), which first generate free-form interpretable pseudo queries before constructing query-dependent event proposals by modeling the event temporal structure. To mitigate the effect of pseudo-label noise, we propose a noise-resistant iterative method that repeatedly re-weight the training sample based on noise estimation to train a grounding model and correct pseudo labels. Experiments on the ActivityNet Captions and Charades-STA datasets demonstrate the advantages of our approach. Code can be found at https://github.com/minghangz/SPL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 796,
      "original_url": "https://aclanthology.org/2023.acl-long.795/",
      "title": "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-long.795.pdf",
      "authors": [
        "Ananya Sai B",
        "Tanay Dixit",
        "Vignesh Nagarajan",
        "Anoop Kunchukuttan",
        "Pratyush Kumar",
        "Mitesh M. Khapra",
        "Raj Dabre"
      ],
      "abstract": "The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from them, and to date, there are no such systematic studies focused solely on English to Indian language MT. This paper fills this gap through a Multidimensional Quality Metric (MQM) dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems. We evaluate 16 metrics and show that, pre-trained metrics like COMET have the highest correlations with annotator scores as opposed to n-gram metrics like BLEU. We further leverage our MQM annotations to develop an Indic-COMET metric and show that it outperforms COMET counterparts in both human scores correlations and robustness scores in Indian languages. Additionally, we show that the Indic-COMET can outperform COMET on some unseen Indian languages. We hope that our dataset and analysis will facilitate further research in Indic MT evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 797,
      "original_url": "https://aclanthology.org/2023.acl-long.796/",
      "title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.796.pdf",
      "authors": [
        "Dawei Zhu",
        "Xiaoyu Shen",
        "Marius Mosbach",
        "Andreas Stephan",
        "Dietrich Klakow"
      ],
      "abstract": "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 798,
      "original_url": "https://aclanthology.org/2023.acl-long.797/",
      "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
      "pdf_url": "https://aclanthology.org/2023.acl-long.797.pdf",
      "authors": [
        "Yingji Li",
        "Mengnan Du",
        "Xin Wang",
        "Ying Wang"
      ],
      "abstract": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs’ encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs’ parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 799,
      "original_url": "https://aclanthology.org/2023.acl-long.798/",
      "title": "Towards Understanding Omission in Dialogue Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.798.pdf",
      "authors": [
        "Yicheng Zou",
        "Kaitao Song",
        "Xu Tan",
        "Zhongkai Fu",
        "Qi Zhang",
        "Dongsheng Li",
        "Tao Gui"
      ],
      "abstract": "Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving summarization quality. Moreover, analyzing and detecting omission relies on summarization datasets with omission labels (i.e., which dialogue utterances are omitted in the summarization), which are not available in the current literature. In this paper, we propose the OLDS dataset, which provides high-quality omission labels for dialogue summarization. By analyzing this dataset, we find that a large improvement in summarization quality can be achieved by providing ground-truth omission labels for the summarization model to recover omission information, which demonstrates the importance of omission detection for omission mitigation in dialogue summarization. Therefore, we formulate an omission detection task and demonstrate our proposed dataset can support the training and evaluation of this task well. We also call for research action on omission detection based on our proposed datasets. Our dataset and codes are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 800,
      "original_url": "https://aclanthology.org/2023.acl-long.799/",
      "title": "Python Code Generation by Asking Clarification Questions",
      "pdf_url": "https://aclanthology.org/2023.acl-long.799.pdf",
      "authors": [
        "Haau-Sing (Xiaocheng) Li",
        "Mohsen Mesgar",
        "André Martins",
        "Iryna Gurevych"
      ],
      "abstract": "Code generation from text requires understanding the user’s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 801,
      "original_url": "https://aclanthology.org/2023.acl-long.800/",
      "title": "A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports",
      "pdf_url": "https://aclanthology.org/2023.acl-long.800.pdf",
      "authors": [
        "Jia-Huei Ju",
        "Yu-Shiang Huang",
        "Cheng-Wei Lin",
        "Che Lin",
        "Chuan-Ju Wang"
      ],
      "abstract": "In this paper, we address the challenge of discovering financial signals in narrative financial reports. As these documents are often lengthy and tend to blend routine information with new information, it is challenging for professionals to discern critical financial signals. To this end, we leverage the inherent nature of the year-to-year structure of reports to define a novel signal-highlighting task; more importantly, we propose a compare-and-contrast multistage pipeline that recognizes different relationships between the reports and locates relevant rationales for these relationships. We also create and publicly release a human-annotated dataset for our task. Our experiments on the dataset validate the effectiveness of our pipeline, and we provide detailed analyses and ablation studies to support our findings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 802,
      "original_url": "https://aclanthology.org/2023.acl-long.801/",
      "title": "Improving the robustness of NLI models with minimax training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.801.pdf",
      "authors": [
        "Michalis Korakakis",
        "Andreas Vlachos"
      ],
      "abstract": "Natural language inference (NLI) models are susceptible to learning shortcuts, i.e. decision rules that spuriously correlate with the label. As a result, they achieve high in-distribution performance, but fail to generalize to out-of-distribution samples where such correlations do not hold. In this paper, we present a training method to reduce the reliance of NLI models on shortcuts and improve their out-of-distribution performance without assuming prior knowledge of the shortcuts being targeted. To this end, we propose a minimax objective between a learner model being trained for the NLI task, and an auxiliary model aiming to maximize the learner’s loss by up-weighting examples from regions of the input space where the learner incurs high losses. This process incentivizes the learner to focus on under-represented “hard” examples with patterns that contradict the shortcuts learned from the prevailing “easy” examples. Experimental results on three NLI datasets demonstrate that our method consistently outperforms other robustness enhancing techniques on out-of-distribution adversarial test sets, while maintaining high in-distribution accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 803,
      "original_url": "https://aclanthology.org/2023.acl-long.802/",
      "title": "USSA: A Unified Table Filling Scheme for Structured Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-long.802.pdf",
      "authors": [
        "Zepeng Zhai",
        "Hao Chen",
        "Ruifan Li",
        "Xiaojie Wang"
      ],
      "abstract": "Most previous studies on Structured Sentiment Analysis (SSA) have cast it as a problem of bi-lexical dependency parsing, which cannot address issues of overlap and discontinuity simultaneously. In this paper, we propose a niche-targeting and effective solution. Our approach involves creating a novel bi-lexical dependency parsing graph, which is then converted to a unified 2D table-filling scheme, namely USSA. The proposed scheme resolves the kernel bottleneck of previous SSA methods by utilizing 13 different types of relations. In addition, to closely collaborate with the USSA scheme, we have developed a model that includes a proposed bi-axial attention module to effectively capture the correlations among relations in the rows and columns of the table. Extensive experimental results on benchmark datasets demonstrate the effectiveness and robustness of our proposed framework, outperforming state-of-the-art methods consistently."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 804,
      "original_url": "https://aclanthology.org/2023.acl-long.803/",
      "title": "PAD-Net: An Efficient Framework for Dynamic Networks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.803.pdf",
      "authors": [
        "Shwai He",
        "Liang Ding",
        "Daize Dong",
        "Boan Liu",
        "Fuqiang Yu",
        "Dacheng Tao"
      ],
      "abstract": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model’s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by +0.7% top-1 acc with only 30% dynamic parameters for ResNet-50 and +1.9% average score in language understanding with only 50% dynamic parameters for BERT. Code will be released at: https://github.com/Shwai-He/PAD-Net."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 805,
      "original_url": "https://aclanthology.org/2023.acl-long.804/",
      "title": "Resolving Ambiguities in Text-to-Image Generative Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.804.pdf",
      "authors": [
        "Ninareh Mehrabi",
        "Palash Goyal",
        "Apurv Verma",
        "Jwala Dhamala",
        "Varun Kumar",
        "Qian Hu",
        "Kai-Wei Chang",
        "Richard Zemel",
        "Aram Galstyan",
        "Rahul Gupta"
      ],
      "abstract": "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 806,
      "original_url": "https://aclanthology.org/2023.acl-long.805/",
      "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.805.pdf",
      "authors": [
        "Joel Jang",
        "Dongkeun Yoon",
        "Sohee Yang",
        "Sungmin Cha",
        "Moontae Lee",
        "Lajanugen Logeswaran",
        "Minjoon Seo"
      ],
      "abstract": "Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 807,
      "original_url": "https://aclanthology.org/2023.acl-long.806/",
      "title": "Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor",
      "pdf_url": "https://aclanthology.org/2023.acl-long.806.pdf",
      "authors": [
        "Or Honovich",
        "Thomas Scialom",
        "Omer Levy",
        "Timo Schick"
      ],
      "abstract": "Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 808,
      "original_url": "https://aclanthology.org/2023.acl-long.807/",
      "title": "To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.807.pdf",
      "authors": [
        "Dheeru Dua",
        "Emma Strubell",
        "Sameer Singh",
        "Pat Verga"
      ],
      "abstract": "Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only under conservative shifts in data distribution and typically focus on a single component (i.e., retriever or reader) rather than an end-to-end system. This work proposes a more realistic end-to-end domain shift evaluation setting covering five diverse domains. We not only find that end-to-end models fail to generalize but that high retrieval scores often still yield poor answer prediction accuracy. To address these failures, we investigate several interventions, in the form of data augmentations, for improving model adaption and use our evaluation set to elucidate the relationship between the efficacy of an intervention scheme and the particular type of dataset shifts we consider. We propose a generalizability test that estimates the type of shift in a target dataset without training a model in the target domain and that the type of shift is predictive of which data augmentation schemes will be effective for domain adaption. Overall, we find that these interventions increase end-to-end performance by up to ~24 points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 809,
      "original_url": "https://aclanthology.org/2023.acl-long.808/",
      "title": "A Survey for Efficient Open Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.808.pdf",
      "authors": [
        "Qin Zhang",
        "Shangsi Chen",
        "Dongkuan Xu",
        "Qingqing Cao",
        "Xiaojun Chen",
        "Trevor Cohn",
        "Meng Fang"
      ],
      "abstract": "Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and have achieved promising progress. However, higher accuracy often requires more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we will survey recent advancements in the efficiency of ODQA models and conclude core techniques for achieving efficiency. Additionally, we will provide a quantitative analysis of memory cost, query speed, accuracy, and overall performance comparison. Our goal is to keep scholars informed of the latest advancements and open challenges in ODQA efficiency research and contribute to the further development of ODQA efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 810,
      "original_url": "https://aclanthology.org/2023.acl-long.809/",
      "title": "Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities",
      "pdf_url": "https://aclanthology.org/2023.acl-long.809.pdf",
      "authors": [
        "Sina Ahmadi",
        "Antonios Anastasopoulos"
      ],
      "abstract": "The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages. This, however, comes with certain challenges in script normalization, particularly where the speakers of a language in a bilingual community rely on another script or orthography to write their native language. This paper addresses the problem of script normalization for several such languages that are mainly written in a Perso-Arabic script. Using synthetic data with various levels of noise and a transformer-based model, we demonstrate that the problem can be effectively remediated. We conduct a small-scale evaluation of real data as well. Our experiments indicate that script normalization is also beneficial to improve the performance of downstream tasks such as machine translation and language identification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 811,
      "original_url": "https://aclanthology.org/2023.acl-long.810/",
      "title": "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.810.pdf",
      "authors": [
        "Matthias Lindemann",
        "Alexander Koller",
        "Ivan Titov"
      ],
      "abstract": "Seq2seq models have been shown to struggle with compositional generalization in semantic parsing, i.e. generalizing to unseen compositions of phenomena that the model handles correctly in isolation. We phrase semantic parsing as a two-step process: we first tag each input token with a multiset of output tokens. Then we arrange the tokens into an output sequence using a new way of parameterizing and predicting permutations. We formulate predicting a permutation as solving a regularized linear program and we backpropagate through the solver. In contrast to prior work, our approach does not place a priori restrictions on possible permutations, making it very expressive. Our model outperforms pretrained seq2seq models and prior work on realistic semantic parsing tasks that require generalization to longer examples. We also outperform non-tree-based models on structural generalization on the COGS benchmark. For the first time, we show that a model without an inductive bias provided by trees achieves high accuracy on generalization to deeper recursion depth."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 812,
      "original_url": "https://aclanthology.org/2023.acl-long.811/",
      "title": "ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.811.pdf",
      "authors": [
        "Xiao Xu",
        "Bei Li",
        "Chenfei Wu",
        "Shao-Yen Tseng",
        "Anahita Bhiwandiwalla",
        "Shachar Rosenman",
        "Vasudev Lal",
        "Wanxiang Che",
        "Nan Duan"
      ],
      "abstract": "Two-Tower Vision-Language (VL) models have shown promising improvements on various downstream VL tasks. Although the most advanced work improves performance by building bridges between encoders, it suffers from ineffective layer-by-layer utilization of uni-modal representations and cannot flexibly exploit different levels of uni-modal semantic knowledge. In this work, we propose ManagerTower, a novel VL model architecture that gathers and combines the insights of pre-trained uni-modal experts at different levels. The managers introduced in each cross-modal layer can adaptively aggregate uni-modal semantic knowledge to facilitate more comprehensive cross-modal alignment and fusion. ManagerTower outperforms previous strong baselines both with and without Vision-Language Pre-training (VLP). With only 4M VLP data, ManagerTower achieves superior performances on various downstream VL tasks, especially 79.15% accuracy on VQAv2 Test-Std, 86.56% IR@1 and 95.64% TR@1 on Flickr30K. Code and checkpoints are available at https://github.com/LooperXX/ManagerTower."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 813,
      "original_url": "https://aclanthology.org/2023.acl-long.812/",
      "title": "Finding the Pillars of Strength for Multi-Head Attention",
      "pdf_url": "https://aclanthology.org/2023.acl-long.812.pdf",
      "authors": [
        "Jinjie Ni",
        "Rui Mao",
        "Zonglin Yang",
        "Han Lei",
        "Erik Cambria"
      ],
      "abstract": "Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Extensive experiments are consistent with our hypothesis. Moreover, our method achieves significant performance gains on three well-established tasks while considerably compressing parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 814,
      "original_url": "https://aclanthology.org/2023.acl-long.813/",
      "title": "Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.813.pdf",
      "authors": [
        "Yandan Zheng",
        "Anran Hao",
        "Anh Tuan Luu"
      ],
      "abstract": "Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framework outperforms the state-of-the-art semi-supervised approaches on NER and RE tasks. We show that the joint semi-supervised learning of the two tasks benefits from their codependency and validates the importance of utilizing the shared information between unlabeled data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 815,
      "original_url": "https://aclanthology.org/2023.acl-long.814/",
      "title": "Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.814.pdf",
      "authors": [
        "Jiajie Zhang",
        "Shulin Cao",
        "Tingjian Zhang",
        "Xin Lv",
        "Juanzi Li",
        "Lei Hou",
        "Jiaxin Shi",
        "Qi Tian"
      ],
      "abstract": "Explainable question answering (XQA) aims to answer a given question and provide an explanation why the answer is selected. Existing XQA methods focus on reasoning on a single knowledge source, e.g., structured knowledge bases, unstructured corpora, etc. However, integrating information from heterogeneous knowledge sources is essential to answer complex questions. In this paper, we propose to leverage question decomposing for heterogeneous knowledge integration, by breaking down a complex question into simpler ones, and selecting the appropriate knowledge source for each sub-question. To facilitate reasoning, we propose a novel two-stage XQA framework, Reasoning over Hierarchical Question Decomposition Tree (RoHT). First, we build the Hierarchical Question Decomposition Tree (HQDT) to understand the semantics of a complex question; then, we conduct probabilistic reasoning over HQDT from root to leaves recursively, to aggregate heterogeneous knowledge at different tree levels and search for a best solution considering the decomposing and answering probabilities. The experiments on complex QA datasets KQA Pro and Musique show that our framework outperforms SOTA methods significantly, demonstrating the effectiveness of leveraging question decomposing for knowledge integration and our RoHT framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 816,
      "original_url": "https://aclanthology.org/2023.acl-long.815/",
      "title": "Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.815.pdf",
      "authors": [
        "Kung-Hsiang Huang",
        "Kathleen McKeown",
        "Preslav Nakov",
        "Yejin Choi",
        "Heng Ji"
      ],
      "abstract": "Despite recent advances in detecting fake news generated by neural models, their results are not readily applicable to effective detection of human-written disinformation. What limits the successful transfer between them is the sizable gap between machine-generated fake news and human-authored ones, including the notable differences in terms of style and underlying intent. With this in mind, we propose a novel framework for generating training examples that are informed by the known styles and strategies of human-authored propaganda. Specifically, we perform self-critical sequence training guided by natural language inference to ensure the validity of the generated articles, while also incorporating propaganda techniques, such as appeal to authority and loaded language. In particular, we create a new training dataset, PropaNews, with 2,256 examples, which we release for future use. Our experimental results show that fake news detectors trained on PropaNews are better at detecting human-written disinformation by 3.62–7.69% F1 score on two public datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 817,
      "original_url": "https://aclanthology.org/2023.acl-long.816/",
      "title": "A Length-Extrapolatable Transformer",
      "pdf_url": "https://aclanthology.org/2023.acl-long.816.pdf",
      "authors": [
        "Yutao Sun",
        "Li Dong",
        "Barun Patra",
        "Shuming Ma",
        "Shaohan Huang",
        "Alon Benhaim",
        "Vishrav Chaudhary",
        "Xia Song",
        "Furu Wei"
      ],
      "abstract": "Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at https://aka.ms/LeX-Transformer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 818,
      "original_url": "https://aclanthology.org/2023.acl-long.817/",
      "title": "A Survey of Deep Learning for Mathematical Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.817.pdf",
      "authors": [
        "Pan Lu",
        "Liang Qiu",
        "Wenhao Yu",
        "Sean Welleck",
        "Kai-Wei Chang"
      ],
      "abstract": "Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 819,
      "original_url": "https://aclanthology.org/2023.acl-long.818/",
      "title": "A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training",
      "pdf_url": "https://aclanthology.org/2023.acl-long.818.pdf",
      "authors": [
        "Nitay Calderon",
        "Subhabrata Mukherjee",
        "Roi Reichart",
        "Amir Kantor"
      ],
      "abstract": "Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target (PT) augmentation methods, substantially extending prior work on sequence-level KD. We propose the Joint-Teaching method, which applies word-level KD to multiple PTs generated by both the teacher and the student. Finally, we validate our findings in an extreme setup with no labeled examples using GPT-4 as the teacher. Our study provides practical model design observations and demonstrates the effectiveness of PT training for task-specific KD in NLG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 820,
      "original_url": "https://aclanthology.org/2023.acl-long.819/",
      "title": "Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.819.pdf",
      "authors": [
        "Chaoya Jiang",
        "Wei Ye",
        "Haiyang Xu",
        "Songfang Huang",
        "Fei Huang",
        "Shikun Zhang"
      ],
      "abstract": "In this paper, we reconsider the problem of (partial) false negative samples from the Mutual Information (MI) Maximization perspective, the traditional contrastive loss (like InfoNCE loss) will equally push away the anchor of all positive samples and negative samples regardless of their possible semantic similarities. We theoretically show that InfoNCE loss will not only maximize the MI between the anchor and positive samples but minimize the MI between the anchor and false negative samples even though they share similar semantic which could provide a possible theoretical explanation for the observation of the existence of false negative samples in the cross-modal contrastive learning will decrease the downstream task performance of VLP models. Above analysis motivate us to propose the VLP model with a novel Semantic Awared Contrastive Learning framework named SACL where different negative samples are assigned with different contrastive weights according to the semantic similarity between them and the anchor."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 821,
      "original_url": "https://aclanthology.org/2023.acl-long.820/",
      "title": "Tell2Design: A Dataset for Language-Guided Floor Plan Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.820.pdf",
      "authors": [
        "Sicong Leng",
        "Yang Zhou",
        "Mohammed Haroon Dupty",
        "Wee Sun Lee",
        "Sam Joyce",
        "Wei Lu"
      ],
      "abstract": "We consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we introduce a novel dataset, Tell2Design (T2D), which contains more than 80k floor plan designs associated with natural language instructions. Second, we propose a Sequence-to-Sequence model that can serve as a strong baseline for future research. Third, we benchmark this task with several text-conditional image generation models. We conclude by conducting human evaluations on the generated samples and providing an analysis of human performance. We hope our contributions will propel the research on language-guided design generation forward."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 822,
      "original_url": "https://aclanthology.org/2023.acl-long.821/",
      "title": "Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.821.pdf",
      "authors": [
        "Bingsheng Yao",
        "Prithviraj Sen",
        "Lucian Popa",
        "James Hendler",
        "Dakuo Wang"
      ],
      "abstract": "Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation’s quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models’ performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 823,
      "original_url": "https://aclanthology.org/2023.acl-long.822/",
      "title": "Rethinking Annotation: Can Language Learners Contribute?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.822.pdf",
      "authors": [
        "Haneul Yoo",
        "Rifki Afina Putri",
        "Changyoon Lee",
        "Youngin Lee",
        "So-Yeon Ahn",
        "Dongyeop Kang",
        "Alice Oh"
      ],
      "abstract": "Researchers have traditionally recruited native speakers to provide annotations for the widely used benchmark datasets. But there are languages for which recruiting native speakers is difficult, and it would help to get learners of those languages to annotate the data. In this paper, we investigate whether language learners can contribute annotations to the benchmark datasets. In a carefully controlled annotation experiment, we recruit 36 language learners, provide two types of additional resources (dictionaries and machine-translated sentences), and perform mini-tests to measure their language proficiency. We target three languages, English, Korean, and Indonesian, and four NLP tasks, sentiment analysis, natural language inference, named entity recognition, and machine reading comprehension. We find that language learners, especially those with intermediate or advanced language proficiency, are able to provide fairly accurate labels with the help of additional resources. Moreover, we show that data annotation improves learners’ language proficiency in terms of vocabulary and grammar. The implication of our findings is that broadening the annotation task to include language learners can open up the opportunity to build benchmark datasets for languages for which it is difficult to recruit native speakers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 824,
      "original_url": "https://aclanthology.org/2023.acl-long.823/",
      "title": "Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-long.823.pdf",
      "authors": [
        "Shengqiong Wu",
        "Hao Fei",
        "Yixin Cao",
        "Lidong Bing",
        "Tat-Seng Chua"
      ],
      "abstract": "Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and external-information exploiting. First, we represent the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG). Based on CMG, we perform structure refinement with the guidance of the graph information bottleneck principle, actively denoising the less-informative features. Next, we perform topic modeling over the input image and text, incorporating latent multimodal topic features to enrich the contexts. On the benchmark MRE dataset, our system outperforms the current best model significantly. With further in-depth analyses, we reveal the great potential of our method for the MRE task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 825,
      "original_url": "https://aclanthology.org/2023.acl-long.824/",
      "title": "MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.824.pdf",
      "authors": [
        "Tao Shi",
        "Shao-Lun Huang"
      ],
      "abstract": "Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping relationships across different modalities. Furthermore, existing state-of-the-art ERC models have difficulty classifying minority and semantically similar emotion categories. To address these challenges, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross-attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion categories on both datasets, the improvements in minority and semantically similar emotions are especially significant."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 826,
      "original_url": "https://aclanthology.org/2023.acl-long.825/",
      "title": "Learning Language-Specific Layers for Multilingual Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.825.pdf",
      "authors": [
        "Telmo Pires",
        "Robin Schmidt",
        "Yi-Hsiu Liao",
        "Stephan Peitz"
      ],
      "abstract": "Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English).On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower. In this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and 1.9 chrF (2.2 spBLEU) on a shared decoder one."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 827,
      "original_url": "https://aclanthology.org/2023.acl-long.826/",
      "title": "Personality Understanding of Fictional Characters during Book Reading",
      "pdf_url": "https://aclanthology.org/2023.acl-long.826.pdf",
      "authors": [
        "Mo Yu",
        "Jiangnan Li",
        "Shunyu Yao",
        "Wenjie Pang",
        "Xiaochen Zhou",
        "Zhou Xiao",
        "Fandong Meng",
        "Jie Zhou"
      ],
      "abstract": "Comprehending characters’ personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 828,
      "original_url": "https://aclanthology.org/2023.acl-long.827/",
      "title": "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing",
      "pdf_url": "https://aclanthology.org/2023.acl-long.827.pdf",
      "authors": [
        "Xuekai Zhu",
        "Jian Guan",
        "Minlie Huang",
        "Juan Liu"
      ],
      "abstract": "Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 829,
      "original_url": "https://aclanthology.org/2023.acl-long.828/",
      "title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.828.pdf",
      "authors": [
        "Qingyu Tan",
        "Hwee Tou Ng",
        "Lidong Bing"
      ],
      "abstract": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 830,
      "original_url": "https://aclanthology.org/2023.acl-long.829/",
      "title": "Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings",
      "pdf_url": "https://aclanthology.org/2023.acl-long.829.pdf",
      "authors": [
        "Daniel Rotem",
        "Michael Hassid",
        "Jonathan Mamou",
        "Roy Schwartz"
      ],
      "abstract": "Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers. We compare SWEET’s speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 831,
      "original_url": "https://aclanthology.org/2023.acl-long.830/",
      "title": "Large Language Models Are Reasoning Teachers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.830.pdf",
      "authors": [
        "Namgyu Ho",
        "Laura Schmid",
        "Se-Young Yun"
      ],
      "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 832,
      "original_url": "https://aclanthology.org/2023.acl-long.831/",
      "title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.831.pdf",
      "authors": [
        "Wenting Zhao",
        "Justin Chiu",
        "Claire Cardie",
        "Alexander Rush"
      ],
      "abstract": "Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 833,
      "original_url": "https://aclanthology.org/2023.acl-long.832/",
      "title": "PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.832.pdf",
      "authors": [
        "Yau-Shian Wang",
        "Ta-Chung Chi",
        "Ruohong Zhang",
        "Yiming Yang"
      ],
      "abstract": "We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 834,
      "original_url": "https://aclanthology.org/2023.acl-long.833/",
      "title": "Visually-augmented pretrained language models for NLP tasks without images",
      "pdf_url": "https://aclanthology.org/2023.acl-long.833.pdf",
      "authors": [
        "Hangyu Guo",
        "Kun Zhou",
        "Wayne Xin Zhao",
        "Qinyu Zhang",
        "Ji-Rong Wen"
      ],
      "abstract": "Although pre-trained language models (PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel **V**isually-**A**ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, **W**ithout using any retrieved or generated **I**mages, namely **VAWI**. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at https://github.com/RUCAIBox/VAWI."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 835,
      "original_url": "https://aclanthology.org/2023.acl-long.834/",
      "title": "Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.834.pdf",
      "authors": [
        "Armineh Nourbakhsh",
        "Sameena Shah",
        "Carolyn Rosé"
      ],
      "abstract": "In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 836,
      "original_url": "https://aclanthology.org/2023.acl-long.835/",
      "title": "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.835.pdf",
      "authors": [
        "Lining Zhang",
        "Simon Mille",
        "Yufang Hou",
        "Daniel Deutsch",
        "Elizabeth Clark",
        "Yixin Liu",
        "Saad Mahamood",
        "Sebastian Gehrmann",
        "Miruna Clinciu",
        "Khyathi Raghavi Chandu",
        "João Sedoc"
      ],
      "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 837,
      "original_url": "https://aclanthology.org/2023.acl-long.836/",
      "title": "TAVT: Towards Transferable Audio-Visual Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.836.pdf",
      "authors": [
        "Wang Lin",
        "Tao Jin",
        "Wenwen Pan",
        "Linjun Li",
        "Xize Cheng",
        "Ye Wang",
        "Zhou Zhao"
      ],
      "abstract": "Audio-visual text generation aims to understand multi-modality contents and translate them into texts. Although various transfer learning techniques of text generation have been proposed, they focused on uni-modal analysis (e.g. text-to-text, visual-to-text) and lack consideration of multi-modal content and cross-modal relation. Motivated by the fact that humans can recognize the timbre of the same low-level concepts (e.g., footstep, rainfall, and laughing), even in different visual conditions, we aim to mitigate the domain discrepancies by audio-visual correlation. In this paper, we propose a novel Transferable Audio-Visual Text Generation framework, named TAVT, which consists of two key components: Audio-Visual Meta-Mapper (AVMM) and Dual Counterfactual Contrastive Learning (DCCL). (1) AVMM first introduces a universal auditory semantic space and drifts the domain-invariant low-level concepts into visual prefixes. Then the reconstruct-based learning encourages the AVMM to learn “which pixels belong to the same sound” and achieve audio-enhanced visual prefix. The well-trained AVMM can be further applied to uni-modal setting. (2) Furthermore, DCCL leverages the destructive counterfactual transformations to provide cross-modal constraints for AVMM from the perspective of feature distribution and text generation. (3) The experimental results show that TAVT outperforms the state-of-the-art methods across multiple domains (cross-datasets, cross-categories) and various modal settings (uni-modal, multi-modal)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 838,
      "original_url": "https://aclanthology.org/2023.acl-long.837/",
      "title": "MeetingQA: Extractive Question-Answering on Meeting Transcripts",
      "pdf_url": "https://aclanthology.org/2023.acl-long.837.pdf",
      "authors": [
        "Archiki Prasad",
        "Trung Bui",
        "Seunghyun Yoon",
        "Hanieh Deilamsalehy",
        "Franck Dernoncourt",
        "Mohit Bansal"
      ],
      "abstract": "With the ubiquitous use of online meeting platforms and robust automatic speech recognition systems, meeting transcripts have emerged as a promising domain for natural language tasks. Most recent works on meeting transcripts primarily focus on summarization and extraction of action items. However, meeting discussions also have a useful question-answering (QA) component, crucial to understanding the discourse or meeting content, and can be used to build interactive interfaces on top of long transcripts. Hence, in this work, we leverage this inherent QA component of meeting discussions and introduce MeetingQA, an extractive QA dataset comprising of questions asked by meeting participants and corresponding responses. As a result, questions can be open-ended and actively seek discussions, while the answers can be multi-span and distributed across multiple speakers. Our comprehensive empirical study of several robust baselines including long-context language models and recent instruction-tuned models reveals that models perform poorly on this task (F1 = 57.3) and severely lag behind human performance (F1 = 84.6), thus presenting a challenging new task for the community to improve upon."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 839,
      "original_url": "https://aclanthology.org/2023.acl-long.838/",
      "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.838.pdf",
      "authors": [
        "Jasivan Sivakumar",
        "Nafise Sadat Moosavi"
      ],
      "abstract": "While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect. The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 840,
      "original_url": "https://aclanthology.org/2023.acl-long.839/",
      "title": "Don’t Forget Your ABC’s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-long.839.pdf",
      "authors": [
        "Sarah E. Finch",
        "James D. Finch",
        "Jinho D. Choi"
      ],
      "abstract": "Despite tremendous advancements in dialogue systems, stable evaluation still requires human judgments producing notoriously high-variance metrics due to their inherent subjectivity. Moreover, methods and labels in dialogue evaluation are not fully standardized, especially for open-domain chats, with a lack of work to compare and assess the validity of those approaches. The use of inconsistent evaluation can misinform the performance of a dialogue system, which becomes a major hurdle to enhance it. Thus, a dimensional evaluation of chat-oriented open-domain dialogue systems that reliably measures several aspects of dialogue capabilities is desired. This paper presents a novel human evaluation method to estimate the rates of many{pasted macro ‘LN’} dialogue system behaviors. Our method is used to evaluate four state-of-the-art open-domain dialogue systems and compared with existing approaches. The analysis demonstrates that our behavior method is more suitable than alternative Likert-style or comparative approaches for dimensional evaluation of these systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 841,
      "original_url": "https://aclanthology.org/2023.acl-long.840/",
      "title": "Decoder Tuning: Efficient Language Understanding as Decoding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.840.pdf",
      "authors": [
        "Ganqu Cui",
        "Wentao Li",
        "Ning Ding",
        "Longtao Huang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sample. Empirically, we conduct extensive natural language understanding experiments and show that DecT significantly outperforms state-of-the-art algorithms with a 200x speed-up. Our code is available at https://github.com/thunlp/DecT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 842,
      "original_url": "https://aclanthology.org/2023.acl-long.841/",
      "title": "The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources",
      "pdf_url": "https://aclanthology.org/2023.acl-long.841.pdf",
      "authors": [
        "Akshatha Arodi",
        "Martin Pömsl",
        "Kaheer Suleman",
        "Adam Trischler",
        "Alexandra Olteanu",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model’s pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied. In this work, we propose a test suite of coreference resolution subtasks that require reasoning over multiple facts. These subtasks differ in terms of which knowledge sources contain the relevant facts. We also introduce subtasks where knowledge is present only at inference time using fictional knowledge. We evaluate state-of-the-art coreference resolution models on our dataset. Our results indicate that several models struggle to reason on-the-fly over knowledge observed both at pretrain time and at inference time. However, with task-specific training, a subset of models demonstrates the ability to integrate certain knowledge types from multiple sources. Still, even the best performing models seem to have difficulties with reliably integrating knowledge presented only at inference time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 843,
      "original_url": "https://aclanthology.org/2023.acl-long.842/",
      "title": "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.842.pdf",
      "authors": [
        "Marcos Treviso",
        "Alexis Ross",
        "Nuno M. Guerreiro",
        "André Martins"
      ],
      "abstract": "Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model’s predictions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 844,
      "original_url": "https://aclanthology.org/2023.acl-long.843/",
      "title": "Towards Unifying Multi-Lingual and Cross-Lingual Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.843.pdf",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Duo Zheng",
        "Yunlong Liang",
        "Zhixu Li",
        "Jianfeng Qu",
        "Jie Zhou"
      ],
      "abstract": "To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directions, where there is no training data from the source-language documents to the target-language summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 845,
      "original_url": "https://aclanthology.org/2023.acl-long.844/",
      "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
      "pdf_url": "https://aclanthology.org/2023.acl-long.844.pdf",
      "authors": [
        "Yixin Liu",
        "Budhaditya Deb",
        "Milagro Teruel",
        "Aaron Halfaker",
        "Dragomir Radev",
        "Ahmed Hassan Awadallah"
      ],
      "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 846,
      "original_url": "https://aclanthology.org/2023.acl-long.845/",
      "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.845.pdf",
      "authors": [
        "Julia Mendelsohn",
        "Ronan Le Bras",
        "Yejin Choi",
        "Maarten Sap"
      ],
      "abstract": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians’ speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 847,
      "original_url": "https://aclanthology.org/2023.acl-long.846/",
      "title": "Exploring Large Language Models for Classical Philology",
      "pdf_url": "https://aclanthology.org/2023.acl-long.846.pdf",
      "authors": [
        "Frederick Riemenschneider",
        "Anette Frank"
      ],
      "abstract": "Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5’s decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 848,
      "original_url": "https://aclanthology.org/2023.acl-long.847/",
      "title": "LayoutMask: Enhance Text-Layout Interaction in Multi-modal Pre-training for Document Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-long.847.pdf",
      "authors": [
        "Yi Tu",
        "Ya Guo",
        "Huan Chen",
        "Jinyang Tang"
      ],
      "abstract": "Visually-rich Document Understanding (VrDU) has attracted much research attention over the past years. Pre-trained models on a large number of document images with transformer-based backbones have led to significant performance gains in this field. The major challenge is how to fusion the different modalities (text, layout, and image) of the documents in a unified model with different pre-training tasks. This paper focuses on improving text-layout interactions and proposes a novel multi-modal pre-training model, LayoutMask. LayoutMask uses local 1D position, instead of global 1D position, as layout input and has two pre-training objectives: (1) Masked Language Modeling: predicting masked tokens with two novel masking strategies; (2) Masked Position Modeling: predicting masked 2D positions to improve layout representation learning. LayoutMask can enhance the interactions between text and layout modalities in a unified model and produce adaptive and robust multi-modal representations for downstream tasks. Experimental results show that our proposed method can achieve state-of-the-art results on a wide variety of VrDU problems, including form understanding, receipt understanding, and document image classification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 849,
      "original_url": "https://aclanthology.org/2023.acl-long.848/",
      "title": "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for Robust Audio-Visual Speech Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.848.pdf",
      "authors": [
        "Yuchen Hu",
        "Ruizhe Li",
        "Chen Chen",
        "Chengwei Qin",
        "Qiu-Shi Zhu",
        "Eng Siong Chng"
      ],
      "abstract": "Audio-visual speech recognition (AVSR) provides a promising solution to ameliorate the noise-robustness of audio-only speech recognition with visual information. However, most existing efforts still focus on audio modality to improve robustness considering its dominance in AVSR task, with noise adaptation techniques such as front-end denoise processing. Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises. In this work, we investigate the noise-invariant visual modality to strengthen robustness of AVSR, which can adapt to any testing noises while without dependence on noisy training data, a.k.a., unsupervised noise adaptation. Inspired by human perception mechanism, we propose a universal viseme-phoneme mapping (UniVPM) approach to implement modality transfer, which can restore clean audio from visual signals to enable speech recognition under any noisy conditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that our approach achieves the state-of-the-art under various noisy as well as clean conditions. In addition, we also outperform previous state-of-the-arts on visual speech recognition task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 850,
      "original_url": "https://aclanthology.org/2023.acl-long.849/",
      "title": "An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.849.pdf",
      "authors": [
        "Xuancheng Huang",
        "Zijun Liu",
        "Peng Li",
        "Tao Li",
        "Maosong Sun",
        "Yang Liu"
      ],
      "abstract": "Recently, multi-aspect controllable text generation that controls the generated text in multiple aspects (e.g., sentiment, topic, and keywords) has attracted increasing attention. Although methods based on parameter efficient tuning like prefix-tuning could achieve multi-aspect controlling in a plug-and-play way, the mutual interference of multiple prefixes leads to significant degeneration of constraints and limits their extensibility to training-time unseen aspect combinations. In this work, we provide a theoretical lower bound for the interference and empirically found that the interference grows with the number of layers where prefixes are inserted. Based on these analyses, we propose using trainable gates to normalize the intervention of prefixes to restrain the growing interference. As a result, controlling training-time unseen combinations of aspects can be realized by simply concatenating corresponding plugins such that new constraints can be extended at a lower cost. In addition, we propose a unified way to process both categorical and free-form constraints. Experiments on text generation and machine translation demonstrate the superiority of our approach over baselines on constraint accuracy, text quality, and extensibility."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 851,
      "original_url": "https://aclanthology.org/2023.acl-long.850/",
      "title": "Double-Branch Multi-Attention based Graph Neural Network for Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.acl-long.850.pdf",
      "authors": [
        "Hongcai Xu",
        "Junpeng Bao",
        "Wenbo Liu"
      ],
      "abstract": "Graph neural networks (GNNs), which effectively use topological structures in the knowledge graphs (KG) to embed entities and relations in low-dimensional spaces, have shown great power in knowledge graph completion (KGC). KG has abundant global and local structural information, however, many GNN-based KGC models cannot capture these two types of information about the graph structure by designing complex aggregation schemes, and are not designed well to learn representations of seen entities with sparse neighborhoods in isolated subgraphs. In this paper, we find that a simple attention-based method can outperform a general GNN-based approach for KGC. We then propose a double-branch multi-attention based graph neural network (MA-GNN) to learn more expressive entity representations which contain rich global-local structural information. Specifically, we first explore the graph attention network-based local aggregator to learn entity representations. Furthermore, we propose a snowball local attention mechanism by leveraging the semantic similarity between two-hop neighbors to enrich the entity embedding. Finally, we use Transformer-based self-attention to learn long-range dependence between entities to obtain richer representations with the global graph structure and entity features. Experimental results on five benchmark datasets show that MA-GNN achieves significant improvements over strong baselines for inductive KGC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 852,
      "original_url": "https://aclanthology.org/2023.acl-long.851/",
      "title": "Dual Cache for Long Document Neural Coreference Resolution",
      "pdf_url": "https://aclanthology.org/2023.acl-long.851.pdf",
      "authors": [
        "Qipeng Guo",
        "Xiangkun Hu",
        "Yue Zhang",
        "Xipeng Qiu",
        "Zheng Zhang"
      ],
      "abstract": "Recent works show the effectiveness of cache-based neural coreference resolution models on long documents. These models incrementally process a long document from left to right and extract relations between mentions and entities in a cache, resulting in much lower memory and computation cost compared to computing all mentions in parallel. However, they do not handle cache misses when high-quality entities are purged from the cache, which causes wrong assignments and leads to prediction errors. We propose a new hybrid cache that integrates two eviction policies to capture global and local entities separately, and effectively reduces the aggregated cache misses up to half as before, while improving F1 score of coreference by 0.7 5.7pt. As such, the hybrid policy can accelerate existing cache-based models and offer a new long document coreference resolution solution. Results show that our method outperforms existing methods on four benchmarks while saving up to 83% of inference time against non-cache-based models. Further, we achieve a new state-of-the-art on a long document coreference benchmark, LitBank."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 853,
      "original_url": "https://aclanthology.org/2023.acl-long.852/",
      "title": "Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.852.pdf",
      "authors": [
        "Kaiyu Huang",
        "Peng Li",
        "Jin Ma",
        "Ting Yao",
        "Yang Liu"
      ],
      "abstract": "In the real-world scenario, a longstanding goal of multilingual neural machine translation (MNMT) is that a single model can incrementally adapt to new language pairs without accessing previous training data. In this scenario, previous studies concentrate on overcoming catastrophic forgetting while lacking encouragement to learn new knowledge from incremental language pairs, especially when the incremental language is not related to the set of original languages. To better acquire new knowledge, we propose a knowledge transfer method that can efficiently adapt original MNMT models to diverse incremental language pairs. The method flexibly introduces the knowledge from an external model into original models, which encourages the models to learn new language pairs, completing the procedure of knowledge transfer. Moreover, all original parameters are frozen to ensure that translation qualities on original language pairs are not degraded. Experimental results show that our method can learn new knowledge from diverse language pairs incrementally meanwhile maintaining performance on original language pairs, outperforming various strong baselines in incremental learning for MNMT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 854,
      "original_url": "https://aclanthology.org/2023.acl-long.853/",
      "title": "DisorBERT: A Double Domain Adaptation Model for Detecting Signs of Mental Disorders in Social Media",
      "pdf_url": "https://aclanthology.org/2023.acl-long.853.pdf",
      "authors": [
        "Mario Aragon",
        "Adrian Pastor Lopez Monroy",
        "Luis Gonzalez",
        "David E. Losada",
        "Manuel Montes"
      ],
      "abstract": "Mental disorders affect millions of people worldwide and cause interference with their thinking and behavior. Through the past years, awareness created by health campaigns and other sources motivated the study of these disorders using information extracted from social media platforms. In this work, we aim to contribute to the study of these disorders and to the understanding of how mental problems reflect on social media. To achieve this goal, we propose a double-domain adaptation of a language model. First, we adapted the model to social media language, and then, we adapted it to the mental health domain. In both steps, we incorporated a lexical resource to guide the masking process of the language model and, therefore, to help it in paying more attention to words related to mental disorders. We have evaluated our model in the detection of signs of three major mental disorders: Anorexia, Self-harm, and Depression. Results are encouraging as they show that the proposed adaptation enhances the classification performance and yields competitive results against state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 855,
      "original_url": "https://aclanthology.org/2023.acl-long.854/",
      "title": "Toward Interactive Dictation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.854.pdf",
      "authors": [
        "Belinda Z. Li",
        "Jason Eisner",
        "Adam Pauls",
        "Sam Thomson"
      ],
      "abstract": "Voice dictation is an increasingly important text input modality. Existing systems that allow both dictation and editing-by-voice restrict their command language to flat templates invoked by trigger words. In this work, we study the feasibility of allowing users to interrupt their dictation with spoken editing commands in open-ended natural language. We introduce a new task and dataset, TERTiUS, to experiment with such systems. To support this flexibility in real-time, a system must incrementally segment and classify spans of speech as either dictation or command, and interpret the spans that are commands. We experiment with using large pre-trained language models to predict the edited text, or alternatively, to predict a small text-editing program. Experiments show a natural trade-off between model accuracy and latency: a smaller model achieves 30% end-state accuracy with 1.3 seconds of latency, while a larger model achieves 55% end-state accuracy with 7 seconds of latency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 856,
      "original_url": "https://aclanthology.org/2023.acl-long.855/",
      "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
      "pdf_url": "https://aclanthology.org/2023.acl-long.855.pdf",
      "authors": [
        "Peng Li",
        "Tianxiang Sun",
        "Qiong Tang",
        "Hang Yan",
        "Yuanbin Wu",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 857,
      "original_url": "https://aclanthology.org/2023.acl-long.856/",
      "title": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.856.pdf",
      "authors": [
        "Barun Patra",
        "Saksham Singhal",
        "Shaohan Huang",
        "Zewen Chi",
        "Li Dong",
        "Furu Wei",
        "Vishrav Chaudhary",
        "Xia Song"
      ],
      "abstract": "In this paper, we elaborate upon recipes for building multilingual representation models that are not only competitive with existing state-of-the-art models but are also more parameter efficient, thereby promoting better adoption in resource-constrained scenarios and practical applications. We show that going beyond English-centric bitexts, coupled with a novel sampling strategy aimed at reducing under-utilization of training data, substantially boosts performance across model sizes for both Electra and MLM pre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language ENcodings using Transformers which not only achieves state-of-the-art performance over 5 cross-lingual tasks within all model size bands, is also competitive across bands. Our XY-LENT XL variant outperforms XLM-R XXL and exhibits competitive performance with mT5 XXL while being 5x and 6x smaller respectively. We then show that our proposed method helps ameliorate the curse of multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and 98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same size band. We then analyze our models performance on extremely low resource languages and posit that scaling alone may not be sufficient for improving the performance in this scenario"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 858,
      "original_url": "https://aclanthology.org/2023.acl-long.857/",
      "title": "Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension",
      "pdf_url": "https://aclanthology.org/2023.acl-long.857.pdf",
      "authors": [
        "Xiao Zhang",
        "Heyan Huang",
        "Zewen Chi",
        "Xian-Ling Mao"
      ],
      "abstract": "Open-retrieval conversational machine reading comprehension (OCMRC) simulates real-life conversational interaction scenes. Machines are required to make a decision of “Yes/No/Inquire” or generate a follow-up question when the decision is “Inquire” based on retrieved rule texts, user scenario, user question and dialogue history. Recent studies try to reduce the information gap between decision-making and question generation, in order to improve the performance of generation. However, the information gap still persists because these methods are still limited in pipeline framework, where decision-making and question generation are performed separately, making it hard to share the entailment reasoning used in decision-making across all stages. To tackle the above problem, we propose a novel one-stage end-to-end framework, called Entailment Fused-T5 (EFT), to bridge the information gap between decision-making and question generation in a global understanding manner. The extensive experimental results demonstrate that our proposed framework achieves new state-of-the-art performance on the OR-ShARC benchmark. Our model and code are publicly available at an anonymous link."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 859,
      "original_url": "https://aclanthology.org/2023.acl-long.858/",
      "title": "LiveChat: A Large-Scale Personalized Dialogue Dataset Automatically Constructed from Live Streaming",
      "pdf_url": "https://aclanthology.org/2023.acl-long.858.pdf",
      "authors": [
        "Jingsheng Gao",
        "Yixin Lian",
        "Ziyi Zhou",
        "Yuzhuo Fu",
        "Baoyuan Wang"
      ],
      "abstract": "Open-domain dialogue systems have made promising progress in recent years. While the state-of-the-art dialogue agents are built upon large-scale social media data and large pre-trained models, there is no guarantee these agents could also perform well in fast-growing scenarios, such as live streaming, due to the bounded transferability of pre-trained models and biased distributions of public datasets from Reddit and Weibo, etc. To improve the essential capability of responding and establish a benchmark in the live open-domain scenario, we introduce the LiveChat dataset, composed of 1.33 million real-life Chinese dialogues with almost 3800 average sessions across 351 personas and fine-grained profiles for each persona. LiveChat is automatically constructed by processing numerous live videos on the Internet and naturally falls within the scope of multi-party conversations, where the issues of Who says What to Whom should be considered. Therefore, we target two critical tasks of response modeling and addressee recognition and propose retrieval-based baselines grounded on advanced techniques. Experimental results have validated the positive effects of leveraging persona profiles and larger average sessions per persona. In addition, we also benchmark the transferability of advanced generation-based models on LiveChat and pose some future directions for current challenges."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 860,
      "original_url": "https://aclanthology.org/2023.acl-long.859/",
      "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
      "pdf_url": "https://aclanthology.org/2023.acl-long.859.pdf",
      "authors": [
        "David Vilar",
        "Markus Freitag",
        "Colin Cherry",
        "Jiaming Luo",
        "Viresh Ratnakar",
        "George Foster"
      ],
      "abstract": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM’s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM’s MT output which reveals some interesting properties and prospects for future work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 861,
      "original_url": "https://aclanthology.org/2023.acl-long.860/",
      "title": "Exploring Lottery Prompts for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.860.pdf",
      "authors": [
        "Yulin Chen",
        "Ning Ding",
        "Xiaobin Wang",
        "Shengding Hu",
        "Haitao Zheng",
        "Zhiyuan Liu",
        "Pengjun Xie"
      ],
      "abstract": "Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning. Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs.Meanwhile, it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features. Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method. Experiments are conducted on various types of NLP classification tasks and demonstrate that the proposed method can achieve comparable results with other gradient-free and optimization-free baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 862,
      "original_url": "https://aclanthology.org/2023.acl-long.861/",
      "title": "A Facial Expression-Aware Multimodal Multi-task Learning Framework for Emotion Recognition in Multi-party Conversations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.861.pdf",
      "authors": [
        "Wenjie Zheng",
        "Jianfei Yu",
        "Rui Xia",
        "Shijin Wang"
      ],
      "abstract": "Multimodal Emotion Recognition in Multiparty Conversations (MERMC) has recently attracted considerable attention. Due to the complexity of visual scenes in multi-party conversations, most previous MERMC studies mainly focus on text and audio modalities while ignoring visual information. Recently, several works proposed to extract face sequences as visual features and have shown the importance of visual information in MERMC. However, given an utterance, the face sequence extracted by previous methods may contain multiple people’s faces, which will inevitably introduce noise to the emotion prediction of the real speaker. To tackle this issue, we propose a two-stage framework named Facial expressionaware Multimodal Multi-Task learning (FacialMMT). Specifically, a pipeline method is first designed to extract the face sequence of the real speaker of each utterance, which consists of multimodal face recognition, unsupervised face clustering, and face matching. With the extracted face sequences, we propose a multimodal facial expression-aware emotion recognition model, which leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Experiments demonstrate the effectiveness of the proposed FacialMMT framework on the benchmark MELD dataset. The source code is publicly released at https://github.com/NUSTM/FacialMMT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 863,
      "original_url": "https://aclanthology.org/2023.acl-long.862/",
      "title": "TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline",
      "pdf_url": "https://aclanthology.org/2023.acl-long.862.pdf",
      "authors": [
        "Jiang Li",
        "Xiangdong Su",
        "Guanglai Gao"
      ],
      "abstract": "Temporal knowledge graph embedding (TKGE) models are commonly utilized to infer the missing facts and facilitate reasoning and decision-making in temporal knowledge graph based systems. However, existing methods fuse temporal information into entities, potentially leading to the evolution of entity information and limiting the link prediction performance of TKG. Meanwhile, current TKGE models often lack the ability to simultaneously model important relation patterns and provide interpretability, which hinders their effectiveness and potential applications. To address these limitations, we propose a novel TKGE model which encodes Temporal knowledge graph embeddings via Archimedean Spiral Timeline (TeAST), which maps relations onto the corresponding Archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor completion problem. Specifically, the Archimedean spiral timeline ensures that relations that occur simultaneously are placed on the same timeline, and all relations evolve over time. Meanwhile, we present a novel temporal spiral regularizer to make the spiral timeline orderly. In addition, we provide mathematical proofs to demonstrate the ability of TeAST to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing TKGE methods. Our code is available at https://github.com/IMU-MachineLearningSXD/TeAST."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 864,
      "original_url": "https://aclanthology.org/2023.acl-long.863/",
      "title": "Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition",
      "pdf_url": "https://aclanthology.org/2023.acl-long.863.pdf",
      "authors": [
        "Yuwei Bao",
        "Barrett Lattimer",
        "Joyce Chai"
      ],
      "abstract": "Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 865,
      "original_url": "https://aclanthology.org/2023.acl-long.864/",
      "title": "Conjunct Lengths in English, Dependency Length Minimization, and Dependency Structure of Coordination",
      "pdf_url": "https://aclanthology.org/2023.acl-long.864.pdf",
      "authors": [
        "Adam Przepiórkowski",
        "Michał Woźniak"
      ],
      "abstract": "This paper confirms that, in English binary coordinations, left conjuncts tend to be shorter than right conjuncts, regardless of the position of the governor of the coordination. We demonstrate that this tendency becomes stronger when length differences are greater, but only when the governor is on the left or absent, not when it is on the right. We explain this effect via Dependency Length Minimization and we show that this explanation provides support for symmetrical dependency structures of coordination (where coordination is multi-headed by all conjuncts, as in Word Grammar or in enhanced Universal Dependencies, or where it single-headed by the conjunction, as in the Prague Dependency Treebank), as opposed to asymmetrical structures (where coordination is headed by the first conjunct, as in the Meaning–Text Theory or in basic Universal Dependencies)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 866,
      "original_url": "https://aclanthology.org/2023.acl-long.865/",
      "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
      "pdf_url": "https://aclanthology.org/2023.acl-long.865.pdf",
      "authors": [
        "Ilias Chalkidis",
        "Nicolas Garneau",
        "Catalina Goanta",
        "Daniel Katz",
        "Anders Søgaard"
      ],
      "abstract": "In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models’ size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model’s size and prior legal knowledge which can be estimated by upstream and probing performance. Based on these findings, we can conclude that both dimensions are important for those seeking the development of domain-specific PLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 867,
      "original_url": "https://aclanthology.org/2023.acl-long.866/",
      "title": "Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge",
      "pdf_url": "https://aclanthology.org/2023.acl-long.866.pdf",
      "authors": [
        "Xuebo Liu",
        "Yutong Wang",
        "Derek F. Wong",
        "Runzhe Zhan",
        "Liangxuan Yu",
        "Min Zhang"
      ],
      "abstract": "The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored in terms of effectively training NMT models with high CR abilities and devising accurate automatic evaluation metrics. This paper presents a comprehensive study aimed at expanding the understanding of CR in NMT.For the training, we confirm the effectiveness of incorporating pretrained knowledge into NMT models and subsequently utilizing these models as robust testbeds for investigating CR in NMT. For the evaluation, we propose a novel entity-aware evaluation method that takes into account both the NMT candidate and important entities in the candidate, which is more aligned with human judgement. Based on the strong testbed and evaluation methods, we identify challenges in training NMT models with high CR abilities and suggest directions for further unlabeled data utilization and model design. We hope that our methods and findings will contribute to advancing the research of CR in NMT. Source data, code and scripts are freely available at https://github.com/YutongWang1216/CR-NMT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 868,
      "original_url": "https://aclanthology.org/2023.acl-long.867/",
      "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.867.pdf",
      "authors": [
        "Kai Mei",
        "Zheng Li",
        "Zhenting Wang",
        "Yang Zhang",
        "Shiqing Ma"
      ],
      "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 869,
      "original_url": "https://aclanthology.org/2023.acl-long.868/",
      "title": "Revisiting Relation Extraction in the era of Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.868.pdf",
      "authors": [
        "Somin Wadhwa",
        "Silvio Amir",
        "Byron Wallace"
      ],
      "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 870,
      "original_url": "https://aclanthology.org/2023.acl-long.869/",
      "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners",
      "pdf_url": "https://aclanthology.org/2023.acl-long.869.pdf",
      "authors": [
        "Xuandong Zhao",
        "Siqi Ouyang",
        "Zhiguo Yu",
        "Ming Wu",
        "Lei Li"
      ],
      "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available at https://anonymous.4open.science/r/NPPrompt."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 871,
      "original_url": "https://aclanthology.org/2023.acl-long.870/",
      "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
      "pdf_url": "https://aclanthology.org/2023.acl-long.870.pdf",
      "authors": [
        "Cheng-Han Chiang",
        "Hung-yi Lee"
      ],
      "abstract": "Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 872,
      "original_url": "https://aclanthology.org/2023.acl-long.871/",
      "title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-long.871.pdf",
      "authors": [
        "Florian Mai",
        "Arnaud Pannatier",
        "Fabio Fehr",
        "Haolin Chen",
        "Francois Marelli",
        "Francois Fleuret",
        "James Henderson"
      ],
      "abstract": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 873,
      "original_url": "https://aclanthology.org/2023.acl-long.872/",
      "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
      "pdf_url": "https://aclanthology.org/2023.acl-long.872.pdf",
      "authors": [
        "Hirofumi Inaguma",
        "Sravya Popuri",
        "Ilia Kulikov",
        "Peng-Jen Chen",
        "Changhan Wang",
        "Yu-An Chung",
        "Yun Tang",
        "Ann Lee",
        "Shinji Watanabe",
        "Juan Pino"
      ],
      "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 874,
      "original_url": "https://aclanthology.org/2023.acl-long.873/",
      "title": "Estimating the Uncertainty in Emotion Attributes using Deep Evidential Regression",
      "pdf_url": "https://aclanthology.org/2023.acl-long.873.pdf",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "abstract": "In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground truth, it ignores the intrinsic uncertainty revealed by the inconsistent labels. This paper proposes a Bayesian approach, deep evidential emotion regression (DEER), to estimate the uncertainty in emotion attributes. Treating the emotion attribute labels of an utterance as samples drawn from an unknown Gaussian distribution, DEER places an utterance-specific normal-inverse gamma prior over the Gaussian likelihood and predicts its hyper-parameters using a deep neural network model. It enables a joint estimation of emotion attributes along with the aleatoric and epistemic uncertainties. AER experiments on the widely used MSP-Podcast and IEMOCAP datasets showed DEER produced state-of-the-art results for both the mean values and the distribution of emotion attributes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 875,
      "original_url": "https://aclanthology.org/2023.acl-long.874/",
      "title": "Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.874.pdf",
      "authors": [
        "Wei Liu",
        "Michael Strube"
      ],
      "abstract": "Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference, we adopt Scheduled Sampling to the joint learning. We evaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results show that our joint model significantly outperforms various baselines on three datasets, demonstrating its superiority for the task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 876,
      "original_url": "https://aclanthology.org/2023.acl-long.875/",
      "title": "Plug-and-Play Document Modules for Pre-trained Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.875.pdf",
      "authors": [
        "Chaojun Xiao",
        "Zhengyan Zhang",
        "Xu Han",
        "Chi-Min Chan",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Xiangyang Li",
        "Zhonghua Li",
        "Zhao Cao",
        "Maosong Sun"
      ],
      "abstract": "Large-scale pre-trained models (PTMs) have been widely used in document-oriented NLP tasks, such as question answering. However, the encoding-task coupling requirement results in the repeated encoding of the same documents for different tasks and queries, which is highly computationally inefficient. To this end, we target to decouple document encoding from downstream tasks, and propose to represent each document as a plug-and-play document module, i.e., a document plugin, for PTMs (PlugD). By inserting document plugins into the backbone PTM for downstream tasks, we can encode a document one time to handle multiple tasks, which is more efficient than conventional encoding-task coupling methods that simultaneously encode documents and input queries using task-specific encoders. Extensive experiments on 8 datasets of 4 typical NLP tasks show that PlugD enables models to encode documents once and for all across different scenarios. Especially, PlugD can save 69% computational costs while achieving comparable performance to state-of-the-art encoding-task coupling methods. Additionally, we show that PlugD can serve as an effective post-processing way to inject knowledge into task-specific models, improving model performance without any additional model training. Our code and checkpoints can be found in https://github.com/thunlp/Document-Plugin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 877,
      "original_url": "https://aclanthology.org/2023.acl-long.876/",
      "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.876.pdf",
      "authors": [
        "Zhongbin Xie",
        "Thomas Lukasiewicz"
      ],
      "abstract": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 878,
      "original_url": "https://aclanthology.org/2023.acl-long.877/",
      "title": "Two-Stage Fine-Tuning for Improved Bias and Variance for Large Pretrained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.877.pdf",
      "authors": [
        "Lijing Wang",
        "Yingya Li",
        "Timothy Miller",
        "Steven Bethard",
        "Guergana Savova"
      ],
      "abstract": "The bias-variance tradeoff is the idea that learning methods need to balance model complexity with data size to minimize both under-fitting and over-fitting. Recent empirical work and theoretical analysis with over-parameterized neural networks challenges the classic bias-variance trade-off notion suggesting that no such trade-off holds: as the width of the network grows, bias monotonically decreases while variance initially increases followed by a decrease. In this work, we first provide a variance decomposition-based justification criteria to examine whether large pretrained neural models in a fine-tuning setting are generalizable enough to have low bias and variance. We then perform theoretical and empirical analysis using ensemble methods explicitly designed to decrease variance due to optimization. This results in essentially a two-stage fine-tuning algorithm that first ratchets down bias and variance iteratively, and then uses a selected fixed-bias model to further reduce variance due to optimization by ensembling. We also analyze the nature of variance change with the ensemble size in low- and high-resource classes. Empirical results show that this two-stage method obtains strong results on SuperGLUE tasks and clinical information extraction tasks. Code and settings are available: https://github.com/christa60/bias-var-fine-tuning-plms.git"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 879,
      "original_url": "https://aclanthology.org/2023.acl-long.878/",
      "title": "A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.878.pdf",
      "authors": [
        "Krithika Ramesh",
        "Arnav Chavan",
        "Shrey Pandit",
        "Sunayana Sitaram"
      ],
      "abstract": "Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 880,
      "original_url": "https://aclanthology.org/2023.acl-long.879/",
      "title": "Ranking-Enhanced Unsupervised Sentence Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.879.pdf",
      "authors": [
        "Yeon Seonwoo",
        "Guoyin Wang",
        "Changmin Seo",
        "Sajal Choudhary",
        "Jiwei Li",
        "Xiang Li",
        "Puyang Xu",
        "Sunghyun Park",
        "Alice Oh"
      ],
      "abstract": "Unsupervised sentence representation learning has progressed through contrastive learning and data augmentation methods such as dropout masking. Despite this progress, sentence encoders are still limited to using only an input sentence when predicting its semantic vector. In this work, we show that the semantic meaning of a sentence is also determined by nearest-neighbor sentences that are similar to the input sentence. Based on this finding, we propose a novel unsupervised sentence encoder, RankEncoder. RankEncoder predicts the semantic vector of an input sentence by leveraging its relationship with other sentences in an external corpus, as well as the input sentence itself. We evaluate RankEncoder on semantic textual benchmark datasets. From the experimental results, we verify that 1) RankEncoder achieves 80.07% Spearman’s correlation, a 1.1% absolute improvement compared to the previous state-of-the-art performance, 2) RankEncoder is universally applicable to existing unsupervised sentence embedding methods, and 3) RankEncoder is specifically effective for predicting the similarity scores of similar sentence pairs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 881,
      "original_url": "https://aclanthology.org/2023.acl-long.880/",
      "title": "To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support",
      "pdf_url": "https://aclanthology.org/2023.acl-long.880.pdf",
      "authors": [
        "Gabriella Skitalinskaya",
        "Henning Wachsmuth"
      ],
      "abstract": "Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying argumentative claims in need of specific revisions. By learning from collaborative editing behaviors in online debates, we seek to capture implicit revision patterns in order to develop approaches aimed at guiding writers in how to further improve their arguments. We systematically compare the ability of common word embedding models to capture the differences between different versions of the same text, and we analyze their impact on various types of writing issues. To deal with the noisy nature of revision-based corpora, we propose a new sampling strategy based on revision distance. Opposed to approaches from prior work, such sampling can be done without employing additional annotations and judgments. Moreover, we provide evidence that using contextual information and domain knowledge can further improve prediction results. How useful a certain type of context is, depends on the issue the claim is suffering from, though."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 882,
      "original_url": "https://aclanthology.org/2023.acl-long.881/",
      "title": "Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments",
      "pdf_url": "https://aclanthology.org/2023.acl-long.881.pdf",
      "authors": [
        "Ethan Mendes",
        "Yang Chen",
        "Wei Xu",
        "Alan Ritter"
      ],
      "abstract": "We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. We make our data and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw user-generated content."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 883,
      "original_url": "https://aclanthology.org/2023.acl-long.882/",
      "title": "Composition-contrastive Learning for Sentence Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-long.882.pdf",
      "authors": [
        "Sachin Chanchani",
        "Ruihong Huang"
      ],
      "abstract": "Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 884,
      "original_url": "https://aclanthology.org/2023.acl-long.883/",
      "title": "Causes and Cures for Interference in Multilingual Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.883.pdf",
      "authors": [
        "Uri Shaham",
        "Maha Elbayad",
        "Vedanuj Goswami",
        "Omer Levy",
        "Shruti Bhosale"
      ],
      "abstract": "Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancing the amount of interference between low and high resource language pairs effectively, and can lead to superior performance overall."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 885,
      "original_url": "https://aclanthology.org/2023.acl-long.884/",
      "title": "Understanding and Bridging the Modality Gap for Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.884.pdf",
      "authors": [
        "Qingkai Fang",
        "Yang Feng"
      ],
      "abstract": "How to achieve better end-to-end speech translation (ST) by leveraging (text) machine translation (MT) data? Among various existing techniques, multi-task learning is one of the effective ways to share knowledge between ST and MT in which additional MT data can help to learn source-to-target mapping. However, due to the differences between speech and text, there is always a gap between ST and MT. In this paper, we first aim to understand this modality gap from the target-side representation differences, and link the modality gap to another well-known problem in neural machine translation: exposure bias. We find that the modality gap is relatively small during training except for some difficult cases, but keeps increasing during inference due to the cascading effect. To address these problems, we propose the Cross-modal Regularization with Scheduled Sampling (Cress) method. Specifically, we regularize the output predictions of ST and MT, whose target-side contexts are derived by sampling between ground truth words and self-generated words with a varying probability. Furthermore, we introduce token-level adaptive training which assigns different training weights to target tokens to handle difficult cases with large modality gaps. Experiments and analysis show that our approach effectively bridges the modality gap, and achieves significant improvements over a strong baseline in all eight directions of the MuST-C dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 886,
      "original_url": "https://aclanthology.org/2023.acl-long.885/",
      "title": "Few-shot Reranking for Multi-hop QA via Language Model Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-long.885.pdf",
      "authors": [
        "Muhammad Khalifa",
        "Lajanugen Logeswaran",
        "Moontae Lee",
        "Honglak Lee",
        "Lu Wang"
      ],
      "abstract": "We study few-shot reranking for multi-hop QA (MQA) with open-domain questions. To alleviate the need for a large number of labeled question-document pairs for retriever training, we propose PromptRank, which relies on language model prompting for multi-hop path reranking. PromptRank first constructs an instruction-based prompt that includes a candidate document path and then computes the relevance score between a given question and the path based on the conditional likelihood of the question given the path prompt according to a language model. PromptRank yields strong retrieval performance on HotpotQA with only 128 training examples compared to state-of-the-art methods trained on thousands of examples — 73.6 recall@10 by PromptRank vs. 77.8 by PathRetriever and 77.5 by multi-hop dense retrieval."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 887,
      "original_url": "https://aclanthology.org/2023.acl-long.886/",
      "title": "DICE: Data-Efficient Clinical Event Extraction with Generative Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.886.pdf",
      "authors": [
        "Mingyu Derek Ma",
        "Alexander Taylor",
        "Wei Wang",
        "Nanyun Peng"
      ],
      "abstract": "Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCROBAT. Our experiments demonstrate state-of-the-art performances of DICE for clinical and news domain event extraction, especially under low data settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 888,
      "original_url": "https://aclanthology.org/2023.acl-long.887/",
      "title": "XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages and Meaning Representations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.887.pdf",
      "authors": [
        "Yusen Zhang",
        "Jun Wang",
        "Zhiguo Wang",
        "Rui Zhang"
      ],
      "abstract": "Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple natural languages (NLs) into meaning representations (MRs) such as SQL, lambda calculus, and logic forms. However, existing CLSP models are separately proposed and evaluated on datasets of limited tasks and applications, impeding a comprehensive and unified evaluation of CLSP on a diverse range of NLs and MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual semantic parsing featured with 22 natural languages and 8 meaning representations by examining and selecting 9 existing datasets to cover 5 tasks and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a wide range of multilingual language models including encoder-based models (mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models (Codex, BLOOM). We design 6 experiment settings covering various lingual combinations (monolingual, multilingual, cross-lingual) and numbers of learning samples (full dataset, few-shot, and zero-shot). Our experiments show that encoder-decoder models (mT5) achieve the highest performance compared with other popular models, and multilingual training can further improve the average performance. Notably, multilingual large language models (e.g., BLOOM) are still inadequate to perform CLSP tasks. We also find that the performance gap between monolingual training and cross-lingual transfer learning is still significant for multilingual models, though it can be mitigated by cross-lingual few-shot training. Our dataset and code are available at https://github.com/psunlpgroup/XSemPLR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 889,
      "original_url": "https://aclanthology.org/2023.acl-long.888/",
      "title": "INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-long.888.pdf",
      "authors": [
        "Wenhao Zhu",
        "Jingjing Xu",
        "Shujian Huang",
        "Lingpeng Kong",
        "Jiajun Chen"
      ],
      "abstract": "Neural machine translation has achieved promising results on many translation tasks. However, previous studies have shown that neural models induce a non-smooth representation space, which harms its generalization results. Recently, kNN-MT has provided an effective paradigm to smooth the prediction based on neighbor representations during inference. Despite promising results, kNN-MT usually requires large inference overhead. We propose an effective training framework INK to directly smooth the representation space via adjusting representations of kNN neighbors with a small number of new parameters. The new parameters are then used to refresh the whole representation datastore to get new kNN knowledge asynchronously. This loop keeps running until convergence. Experiments on four benchmark datasets show that INK achieves average gains of 1.99 COMET and 1.0 BLEU, outperforming the state-of-the-art kNN-MT system with 0.02x memory space and 1.9x inference speedup."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 890,
      "original_url": "https://aclanthology.org/2023.acl-long.889/",
      "title": "Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.889.pdf",
      "authors": [
        "Qi Sun",
        "Kun Huang",
        "Xiaocui Yang",
        "Pengfei Hong",
        "Kun Zhang",
        "Soujanya Poria"
      ],
      "abstract": "Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further considering the long-tail problem, we design dynamic uncertainty thresholds for different types of relations to filter high-uncertainty pseudo labels. We conduct experiments on two public datasets. Our framework outperforms strong baselines by 1.91 F1 and 2.28 Ign F1 on the RE-DocRED dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 891,
      "original_url": "https://aclanthology.org/2023.acl-long.890/",
      "title": "Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.890.pdf",
      "authors": [
        "Shivaen Ramshetty",
        "Gaurav Verma",
        "Srijan Kumar"
      ],
      "abstract": "The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the cross-modal information present in multimodal data. Such information from the visual modality, such as color, size, and shape, provides additional attributes that users can include in their inputs. Thus, we propose cross-modal attribute insertions as a realistic perturbation strategy for vision-and-language data that inserts visual attributes of the objects in the image into the corresponding text (e.g., “girl on a chair” to “little girl on a wooden chair”). Our proposed approach for cross-modal attribute insertions is modular, controllable, and task-agnostic. We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of ~15% in MRR and ~20% in F1 score, respectively. Crowd-sourced annotations demonstrate that cross-modal insertions lead to higher quality augmentations for multimodal data than augmentations using text-only data, and are equivalent in quality to original examples. We release the code to encourage robustness evaluations of deep vision-and-language models: https://github.com/claws-lab/multimodal-robustness-xmai"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 892,
      "original_url": "https://aclanthology.org/2023.acl-long.891/",
      "title": "Crosslingual Generalization through Multitask Finetuning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.891.pdf",
      "authors": [
        "Niklas Muennighoff",
        "Thomas Wang",
        "Lintang Sutawika",
        "Adam Roberts",
        "Stella Biderman",
        "Teven Le Scao",
        "M Saiful Bari",
        "Sheng Shen",
        "Zheng Xin Yong",
        "Hailey Schoelkopf",
        "Xiangru Tang",
        "Dragomir Radev",
        "Alham Fikri Aji",
        "Khalid Almubarak",
        "Samuel Albanie",
        "Zaid Alyafeai",
        "Albert Webson",
        "Edward Raff",
        "Colin Raffel"
      ],
      "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task genrealization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 893,
      "original_url": "https://aclanthology.org/2023.acl-long.892/",
      "title": "Evaluate AMR Graph Similarity via Self-supervised Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.892.pdf",
      "authors": [
        "Ziyi Shou",
        "Fangzhen Lin"
      ],
      "abstract": "In work on AMR (Abstract Meaning Representation), similarity metrics are crucial as they are used to evaluate AMR systems such as AMR parsers. Current AMR metrics are all based on nodes or triples matching without considering the entire structures of AMR graphs. To address this problem, and inspired by learned similarity evaluation on plain text, we propose AMRSim, an automatic AMR graph similarity evaluation metric. To overcome the high cost of collecting human-annotated data, AMRSim automatically generates silver AMR graphs and utilizes self-supervised learning methods. We evaluated AMRSim on various datasets and found that AMRSim significantly improves the correlations with human semantic scores and remains robust under diverse challenges. We also discuss how AMRSim can be extended to multilingual cases."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 894,
      "original_url": "https://aclanthology.org/2023.acl-long.893/",
      "title": "Analyzing Transformers in Embedding Space",
      "pdf_url": "https://aclanthology.org/2023.acl-long.893.pdf",
      "authors": [
        "Guy Dar",
        "Mor Geva",
        "Ankit Gupta",
        "Jonathan Berant"
      ],
      "abstract": "Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by “translating” the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 895,
      "original_url": "https://aclanthology.org/2023.acl-long.894/",
      "title": "Few-Shot Data-to-Text Generation via Unified Representation and Multi-Source Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-long.894.pdf",
      "authors": [
        "Alexander Hanbo Li",
        "Mingyue Shang",
        "Evangelia Spiliopoulou",
        "Jie Ma",
        "Patrick Ng",
        "Zhiguo Wang",
        "Bonan Min",
        "William Yang Wang",
        "Kathleen McKeown",
        "Vittorio Castelli",
        "Dan Roth",
        "Bing Xiang"
      ],
      "abstract": "In this paper, we present a novel approach for data-to-text generation that addresses the limitations of current methods that primarily focus on specific types of structured data. Our proposed method aims to improve performance in multi-task training, zero-shot and few-shot scenarios by providing a unified representation that can handle various forms of structured data such as tables, knowledge graph triples, and meaning representations. We demonstrate that our proposed approach can effectively adapt to new structured forms, and can improve performance in comparison to current methods. For example, our method resulted in a 66% improvement in zero-shot BLEU scores when transferring models trained on table inputs to a knowledge graph dataset. Our proposed method is an important step towards a more general data-to-text generation framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 896,
      "original_url": "https://aclanthology.org/2023.acl-long.895/",
      "title": "FactKG: Fact Verification via Reasoning on Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.acl-long.895.pdf",
      "authors": [
        "Jiho Kim",
        "Sungjin Park",
        "Yeonsu Kwon",
        "Yohan Jo",
        "James Thorne",
        "Edward Choi"
      ],
      "abstract": "In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verificationvia Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 897,
      "original_url": "https://aclanthology.org/2023.acl-long.896/",
      "title": "DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains",
      "pdf_url": "https://aclanthology.org/2023.acl-long.896.pdf",
      "authors": [
        "Yanis Labrak",
        "Adrien Bazoge",
        "Richard Dufour",
        "Mickael Rouvier",
        "Emmanuel Morin",
        "Béatrice Daille",
        "Pierre-Antoine Gourraud"
      ],
      "abstract": "In recent years, pre-trained language models (PLMs) achieve the best performance on a wide range of natural language processing (NLP) tasks. While the first models were trained on general domain data, specialized ones have emerged to more effectively treat specific domains. In this paper, we propose an original study of PLMs in the medical domain on French language. We compare, for the first time, the performance of PLMs trained on both public data from the web and private data from healthcare establishments. We also evaluate different learning strategies on a set of biomedical tasks. In particular, we show that we can take advantage of already existing biomedical PLMs in a foreign language by further pre-train it on our targeted data. Finally, we release the first specialized PLMs for the biomedical field in French, called DrBERT, as well as the largest corpus of medical data under free license on which these models are trained."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 898,
      "original_url": "https://aclanthology.org/2023.acl-long.897/",
      "title": "Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.897.pdf",
      "authors": [
        "Changsen Yuan",
        "Heyan Huang",
        "Yixin Cao",
        "Yonggang Wen"
      ],
      "abstract": "Document-level Event Causality Identification (DECI) aims to extract causal relations between events in a document. It challenges conventional sentence-level task (SECI) with difficult long-text understanding. In this paper, we propose a novel DECI model (SENDIR) for better document-level reasoning. Different from existing works that build an event graph via linguistic tools, SENDIR does not require any prior knowledge. The basic idea is to discriminate event pairs in the same sentence or span multiple sentences by assuming their different information density: 1) low density in the document suggests sparse attention to skip irrelevant information. Our module 1 designs various types of attention for event representation learning to capture long-distance dependence. 2) High density in a sentence makes SECI relatively easy. Module 2 uses different weights to highlight the roles and contributions of intra- and inter-sentential reasoning, which introduces supportive event pairs for joint modeling. Extensive experiments demonstrate great improvements in SENDIR and the effectiveness of various sparse attention for document-level representations. Codes will be released later."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 899,
      "original_url": "https://aclanthology.org/2023.acl-long.898/",
      "title": "Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks",
      "pdf_url": "https://aclanthology.org/2023.acl-long.898.pdf",
      "authors": [
        "Junyu Lu",
        "Bo Xu",
        "Xiaokun Zhang",
        "Changrong Min",
        "Liang Yang",
        "Hongfei Lin"
      ],
      "abstract": "The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly due to limited datasets. Existing datasets suffer from a lack of fine-grained annotations, such as the toxic type and expressions with indirect toxicity. These fine-grained annotations are crucial factors for accurately detecting the toxicity of posts involved with lexical knowledge, which has been a challenge for researchers. To tackle this problem, we facilitate the fine-grained detection of Chinese toxic language by building a new dataset with benchmark results. First, we devised Monitor Toxic Frame, a hierarchical taxonomy to analyze the toxic type and expressions. Then, we built a fine-grained dataset ToxiCN, including both direct and indirect toxic samples. ToxiCN is based on an insulting vocabulary containing implicit profanity. We further propose a benchmark model, Toxic Knowledge Enhancement (TKE), by incorporating lexical features to detect toxic language. We demonstrate the usability of ToxiCN and the effectiveness of TKE based on a systematic quantitative and qualitative analysis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 900,
      "original_url": "https://aclanthology.org/2023.acl-long.899/",
      "title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations",
      "pdf_url": "https://aclanthology.org/2023.acl-long.899.pdf",
      "authors": [
        "Paul-Ambroise Duquenne",
        "Hongyu Gong",
        "Ning Dong",
        "Jingfei Du",
        "Ann Lee",
        "Vedanuj Goswami",
        "Changhan Wang",
        "Juan Pino",
        "Benoît Sagot",
        "Holger Schwenk"
      ],
      "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models will be publicly released"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 901,
      "original_url": "https://aclanthology.org/2023.acl-long.900/",
      "title": "Character-Aware Models Improve Visual Text Rendering",
      "pdf_url": "https://aclanthology.org/2023.acl-long.900.pdf",
      "authors": [
        "Rosanne Liu",
        "Dan Garrette",
        "Chitwan Saharia",
        "William Chan",
        "Adam Roberts",
        "Sharan Narang",
        "Irina Blok",
        "Rj Mical",
        "Mohammad Norouzi",
        "Noah Constant"
      ],
      "abstract": "Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word’s visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 902,
      "original_url": "https://aclanthology.org/2023.acl-long.901/",
      "title": "IDRISI-RA: The First Arabic Location Mention Recognition Dataset of Disaster Tweets",
      "pdf_url": "https://aclanthology.org/2023.acl-long.901.pdf",
      "authors": [
        "Reem Suwaileh",
        "Muhammad Imran",
        "Tamer Elsayed"
      ],
      "abstract": "Extracting geolocation information from social media data enables effective disaster management, as it helps response authorities; for example, in locating incidents for planning rescue activities, and affected people for evacuation. Nevertheless, geolocation extraction is greatly understudied for the low resource languages such as Arabic. To fill this gap, we introduce IDRISI-RA, the first publicly-available Arabic Location Mention Recognition (LMR) dataset that provides human- and automatically-labeled versions in order of thousands and millions of tweets, respectively. It contains both location mentions and their types (e.g., district, city). Our extensive analysis shows the decent geographical, domain, location granularity, temporal, and dialectical coverage of IDRISI-RA. Furthermore, we establish baselines using the standard Arabic NER models and build two simple, yet effective, LMR models. Our rigorous experiments confirm the need for developing specific models for Arabic LMR in the disaster domain. Moreover, experiments show the promising domain and geographical generalizability of IDRISI-RA under zero-shot learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 903,
      "original_url": "https://aclanthology.org/2023.acl-long.902/",
      "title": "FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-long.902.pdf",
      "authors": [
        "Tianshuo Peng",
        "Zuchao Li",
        "Lefei Zhang",
        "Bo Du",
        "Hai Zhao"
      ],
      "abstract": "Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To address these deficiencies, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework. Specifically, our contribution consists of two concepts: fuzzy span loss and fuzzy span attention. Our experimental results on a series of main IE tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of FSUIE in different tasks, settings, and scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 904,
      "original_url": "https://aclanthology.org/2023.acl-long.903/",
      "title": "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey",
      "pdf_url": "https://aclanthology.org/2023.acl-long.903.pdf",
      "authors": [
        "Julian Michael",
        "Ari Holtzman",
        "Alicia Parrish",
        "Aaron Mueller",
        "Alex Wang",
        "Angelica Chen",
        "Divyam Madaan",
        "Nikita Nangia",
        "Richard Yuanzhe Pang",
        "Jason Phang",
        "Samuel R. Bowman"
      ],
      "abstract": "We present the results of the NLP Community Metasurvey. Run from May to June 2022, it elicited opinions on controversial issues, including industry influence in the field, concerns about AGI, and ethics. Our results put concrete numbers to several controversies: For example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems. In addition, the survey posed meta-questions, asking respondents to predict the distribution of survey responses. This allows us to uncover false sociological beliefs where the community’s predictions don’t match reality. Among other results, we find that the community greatly overestimates its own belief in the usefulness of benchmarks and the potential for scaling to solve real-world problems, while underestimating its belief in the importance of linguistic structure, inductive bias, and interdisciplinary science."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 905,
      "original_url": "https://aclanthology.org/2023.acl-long.904/",
      "title": "Prototype-Guided Pseudo Labeling for Semi-Supervised Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.904.pdf",
      "authors": [
        "Weiyi Yang",
        "Richong Zhang",
        "Junfan Chen",
        "Lihong Wang",
        "Jaein Kim"
      ],
      "abstract": "Semi-supervised text classification (SSTC) aims at text classification with few labeled data and massive unlabeled data. Recent works achieve this task by pseudo-labeling methods, with the belief that the unlabeled and labeled data have identical data distribution, and assign the unlabeled data with pseudo-labels as additional supervision. However, existing pseudo-labeling methods usually suffer from ambiguous categorical boundary issues when training the pseudo-labeling phase, and simply select pseudo-labels without considering the unbalanced categorical distribution of the unlabeled data, making it difficult to generate reliable pseudo-labels for each category. We propose a novel semi-supervised framework, namely ProtoS2, with prototypical cluster separation (PCS) and prototypical-center data selection (CDS) technology to address the issue. Particularly, PCS exploits categorical prototypes to assimilate instance representations within the same category, thus emphasizing low-density separation for the pseudo-labeled data to alleviate ambiguous boundaries. Besides, CDS selects central pseudo-labeled data considering the categorical distribution, avoiding the model from biasing on dominant categories. Empirical studies and extensive analysis with four benchmarks demonstrate the effectiveness of the proposed model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 906,
      "original_url": "https://aclanthology.org/2023.acl-long.905/",
      "title": "LENS: A Learnable Evaluation Metric for Text Simplification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.905.pdf",
      "authors": [
        "Mounica Maddela",
        "Yao Dou",
        "David Heineman",
        "Wei Xu"
      ],
      "abstract": "Training learnable metrics using modern language models has recently emerged as a promising method for the automatic evaluation of machine translation. However, existing human evaluation datasets for text simplification have limited annotations that are based on unitary or outdated models, making them unsuitable for this approach. To address these issues, we introduce the SimpEval corpus that contains: SimpEval_past, comprising 12K human ratings on 2.4K simplifications of 24 past systems, and SimpEval_2022, a challenging simplification benchmark consisting of over 1K human ratings of 360 simplifications including GPT-3.5 generated text. Training on SimpEval, we present LENS, a Learnable Evaluation Metric for Text Simplification. Extensive empirical results show that LENS correlates much better with human judgment than existing metrics, paving the way for future progress in the evaluation of text simplification. We also introduce Rank & Rate, a human evaluation framework that rates simplifications from several models in a list-wise manner using an interactive interface, which ensures both consistency and accuracy in the evaluation process and is used to create the SimpEval datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 907,
      "original_url": "https://aclanthology.org/2023.acl-long.906/",
      "title": "MeetingBank: A Benchmark Dataset for Meeting Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-long.906.pdf",
      "authors": [
        "Yebowen Hu",
        "Timothy Ganter",
        "Hanieh Deilamsalehy",
        "Franck Dernoncourt",
        "Hassan Foroosh",
        "Fei Liu"
      ],
      "abstract": "As the number of recorded meetings increases, it becomes increasingly important to utilize summarization technology to create useful summaries of these recordings. However, there is a crucial lack of annotated meeting corpora for developing this technology, as it can be hard to collect meetings, especially when the topics discussed are confidential. Furthermore, meeting summaries written by experienced writers are scarce, making it hard for abstractive summarizers to produce sensible output without a reliable reference. This lack of annotated corpora has hindered the development of meeting summarization technology. In this paper, we present MeetingBank, a new benchmark dataset of city council meetings over the past decade. MeetingBank is unique among other meeting corpora due to its divide-and-conquer approach, which involves dividing professionally written meeting minutes into shorter passages and aligning them with specific segments of the meeting. This breaks down the process of summarizing a lengthy meeting into smaller, more manageable tasks. The dataset provides a new testbed of various meeting summarization systems and also allows the public to gain insight into how council decisions are made. We make the collection, including meeting video links, transcripts, reference summaries, agenda, and other metadata, publicly available to facilitate the development of better meeting summarization techniques."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 908,
      "original_url": "https://aclanthology.org/2023.acl-long.907/",
      "title": "UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective",
      "pdf_url": "https://aclanthology.org/2023.acl-long.907.pdf",
      "authors": [
        "Yang Ping",
        "JunYu Lu",
        "Ruyi Gan",
        "Junjie Wang",
        "Yuxiang Zhang",
        "Pingjian Zhang",
        "Jiaxing Zhang"
      ],
      "abstract": "We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on 14 benchmarks IE datasets with the supervised setting. The state-of-the-art performance in low-resource scenarios also verifies the transferability and effectiveness of UniEX."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 909,
      "original_url": "https://aclanthology.org/2023.acl-long.908/",
      "title": "DEplain: A German Parallel Corpus with Intralingual Translations into Plain Language for Sentence and Document Simplification",
      "pdf_url": "https://aclanthology.org/2023.acl-long.908.pdf",
      "authors": [
        "Regina Stodden",
        "Omar Momen",
        "Laura Kallmeyer"
      ],
      "abstract": "Text simplification is an intralingual translation task in which documents, or sentences of a complex source text are simplified for a target audience. The success of automatic text simplification systems is highly dependent on the quality of parallel data used for training and evaluation. To advance sentence simplification and document simplification in German, this paper presents DEplain, a new dataset of parallel, professionally written and manually aligned simplifications in plain German “plain DE” or in German: “Einfache Sprache”. DEplain consists of a news-domain (approx. 500 document pairs, approx. 13k sentence pairs) and a web-domain corpus (approx. 150 aligned documents, approx. 2k aligned sentence pairs). In addition, we are building a web harvester and experimenting with automatic alignment methods to facilitate the integration of non-aligned and to be-published parallel documents. Using this approach, we are dynamically increasing the web-domain corpus, so it is currently extended to approx. 750 document pairs and approx. 3.5k aligned sentence pairs. We show that using DEplain to train a transformer-based seq2seq text simplification model can achieve promising results. We make available the corpus, the adapted alignment methods for German, the web harvester and the trained models here: https://github.com/rstodden/DEPlain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 910,
      "original_url": "https://aclanthology.org/2023.acl-long.909/",
      "title": "A Neural Divide-and-Conquer Reasoning Framework for Image Retrieval from Linguistically Complex Text",
      "pdf_url": "https://aclanthology.org/2023.acl-long.909.pdf",
      "authors": [
        "Yunxin Li",
        "Baotian Hu",
        "Yuxin Ding",
        "Lin Ma",
        "Min Zhang"
      ],
      "abstract": "Pretrained Vision-Language Models (VLMs) have achieved remarkable performance in image retrieval from text. However, their performance drops drastically when confronted with linguistically complex texts that they struggle to comprehend. Inspired by the Divide-and-Conquer algorithm and dual-process theory, in this paper, we regard linguistically complex texts as compound proposition texts composed of multiple simple proposition sentences and propose an end-to-end Neural Divide-and-Conquer Reasoning framework, dubbed NDCR. It contains three main components: 1) Divide: a proposition generator divides the compound proposition text into simple proposition sentences and produces their corresponding representations, 2) Conquer: a pretrained VLMs-based visual-linguistic interactor achieves the interaction between decomposed proposition sentences and images, 3) Combine: a neural-symbolic reasoner combines the above reasoning states to obtain the final solution via a neural logic reasoning approach. According to the dual-process theory, the visual-linguistic interactor and neural-symbolic reasoner could be regarded as analogical reasoning System 1 and logical reasoning System 2. We conduct extensive experiments on a challenging image retrieval from contextual descriptions data set. Experimental results and analyses indicate NDCR significantly improves performance in the complex image-text reasoning problem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 911,
      "original_url": "https://aclanthology.org/2023.acl-long.910/",
      "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-long.910.pdf",
      "authors": [
        "Luyu Gao",
        "Zhuyun Dai",
        "Panupong Pasupat",
        "Anthony Chen",
        "Arun Tejasvi Chaganty",
        "Yicheng Fan",
        "Vincent Zhao",
        "Ni Lao",
        "Hongrae Lee",
        "Da-Cheng Juan",
        "Kelvin Guu"
      ],
      "abstract": "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 912,
      "original_url": "https://aclanthology.org/2023.acl-short.1/",
      "title": "Should you marginalize over possible tokenizations?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.1.pdf",
      "authors": [
        "Nadezhda Chirkova",
        "Germán Kruszewski",
        "Jos Rozen",
        "Marc Dymetman"
      ],
      "abstract": "Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5% in most cases, but that it becomes more pronounced for data with long complex words."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 913,
      "original_url": "https://aclanthology.org/2023.acl-short.2/",
      "title": "Back to Patterns: Efficient Japanese Morphological Analysis with Feature-Sequence Trie",
      "pdf_url": "https://aclanthology.org/2023.acl-short.2.pdf",
      "authors": [
        "Naoki Yoshinaga"
      ],
      "abstract": "Accurate neural models are much less efficient than non-neural models and are useless for processing billions of social media posts or handling user queries in real time with a limited budget. This study revisits the fastest pattern-based NLP methods to make them as accurate as possible, thus yielding a strikingly simple yet surprisingly accurate morphological analyzer for Japanese. The proposed method induces reliable patterns from a morphological dictionary and annotated data. Experimental results on two standard datasets confirm that the method exhibits comparable accuracy to learning-based baselines, while boasting a remarkable throughput of over 1,000,000 sentences per second on a single modern CPU. The source code is available at https://www.tkl.iis.u-tokyo.ac.jp/ynaga/jagger/"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 914,
      "original_url": "https://aclanthology.org/2023.acl-short.3/",
      "title": "Transformed Protoform Reconstruction",
      "pdf_url": "https://aclanthology.org/2023.acl-short.3.pdf",
      "authors": [
        "Young Min Kim",
        "Kalvin Chang",
        "Chenxuan Cui",
        "David R. Mortensen"
      ],
      "abstract": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 915,
      "original_url": "https://aclanthology.org/2023.acl-short.4/",
      "title": "Ellipsis-Dependent Reasoning: a New Challenge for Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.4.pdf",
      "authors": [
        "Daniel Hardt"
      ],
      "abstract": "We propose a novel challenge for large language models: ellipsis-dependent reasoning. We define several structures of paired examples, where an ellipsis example is matched to its non-ellipsis counterpart, and a question is posed which requires resolution of the ellipsis. Test results show that the best models perform well on non-elliptical examples but struggle with all but the simplest ellipsis structures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 916,
      "original_url": "https://aclanthology.org/2023.acl-short.5/",
      "title": "Bootstrapping Neural Relation and Explanation Classifiers",
      "pdf_url": "https://aclanthology.org/2023.acl-short.5.pdf",
      "authors": [
        "Zheng Tang",
        "Mihai Surdeanu"
      ],
      "abstract": "We introduce a method that self trains (or bootstraps) neural relation and explanation classifiers. Our work expands the supervised approach of CITATION, which jointly trains a relation classifier with an explanation classifier that identifies context words important for the relation at hand, to semi-supervised scenarios. In particular, our approach iteratively converts the explainable models’ outputs to rules and applies them to unlabeled text to produce new annotations. Our evaluation on the TACRED dataset shows that our method outperforms the rule-based model we started from by 15 F1 points, outperforms traditional self-training that relies just on the relation classifier by 5 F1 points, and performs comparatively with the prompt-based approach of CITATION (without requiring an additional natural language inference component)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 917,
      "original_url": "https://aclanthology.org/2023.acl-short.6/",
      "title": "A Fast Algorithm for Computing Prefix Probabilities",
      "pdf_url": "https://aclanthology.org/2023.acl-short.6.pdf",
      "authors": [
        "Franz Nowak",
        "Ryan Cotterell"
      ],
      "abstract": "Multiple algorithms are known for efficiently calculating the prefix probability of a string under a probabilistic context-free grammar (PCFG). Good algorithms for the problem have a runtime cubic in the length of the input string. However, some proposed algorithms are suboptimal with respect to the size of the grammar. This paper proposes a new speed-up of Jelinek and Lafferty’s (1991) algorithm, which runs in O(n3|N|3 + |N|4), where n is the input length and |N| is the number of non-terminals in the grammar. In contrast, our speed-up runs in O(n2|N|3 + n3|N|2)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 918,
      "original_url": "https://aclanthology.org/2023.acl-short.7/",
      "title": "Analyzing Text Representations by Measuring Task Alignment",
      "pdf_url": "https://aclanthology.org/2023.acl-short.7.pdf",
      "authors": [
        "Cesar Gonzalez-Gutierrez",
        "Audi Primadhanty",
        "Francesco Cazzaro",
        "Ariadna Quattoni"
      ],
      "abstract": "Textual representations based on pre-trained language models are key, especially in few-shot learning scenarios. What makes a representation good for text classification? Is it due to the geometric properties of the space or because it is well aligned with the task? We hypothesize the second claim. To test it, we develop a task alignment score based on hierarchical clustering that measures alignment at different levels of granularity. Our experiments on text classification validate our hypothesis by showing that task alignment can explain the classification performance of a given representation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 919,
      "original_url": "https://aclanthology.org/2023.acl-short.8/",
      "title": "Tracing Linguistic Markers of Influence in a Large Online Organisation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.8.pdf",
      "authors": [
        "Prashant Khare",
        "Ravi Shekhar",
        "Mladen Karan",
        "Stephen McQuistin",
        "Colin Perkins",
        "Ignacio Castro",
        "Gareth Tyson",
        "Patrick Healey",
        "Matthew Purver"
      ],
      "abstract": "Social science and psycholinguistic research have shown that power and status affect how people use language in a range of domains. Here, we investigate a similar question in a large, distributed, consensus-driven community with little traditional power hierarchy – the Internet Engineering Task Force (IETF), a collaborative organisation that designs internet standards. Our analysis based on lexical categories (LIWC) and BERT, shows that participants’ levels of influence can be predicted from their email text, and identify key linguistic differences (e.g., certain LIWC categories, such as “WE” are positively correlated with high-influence). We also identify the differences in language use for the same person before and after becoming influential."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 920,
      "original_url": "https://aclanthology.org/2023.acl-short.9/",
      "title": "Metaphor Detection via Explicit Basic Meanings Modelling",
      "pdf_url": "https://aclanthology.org/2023.acl-short.9.pdf",
      "authors": [
        "Yucheng Li",
        "Shun Wang",
        "Chenghua Lin",
        "Frank Guerin"
      ],
      "abstract": "One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design. While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its contextual meaning and its basic meaning, existing work does not strictly follow this principle, typically using the aggregated meaning to approximate the basic meaning of target words. In this paper, we propose a novel metaphor detection method, which models the basic meaning of the word based on literal annotation from the training set, and then compares this with the contextual meaning in a target sentence to identify metaphors. Empirical results show that our method outperforms the state-of-the-art method significantly by 1.0% in F1 score. Moreover, our performance even reaches the theoretical upper bound on the VUA18 benchmark for targets with basic annotations, which demonstrates the importance of modelling basic meanings for metaphor detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 921,
      "original_url": "https://aclanthology.org/2023.acl-short.10/",
      "title": "xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-short.10.pdf",
      "authors": [
        "Mingda Chen",
        "Kevin Heffernan",
        "Onur Çelebi",
        "Alexandre Mourachko",
        "Holger Schwenk"
      ],
      "abstract": "We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xsim, we show that xsim++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xsim++ also reports performance for different error types, offering more fine-grained feedbacks for model development."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 922,
      "original_url": "https://aclanthology.org/2023.acl-short.11/",
      "title": "Improving Low-resource Named Entity Recognition with Graph Propagated Data Augmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.11.pdf",
      "authors": [
        "Jiong Cai",
        "Shen Huang",
        "Yong Jiang",
        "Zeqi Tan",
        "Pengjun Xie",
        "Kewei Tu"
      ],
      "abstract": "Data augmentation is an effective solution to improve model performance and robustness for low-resource named entity recognition (NER). However, synthetic data often suffer from poor diversity, which leads to performance limitations. In this paper, we propose a novel Graph Propagated Data Augmentation (GPDA) framework for Named Entity Recognition (NER), leveraging graph propagation to build relationships between labeled data and unlabeled natural texts. By projecting the annotations from the labeled text to the unlabeled text, the unlabeled texts are partially labeled, which has more diversity rather than synthetic annotated data. To strengthen the propagation precision, a simple search engine built on Wikipedia is utilized to fetch related texts of labeled data and to propagate the entity labels to them in the light of the anchor links. Besides, we construct and perform experiments on a real-world low-resource dataset of the E-commerce domain, which will be publicly available to facilitate the low-resource NER research. Experimental results show that GPDA presents substantial improvements over previous data augmentation methods on multiple low-resource NER datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 923,
      "original_url": "https://aclanthology.org/2023.acl-short.12/",
      "title": "Dataset Distillation with Attention Labels for Fine-tuning BERT",
      "pdf_url": "https://aclanthology.org/2023.acl-short.12.pdf",
      "authors": [
        "Aru Maekawa",
        "Naoki Kobayashi",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "abstract": "Dataset distillation aims to create a small dataset of informative synthetic samples to rapidly train neural networks that retain the performance of the original dataset. In this paper, we focus on constructing distilled few-shot datasets for natural language processing (NLP) tasks to fine-tune pre-trained transformers. Specifically, we propose to introduce attention labels, which can efficiently distill the knowledge from the original dataset and transfer it to the transformer models via attention probabilities. We evaluated our dataset distillation methods in four various NLP tasks and demonstrated that it is possible to create distilled few-shot datasets with the attention labels, yielding impressive performances for fine-tuning BERT. Specifically, in AGNews, a four-class news classification task, our distilled few-shot dataset achieved up to 93.2% accuracy, which is 98.5% performance of the original dataset even with only one sample per class and only one gradient step."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 924,
      "original_url": "https://aclanthology.org/2023.acl-short.13/",
      "title": "Multi-Document Summarization with Centroid-Based Pretraining",
      "pdf_url": "https://aclanthology.org/2023.acl-short.13.pdf",
      "authors": [
        "Ratish Surendran Puduppully",
        "Parag Jain",
        "Nancy Chen",
        "Mark Steedman"
      ],
      "abstract": "In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research community https://github.com/ratishsp/centrum."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 925,
      "original_url": "https://aclanthology.org/2023.acl-short.14/",
      "title": "Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times",
      "pdf_url": "https://aclanthology.org/2023.acl-short.14.pdf",
      "authors": [
        "Andrea de Varda",
        "Marco Marelli"
      ],
      "abstract": "Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 926,
      "original_url": "https://aclanthology.org/2023.acl-short.15/",
      "title": "Improving Generalization in Language Model-based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-based Techniques",
      "pdf_url": "https://aclanthology.org/2023.acl-short.15.pdf",
      "authors": [
        "Daking Rai",
        "Bailin Wang",
        "Yilun Zhou",
        "Ziyu Yao"
      ],
      "abstract": "Compositional and domain generalization present significant challenges in semantic parsing, even for state-of-the-art semantic parsers based on pre-trained language models (LMs). In this study, we empirically investigate improving an LM’s generalization in semantic parsing with two simple techniques: at the token level, we introduce a token preprocessing method to preserve the semantic boundaries of tokens produced by LM tokenizers; at the sequence level, we propose to use special tokens to mark the boundaries of components aligned between input and output. Our experimental results on two text-to-SQL semantic parsing datasets show that our token preprocessing, although simple, can substantially improve the LM performance on both types of generalization, and our component boundary marking method is particularly helpful for compositional generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 927,
      "original_url": "https://aclanthology.org/2023.acl-short.16/",
      "title": "HiPool: Modeling Long Documents Using Graph Neural Networks",
      "pdf_url": "https://aclanthology.org/2023.acl-short.16.pdf",
      "authors": [
        "Irene Li",
        "Aosong Feng",
        "Dragomir Radev",
        "Rex Ying"
      ],
      "abstract": "Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens’ length. Evaluation shows our model surpasses competitive baselines by 2.6% in F1 score, and 4.8% on the longest sequence dataset. Our method is shown to outperform hierarchical sequential models with better performance and scalability, especially for longer sequences."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 928,
      "original_url": "https://aclanthology.org/2023.acl-short.17/",
      "title": "A Weakly Supervised Classifier and Dataset of White Supremacist Language",
      "pdf_url": "https://aclanthology.org/2023.acl-short.17.pdf",
      "authors": [
        "Michael Yoder",
        "Ahmad Diab",
        "David Brown",
        "Kathleen Carley"
      ],
      "abstract": "We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 929,
      "original_url": "https://aclanthology.org/2023.acl-short.18/",
      "title": "BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases",
      "pdf_url": "https://aclanthology.org/2023.acl-short.18.pdf",
      "authors": [
        "Xin Liu",
        "Muhammad Khalifa",
        "Lu Wang"
      ],
      "abstract": "Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model’s output logits. Unlike prior work, BOLT maintains the generator’s autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), BOLT demonstrates significantly improved efficiency and fluency. On sentiment control, BOLT is 7x faster than competitive baselines, and more fluent in 74.4% of the evaluation samples according to human judges."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 930,
      "original_url": "https://aclanthology.org/2023.acl-short.19/",
      "title": "mOKB6: A Multilingual Open Knowledge Base Completion Benchmark",
      "pdf_url": "https://aclanthology.org/2023.acl-short.19.pdf",
      "authors": [
        "Shubham Mittal",
        "Keshav Kolluru",
        "Soumen Chakrabarti",
        "Mausam"
      ],
      "abstract": "Automated completion of open knowledge bases (Open KBs), which are constructed from triples of the form (subject phrase, relation phrase, object phrase), obtained via open information extraction (Open IE) system, are useful for discovering novel facts that may not be directly present in the text. However, research in Open KB completion (Open KBC) has so far been limited to resource-rich languages like English. Using the latest advances in multilingual Open IE, we construct the first multilingual Open KBC dataset, called mOKB6, containing facts from Wikipedia in six languages (including English). Improvingthe previous Open KB construction pipeline by doing multilingual coreference resolution andkeeping only entity-linked triples, we create a dense Open KB. We experiment with several models for the task and observe a consistent benefit of combining languages with the help of shared embedding space as well as translations of facts. We also observe that current multilingual models struggle to remember facts seen in languages of different scripts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 931,
      "original_url": "https://aclanthology.org/2023.acl-short.20/",
      "title": "Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment",
      "pdf_url": "https://aclanthology.org/2023.acl-short.20.pdf",
      "authors": [
        "Roni Rabin",
        "Alexandre Djerbetian",
        "Roee Engelberg",
        "Lidan Hackmon",
        "Gal Elidan",
        "Reut Tsarfaty",
        "Amir Globerson"
      ],
      "abstract": "Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically. We define the task, highlight key desired aspects of a good GFQ, and propose a model that satisfies these. Finally, we provide an evaluation by human annotators of our generated questions compared against human generated ones, demonstrating competitive performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 932,
      "original_url": "https://aclanthology.org/2023.acl-short.21/",
      "title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts",
      "pdf_url": "https://aclanthology.org/2023.acl-short.21.pdf",
      "authors": [
        "Skyler Hallinan",
        "Alisa Liu",
        "Yejin Choi",
        "Maarten Sap"
      ],
      "abstract": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo’s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 933,
      "original_url": "https://aclanthology.org/2023.acl-short.22/",
      "title": "A Natural Bias for Language Generation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.22.pdf",
      "authors": [
        "Clara Meister",
        "Wojciech Stokowiec",
        "Tiago Pimentel",
        "Lei Yu",
        "Laura Rimell",
        "Adhiguna Kuncoro"
      ],
      "abstract": "After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model’s final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall performance; and perhaps most importantly (iii) appears to disentangle strong frequency effects by encouraging the model to specialise in non-frequency-related aspects of language."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 934,
      "original_url": "https://aclanthology.org/2023.acl-short.23/",
      "title": "Simple Augmentations of Logical Rules for Neuro-Symbolic Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.acl-short.23.pdf",
      "authors": [
        "Ananjan Nandi",
        "Navdeep Kaur",
        "Parag Singla",
        "Mausam"
      ],
      "abstract": "High-quality and high-coverage rule sets are imperative to the success of Neuro-Symbolic Knowledge Graph Completion (NS-KGC) models, because they form the basis of all symbolic inferences. Recent literature builds neural models for generating rule sets, however, preliminary experiments show that they struggle with maintaining high coverage. In this work, we suggest three simple augmentations to existing rule sets: (1) transforming rules to their abductive forms, (2) generating equivalent rules that use inverse forms of constituent relations and (3) random walks that propose new rules. Finally, we prune potentially low quality rules. Experiments over four datasets and five ruleset-baseline settings suggest that these simple augmentations consistently improve results, and obtain up to 7.1 pt MRR and 8.5 pt Hits@1 gains over using rules without augmentations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 935,
      "original_url": "https://aclanthology.org/2023.acl-short.24/",
      "title": "Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer",
      "pdf_url": "https://aclanthology.org/2023.acl-short.24.pdf",
      "authors": [
        "Xingtai Lv",
        "Ning Ding",
        "Yujia Qin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Recent studies show that large-scale pre-trained language models could be efficaciously adapted to particular tasks in a parameter-efficient manner. The trained lightweight set of parameters, such as adapters, can be easily stored and shared as a capability equipped with the corresponding models. Owning many lightweight parameters, we focus on transferring them between tasks to acquire an improvement in performance of new tasks, the key point of which is to obtain the similarity between tasks. In this paper, we explore 5 parameter-efficient weight ensembling methods to achieve such transferability and verify the effectiveness of them. These methods extract the information of datasets and trained lightweight parameters from different perspectives to obtain the similarity between tasks, and weight the existing lightweight parameters according to the comparability to acquire a suitable module for the initialization of new tasks. We apply them to three parameter-efficient tuning methods and test them on a wide set of downstream tasks. Experimental results show that our methods show an improvement of 5%~8% over baselines and could largely facilitate task-level knowledge transfer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 936,
      "original_url": "https://aclanthology.org/2023.acl-short.25/",
      "title": "Faithfulness Tests for Natural Language Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-short.25.pdf",
      "authors": [
        "Pepa Atanasova",
        "Oana-Maria Camburu",
        "Christina Lioma",
        "Thomas Lukasiewicz",
        "Jakob Grue Simonsen",
        "Isabelle Augenstein"
      ],
      "abstract": "Explanations of neural models aim to reveal a model’s decision-making process for its predictions. However, recent work shows that current methods giving explanations such as saliency maps or counterfactuals can be misleading, as they are prone to present reasons that are unfaithful to the model’s inner workings. This work explores the challenging question of evaluating the faithfulness of natural language explanations (NLEs). To this end, we present two tests. First, we propose a counterfactual input editor for inserting reasons that lead to counterfactual predictions but are not reflected by the NLEs. Second, we reconstruct inputs from the reasons stated in the generated NLEs and check how often they lead to the same predictions. Our tests can evaluate emerging NLE models, proving a fundamental tool in the development of faithful NLEs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 937,
      "original_url": "https://aclanthology.org/2023.acl-short.26/",
      "title": "COGEN: Abductive Commonsense Language Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.26.pdf",
      "authors": [
        "Rohola Zandie",
        "Diwanshu Shekhar",
        "Mohammad Mahoor"
      ],
      "abstract": "Reasoning is one of the most important elements in achieving Artificial General Intelligence (AGI), specifically when it comes to Abductive and counterfactual reasoning. In order to introduce these capabilities of reasoning in Natural Language Processing (NLP) models, there have been recent advances towards training NLP models to better perform on two main tasks - Abductive Natural Language Inference (alphaNLI) and Abductive Natural Language Generation Task (alphaNLG). This paper proposes CoGen, a model for both alphaNLI and alphaNLG tasks that employ a novel approach of combining the temporal commonsense reasoning for each observation (before and after a real hypothesis) from pre-trained models with contextual filtering for training. Additionally, we use state-of-the-art semantic entailment to filter out the contradictory hypothesis during the inference. Our experimental results show that CoGen outperforms current models and set a new state of the art in regards to alphaNLI and alphaNLG tasks. We make the source code of CoGen model publicly available for reproducibility and to facilitate relevant future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 938,
      "original_url": "https://aclanthology.org/2023.acl-short.27/",
      "title": "Multimodal Relation Extraction with Cross-Modal Retrieval and Synthesis",
      "pdf_url": "https://aclanthology.org/2023.acl-short.27.pdf",
      "authors": [
        "Xuming Hu",
        "Zhijiang Guo",
        "Zhiyang Teng",
        "Irwin King",
        "Philip S. Yu"
      ],
      "abstract": "Multimodal relation extraction (MRE) is the task of identifying the semantic relationships between two entities based on the context of the sentence image pair. Existing retrieval-augmented approaches mainly focused on modeling the retrieved textual knowledge, but this may not be able to accurately identify complex relations. To improve the prediction, this research proposes to retrieve textual and visual evidence based on the object, sentence, and whole image. We further develop a novel approach to synthesize the object-level, image-level, and sentence-level information for better reasoning between the same and different modalities. Extensive experiments and analyses show that the proposed method is able to effectively select and compare evidence across modalities and significantly outperforms state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 939,
      "original_url": "https://aclanthology.org/2023.acl-short.28/",
      "title": "Characterization of Stigmatizing Language in Medical Records",
      "pdf_url": "https://aclanthology.org/2023.acl-short.28.pdf",
      "authors": [
        "Keith Harrigian",
        "Ayah Zirikly",
        "Brant Chee",
        "Alya Ahmad",
        "Anne Links",
        "Somnath Saha",
        "Mary Catherine Beach",
        "Mark Dredze"
      ],
      "abstract": "Widespread disparities in clinical outcomes exist between different demographic groups in the United States. A new line of work in medical sociology has demonstrated physicians often use stigmatizing language in electronic medical records within certain groups, such as black patients, which may exacerbate disparities. In this study, we characterize these instances at scale using a series of domain-informed NLP techniques. We highlight important differences between this task and analogous bias-related tasks studied within the NLP community (e.g., classifying microaggressions). Our study establishes a foundation for NLP researchers to contribute timely insights to a problem domain brought to the forefront by recent legislation regarding clinical documentation transparency. We release data, code, and models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 940,
      "original_url": "https://aclanthology.org/2023.acl-short.29/",
      "title": "Abstractive Summarizers are Excellent Extractive Summarizers",
      "pdf_url": "https://aclanthology.org/2023.acl-short.29.pdf",
      "authors": [
        "Daniel Varab",
        "Yumo Xu"
      ],
      "abstract": "Extractive and abstractive summarization designs have historically been fragmented, limiting the benefits that often arise from compatible model architectures. In this paper, we explore the potential synergies of modeling extractive summarization with an abstractive summarization system and propose three novel inference algorithms using the sequence-to-sequence architecture. We evaluate them on the CNN & Dailymail dataset and show that recent advancements in abstractive system designs enable abstractive systems to not only compete, but even surpass the performance of extractive systems with custom architectures. To our surprise, abstractive systems achieve this without being exposed to extractive oracle summaries and, therefore, for the first time allow a single model to produce both abstractive and extractive summaries. This evidence questions our fundamental understanding of extractive system design, and the necessity for extractive labels while pathing the way for promising research directions in hybrid models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 941,
      "original_url": "https://aclanthology.org/2023.acl-short.30/",
      "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
      "pdf_url": "https://aclanthology.org/2023.acl-short.30.pdf",
      "authors": [
        "Himanshu Thakur",
        "Atishay Jain",
        "Praneetha Vaddamanu",
        "Paul Pu Liang",
        "Louis-Philippe Morency"
      ],
      "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 942,
      "original_url": "https://aclanthology.org/2023.acl-short.31/",
      "title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English",
      "pdf_url": "https://aclanthology.org/2023.acl-short.31.pdf",
      "authors": [
        "Jianfeng Chi",
        "Wasi Uddin Ahmad",
        "Yuan Tian",
        "Kai-Wei Chang"
      ],
      "abstract": "Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks. The code and models are released at https://github.com/JFChi/PLUE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 943,
      "original_url": "https://aclanthology.org/2023.acl-short.32/",
      "title": "Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-short.32.pdf",
      "authors": [
        "Yasmine Karoui",
        "Rémi Lebret",
        "Negar Foroutan Eghlidi",
        "Karl Aberer"
      ],
      "abstract": "Vision-Language Pre-training (VLP) has advanced the performance of many vision-language tasks, such as image-text retrieval, visual entailment, and visual reasoning. The pre-training mostly utilizes lexical databases and image queries in English. Previous work has demonstrated that the pre-training in English does not transfer well to other languages in a zero-shot setting. However, multilingual pre-trained language models (MPLM) have excelled at a variety of single-modal language tasks. In this paper, we propose a simple yet efficient approach to adapt VLP to unseen languages using MPLM.We utilize a cross-lingual contextualised token embeddings alignment approach to train text encoders for non-English languages. Our approach does not require image input and primarily uses machine translation, eliminating the need for target language data. Our evaluation across three distinct tasks (image-text retrieval, visual entailment, and natural language visual reasoning) demonstrates that this approach outperforms the state-of-the-art multilingual vision-language models without requiring large parallel corpora. Our code is available at https://github.com/Yasminekaroui/CliCoTea."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 944,
      "original_url": "https://aclanthology.org/2023.acl-short.33/",
      "title": "BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.33.pdf",
      "authors": [
        "Jie He",
        "Simon U",
        "Victor Gutierrez-Basulto",
        "Jeff Pan"
      ],
      "abstract": "Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 945,
      "original_url": "https://aclanthology.org/2023.acl-short.34/",
      "title": "Nichelle and Nancy: The Influence of Demographic Attributes and Tokenization Length on First Name Biases",
      "pdf_url": "https://aclanthology.org/2023.acl-short.34.pdf",
      "authors": [
        "Haozhe An",
        "Rachel Rudinger"
      ],
      "abstract": "Through the use of first name substitution experiments, prior research has demonstrated the tendency of social commonsense reasoning models to systematically exhibit social biases along the dimensions of race, ethnicity, and gender (An et al., 2023). Demographic attributes of first names, however, are strongly correlated with corpus frequency and tokenization length, which may influence model behavior independent of or in addition to demographic factors. In this paper, we conduct a new series of first name substitution experiments that measures the influence of these factors while controlling for the others. We find that demographic attributes of a name (race, ethnicity, and gender) and name tokenization length are both factors that systematically affect the behavior of social commonsense reasoning models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 946,
      "original_url": "https://aclanthology.org/2023.acl-short.35/",
      "title": "Improving Syntactic Probing Correctness and Robustness with Control Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-short.35.pdf",
      "authors": [
        "Weicheng Ma",
        "Brian Wang",
        "Hefan Zhang",
        "Lili Wang",
        "Rolando Coto-Solano",
        "Saeed Hassanpour",
        "Soroush Vosoughi"
      ],
      "abstract": "Syntactic probing methods have been used to examine whether and how pre-trained language models (PLMs) encode syntactic features. However, the probing methods are usually biased by the PLMs’ memorization of common word co-occurrences, even if they do not form syntactic relations. This paper presents a random-word-substitution and random-label-matching control task to reduce these biases and improve the robustness of syntactic probing methods. Our control tasks are also shown to notably improve the consistency of probing results between different probing methods and make the methods more robust with respect to the text attributes of the probing instances. Our control tasks make syntactic probing methods better at reconstructing syntactic features and more generalizable to unseen text domains. Our experiments show that our proposed control tasks are effective on different PLMs, probing methods, and syntactic features."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 947,
      "original_url": "https://aclanthology.org/2023.acl-short.36/",
      "title": "Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications",
      "pdf_url": "https://aclanthology.org/2023.acl-short.36.pdf",
      "authors": [
        "Jatin Arora",
        "Youngja Park"
      ],
      "abstract": "In this work, we address the NER problem by splitting it into two logical sub-tasks: (1) Span Detection which simply extracts entity mention spans irrespective of entity type; (2) Span Classification which classifies the spans into their entity types. Further, we formulate both sub-tasks as question-answering (QA) problems and produce two leaner models which can be optimized separately for each sub-task. Experiments with four cross-domain datasets demonstrate that this two-step approach is both effective and time efficient. Our system, SplitNER outperforms baselines on OntoNotes5.0, WNUT17 and a cybersecurity dataset and gives on-par performance on BioNLP13CG. In all cases, it achieves a significant reduction in training time compared to its QA baseline counterpart. The effectiveness of our system stems from fine-tuning the BERT model twice, separately for span detection and classification. The source code can be found at https://github.com/c3sr/split-ner."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 948,
      "original_url": "https://aclanthology.org/2023.acl-short.37/",
      "title": "Credible without Credit: Domain Experts Assess Generative Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.37.pdf",
      "authors": [
        "Denis Peskoff",
        "Brandon Stewart"
      ],
      "abstract": "Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy, and sourcing of two language models across 100 expert-written questions. While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 949,
      "original_url": "https://aclanthology.org/2023.acl-short.38/",
      "title": "Grokking of Hierarchical Structure in Vanilla Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-short.38.pdf",
      "authors": [
        "Shikhar Murty",
        "Pratyusha Sharma",
        "Jacob Andreas",
        "Christopher Manning"
      ],
      "abstract": "For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods—far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 950,
      "original_url": "https://aclanthology.org/2023.acl-short.39/",
      "title": "Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data",
      "pdf_url": "https://aclanthology.org/2023.acl-short.39.pdf",
      "authors": [
        "Ujan Deb",
        "Ridayesh Parab",
        "Preethi Jyothi"
      ],
      "abstract": "Adapters have emerged as a parameter-efficient Transformer-based framework for cross-lingual transfer by inserting lightweight language-specific modules (language adapters) and task-specific modules (task adapters) within pretrained multilingual models. Zero-shot transfer is enabled by pairing the language adapter in the target language with an appropriate task adapter in a source language. If our target languages are known apriori, we explore how zero-shot transfer can be further improved within the adapter framework by utilizing unlabeled text during task-specific finetuning. We construct language-specific subspaces using standard linear algebra constructs and selectively project source-language representations into the target language subspace during task-specific finetuning using two schemes. Our experiments on three cross-lingual tasks, Named Entity Recognition (NER), Question Answering (QA) and Natural Language Inference (NLI) yield consistent benefits compared to adapter baselines over a wide variety of target languages with up to 11% relative improvement in NER, 2% relative improvement in QA and 5% relative improvement in NLI."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 951,
      "original_url": "https://aclanthology.org/2023.acl-short.40/",
      "title": "Context-Aware Transformer Pre-Training for Answer Sentence Selection",
      "pdf_url": "https://aclanthology.org/2023.acl-short.40.pdf",
      "authors": [
        "Luca Di Liello",
        "Siddhant Garg",
        "Alessandro Moschitti"
      ],
      "abstract": "Answer Sentence Selection (AS2) is a core component for building an accurate Question Answering pipeline. AS2 models rank a set of candidate sentences based on how likely they answer a given question. The state of the art in AS2 exploits pre-trained transformers by transferring them on large annotated datasets, while using local contextual information around the candidate sentence. In this paper, we propose three pre-training objectives designed to mimic the downstream fine-tuning task of contextual AS2. This allows for specializing LMs when fine-tuning for contextual AS2. Our experiments on three public and two large-scale industrial datasets show that our pre-training approaches (applied to RoBERTa and ELECTRA) can improve baseline contextual AS2 accuracy by up to 8% on some datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 952,
      "original_url": "https://aclanthology.org/2023.acl-short.41/",
      "title": "Toward Expanding the Scope of Radiology Report Summarization to Multiple Anatomies and Modalities",
      "pdf_url": "https://aclanthology.org/2023.acl-short.41.pdf",
      "authors": [
        "Zhihong Chen",
        "Maya Varma",
        "Xiang Wan",
        "Curtis Langlotz",
        "Jean-Benoit Delbrouck"
      ],
      "abstract": "Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations. First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 953,
      "original_url": "https://aclanthology.org/2023.acl-short.42/",
      "title": "Efficient Diagnosis Assignment Using Unstructured Clinical Notes",
      "pdf_url": "https://aclanthology.org/2023.acl-short.42.pdf",
      "authors": [
        "Louis Blankemeier",
        "Jason Fries",
        "Robert Tinn",
        "Joseph Preston",
        "Nigam Shah",
        "Akshay Chaudhari"
      ],
      "abstract": "Electronic phenotyping entails using electronic health records (EHRs) to identify patients with specific health outcomes and determine when those outcomes occurred. Unstructured clinical notes, which contain a vast amount of information, are a valuable resource for electronic phenotyping. However, traditional methods, such as rule-based labeling functions or neural networks, require significant manual effort to tune and may not generalize well to multiple indications. To address these challenges, we propose HyDE (hybrid diagnosis extractor). HyDE is a simple framework for electronic phenotyping that integrates labeling functions and a disease-agnostic neural network to assign diagnoses to patients. By training HyDE’s model to correct predictions made by labeling functions, we are able to disambiguate hypertension true positives and false positives with a supervised area under the precision-recall curve (AUPRC) of 0.85. We extend this hypertension-trained model to zero-shot evaluation of four other diseases, generating AUPRC values ranging from 0.82 - 0.95 and outperforming a labeling function baseline by 44 points in F1 score and a Word2Vec baseline by 24 points in F1 score on average. Furthermore, we demonstrate a speedup of >4x by pruning the length of inputs into our language model to ~2.3% of the full clinical notes, with negligible impact to the AUPRC. HyDE has the potential to improve the efficiency and efficacy of interpreting large-scale unstructured clinical notes for accurate EHR phenotyping."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 954,
      "original_url": "https://aclanthology.org/2023.acl-short.43/",
      "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.43.pdf",
      "authors": [
        "Masoud Monajatipoor",
        "Liunian Harold Li",
        "Mozhdeh Rouhsedaghat",
        "Lin Yang",
        "Kai-Wei Chang"
      ],
      "abstract": "Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 955,
      "original_url": "https://aclanthology.org/2023.acl-short.44/",
      "title": "On the Interpretability and Significance of Bias Metrics in Texts: a PMI-based Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-short.44.pdf",
      "authors": [
        "Francisco Valentini",
        "Germán Rosati",
        "Damián Blasi",
        "Diego Fernandez Slezak",
        "Edgar Altszyler"
      ],
      "abstract": "In recent years, word embeddings have been widely used to measure biases in texts. Even if they have proven to be effective in detecting a wide variety of biases, metrics based on word embeddings lack transparency and interpretability. We analyze an alternative PMI-based metric to quantify biases in texts. It can be expressed as a function of conditional probabilities, which provides a simple interpretation in terms of word co-occurrences. We also prove that it can be approximated by an odds ratio, which allows estimating confidence intervals and statistical significance of textual biases. This approach produces similar results to metrics based on word embeddings when capturing gender gaps of the real world embedded in large corpora."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 956,
      "original_url": "https://aclanthology.org/2023.acl-short.45/",
      "title": "Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.45.pdf",
      "authors": [
        "Ehsan Doostmohammadi",
        "Tobias Norlund",
        "Marco Kuhlmann",
        "Richard Johansson"
      ],
      "abstract": "Augmenting language models with a retrieval mechanism has been shown to significantly improve their performance while keeping the number of parameters low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism based on the similarity between dense representations of the query chunk and potential neighbors. In this paper, we study the state-of-the-art Retro model and observe that its performance gain is better explained by surface-level similarities, such as token overlap. Inspired by this, we replace the semantic retrieval in Retro with a surface-level method based on BM25, obtaining a significant reduction in perplexity. As full BM25 retrieval can be computationally costly for large datasets, we also apply it in a re-ranking scenario, gaining part of the perplexity reduction with minimal computational overhead."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 957,
      "original_url": "https://aclanthology.org/2023.acl-short.46/",
      "title": "MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents",
      "pdf_url": "https://aclanthology.org/2023.acl-short.46.pdf",
      "authors": [
        "Anastasiia Razdaibiedina",
        "Aleksandr Brechalov"
      ],
      "abstract": "Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns highquality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 958,
      "original_url": "https://aclanthology.org/2023.acl-short.47/",
      "title": "KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations",
      "pdf_url": "https://aclanthology.org/2023.acl-short.47.pdf",
      "authors": [
        "Myeongjun Jang",
        "Bodhisattwa Prasad Majumder",
        "Julian McAuley",
        "Thomas Lukasiewicz",
        "Oana-Maria Camburu"
      ],
      "abstract": "While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not necessarily generate fewer inconsistencies. Moreover, we propose an off-the-shelf mitigation method to alleviate inconsistencies by grounding the model into external background knowledge. Our method decreases the inconsistencies of previous high-performing NLE models as detected by our attack."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 959,
      "original_url": "https://aclanthology.org/2023.acl-short.48/",
      "title": "Measuring the Effect of Influential Messages on Varying Personas",
      "pdf_url": "https://aclanthology.org/2023.acl-short.48.pdf",
      "authors": [
        "Chenkai Sun",
        "Jinning Li",
        "Hou Pong Chan",
        "ChengXiang Zhai",
        "Heng Ji"
      ],
      "abstract": "Predicting how a user responds to news events enables important applications such as allowing intelligent agents or content producers to estimate the effect on different communities and revise unreleased messages to prevent unexpected bad outcomes such as social conflict and moral injury. We present a new task, Response Forecasting on Personas for News Media, to estimate the response a persona (characterizing an individual or a group) might have upon seeing a news message. Compared to the previous efforts which only predict generic comments to news, the proposed task not only introduces personalization in the modeling but also predicts the sentiment polarity and intensity of each response. This enables more accurate and comprehensive inference on the mental state of the persona. Meanwhile, the generated sentiment dimensions make the evaluation and application more reliable. We create the first benchmark dataset, which consists of 13,357 responses to 3,847 news headlines from Twitter. We further evaluate the SOTA neural language models with our dataset. The empirical results suggest that the included persona attributes are helpful for the performance of all response dimensions. Our analysis shows that the best-performing models are capable of predicting responses that are consistent with the personas, and as a byproduct, the task formulation also enables many interesting applications in the analysis of social network groups and their opinions, such as the discovery of extreme opinion groups."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 960,
      "original_url": "https://aclanthology.org/2023.acl-short.49/",
      "title": "Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity",
      "pdf_url": "https://aclanthology.org/2023.acl-short.49.pdf",
      "authors": [
        "Hongwei Wang",
        "Dong Yu"
      ],
      "abstract": "Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability. This paper presents a token-level matching inference algorithm, which can be applied on top of any language model to improve its performance on STS task. Our method calculates pairwise token-level similarity and token matching scores, and then aggregates them with pretrained token weights to produce sentence similarity. Experimental results on seven STS datasets show that our method improves the performance of almost all language models, with up to 12.7% gain in Spearman’s correlation. We also demonstrate that our method is highly explainable and computationally efficient."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 961,
      "original_url": "https://aclanthology.org/2023.acl-short.50/",
      "title": "Robust Learning for Multi-party Addressee Recognition with Discrete Addressee Codebook",
      "pdf_url": "https://aclanthology.org/2023.acl-short.50.pdf",
      "authors": [
        "Pengcheng Zhu",
        "Wei Zhou",
        "Kuncai Zhang",
        "Yuankai Ma",
        "Haiqing Chen"
      ],
      "abstract": "Addressee recognition aims to identify addressees in multi-party conversations. While state-of-the-art addressee recognition models have achieved promising performance, they still suffer from the issue of robustness when applied in real-world scenes. When exposed to a noisy environment, these models regard the noise as input and identify the addressee in a pre-given addressee closed set, while the addressees of the noise do not belong to this closed set, thus leading to the wrong identification of addressee. To this end, we propose a Robust Addressee Recognition (RAR) method, which discrete the addressees into a character codebook, making it able to represent open set addressees and robust in a noisy environment. Experimental results show that the introduction of the addressee character codebook helps to represent the open set addressees and highly improves the robustness of addressee recognition even if the input is noise."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 962,
      "original_url": "https://aclanthology.org/2023.acl-short.51/",
      "title": "TwistList: Resources and Baselines for Tongue Twister Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.51.pdf",
      "authors": [
        "Tyler Loakman",
        "Chen Tang",
        "Chenghua Lin"
      ],
      "abstract": "Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present TwistList, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance ofexisting mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is challenging for models under these conditions, yet some models are still capable of generating acceptable examples of this language type."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 963,
      "original_url": "https://aclanthology.org/2023.acl-short.52/",
      "title": "Substitution-based Semantic Change Detection using Contextual Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-short.52.pdf",
      "authors": [
        "Dallas Card"
      ],
      "abstract": "Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 964,
      "original_url": "https://aclanthology.org/2023.acl-short.53/",
      "title": "Probing Physical Reasoning with Counter-Commonsense Context",
      "pdf_url": "https://aclanthology.org/2023.acl-short.53.pdf",
      "authors": [
        "Kazushi Kondo",
        "Saku Sugawara",
        "Akiko Aizawa"
      ],
      "abstract": "In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not. This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates. We measure the ability of several masked language models and encoder-decoder models. The results show that while large language models can use prepositions such as “in” and “into” in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 965,
      "original_url": "https://aclanthology.org/2023.acl-short.54/",
      "title": "Morphological Inflection with Phonological Features",
      "pdf_url": "https://aclanthology.org/2023.acl-short.54.pdf",
      "authors": [
        "David Guriel",
        "Omer Goldman",
        "Reut Tsarfaty"
      ],
      "abstract": "Recent years have brought great advances into solving morphological tasks, mostly due to powerful neural models applied to various tasks as (re)inflection and analysis. Yet, such morphological tasks cannot be considered solved, especially when little training data is available or when generalizing to previously unseen lemmas. This work explores effects on performance obtained through various ways in which morphological models get access to sub-character phonological features that are often the targets of morphological processes. We design two methods to achieve this goal: one that leaves models as is but manipulates the data to include features instead of characters, and another that manipulates models to take phonological features into account when building representations for phonemes. We elicit phonemic data from standard graphemic data using language-specific grammars for languages with shallow grapheme-to-phoneme mapping, and we experiment with two reinflection models over eight languages. Our results show that our methods yield comparable results to the grapheme-based baseline overall, with minor improvements in some of the languages. All in all, we conclude that patterns in character distributions are likely to allow models to infer the underlying phonological characteristics, even when phonemes are not explicitly represented."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 966,
      "original_url": "https://aclanthology.org/2023.acl-short.55/",
      "title": "A Holistic Approach to Reference-Free Evaluation of Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.55.pdf",
      "authors": [
        "Hanming Wu",
        "Wenjuan Han",
        "Hui Di",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "Traditional machine translation evaluation relies on reference written by humans. While reference-free evaluation gets rid of the constraints of labor-intensive annotations, which can pivot easily to new domains and is more scalable. In this paper, we propose a reference-free evaluation approach that characterizes evaluation as two aspects: (1) fluency: how well the translated text conforms to normal human language usage; (2) faithfulness: how well the translated text reflects the source data. We further split the faithfulness into word-level and sentence-level. Extensive experiments spanning WMT18/19/21 Metrics segment-level daRR and MQM datasets demonstrate that our proposed reference-free approach, ReFreeEval, outperforms SOTA reference-fee metrics like YiSi-2."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 967,
      "original_url": "https://aclanthology.org/2023.acl-short.56/",
      "title": "Balancing Lexical and Semantic Quality in Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.acl-short.56.pdf",
      "authors": [
        "Jeewoo Sul",
        "Yong Suk Choi"
      ],
      "abstract": "An important problem of the sequence-to-sequence neural models widely used in abstractive summarization is exposure bias. To alleviate this problem, re-ranking systems have been applied in recent years. Despite some performance improvements, this approach remains underexplored. Previous works have mostly specified the rank through the ROUGE score and aligned candidate summaries, but there can be quite a large gap between the lexical overlap metric and semantic similarity. In this paper, we propose a novel training method in which a re-ranker balances the lexical and semantic quality. We further newly define false positives in ranking and present a strategy to reduce their influence. Experiments on the CNN/DailyMail and XSum datasets show that our method can estimate the meaning of summaries without seriously degrading the lexical aspect. More specifically, it achieves an 89.67 BERTScore on the CNN/DailyMail dataset, reaching new state-of-the-art performance. Our code is publicly available at https://github.com/jeewoo1025/BalSum."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 968,
      "original_url": "https://aclanthology.org/2023.acl-short.57/",
      "title": "Learning Neuro-Symbolic World Models with Conversational Proprioception",
      "pdf_url": "https://aclanthology.org/2023.acl-short.57.pdf",
      "authors": [
        "Don Joven Agravante",
        "Daiki Kimura",
        "Michiaki Tatsubori",
        "Asim Munawar",
        "Alexander Gray"
      ],
      "abstract": "The recent emergence of Neuro-Symbolic Agent (NeSA) approaches to natural language-based interactions calls for the investigation of model-based approaches. In contrast to model-free approaches, which existing NeSAs take, learning an explicit world model has an interesting potential especially in the explainability, which is one of the key selling points of NeSA. To learn useful world models, we leverage one of the recent neuro-symbolic architectures, Logical Neural Networks (LNN). Here, we describe a method that can learn neuro-symbolic world models on the TextWorld-Commonsense set of games. We then show how this can be improved further by taking inspiration from the concept of proprioception, but for conversation. This is done by enhancing the internal logic state with a memory of previous actions while also guiding future actions by augmenting the learned model with constraints based on this memory. This greatly improves the game-solving agents performance in a TextWorld setting, where the advantage over the baseline is an 85% average steps reduction and x2.3 average score."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 969,
      "original_url": "https://aclanthology.org/2023.acl-short.58/",
      "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
      "pdf_url": "https://aclanthology.org/2023.acl-short.58.pdf",
      "authors": [
        "Yahan Yang",
        "Soham Dan",
        "Dan Roth",
        "Insup Lee"
      ],
      "abstract": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 970,
      "original_url": "https://aclanthology.org/2023.acl-short.59/",
      "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.59.pdf",
      "authors": [
        "Amirhossein Abaskohi",
        "Sascha Rothe",
        "Yadollah Yaghoobzadeh"
      ],
      "abstract": "In recent years, there has been significant progress in developing pre-trained language models for NLP. However, these models often struggle when fine-tuned on small datasets. To address this issue, researchers have proposed various adaptation approaches. Prompt-based tuning is arguably the most common way, especially for larger models. Previous research shows that adding contrastive learning to prompt-based fine-tuning is effective as it helps the model generate embeddings that are more distinguishable between classes, and it can also be more sample-efficient as the model learns from positive and negative examples simultaneously. One of the most important components of contrastive learning is data augmentation, but unlike computer vision, effective data augmentation for NLP is still challenging. This paper proposes LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language Models, which leverages prompt-based few-shot paraphrasing using generative language models, especially large language models such as GPT-3 and OPT-175B, for data augmentation. Our experiments on multiple text classification benchmarks show that this augmentation method outperforms other methods, such as easy data augmentation, back translation, and multiple templates."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 971,
      "original_url": "https://aclanthology.org/2023.acl-short.60/",
      "title": "Considerations for meaningful sign language machine translation based on glosses",
      "pdf_url": "https://aclanthology.org/2023.acl-short.60.pdf",
      "authors": [
        "Mathias Müller",
        "Zifan Jiang",
        "Amit Moryossef",
        "Annette Rios",
        "Sarah Ebling"
      ],
      "abstract": "Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation. To address these issues, we put forward concrete recommendations for future research on gloss translation. Our suggestions advocate awareness of the inherent limitations of gloss-based approaches, realistic datasets, stronger baselines and convincing evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 972,
      "original_url": "https://aclanthology.org/2023.acl-short.61/",
      "title": "Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature",
      "pdf_url": "https://aclanthology.org/2023.acl-short.61.pdf",
      "authors": [
        "Daniel Sosa",
        "Malavika Suresh",
        "Christopher Potts",
        "Russ Altman"
      ],
      "abstract": "The COVID-19 pandemic created a deluge of questionable and contradictory scientific claims about drug efficacy – an “infodemic” with lasting consequences for science and society. In this work, we argue that NLP models can help domain experts distill and understand the literature in this complex, high-stakes area. Our task is to automatically identify contradictory claims about COVID-19 drug efficacy. We frame this as a natural language inference problem and offer a new NLI dataset created by domain experts. The NLI framing allows us to create curricula combining existing datasets and our own. The resulting models are useful investigative tools. We provide a case study of how these models help a domain expert summarize and assess evidence concerning remdisivir and hydroxychloroquine."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 973,
      "original_url": "https://aclanthology.org/2023.acl-short.62/",
      "title": "The Role of Global and Local Context in Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-short.62.pdf",
      "authors": [
        "Arthur Amalvy",
        "Vincent Labatut",
        "Richard Dufour"
      ],
      "abstract": "Pre-trained transformer-based models have recently shown great performance when applied to Named Entity Recognition (NER). As the complexity of their self-attention mechanism prevents them from processing long documents at once, these models are usually applied in a sequential fashion. Such an approach unfortunately only incorporates local context and prevents leveraging global document context in long documents such as novels, which might hinder performance. In this article, we explore the impact of global document context, and its relationships with local context. We find that correctly retrieving global document context has a greater impact on performance than only leveraging local context, prompting for further research on how to better retrieve that context."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 974,
      "original_url": "https://aclanthology.org/2023.acl-short.63/",
      "title": "Joint End-to-end Semantic Proto-role Labeling",
      "pdf_url": "https://aclanthology.org/2023.acl-short.63.pdf",
      "authors": [
        "Elizabeth Spaulding",
        "Gary Kazantsev",
        "Mark Dredze"
      ],
      "abstract": "Semantic proto-role labeling (SPRL) assigns properties to arguments based on a series of binary labels. While multiple studies have evaluated various approaches to SPRL, it has only been studied in-depth as a standalone task using gold predicate/argument pairs. How do SPRL systems perform as part of an information extraction pipeline? We model SPRL jointly with predicate-argument extraction using a deep transformer model. We find that proto-role labeling is surprisingly robust in this setting, with only a small decrease when using predicted arguments. We include a detailed analysis of each component of the joint system, and an error analysis to understand correlations in errors between system stages. Finally, we study the effects of annotation errors on SPRL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 975,
      "original_url": "https://aclanthology.org/2023.acl-short.64/",
      "title": "Improving Automatic Quotation Attribution in Literary Novels",
      "pdf_url": "https://aclanthology.org/2023.acl-short.64.pdf",
      "authors": [
        "Krishnapriya Vishnubhotla",
        "Frank Rudzicz",
        "Graeme Hirst",
        "Adam Hammond"
      ],
      "abstract": "Current models for quotation attribution in literary novels assume varying levels of available information in their training and test data, which poses a challenge for in-the-wild inference. Here, we approach quotation attribution as a set of four interconnected sub-tasks: character identification, coreference resolution, quotation identification, and speaker attribution. We benchmark state-of-the-art models on each of these sub-tasks independently, using a large dataset of annotated coreferences and quotations in literary novels (the Project Dialogism Novel Corpus). We also train and evaluate models for the speaker attribution task in particular, showing that a simple sequential prediction model achieves accuracy scores on par with state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 976,
      "original_url": "https://aclanthology.org/2023.acl-short.65/",
      "title": "Modular Visual Question Answering via Code Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.65.pdf",
      "authors": [
        "Sanjay Subramanian",
        "Medhini Narasimhan",
        "Kushal Khangaonkar",
        "Kevin Yang",
        "Arsha Nagrani",
        "Cordelia Schmid",
        "Andy Zeng",
        "Trevor Darrell",
        "Dan Klein"
      ],
      "abstract": "We present a framework that formulates visual question answering as modular code generation. In contrast to prior work on modular approaches to VQA, our approach requires no additional training and relies on pre-trained language models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning. The generated Python programs invoke and compose the outputs of the visual models using arithmetic and conditional logic. Our approach improves accuracy on the COVR dataset by at least 3% and on the GQA dataset by 2% compared to the few-shot baseline that does not employ code generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 977,
      "original_url": "https://aclanthology.org/2023.acl-short.66/",
      "title": "Target-Based Offensive Language Identification",
      "pdf_url": "https://aclanthology.org/2023.acl-short.66.pdf",
      "authors": [
        "Marcos Zampieri",
        "Skye Morgan",
        "Kai North",
        "Tharindu Ranasinghe",
        "Austin Simmmons",
        "Paridhi Khandelwal",
        "Sara Rosenthal",
        "Preslav Nakov"
      ],
      "abstract": "We present TBO, a new dataset for Target-based Offensive language identification. TBO contains post-level annotations regarding the harmfulness of an offensive post and token-level annotations comprising of the target and the offensive argument expression. Popular offensive language identification datasets for social media focus on annotation taxonomies only at the post level and more recently, some datasets have been released that feature only token-level annotations. TBO is an important resource that bridges the gap between post-level and token-level annotation datasets by introducing a single comprehensive unified annotation taxonomy. We use the TBO taxonomy to annotate post-level and token-level offensive language on English Twitter posts. We release an initial dataset of over 4,500 instances collected from Twitter and we carry out multiple experiments to compare the performance of different models trained and tested on TBO."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 978,
      "original_url": "https://aclanthology.org/2023.acl-short.67/",
      "title": "Unsupervised Subtitle Segmentation with Masked Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.67.pdf",
      "authors": [
        "David Ponce",
        "Thierry Etchegoyhen",
        "Victor Ruiz"
      ],
      "abstract": "We describe a novel unsupervised approach to subtitle segmentation, based on pretrained masked language models, where line endings and subtitle breaks are predicted according to the likelihood of punctuation to occur at candidate segmentation points. Our approach obtained competitive results in terms of segmentation accuracy across metrics, while also fully preserving the original text and complying with length constraints. Although supervised models trained on in-domain data and with access to source audio information can provide better segmentation accuracy, our approach is highly portable across languages and domains and may constitute a robust off-the-shelf solution for subtitle segmentation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 979,
      "original_url": "https://aclanthology.org/2023.acl-short.68/",
      "title": "Exploring Continual Learning for Code Generation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.68.pdf",
      "authors": [
        "Prateek Yadav",
        "Qing Sun",
        "Hantian Ding",
        "Xiaopeng Li",
        "Dejiao Zhang",
        "Ming Tan",
        "Parminder Bhatia",
        "Xiaofei Ma",
        "Ramesh Nallapati",
        "Murali Krishna Ramanathan",
        "Mohit Bansal",
        "Bing Xiang"
      ],
      "abstract": "Large-scale code generation models such as Copilot and CodeT5 have achieved impressive performance. However, libraries are upgraded or deprecated very frequently and re-training large-scale language models is computationally expensive. Therefore, Continual Learning (CL) is an important aspect that remains under-explored in the code domain. In this paper, we introduce a benchmark called CodeTask-CL that covers a wide range of tasks, including code generation, translation, summarization, and refinement, with different input and output programming languages. Next, on our CodeTask-CL benchmark, we compare popular CL techniques from NLP and Vision domains. We find that effective methods like Prompt Pooling (PP) suffer from catastrophic forgetting due to the unstable training of the prompt selection mechanism caused by stark distribution shifts in coding tasks. We address this issue with our proposed method, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training by enforcing constraints on the prompt selection mechanism and leads to a 21.54% improvement over Prompt Pooling. Along with the benchmark, we establish a training pipeline that can be used for CL on code models, which we believe can motivate further development of CL methods for code models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 980,
      "original_url": "https://aclanthology.org/2023.acl-short.69/",
      "title": "Deep Active Learning for Morphophonological Processing",
      "pdf_url": "https://aclanthology.org/2023.acl-short.69.pdf",
      "authors": [
        "Seyed Morteza Mirbostani",
        "Yasaman Boreshban",
        "Salam Khalifa",
        "SeyedAbolghasem Mirroshandel",
        "Owen Rambow"
      ],
      "abstract": "Building a system for morphological processing is a challenging task in morphologically complex languages like Arabic. Although there are some deep learning based models that achieve successful results, these models rely on a large amount of annotated data. Building such datasets, specially for some of the lower-resource Arabic dialects, is very difficult, time-consuming, and expensive. In addition, some parts of the annotated data do not contain useful information for training machine learning models. Active learning strategies allow the learner algorithm to select the most informative samples for annotation. There has been little research that focuses on applying active learning for morphological inflection and morphophonological processing. In this paper, we have proposed a deep active learning method for this task. Our experiments on Egyptian Arabic show that with only about 30% of annotated data, we achieve the same results as does the state-of-the-art model on the whole dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 981,
      "original_url": "https://aclanthology.org/2023.acl-short.70/",
      "title": "Counterfactual reasoning: Testing language models’ understanding of hypothetical scenarios",
      "pdf_url": "https://aclanthology.org/2023.acl-short.70.pdf",
      "authors": [
        "Jiaxuan Li",
        "Lang Yu",
        "Allyson Ettinger"
      ],
      "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 982,
      "original_url": "https://aclanthology.org/2023.acl-short.71/",
      "title": "Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages",
      "pdf_url": "https://aclanthology.org/2023.acl-short.71.pdf",
      "authors": [
        "Yash Madhani",
        "Mitesh M. Khapra",
        "Anoop Kunchukuttan"
      ],
      "abstract": "We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better language coverage than existing LIDs and is competitive or better than other LIDs. IndicLID is the first LID for romanized text in Indian languages. Two major challenges for romanized text LID are the lack of training data and low-LID performance when languages are similar. We provide simple and effective solutions to these problems. In general, there has been limited work on romanized text in any language, and our findings are relevant to other languages that need romanized language identification. Our models are publicly available at https://github.com/AI4Bharat/IndicLID under open-source licenses. Our training and test sets are also publicly available at https://huggingface.co/datasets/ai4bharat/Bhasha-Abhijnaanam under open-source licenses."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 983,
      "original_url": "https://aclanthology.org/2023.acl-short.72/",
      "title": "Using contradictions improves question answering systems",
      "pdf_url": "https://aclanthology.org/2023.acl-short.72.pdf",
      "authors": [
        "Etienne Fortier-Dubois",
        "Domenic Rosati"
      ],
      "abstract": "This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is entailed (supported) by some background context. But is it useful to also determine if an answer contradicts the context? We test this in two settings, multiple choice and extractive QA, and find that systems that incorporate contradiction can do slightly better than entailment-only systems on certain datasets. However, the best performances come from using contradiction, entailment, and QA model confidence scores together. This has implications for the deployment of QA systems in domains such as medicine and science where safety is an issue."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 984,
      "original_url": "https://aclanthology.org/2023.acl-short.73/",
      "title": "Token-Level Self-Evolution Training for Sequence-to-Sequence Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.73.pdf",
      "authors": [
        "Keqin Peng",
        "Liang Ding",
        "Qihuang Zhong",
        "Yuanxin Ouyang",
        "Wenge Rong",
        "Zhang Xiong",
        "Dacheng Tao"
      ],
      "abstract": "Adaptive training approaches, widely used in sequence-to-sequence models, commonly reweigh the losses of different target tokens based on priors, e.g. word frequency. However, most of them do not consider the variation of learning difficulty in different training steps, and overly emphasize the learning of difficult one-hot labels, making the learning deterministic and sub-optimal. In response, we present Token-Level Self-Evolution Training (SE), a simple and effective dynamic training method to fully and wisely exploit the knowledge from data. SE focuses on dynamically learning the under-explored tokens for each forward pass and adaptively regularizes the training by introducing a novel token-specific label smoothing approach. Empirically, SE yields consistent and significant improvements in three tasks, i.e. machine translation, summarization, and grammatical error correction. Encouragingly, we achieve averaging +0.93 BLEU improvement on three machine translation tasks. Analyses confirm that, besides improving lexical accuracy, SE enhances generation diversity and model generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 985,
      "original_url": "https://aclanthology.org/2023.acl-short.74/",
      "title": "Gradient Ascent Post-training Enhances Language Model Generalization",
      "pdf_url": "https://aclanthology.org/2023.acl-short.74.pdf",
      "authors": [
        "Dongkeun Yoon",
        "Joel Jang",
        "Sungdong Kim",
        "Minjoon Seo"
      ],
      "abstract": "In this work, we empirically show that updating pretrained LMs (350M, 1.3B, 2.7B) with just a few steps of Gradient Ascent Post-training (GAP) on random, unlabeled text corpora enhances its zero-shot generalization capabilities across diverse NLP tasks. Specifically, we show that GAP can allow LMs to become comparable to 2-3x times larger LMs across 12 different NLP tasks. We also show that applying GAP on out-of-distribution corpora leads to the most reliable performance improvements. Our findings indicate that GAP can be a promising method for improving the generalization capability of LMs without any task-specific fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 986,
      "original_url": "https://aclanthology.org/2023.acl-short.75/",
      "title": "An Open Dataset and Model for Language Identification",
      "pdf_url": "https://aclanthology.org/2023.acl-short.75.pdf",
      "authors": [
        "Laurie Burchell",
        "Alexandra Birch",
        "Nikolay Bogoychev",
        "Kenneth Heafield"
      ],
      "abstract": "Language identification (LID) is a fundamental step in many natural language processing pipelines. However, current LID systems are far from perfect, particularly on lower-resource languages. We present a LID model which achieves a macro-average F1 score of 0.93 and a false positive rate of 0.033% across 201 languages, outperforming previous work. We achieve this by training on a curated dataset of monolingual data, which we audit manually to ensure reliability. We make both the model and the dataset available to the research community. Finally, we carry out detailed analysis into our model’s performance, both in comparison to existing open models and by language class."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 987,
      "original_url": "https://aclanthology.org/2023.acl-short.76/",
      "title": "Evaluating Paraphrastic Robustness in Textual Entailment Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.76.pdf",
      "authors": [
        "Dhruv Verma",
        "Yash Kumar Lal",
        "Shreyashee Sinha",
        "Benjamin Van Durme",
        "Adam Poliak"
      ],
      "abstract": "We present PaRTE, a collection of 1,126 pairs of Recognizing Textual Entailment (RTE) examples to evaluate whether models are robust to paraphrasing. We posit that if RTE models understand language, their predictions should be consistent across inputs that share the same meaning. We use the evaluation set to determine if RTE models’ predictions change when examples are paraphrased. In our experiments, contemporary models change their predictions on 8-16% of paraphrased examples, indicating that there is still room for improvement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 988,
      "original_url": "https://aclanthology.org/2023.acl-short.77/",
      "title": "Are Pre-trained Language Models Useful for Model Ensemble in Chinese Grammatical Error Correction?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.77.pdf",
      "authors": [
        "Chenming Tang",
        "Xiuyu Wu",
        "Yunfang Wu"
      ],
      "abstract": "Model ensemble has been in widespread use for Grammatical Error Correction (GEC), boosting model performance. We hypothesize that model ensemble based on the perplexity (PPL) computed by pre-trained language models (PLMs) should benefit the GEC system. To this end, we explore several ensemble strategies based on strong PLMs with four sophisticated single models. However, the performance does not improve but even gets worse after the PLM-based ensemble. This surprising result sets us doing a detailed analysis on the data and coming up with some insights on GEC. The human references of correct sentences is far from sufficient in the test data, and the gap between a correct sentence and an idiomatic one is worth our attention. Moreover, the PLM-based ensemble strategies provide an effective way to extend and improve GEC benchmark data. Our source code is available at https://github.com/JamyDon/PLM-based-CGEC-Model-Ensemble."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 989,
      "original_url": "https://aclanthology.org/2023.acl-short.78/",
      "title": "Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality",
      "pdf_url": "https://aclanthology.org/2023.acl-short.78.pdf",
      "authors": [
        "Tanay Dixit",
        "Fei Wang",
        "Muhao Chen"
      ],
      "abstract": "Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro ‘MODEL’}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 990,
      "original_url": "https://aclanthology.org/2023.acl-short.79/",
      "title": "With a Little Push, NLI Models can Robustly and Efficiently Predict Faithfulness",
      "pdf_url": "https://aclanthology.org/2023.acl-short.79.pdf",
      "authors": [
        "Julius Steen",
        "Juri Opitz",
        "Anette Frank",
        "Katja Markert"
      ],
      "abstract": "Conditional language models still generate unfaithful output that is not supported by their input. These unfaithful generations jeopardize trust in real-world applications such as summarization or human-machine interaction, motivating a need for automatic faithfulness metrics. To implement such metrics, NLI models seem attractive, since they solve a strongly related task that comes with a wealth of prior research and data. But recent research suggests that NLI models require costly additional machinery to perform reliably across datasets, e.g., by running inference on a cartesian product of input and generated sentences, or supporting them with a question-generation/answering step. In this work we show that pure NLI models _can_ outperform more complex metrics when combining task-adaptive data augmentation with robust inference procedures. We propose: (1) Augmenting NLI training data toadapt NL inferences to the specificities of faithfulness prediction in dialogue;(2) Making use of both entailment and contradiction probabilities in NLI, and(3) Using Monte-Carlo dropout during inference. Applied to the TRUE benchmark, which combines faithfulness datasets across diverse domains and tasks, our approach strongly improves a vanilla NLI model and significantly outperforms previous work, while showing favourable computational cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 991,
      "original_url": "https://aclanthology.org/2023.acl-short.80/",
      "title": "A Better Way to Do Masked Language Model Scoring",
      "pdf_url": "https://aclanthology.org/2023.acl-short.80.pdf",
      "authors": [
        "Carina Kauf",
        "Anna Ivanova"
      ],
      "abstract": "Estimating the log-likelihood of a given sentence under an autoregressive language model is straightforward: one can simply apply the chain rule and sum the log-likelihood values for each successive token. However, for masked language models (MLMs), there is no direct way to estimate the log-likelihood of a sentence. To address this issue, Salazar et al. (2020) propose to estimate sentence pseudo-log-likelihood (PLL) scores, computed by successively masking each sentence token, retrieving its score using the rest of the sentence as context, and summing the resulting values. Here, we demonstrate that the original PLL method yields inflated scores for out-of-vocabulary words and propose an adapted metric, in which we mask not only the target token, but also all within-word tokens to the right of the target. We show that our adapted metric (PLL-word-l2r) outperforms both the original PLL metric and a PLL metric in which all within-word tokens are masked. In particular, it better satisfies theoretical desiderata and better correlates with scores from autoregressive models. Finally, we show that the choice of metric affects even tightly controlled, minimal pair evaluation benchmarks (such as BLiMP), underscoring the importance of selecting an appropriate scoring metric for evaluating MLM properties."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 992,
      "original_url": "https://aclanthology.org/2023.acl-short.81/",
      "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.81.pdf",
      "authors": [
        "Michael Heck",
        "Nurul Lubis",
        "Benjamin Ruppik",
        "Renato Vukovic",
        "Shutong Feng",
        "Christian Geishauser",
        "Hsien-chin Lin",
        "Carel van Niekerk",
        "Milica Gasic"
      ],
      "abstract": "Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 993,
      "original_url": "https://aclanthology.org/2023.acl-short.82/",
      "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-short.82.pdf",
      "authors": [
        "Maximillian Chen",
        "Xiao Yu",
        "Weiyan Shi",
        "Urvi Awasthi",
        "Zhou Yu"
      ],
      "abstract": "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 994,
      "original_url": "https://aclanthology.org/2023.acl-short.83/",
      "title": "Enhancing Event Causality Identification with Counterfactual Reasoning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.83.pdf",
      "authors": [
        "Feiteng Mu",
        "Wenjie Li"
      ],
      "abstract": "Existing methods for event causality identification (ECI) focus on mining potential causal signals, i.e., causal context keywords and event pairs. However, causal signals are ambiguous, which may lead to the context-keywords bias and the event-pairs bias. To solve this issue, we propose the counterfactual reasoning that explicitly estimates the influence of context keywords and event pairs in training, so that we are able to eliminate the biases in inference.Experiments are conducted on two datasets, the result demonstrates the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 995,
      "original_url": "https://aclanthology.org/2023.acl-short.84/",
      "title": "Contrastive Bootstrapping for Label Refinement",
      "pdf_url": "https://aclanthology.org/2023.acl-short.84.pdf",
      "authors": [
        "Shudi Hou",
        "Yu Xia",
        "Muhao Chen",
        "Sujian Li"
      ],
      "abstract": "Traditional text classification typically categorizes texts into pre-defined coarse-grained classes, from which the produced models cannot handle the real-world scenario where finer categories emerge periodically for accurate services. In this work, we investigate the setting where fine-grained classification is done only using the annotation of coarse-grained categories and the coarse-to-fine mapping. We propose a lightweight contrastive clustering-based bootstrapping method to iteratively refine the labels of passages. During clustering, it pulls away negative passage-prototype pairs under the guidance of the mapping from both global and local perspectives. Experiments on NYT and 20News show that our method outperforms the state-of-the-art methods by a large margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 996,
      "original_url": "https://aclanthology.org/2023.acl-short.85/",
      "title": "NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-short.85.pdf",
      "authors": [
        "Iyanuoluwa Shode",
        "David Ifeoluwa Adelani",
        "JIng Peng",
        "Anna Feldman"
      ],
      "abstract": "Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labelled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross-domain adaptation. We create a new dataset, Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian Pidgin, and Yoruba). We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. By leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in the same language. To further mitigate the domain difference, we leverage machine translation from English to other Nigerian languages, which leads to a further improvement of 7% over cross-lingual evaluation. While machine translation to low-resource languages are often of low quality, our analysis shows that sentiment related words are often preserved."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 997,
      "original_url": "https://aclanthology.org/2023.acl-short.86/",
      "title": "Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement",
      "pdf_url": "https://aclanthology.org/2023.acl-short.86.pdf",
      "authors": [
        "Samuel Mensah",
        "Kai Sun",
        "Nikolaos Aletras"
      ],
      "abstract": "State-of-the-art target-oriented opinion word extraction (TOWE) models typically use BERT-based text encoders that operate on the word level, along with graph convolutional networks (GCNs) that incorporate syntactic information extracted from syntax trees. These methods achieve limited gains with GCNs and have difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to be effective at representing rare words or words with insufficient context information. To address this issue, this work trades syntax trees for BERT wordpieces by entirely removing the GCN component from the methods’ architectures. To enhance TOWE performance, we tackle the issue of aspect representation loss during encoding. Instead of solely utilizing a sentence as the input, we use a sentence-aspect pair. Our relatively simple approach achieves state-of-the-art results on benchmark datasets and should serve as a strong baseline for further research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 998,
      "original_url": "https://aclanthology.org/2023.acl-short.87/",
      "title": "An (unhelpful) guide to selecting the best ASR architecture for your under-resourced language",
      "pdf_url": "https://aclanthology.org/2023.acl-short.87.pdf",
      "authors": [
        "Robert Jimerson",
        "Zoey Liu",
        "Emily Prud’hommeaux"
      ],
      "abstract": "Advances in deep neural models for automatic speech recognition (ASR) have yielded dramatic improvements in ASR quality for resource-rich languages, with English ASR now achieving word error rates comparable to that of human transcribers. The vast majority of the world’s languages, however, lack the quantity of data necessary to approach this level of accuracy. In this paper we use four of the most popular ASR toolkits to train ASR models for eleven languages with limited ASR training resources: eleven widely spoken languages of Africa, Asia, and South America, one endangered language of Central America, and three critically endangered languages of North America. We find that no single architecture consistently outperforms any other. These differences in performance so far do not appear to be related to any particular feature of the datasets or characteristics of the languages. These findings have important implications for future research in ASR for under-resourced languages. ASR systems for languages with abundant existing media and available speakers may derive the most benefit simply by collecting large amounts of additional acoustic and textual training data. Communities using ASR to support endangered language documentation efforts, who cannot easily collect more data, might instead focus on exploring multiple architectures and hyperparameterizations to optimize performance within the constraints of their available data and resources."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 999,
      "original_url": "https://aclanthology.org/2023.acl-short.88/",
      "title": "The Ecological Fallacy in Annotation: Modeling Human Label Variation goes beyond Sociodemographics",
      "pdf_url": "https://aclanthology.org/2023.acl-short.88.pdf",
      "authors": [
        "Matthias Orlikowski",
        "Paul Röttger",
        "Philipp Cimiano",
        "Dirk Hovy"
      ],
      "abstract": "Many NLP tasks exhibit human label variation, where different annotators give different labels to the same texts. This variation is known to depend, at least in part, on the sociodemographics of annotators. Recent research aims to model individual annotator behaviour rather than predicting aggregated labels, and we would expect that sociodemographic information is useful for these models. On the other hand, the ecological fallacy states that aggregate group behaviour, such as the behaviour of the average female annotator, does not necessarily explain individual behaviour. To account for sociodemographics in models of individual annotator behaviour, we introduce group-specific layers to multi-annotator models. In a series of experiments for toxic content detection, we find that explicitly accounting for sociodemographic attributes in this way does not significantly improve model performance. This result shows that individual annotation behaviour depends on much more than just sociodemographics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1000,
      "original_url": "https://aclanthology.org/2023.acl-short.89/",
      "title": "Decomposed scoring of CCG dependencies",
      "pdf_url": "https://aclanthology.org/2023.acl-short.89.pdf",
      "authors": [
        "Aditya Bhargava",
        "Gerald Penn"
      ],
      "abstract": "In statistical parsing with CCG, the standard evaluation method is based on predicate-argument structure and evaluates dependencies labelled in part by lexical categories. When a predicate has multiple argument slots that can be filled, the same lexical category is used for the label of multiple dependencies. In this paper, we show that this evaluation can result in disproportionate penalization of supertagging errors and obfuscate the truly erroneous dependencies. Enabled by the compositional nature of CCG lexical categories, we propose *decomposed scoring* based on subcategorial labels to address this. To evaluate our scoring method, we engage fellow categorial grammar researchers in two English-language judgement tasks: (1) directly ranking the outputs of the standard and experimental scoring methods; and (2) determining which of two sentences has the better parse in cases where the two scoring methods disagree on their ranks. Overall, the judges prefer decomposed scoring in each task; but there is substantial disagreement among the judges in 24% of the given cases, pointing to potential issues with parser evaluations in general."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1001,
      "original_url": "https://aclanthology.org/2023.acl-short.90/",
      "title": "Do GPTs Produce Less Literal Translations?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.90.pdf",
      "authors": [
        "Vikas Raunak",
        "Arul Menezes",
        "Matt Post",
        "Hany Hassan"
      ],
      "abstract": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating sentences that contain idiomatic expressions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1002,
      "original_url": "https://aclanthology.org/2023.acl-short.91/",
      "title": "Environmental Claim Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-short.91.pdf",
      "authors": [
        "Dominik Stammbach",
        "Nicolas Webersinke",
        "Julia Bingler",
        "Mathias Kraus",
        "Markus Leippold"
      ],
      "abstract": "To transition to a green economy, environmental claims made by companies must be reliable, comparable, and verifiable. To analyze such claims at scale, automated methods are needed to detect them in the first place. However, there exist no datasets or models for this. Thus, this paper introduces the task of environmental claim detection. To accompany the task, we release an expert-annotated dataset and models trained on this dataset. We preview one potential application of such models: We detect environmental claims made in quarterly earning calls and find that the number of environmental claims has steadily increased since the Paris Agreement in 2015."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1003,
      "original_url": "https://aclanthology.org/2023.acl-short.92/",
      "title": "Black-box language model explanation by context length probing",
      "pdf_url": "https://aclanthology.org/2023.acl-short.92.pdf",
      "authors": [
        "Ondřej Cífka",
        "Antoine Liutkus"
      ],
      "abstract": "The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present *context length probing*, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign *differential importance scores* to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The [source code](https://github.com/cifkao/context-probing/) and an [interactive demo](https://cifkao.github.io/context-probing/) of the method are available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1004,
      "original_url": "https://aclanthology.org/2023.acl-short.93/",
      "title": "Let Me Check the Examples: Enhancing Demonstration Learning via Explicit Imitation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.93.pdf",
      "authors": [
        "Sirui Wang",
        "Kaiwen Wei",
        "Hongzhi Zhang",
        "Yuntao Li",
        "Wei Wu"
      ],
      "abstract": "Demonstration learning aims to guide the prompt prediction by providing answered demonstrations in the few shot settings. Despite achieving promising results, existing work only concatenates the answered examples as demonstrations to the prompt template (including the raw context) without any additional operation, neglecting the prompt-demonstration dependencies. Besides, prior research found that randomly replacing the labels of demonstrations marginally hurts performance, illustrating that the model could not properly learn the knowledge brought by the demonstrations. Inspired by the human learning process, in this paper, we introduce Imitation DEMOnstration learning (Imitation-Demo) to strengthen demonstration learning via explicitly imitating human review behaviour, which includes: (1) contrastive learning mechanism to concentrate on similar demonstrations.(2) demonstration-label re-prediction method to consolidate known knowledge. Experiment results show that our proposed method achieves state-of-the-art performance on 5 out of 14 classification corpus. Further studies also prove that Imitation-Demo strengthens the associations between the prompt and demonstrations, which could provide the basis for exploring how demonstration learning works."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1005,
      "original_url": "https://aclanthology.org/2023.acl-short.94/",
      "title": "The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics",
      "pdf_url": "https://aclanthology.org/2023.acl-short.94.pdf",
      "authors": [
        "Ricardo Rei",
        "Nuno M. Guerreiro",
        "Marcos Treviso",
        "Luisa Coheur",
        "Alon Lavie",
        "André Martins"
      ],
      "abstract": "Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, “black boxes” returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1006,
      "original_url": "https://aclanthology.org/2023.acl-short.95/",
      "title": "Typo-Robust Representation Learning for Dense Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-short.95.pdf",
      "authors": [
        "Panuthep Tasawong",
        "Wuttikorn Ponwitayarat",
        "Peerat Limkonchotiwat",
        "Can Udomcharoenchaikit",
        "Ekapol Chuangsuwanich",
        "Sarana Nutanong"
      ],
      "abstract": "Dense retrieval is a basic building block of information retrieval applications. One of the main challenges of dense retrieval in real-world settings is the handling of queries containing misspelled words. A popular approach for handling misspelled queries is minimizing the representations discrepancy between misspelled queries and their pristine ones. Unlike the existing approaches, which only focus on the alignment between misspelled and pristine queries, our method also improves the contrast between each misspelled query and its surrounding queries. To assess the effectiveness of our proposed method, we compare it against the existing competitors using two benchmark datasets and two base encoders. Our method outperforms the competitors in all cases with misspelled queries. Our code and models are available at https://github.com/panuthept/DST-DenseRetrieval."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1007,
      "original_url": "https://aclanthology.org/2023.acl-short.96/",
      "title": "Focused Prefix Tuning for Controllable Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.96.pdf",
      "authors": [
        "Congda Ma",
        "Tianyu Zhao",
        "Makoto Shing",
        "Kei Sawada",
        "Manabu Okumura"
      ],
      "abstract": "In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1008,
      "original_url": "https://aclanthology.org/2023.acl-short.97/",
      "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.97.pdf",
      "authors": [
        "Jianyi Zhang",
        "Aashiq Muhamed",
        "Aditya Anantharaman",
        "Guoyin Wang",
        "Changyou Chen",
        "Kai Zhong",
        "Qingjun Cui",
        "Yi Xu",
        "Belinda Zeng",
        "Trishul Chilimbi",
        "Yiran Chen"
      ],
      "abstract": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models. Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher’s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1009,
      "original_url": "https://aclanthology.org/2023.acl-short.98/",
      "title": "Debiasing Generative Named Entity Recognition by Calibrating Sequence Likelihood",
      "pdf_url": "https://aclanthology.org/2023.acl-short.98.pdf",
      "authors": [
        "Yu Xia",
        "Yongwei Zhao",
        "Wenhao Wu",
        "Sujian Li"
      ],
      "abstract": "Recognizing flat, overlapped and discontinuous entities uniformly has been paid increasing attention. Among these works, Seq2Seq formulation prevails for its flexibility and effectiveness. It arranges the output entities into a specific target sequence. However, it introduces bias by assigning all the probability mass to the observed sequence. To alleviate the bias, previous works either augment the data with possible sequences or resort to other formulations. In this paper, we stick to the Seq2Seq formulation and propose a reranking-based approach. It redistributes the likelihood among candidate sequences depending on their performance via a contrastive loss. Extensive experiments show that our simple yet effective method consistently boosts the baseline, and yields competitive or better results compared with the state-of-the-art methods on 8 widely-used datasets for Named Entity Recognition."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1010,
      "original_url": "https://aclanthology.org/2023.acl-short.99/",
      "title": "Deriving Language Models from Masked Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.99.pdf",
      "authors": [
        "Lucas Torroba Hennigen",
        "Yoon Kim"
      ],
      "abstract": "Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions from MLMs, focusing on distributions over two tokens, which makes it possible to calculate exact distributional properties. We find that an approach based on identifying joints whose conditionals are closest to those of the MLM works well and outperforms existing Markov random field-based approaches. We further find that this derived model’s conditionals can even occasionally outperform the original MLM’s conditionals."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1011,
      "original_url": "https://aclanthology.org/2023.acl-short.100/",
      "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.100.pdf",
      "authors": [
        "Zhiming Mao",
        "Huimin Wang",
        "Yiming Du",
        "Kam-Fai Wong"
      ],
      "abstract": "Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1012,
      "original_url": "https://aclanthology.org/2023.acl-short.101/",
      "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-short.101.pdf",
      "authors": [
        "Hao Fei",
        "Bobo Li",
        "Qian Liu",
        "Lidong Bing",
        "Fei Li",
        "Tat-Seng Chua"
      ],
      "abstract": "While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1013,
      "original_url": "https://aclanthology.org/2023.acl-short.102/",
      "title": "Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-short.102.pdf",
      "authors": [
        "Ta-Chung Chi",
        "Ting-Han Fan",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Peter Ramadge"
      ],
      "abstract": "The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1014,
      "original_url": "https://aclanthology.org/2023.acl-short.103/",
      "title": "Is Anisotropy Truly Harmful? A Case Study on Text Clustering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.103.pdf",
      "authors": [
        "Mira Ait-Saada",
        "Mohamed Nadif"
      ],
      "abstract": "In the last few years, several studies have been devoted to dissecting dense text representations in order to understand their effectiveness and further improve their quality. Particularly, the anisotropy of such representations has been observed, which means that the directions of the word vectors are not evenly distributed across the space but rather concentrated in a narrow cone. This has led to several attempts to counteract this phenomenon both on static and contextualized text representations. However, despite this effort, there is no established relationship between anisotropy and performance. In this paper, we aim to bridge this gap by investigating the impact of different transformations on both the isotropy and the performance in order to assess the true impact of anisotropy. To this end, we rely on the clustering task as a means of evaluating the ability of text representations to produce meaningful groups. Thereby, we empirically show a limited impact of anisotropy on the expressiveness of sentence representations both in terms of directions and L2 closeness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1015,
      "original_url": "https://aclanthology.org/2023.acl-short.104/",
      "title": "Class based Influence Functions for Error Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-short.104.pdf",
      "authors": [
        "Thang Nguyen-Duc",
        "Hoang Thanh-Tung",
        "Quan Hung Tran",
        "Dang Huu-Tien",
        "Hieu Nguyen",
        "Anh T. V. Dau",
        "Nghi Bui"
      ],
      "abstract": "Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs.Extensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1016,
      "original_url": "https://aclanthology.org/2023.acl-short.105/",
      "title": "Leveraging Prefix Transfer for Multi-Intent Text Revision",
      "pdf_url": "https://aclanthology.org/2023.acl-short.105.pdf",
      "authors": [
        "Ruining Chong",
        "Cunliang Kong",
        "Liu Wu",
        "Zhenghao Liu",
        "Ziye Jin",
        "Liner Yang",
        "Yange Fan",
        "Hanghang Fan",
        "Erhong Yang"
      ],
      "abstract": "Text revision is a necessary process to improve text quality. During this process, writers constantly edit texts out of different edit intentions. Identifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts according to one specific edit intention. In this work, we aim to build a multi-intent text revision system that could revise texts without explicit intent annotation. Our system is based on prefix-tuning, which first gets prefixes for every edit intent, and then trains a prefix transfer module, enabling the system to selectively leverage the knowledge from various prefixes according to the input text. We conduct experiments on the IteraTeR dataset, and the results show that our system outperforms baselines. The system can significantly improve the SARI score with more than 3% improvements, which thrives on the learned editing intention prefixes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1017,
      "original_url": "https://aclanthology.org/2023.acl-short.106/",
      "title": "Learning Multi-Step Reasoning by Solving Arithmetic Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-short.106.pdf",
      "authors": [
        "Tianduo Wang",
        "Wei Lu"
      ],
      "abstract": "Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs’ impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs’ math reasoning abilities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1018,
      "original_url": "https://aclanthology.org/2023.acl-short.107/",
      "title": "Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.107.pdf",
      "authors": [
        "Zhen-Ru Zhang",
        "Chuanqi Tan",
        "Haiyang Xu",
        "Chengyu Wang",
        "Jun Huang",
        "Songfang Huang"
      ],
      "abstract": "Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1019,
      "original_url": "https://aclanthology.org/2023.acl-short.108/",
      "title": "Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting",
      "pdf_url": "https://aclanthology.org/2023.acl-short.108.pdf",
      "authors": [
        "Zahra Fatemi",
        "Chen Xing",
        "Wenhao Liu",
        "Caimming Xiong"
      ],
      "abstract": "Existing studies addressing gender bias of pre-trained language models, usually build a small gender-neutral data set and conduct a second phase pre-training on the model with such data. However, given the limited size and concentrated focus of the gender-neutral data, catastrophic forgetting would occur during second-phase pre-training. Forgetting information in the original training data may damage the model’s downstream performance by a large margin. In this work, we empirically show that catastrophic forgetting occurs in such methods by evaluating them with general NLP tasks in GLUE. Then, we propose a new method, GEnder Equality Prompt (GEEP), to improve gender fairness of pre-trained models with less forgetting. GEEP freezes the pre-trained model and learns gender-related prompts with gender-neutral data. Empirical results show that GEEP not only achieves SOTA performances on gender fairness tasks, but also forgets less and performs better on GLUE by a large margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1020,
      "original_url": "https://aclanthology.org/2023.acl-short.109/",
      "title": "Class-Incremental Learning based on Label Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.109.pdf",
      "authors": [
        "Yijia Shao",
        "Yiduo Guo",
        "Dongyan Zhao",
        "Bing Liu"
      ],
      "abstract": "Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1021,
      "original_url": "https://aclanthology.org/2023.acl-short.110/",
      "title": "Evaluating pragmatic abilities of image captioners on A3DS",
      "pdf_url": "https://aclanthology.org/2023.acl-short.110.pdf",
      "authors": [
        "Polina Tsvilodub",
        "Michael Franke"
      ],
      "abstract": "Evaluating grounded neural language model performance with respect to pragmatic qualities like the trade off between truthfulness, contrastivity and overinformativity of generated utterances remains a challenge in absence of data collected from humans. To enable such evaluation, we present a novel open source image-text dataset “Annotated 3D Shapes” (A3DS) comprising over nine million exhaustive natural language annotations and over 12 million variable-granularity captions for the 480,000 images provided by Burgess & Kim (2018).We showcase the evaluation of pragmatic abilities developed by a task-neutral image captioner fine-tuned in a multi-agent communication setting to produce contrastive captions. The evaluation is enabled by the dataset because the exhaustive annotations allow to quantify the presence of contrastive features in the model’s generations. We show that the model develops human-like patterns (informativity, brevity, over-informativity for specific features (e.g., shape, color biases))."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1022,
      "original_url": "https://aclanthology.org/2023.acl-short.111/",
      "title": "The Art of Prompting: Event Detection based on Type Specific Prompts",
      "pdf_url": "https://aclanthology.org/2023.acl-short.111.pdf",
      "authors": [
        "Sijia Wang",
        "Mo Yu",
        "Lifu Huang"
      ],
      "abstract": "We compare various forms of prompts to represent event types and develop a unified framework to incorporate the event type specific prompts for supervised, few-shot, and zero-shot event detection. The experimental results demonstrate that a well-defined and comprehensive event type prompt can significantly improve event detection performance, especially when the annotated data is scarce (few-shot event detection) or not available (zero-shot event detection). By leveraging the semantics of event types, our unified framework shows up to 22.2% F-score gain over the previous state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1023,
      "original_url": "https://aclanthology.org/2023.acl-short.112/",
      "title": "Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.112.pdf",
      "authors": [
        "Zhuoyuan Mao",
        "Raj Dabre",
        "Qianying Liu",
        "Haiyue Song",
        "Chenhui Chu",
        "Sadao Kurohashi"
      ],
      "abstract": "This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1024,
      "original_url": "https://aclanthology.org/2023.acl-short.113/",
      "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.113.pdf",
      "authors": [
        "Po-Nien Kung",
        "Nanyun Peng"
      ],
      "abstract": "Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive performance gains, what models learn from IT remains understudied. In this work, we analyze how models utilize instructions during IT by comparing model training with altered vs. original instructions. Specifically, we create simplified task definitions by removing all semantic components and only leaving the output space information, and delusive examples that contain incorrect input-output mapping. Our experiments show that models trained on simplified task definition or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Furthermore, we introduce a random baseline to perform zeroshot classification tasks, and find it achieves similar performance (42.6% exact-match) as IT does (43% exact-match) in low resource setting, while both methods outperform naive T5 significantly (30% per exact-match). Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing. Our study highlights the urgent need for more reliable IT methods and evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1025,
      "original_url": "https://aclanthology.org/2023.acl-short.114/",
      "title": "Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.114.pdf",
      "authors": [
        "James O’Neill",
        "Sourav Dutta"
      ],
      "abstract": "We investigate the effects of post-training quantization and quantization-aware training on the generalization of Transformer language models. We present a new method called self-distilled quantization (SDQ) that minimizes accumulative quantization errors and outperforms baselines. We apply SDQ to multilingual models XLM-RBase and InfoXLMBase and demonstrate that both models can be reduced from 32-bit floating point weights to 8-bit integer weights while maintaining a high level of performance on the XGLUE benchmark. Our results also highlight the challenges of quantizing multilingual models, which must generalize to languages they were not fine-tuned on."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1026,
      "original_url": "https://aclanthology.org/2023.acl-short.115/",
      "title": "Modality Adaption or Regularization? A Case Study on End-to-End Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.115.pdf",
      "authors": [
        "Yuchen Han",
        "Chen Xu",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "abstract": "Pre-training and fine-tuning is a paradigm for alleviating the data scarcity problem in end-to-end speech translation (E2E ST). The commonplace ”modality gap” between speech and text data often leads to inconsistent inputs between pre-training and fine-tuning. However, we observe that this gap occurs in the early stages of fine-tuning, but does not have a major impact on the final performance. On the other hand, we find that there has another gap, which we call the ”capacity gap”: high resource tasks (such as ASR and MT) always require a large model to fit, when the model is reused for a low resource task (E2E ST), it will get a sub-optimal performance due to the over-fitting. In a case study, we find that the regularization plays a more important role than the well-designed modality adaption method, which achieves 29.0 for en-de and 40.3 for en-fr on the MuST-C dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1027,
      "original_url": "https://aclanthology.org/2023.acl-short.116/",
      "title": "Uncertainty-Aware Bootstrap Learning for Joint Extraction on Distantly-Supervised Data",
      "pdf_url": "https://aclanthology.org/2023.acl-short.116.pdf",
      "authors": [
        "Yufei Li",
        "Xiao Yu",
        "Yanchi Liu",
        "Haifeng Chen",
        "Cong Liu"
      ],
      "abstract": "Jointly extracting entity pairs and their relations is challenging when working on distantly-supervised data with ambiguous or noisy labels. To mitigate such impact, we propose uncertainty-aware bootstrap learning, which is motivated by the intuition that the higher uncertainty of an instance, the more likely the model confidence is inconsistent with the ground truths. Specifically, we first explore instance-level data uncertainty to create an initial high-confident examples. Such subset serves as filtering noisy instances and facilitating the model to converge fast at the early stage. During bootstrap learning, we propose self-ensembling as a regularizer to alleviate inter-model uncertainty produced by noisy labels. We further define probability variance of joint tagging probabilities to estimate inner-model parametric uncertainty, which is used to select and build up new reliable training instances for the next iteration. Experimental results on two large datasets reveal that our approach outperforms existing strong baselines and related methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1028,
      "original_url": "https://aclanthology.org/2023.acl-short.117/",
      "title": "Text-to-SQL Error Correction with Language Models of Code",
      "pdf_url": "https://aclanthology.org/2023.acl-short.117.pdf",
      "authors": [
        "Ziru Chen",
        "Shijie Chen",
        "Michael White",
        "Raymond Mooney",
        "Ali Payani",
        "Jayanth Srinivasa",
        "Yu Su",
        "Huan Sun"
      ],
      "abstract": "Despite recent progress in text-to-SQL parsing, current semantic parsers are still not accurate enough for practical use. In this paper, we investigate how to build automatic text-to-SQL error correction models. Noticing that token-level edits are out of context and sometimes ambiguous, we propose building clause-level edit models instead. Besides, while most language models of code are not specifically pre-trained for SQL, they know common data structures and their operations in programming languages such as Python. Thus, we propose a novel representation for SQL queries and their edits that adheres more closely to the pre-training corpora of language models of code. Our error correction model improves the exact set match accuracy of different parsers by 2.4-6.5 and obtains up to 4.3 point absolute improvement over two strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1029,
      "original_url": "https://aclanthology.org/2023.acl-short.118/",
      "title": "The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks",
      "pdf_url": "https://aclanthology.org/2023.acl-short.118.pdf",
      "authors": [
        "Nikil Selvam",
        "Sunipa Dev",
        "Daniel Khashabi",
        "Tushar Khot",
        "Kai-Wei Chang"
      ],
      "abstract": "How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given model? In this work, we study this question by contrasting social biases with non-social biases that stem from choices made during dataset construction (which might not even be discernible to the human eye). To do so, we empirically simulate various alternative constructions for a given benchmark based on seemingly innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI), we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models and consequently the relative ordering of these models when ranked by measured bias. We hope these troubling observations motivate more robust measures of social biases."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1030,
      "original_url": "https://aclanthology.org/2023.acl-short.119/",
      "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",
      "pdf_url": "https://aclanthology.org/2023.acl-short.119.pdf",
      "authors": [
        "Chantal Shaib",
        "Millicent Li",
        "Sebastian Joseph",
        "Iain Marshall",
        "Junyi Jessy Li",
        "Byron Wallace"
      ],
      "abstract": "Large language models, particularly GPT-3, are able to produce high quality summaries ofgeneral domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized domains such as biomedicine. In this paper we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given no supervision. We consider bothsingle- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in thelatter, we assess the degree to which GPT-3 is able to synthesize evidence reported acrossa collection of articles. We design an annotation scheme for evaluating model outputs, withan emphasis on assessing the factual accuracy of generated summaries. We find that whileGPT-3 is able to summarize and simplify single biomedical articles faithfully, it strugglesto provide accurate aggregations of findings over multiple documents. We release all data,code, and annotations used in this work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1031,
      "original_url": "https://aclanthology.org/2023.acl-short.120/",
      "title": "Prefix Propagation: Parameter-Efficient Tuning for Long Sequences",
      "pdf_url": "https://aclanthology.org/2023.acl-short.120.pdf",
      "authors": [
        "Jonathan Li",
        "Will Aitken",
        "Rohan Bhambhoria",
        "Xiaodan Zhu"
      ],
      "abstract": "Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model’s parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning for long-sequence language tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1032,
      "original_url": "https://aclanthology.org/2023.acl-short.121/",
      "title": "Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain",
      "pdf_url": "https://aclanthology.org/2023.acl-short.121.pdf",
      "authors": [
        "Shih-Lun Wu",
        "Yi-Hui Chou",
        "Liangze Li"
      ],
      "abstract": "PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common. It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively. Methods developed in the literature, however, cannot be deployed to real gameplaysince they only tackle some subtasks of the game,and they require additional reference chains inputs, whose extraction process is imperfect. Therefore, we propose a reference chain-free listener modelthat directly addresses the game’s predictive task, i.e., deciding whether an image is shared with partner. Our DeBERTa-based listener model reads the full dialogue, and utilizesCLIPScore features to assess utterance-image relevance. We achieve >77% accuracy on unseen sets of images/game themes, outperforming baseline by >17 points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1033,
      "original_url": "https://aclanthology.org/2023.acl-short.122/",
      "title": "Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations",
      "pdf_url": "https://aclanthology.org/2023.acl-short.122.pdf",
      "authors": [
        "Baikjin Jung",
        "Myungji Lee",
        "Jong-Hyeok Lee",
        "Yunsu Kim"
      ],
      "abstract": "Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English–German: the better the given MT is, the harder it is to decide what parts to edit and how to fix these errors. One possible solution to this problem is to instill deeper knowledge about the target language into the model. Thus, we propose a linguistically motivated method of regularization that is expected to enhance APE models’ understanding of the target language: a loss function that encourages symmetric self-attention on the given MT. Our analysis of experimental results demonstrates that the proposed method helps improving the state-of-the-art architecture’s APE quality for high-quality MTs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1034,
      "original_url": "https://aclanthology.org/2023.acl-short.123/",
      "title": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-short.123.pdf",
      "authors": [
        "Hang Yan",
        "Yu Sun",
        "Xiaonan Li",
        "Xipeng Qiu"
      ],
      "abstract": "Named entity recognition (NER) is the task to detect and classify entity spans in the text. When entity spans overlap between each other, the task is named as nested NER. Span-based methods have been widely used to tackle nested NER. Most of these methods get a score matrix, where each entry corresponds to a span. However, previous work ignores spatial relations in the score matrix. In this paper, we propose using Convolutional Neural Network (CNN) to model these spatial relations. Despite being simple, experiments in three commonly used nested NER datasets show that our model surpasses several recently proposed methods with the same pre-trained encoders. Further analysis shows that using CNN can help the model find more nested entities. Besides, we find that different papers use different sentence tokenizations for the three nested NER datasets, which will influence the comparison. Thus, we release a pre-processing script to facilitate future comparison."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1035,
      "original_url": "https://aclanthology.org/2023.acl-short.124/",
      "title": "Hexatagging: Projective Dependency Parsing as Tagging",
      "pdf_url": "https://aclanthology.org/2023.acl-short.124.pdf",
      "authors": [
        "Afra Amini",
        "Tianyu Liu",
        "Ryan Cotterell"
      ],
      "abstract": "We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser’s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1036,
      "original_url": "https://aclanthology.org/2023.acl-short.125/",
      "title": "Understanding Demonstration-based Learning from a Causal Perspective",
      "pdf_url": "https://aclanthology.org/2023.acl-short.125.pdf",
      "authors": [
        "Ruiyi Zhang",
        "Tong Yu"
      ],
      "abstract": "Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate the causal effects and find that the concurrence of specific words in the demonstration will induce bias, while randomly sampled tokens in the demonstration do not. Based on this finding, we further propose simple ways to construct random demonstrations, which even outperform hand-crafted, meaningful demonstrations on public sequence labeling benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1037,
      "original_url": "https://aclanthology.org/2023.acl-short.126/",
      "title": "RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.126.pdf",
      "authors": [
        "Gabriele Sarti",
        "Phu Mon Htut",
        "Xing Niu",
        "Benjamin Hsu",
        "Anna Currey",
        "Georgiana Dinu",
        "Maria Nadejde"
      ],
      "abstract": "Attribute-controlled translation (ACT) is a subtask of machine translation that involves controlling stylistic or linguistic attributes (like formality and gender) of translation outputs. While ACT has garnered attention in recent years due to its usefulness in real-world applications, progress in the task is currently limited by dataset availability, since most prior approaches rely on supervised methods. To address this limitation, we propose Retrieval and Attribute-Marking enhanced Prompting (RAMP), which leverages large multilingual language models to perform ACT in few-shot and zero-shot settings. RAMP improves generation accuracy over the standard prompting approach by (1) incorporating a semantic similarity retrieval component for selecting similar in-context examples, and (2) marking in-context examples with attribute annotations. Our comprehensive experiments show that RAMP is a viable approach in both zero-shot and few-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1038,
      "original_url": "https://aclanthology.org/2023.acl-short.127/",
      "title": "Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.127.pdf",
      "authors": [
        "Haoyang Wen",
        "Alexander Hauptmann"
      ],
      "abstract": "Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1039,
      "original_url": "https://aclanthology.org/2023.acl-short.128/",
      "title": "Discourse-Level Representations can Improve Prediction of Degree of Anxiety",
      "pdf_url": "https://aclanthology.org/2023.acl-short.128.pdf",
      "authors": [
        "Swanie Juhng",
        "Matthew Matero",
        "Vasudha Varadarajan",
        "Johannes Eichstaedt",
        "Adithya V Ganesan",
        "H. Andrew Schwartz"
      ],
      "abstract": "Anxiety disorders are the most common of mental illnesses, but relatively little is known about how to detect them from language. The primary clinical manifestation of anxiety is worry associated cognitive distortions, which are likely expressed at the discourse-level of semantics. Here, we investigate the development of a modern linguistic assessment for degree of anxiety, specifically evaluating the utility of discourse-level information in addition to lexical-level large language model embeddings. We find that a combined lexico-discourse model outperforms models based solely on state-of-the-art contextual embeddings (RoBERTa), with discourse-level representations derived from Sentence-BERT and DiscRE both providing additional predictive power not captured by lexical-level representations. Interpreting the model, we find that discourse patterns of causal explanations, among others, were used significantly more by those scoring high in anxiety, dovetailing with psychological literature."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1040,
      "original_url": "https://aclanthology.org/2023.acl-short.129/",
      "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.129.pdf",
      "authors": [
        "Mustafa Ozdayi",
        "Charith Peris",
        "Jack FitzGerald",
        "Christophe Dupuy",
        "Jimit Majmudar",
        "Haidar Khan",
        "Rahil Parikh",
        "Rahul Gupta"
      ],
      "abstract": "Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1041,
      "original_url": "https://aclanthology.org/2023.acl-short.130/",
      "title": "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting",
      "pdf_url": "https://aclanthology.org/2023.acl-short.130.pdf",
      "authors": [
        "Tatsuro Inaba",
        "Hirokazu Kiyomaru",
        "Fei Cheng",
        "Sadao Kurohashi"
      ],
      "abstract": "Large language models (LLMs) have achieved impressive performance on various reasoning tasks. To further improve the performance, we propose MultiTool-CoT, a novel framework that leverages chain-of-thought (CoT) prompting to incorporate multiple external tools, such as a calculator and a knowledge retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2 dataset of NumGLUE, which requires both numerical reasoning and domain-specific knowledge. The experiments show that our method significantly outperforms strong baselines and achieves state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1042,
      "original_url": "https://aclanthology.org/2023.acl-short.131/",
      "title": "mPMR: A Multilingual Pre-trained Machine Reader at Scale",
      "pdf_url": "https://aclanthology.org/2023.acl-short.131.pdf",
      "authors": [
        "Weiwen Xu",
        "Xin Li",
        "Wai Lam",
        "Lidong Bing"
      ],
      "abstract": "We present multilingual Pre-trained Machine Reader (mPMR), a novel method for multilingual machine reading comprehension (MRC)-style pre-training. mPMR aims to guide multilingual pre-trained language models (mPLMs) to perform natural language understanding (NLU) including both sequence classification and span extraction in multiple languages. To achieve cross-lingual generalization when only source-language fine-tuning data is available, existing mPLMs solely transfer NLU capability from a source language to target languages. In contrast, mPMR allows the direct inheritance of multilingual NLU capability from the MRC-style pre-training to downstream tasks. Therefore, mPMR acquires better NLU capability for target languages. mPMR also provides a unified solver for tackling cross-lingual span extraction and sequence classification, thereby enabling the extraction of rationales to explain the sentence-pair classification process."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1043,
      "original_url": "https://aclanthology.org/2023.acl-short.132/",
      "title": "MOSPC: MOS Prediction Based on Pairwise Comparison",
      "pdf_url": "https://aclanthology.org/2023.acl-short.132.pdf",
      "authors": [
        "Kexin Wang",
        "Yunlong Zhao",
        "Qianqian Dong",
        "Tom Ko",
        "Mingxuan Wang"
      ],
      "abstract": "As a subjective metric to evaluate the quality of synthesized speech, Mean opinion score(MOS) usually requires multiple annotators to score the same speech. Such an annotation approach requires a lot of manpower and is also time-consuming. MOS prediction model for automatic evaluation can significantly reduce labor cost. In previous works, it is difficult to accurately rank the quality of speech when the MOS scores are close. However, in practical applications, it is more important to correctly rank the quality of synthesis systems or sentences than simply predicting MOS scores. Meanwhile, as each annotator scores multiple audios during annotation, the score is probably a relative value based on the first or the first few speech scores given by the annotator. Motivated by the above two points, we propose a general framework for MOS prediction based on pair comparison (MOSPC), and we utilize C-Mixup algorithm to enhance the generalization performance of MOSPC.The experiments on BVCC and VCC2018 show that our framework outperforms the baselines on most of the correlation coefficient metrics, especially on the metric KTAU related to quality ranking. And our framework also surpasses the strong baseline in ranking accuracy on each fine-grained segment. These results indicate that our framework contributes to improving the ranking accuracy of speech quality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1044,
      "original_url": "https://aclanthology.org/2023.acl-short.133/",
      "title": "LI-RAGE: Late Interaction Retrieval Augmented Generation with Explicit Signals for Open-Domain Table Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.133.pdf",
      "authors": [
        "Weizhe Lin",
        "Rexhina Blloshmi",
        "Bill Byrne",
        "Adria de Gispert",
        "Gonzalo Iglesias"
      ],
      "abstract": "Recent open-domain TableQA models are typically implemented as retriever-reader pipelines. The retriever component is usually a variant of the Dense Passage Retriever, which computes the similarities between questions and tables based on a single representation of each. These fixed vectors can be insufficient to capture fine-grained features of potentially very big tables with heterogeneous row/column information. We address this limitation by 1) applying late interaction models which enforce a finer-grained interaction between question and table embeddings at retrieval time. In addition, we 2) incorporate a joint training scheme of the retriever and reader with explicit table-level signals, and 3) embed a binary relevance token as a prefix to the answer generated by the reader, so we can determine at inference time whether the table used to answer the question is reliable and filter accordingly. The combined strategies set a new state-to-the-art performance on two public open-domain TableQA datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1045,
      "original_url": "https://aclanthology.org/2023.acl-short.134/",
      "title": "How Well Apply Simple MLP to Incomplete Utterance Rewriting?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.134.pdf",
      "authors": [
        "Jiang Li",
        "Xiangdong Su",
        "Xinlan Ma",
        "Guanglai Gao"
      ],
      "abstract": "Incomplete utterance rewriting (IUR) aims to restore the incomplete utterance with sufficient context information for comprehension. This paper introduces a simple yet efficient IUR method. Different from prior studies, we first employ only one-layer MLP architecture to mine latent semantic information between joint utterances for IUR task (MIUR). After that, we conduct a joint feature matrix to predict the token type and thus restore the incomplete utterance. The well-designed network and simple architecture make our method significantly superior to existing methods in terms of quality and inference speedOur code is available at https://github.com/IMU-MachineLearningSXD/MIUR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1046,
      "original_url": "https://aclanthology.org/2023.acl-short.135/",
      "title": "XL-LEXEME: WiC Pretrained Model for Cross-Lingual LEXical sEMantic changE",
      "pdf_url": "https://aclanthology.org/2023.acl-short.135.pdf",
      "authors": [
        "Pierluigi Cassotti",
        "Lucia Siciliani",
        "Marco DeGemmis",
        "Giovanni Semeraro",
        "Pierpaolo Basile"
      ],
      "abstract": "The recent introduction of large-scale datasets for the WiC (Word in Context) task enables the creation of more reliable and meaningful contextualized word embeddings.However, most of the approaches to the WiC task use cross-encoders, which prevent the possibility of deriving comparable word embeddings.In this work, we introduce XL-LEXEME, a Lexical Semantic Change Detection model.XL-LEXEME extends SBERT, highlighting the target word in the sentence. We evaluate XL-LEXEME on the multilingual benchmarks for SemEval-2020 Task 1 - Lexical Semantic Change (LSC) Detection and the RuShiftEval shared task involving five languages: English, German, Swedish, Latin, and Russian.XL-LEXEME outperforms the state-of-the-art in English, German and Swedish with statistically significant differences from the baseline results and obtains state-of-the-art performance in the RuShiftEval shared task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1047,
      "original_url": "https://aclanthology.org/2023.acl-short.136/",
      "title": "Theory-Grounded Computational Text Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-short.136.pdf",
      "authors": [
        "Arya D. McCarthy",
        "Giovanna Maria Dora Dore"
      ],
      "abstract": "In this position paper, we argue that computational text analysis lacks and requires organizing principles. A broad space separates its two constituent disciplines—natural language processing and social science—which has to date been sidestepped rather than filled by applying increasingly complex computational models to problems in social science research. We contrast descriptive and integrative findings, and our review of approximately 60 papers on computational text analysis reveals that those from *ACL venues are typically descriptive. The lack of theory began at the area’s inception and has over the decades, grown more important and challenging. A return to theoretically grounded research questions will propel the area from both theoretical and methodological points of view."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1048,
      "original_url": "https://aclanthology.org/2023.acl-short.137/",
      "title": "AMRs Assemble! Learning to Ensemble with Autoregressive Models for AMR Parsing",
      "pdf_url": "https://aclanthology.org/2023.acl-short.137.pdf",
      "authors": [
        "Abelardo Carlos Martínez Lorenzo",
        "Pere Lluís Huguet Cabot",
        "Roberto Navigli"
      ],
      "abstract": "In this paper, we examine the current state-of-the-art in AMR parsing, which relies on ensemble strategies by merging multiple graph predictions. Our analysis reveals that the present models often violate AMR structural constraints. To address this issue, we develop a validation method, and show how ensemble models can exploit SMATCH metric weaknesses to obtain higher scores, but sometimes result in corrupted graphs. Additionally, we highlight the demanding need to compute the SMATCH score among all possible predictions. To overcome these challenges, we propose two novel ensemble strategies based on Transformer models, improving robustness to structural constraints, while also reducing the computational time. Our methods provide new insights for enhancing AMR parsers and metrics. Our code is available at [https://www.github.com/babelscape/AMRs-Assemble](https://www.github.com/babelscape/AMRs-Assemble)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1049,
      "original_url": "https://aclanthology.org/2023.acl-short.138/",
      "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training",
      "pdf_url": "https://aclanthology.org/2023.acl-short.138.pdf",
      "authors": [
        "Zequn Liu",
        "Wei Zhang",
        "Yingce Xia",
        "Lijun Wu",
        "Shufang Xie",
        "Tao Qin",
        "Ming Zhang",
        "Tie-Yan Liu"
      ],
      "abstract": "Generative pre-trained Transformer (GPT) has demonstrates its great success in natural language processing and related techniques have been adapted into molecular modeling. Considering that text is the most important record for scientific discovery, in this paper, we propose MolXPT, a unified language model of text and molecules pre-trained on SMILES (a sequence representation of molecules) wrapped by text. Briefly, we detect the molecule names in each sequence and replace them to the corresponding SMILES. In this way, the SMILES could leverage the information from surrounding text, and vice versa. The above wrapped sequences, text sequences from PubMed and SMILES sequences from PubChem are all fed into a language model for pre-training. Experimental results demonstrate that MolXPT outperforms strong baselines of molecular property prediction on MoleculeNet, performs comparably to the best model in text-molecule translation while using less than half of its parameters, and enables zero-shot molecular generation without finetuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1050,
      "original_url": "https://aclanthology.org/2023.acl-short.139/",
      "title": "A Study on the Efficiency and Generalization of Light Hybrid Retrievers",
      "pdf_url": "https://aclanthology.org/2023.acl-short.139.pdf",
      "authors": [
        "Man Luo",
        "Shashank Jain",
        "Anchit Gupta",
        "Arash Einolghozati",
        "Barlas Oguz",
        "Debojeet Chatterjee",
        "Xilun Chen",
        "Chitta Baral",
        "Peyman Heidari"
      ],
      "abstract": "Hybrid retrievers can take advantage of both sparse and dense retrievers. Previous hybrid retrievers leverage indexing-heavy dense retrievers. In this work, we study “Is it possible to reduce the indexing memory of hybrid retrievers without sacrificing performance”? Driven by this question, we leverage an indexing-efficient dense retriever (i.e. DrBoost) and introduce a LITE retriever that further reduces the memory of DrBoost. LITE is jointly trained on contrastive learning and knowledge distillation from DrBoost. Then, we integrate BM25, a sparse retriever, with either LITE or DrBoost to form light hybrid retrievers. Our Hybrid-LITE retriever saves 13× memory while maintaining 98.0% performance of the hybrid retriever of BM25 and DPR. In addition, we study the generalization capacity of our light hybrid retrievers on out-of-domain dataset and a set of adversarial attacks datasets. Experiments showcase that light hybrid retrievers achieve better generalization performance than individual sparse and dense retrievers. Nevertheless, our analysis shows that there is a large room to improve the robustness of retrievers, suggesting a new research direction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1051,
      "original_url": "https://aclanthology.org/2023.acl-short.140/",
      "title": "The Mechanical Bard: An Interpretable Machine Learning Approach to Shakespearean Sonnet Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.140.pdf",
      "authors": [
        "Edwin Agnew",
        "Michelle Qiu",
        "Lily Zhu",
        "Sam Wiseman",
        "Cynthia Rudin"
      ],
      "abstract": "We consider the automated generation of sonnets, a poetic form constrained according to meter, rhyme scheme, and length. Sonnets generally also use rhetorical figures, expressive language, and a consistent theme or narrative. Our constrained decoding approach allows for the generation of sonnets within preset poetic constraints, while using a relatively modest neural backbone. Human evaluation confirms that our approach produces Shakespearean sonnets that resemble human-authored sonnets, and which adhere to the genre’s defined constraints and contain lyrical language and literary devices."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1052,
      "original_url": "https://aclanthology.org/2023.acl-short.141/",
      "title": "When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants",
      "pdf_url": "https://aclanthology.org/2023.acl-short.141.pdf",
      "authors": [
        "Anuj Diwan",
        "Eunsol Choi",
        "David Harwath"
      ],
      "abstract": "We present the first unified study of the efficiency of self-attention-based Transformer variants spanning text, speech and vision. We identify input length thresholds (tipping points) at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency metrics (latency, throughput, and memory). To conduct this analysis for speech, we introduce L-HuBERT, a novel local-attention variant of a self-supervised speech model. We observe that these thresholds are (a) much higher than typical dataset sequence lengths and (b) dependent on the metric and modality, showing that choosing the right model depends on modality, task type (long-form vs. typical context) and resource constraints (time vs. memory). By visualising the breakdown of the computational costs for transformer components, we also show that non-self-attention components exhibit significant computational costs. We release our profiling toolkit at https://github.com/ajd12342/profiling-transformers ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1053,
      "original_url": "https://aclanthology.org/2023.acl-short.142/",
      "title": "Evaluating Zero-Shot Event Structures: Recommendations for Automatic Content Extraction (ACE) Annotations",
      "pdf_url": "https://aclanthology.org/2023.acl-short.142.pdf",
      "authors": [
        "Erica Cai",
        "Brendan O’Connor"
      ],
      "abstract": "Zero-shot event extraction (EE) methods infer richly structured event records from text, based only on a minimal user specification and no training examples, which enables flexibility in exploring and developing applications. Most event extraction research uses the Automatic Content Extraction (ACE) annotated dataset to evaluate supervised EE methods, but can it be used to evaluate zero-shot and other low-supervision EE? We describe ACE’s event structures and identify significant ambiguities and issues in current evaluation practice, including (1) coreferent argument mentions, (2) conflicting argument head conventions, and (3) ignorance of modality and event class details. By sometimes mishandling these subtleties, current work may dramatically understate the actual performance of zero-shot and other low-supervision EE, considering up to 32% of correctly identified arguments and 25% of correctly ignored event mentions as false negatives. For each issue, we propose recommendations for future evaluations so the research community can better utilize ACE as an event evaluation resource."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1054,
      "original_url": "https://aclanthology.org/2023.acl-short.143/",
      "title": "Event Extraction as Question Generation and Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.143.pdf",
      "authors": [
        "Di Lu",
        "Shihao Ran",
        "Joel Tetreault",
        "Alejandro Jaimes"
      ],
      "abstract": "Recent work on Event Extraction has reframed the task as Question Answering (QA), with promising results. The advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model. Experiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1055,
      "original_url": "https://aclanthology.org/2023.acl-short.144/",
      "title": "Are Sample-Efficient NLP Models More Robust?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.144.pdf",
      "authors": [
        "Nelson F. Liu",
        "Ananya Kumar",
        "Percy Liang",
        "Robin Jia"
      ],
      "abstract": "Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-ofdistribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness improvements, since such improvements are highly dataset- and task-dependent. Even in an era of large, multi-purpose pre-trained models, task-specific decisions may often be necessary for OOD generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1056,
      "original_url": "https://aclanthology.org/2023.acl-short.145/",
      "title": "Diversity-Aware Coherence Loss for Improving Neural Topic Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.145.pdf",
      "authors": [
        "Raymond Li",
        "Felipe Gonzalez-Pizarro",
        "Linzi Xing",
        "Gabriel Murray",
        "Giuseppe Carenini"
      ],
      "abstract": "The standard approach for neural topic modeling uses a variational autoencoder (VAE) framework that jointly minimizes the KL divergence between the estimated posterior and prior, in addition to the reconstruction loss. Since neural topic models are trained by recreating individual input documents, they do not explicitly capture the coherence between words on the corpus level. In this work, we propose a novel diversity-aware coherence loss that encourages the model to learn corpus-level coherence scores while maintaining high diversity between topics. Experimental results on multiple datasets show that our method significantly improves the performance of neural topic models without requiring any pretraining or additional parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1057,
      "original_url": "https://aclanthology.org/2023.acl-short.146/",
      "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
      "pdf_url": "https://aclanthology.org/2023.acl-short.146.pdf",
      "authors": [
        "Haoxin Li",
        "Phillip Keung",
        "Daniel Cheng",
        "Jungo Kasai",
        "Noah A. Smith"
      ],
      "abstract": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than 2x. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as 3.5x with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1058,
      "original_url": "https://aclanthology.org/2023.acl-short.147/",
      "title": "S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.147.pdf",
      "authors": [
        "Fangyu Lei",
        "Xiang Li",
        "Yifan Wei",
        "Shizhu He",
        "Yiming Huang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator (first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1059,
      "original_url": "https://aclanthology.org/2023.acl-short.148/",
      "title": "Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-short.148.pdf",
      "authors": [
        "Bin Sun",
        "Yitong Li",
        "Fei Mi",
        "Fanhu Bie",
        "Yiwei Li",
        "Kan Li"
      ],
      "abstract": "Existing knowledge-grounded open-domain dialogue generation models often face the hallucination problem, i.e. the dialogue generative model will persist in an inappropriate knowledge and generate responses that inconsistent with the facts. We argue that this problem mainly stems from the polarized optimization objectives and weak knowledge generation ability. To mitigate the hallucination, we take inspiration from human communicating that people will replay euphemistic responses for the unclear or unrecognizable knowledge, and propose an Augmentative and Contrastive Knowledge Dialogue Expansion Framework (ACK-DEF). ACK-DEF constructs the augmentative and contrastive knowledge dialogue samples, which consist of the knowledge of different degrees of errors and the response of manual design, to expand the original training set and smooth the polarized optimization objective that enables models to generate ground-truth with or without gold knowledge. Not only the knowledge, ACK-DEF also provides the tactful responses of manual design corresponding to the incomplete correct knowledge. Experimental results on the Wikipedia of Wizard dataset show that employing the ACK-DEF is effective to alleviate the hallucination problem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1060,
      "original_url": "https://aclanthology.org/2023.acl-short.149/",
      "title": "AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.149.pdf",
      "authors": [
        "Siheng Li",
        "Cheng Yang",
        "Yichun Yin",
        "Xinyu Zhu",
        "Zesen Cheng",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Yujiu Yang"
      ],
      "abstract": "Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1061,
      "original_url": "https://aclanthology.org/2023.acl-short.150/",
      "title": "STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions",
      "pdf_url": "https://aclanthology.org/2023.acl-short.150.pdf",
      "authors": [
        "Michel Plüss",
        "Jan Deriu",
        "Yanick Schraner",
        "Claudio Paonessa",
        "Julia Hartmann",
        "Larissa Schmidt",
        "Christian Scheller",
        "Manuela Hürlimann",
        "Tanja Samardžić",
        "Manfred Vogel",
        "Mark Cieliebak"
      ],
      "abstract": "We present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The data is collected using a web app in which the speakers are shown Standard German sentences, which they translate to Swiss German and record. We make the corpus publicly available. It contains 343 hours of speech from all dialect regions and is the largest public speech corpus for Swiss German to date. Application areas include automatic speech recognition (ASR), text-to-speech, dialect identification, and speaker recognition. Dialect information, age group, and gender of the 316 speakers are provided. Genders are equally represented and the corpus includes speakers of all ages. Roughly the same amount of speech is provided per dialect region, which makes the corpus ideally suited for experiments with speech technology for different dialects. We provide training, validation, and test splits of the data. The test set consists of the same spoken sentences for each dialect region and allows a fair evaluation of the quality of speech technologies in different dialects. We train an ASR model on the training set and achieve an average BLEU score of 74.7 on the test set. The model beats the best published BLEU scores on 2 other Swiss German ASR test sets, demonstrating the quality of the corpus."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1062,
      "original_url": "https://aclanthology.org/2023.acl-short.151/",
      "title": "Teaching Small Language Models to Reason",
      "pdf_url": "https://aclanthology.org/2023.acl-short.151.pdf",
      "authors": [
        "Lucie Charlotte Magister",
        "Jonathan Mallinson",
        "Jakub Adamek",
        "Eric Malmi",
        "Aliaksei Severyn"
      ],
      "abstract": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1063,
      "original_url": "https://aclanthology.org/2023.acl-short.152/",
      "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-short.152.pdf",
      "authors": [
        "Rohan Bhambhoria",
        "Lei Chen",
        "Xiaodan Zhu"
      ],
      "abstract": "In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1064,
      "original_url": "https://aclanthology.org/2023.acl-short.153/",
      "title": "A Simple Concatenation can Effectively Improve Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.153.pdf",
      "authors": [
        "Linlin Zhang",
        "Kai Fan",
        "Boxing Chen",
        "Luo Si"
      ],
      "abstract": "A triple speech translation data comprises speech, transcription, and translation. In the end-to-end paradigm, text machine translation (MT) usually plays the role of a teacher model for the speech translation (ST) via knowledge distillation. Parameter sharing with the teacher is often adopted to construct the ST model architecture, however, the two modalities are independently fed and trained via different losses. This situation does not match ST’s properties across two modalities and also limits the upper bound of the performance. Inspired by the works of video Transformer, we propose a simple unified cross-modal ST method, which concatenates speech and text as the input, and builds a teacher that can utilize both cross-modal information simultaneously. Experimental results show that in our unified ST framework, models can effectively utilize the auxiliary information from speech and text, and achieve compelling results on MuST-C datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1065,
      "original_url": "https://aclanthology.org/2023.acl-short.154/",
      "title": "ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-short.154.pdf",
      "authors": [
        "Jingyuan S. She",
        "Christopher Potts",
        "Samuel R. Bowman",
        "Atticus Geiger"
      ],
      "abstract": "A number of recent benchmarks seek to assess how well models handle natural language negation. However, these benchmarks lack the controlled example paradigms that would allow us to infer whether a model had truly learned how negation morphemes semantically scope. To fill these analytical gaps, we present the Scoped Negation NLI (ScoNe-NLI) benchmark, which contains contrast sets of six examples with up to two negations where either zero, one, or both negative morphemes affect the NLI label. We use ScoNe-NLI to assess fine-tuning and in-context learning strategies. We find that RoBERTa and DeBERTa models solve ScoNe-NLI after many shot fine-tuning. For in-context learning, we test the latest InstructGPT models and find that most prompt strategies are not successful, including those using step-by-step reasoning. To better understand this result, we extend ScoNe with ScoNe-NLG, a sentence completion test set that embeds negation reasoning in short narratives. Here, InstructGPT is successful, which reveals the model can correctly reason about negation, but struggles to do so on NLI examples outside of its core pretraining regime."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1066,
      "original_url": "https://aclanthology.org/2023.acl-short.155/",
      "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
      "pdf_url": "https://aclanthology.org/2023.acl-short.155.pdf",
      "authors": [
        "Yulin Zhou",
        "Yiren Zhao",
        "Ilia Shumailov",
        "Robert Mullins",
        "Yarin Gal"
      ],
      "abstract": "Current literature demonstrates that Large Language Models (LLMs) are great few-shot learners, and prompting significantly increases their performance on a range of downstream tasks in a few-shot learning setting. An attempt to automate human-led prompting followed, with some progress achieved. In particular, subsequent work demonstrates that automation can outperform fine-tuning in certain K-shot learning scenarios. In this paper, we revisit techniques for automated prompting on six different downstream tasks and a larger range of K-shot learning settings. We find that automated prompting does not consistently outperform simple manual prompting. Our work suggests that, in addition to fine-tuning, manual prompting should be used as a baseline in this line of research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1067,
      "original_url": "https://aclanthology.org/2023.acl-short.156/",
      "title": "Mind the Gap between the Application Track and the Real World",
      "pdf_url": "https://aclanthology.org/2023.acl-short.156.pdf",
      "authors": [
        "Ananya Ganesh",
        "Jie Cao",
        "E. Margaret Perkoff",
        "Rosy Southwell",
        "Martha Palmer",
        "Katharina Kann"
      ],
      "abstract": "Recent advances in NLP have led to a rise in inter-disciplinary and application-oriented research. While this demonstrates the growing real-world impact of the field, research papers frequently feature experiments that do not account for the complexities of realistic data and environments. To explore the extent of this gap, we investigate the relationship between the real-world motivations described in NLP papers and the models and evaluation which comprise the proposed solution. We first survey papers from the NLP Applications track from ACL 2020 and EMNLP 2020, asking which papers have differences between their stated motivation and their experimental setting, and if so, mention them. We find that many papers fall short of considering real-world input and output conditions due to adopting simplified modeling or evaluation settings. As a case study, we then empirically show that the performance of an educational dialog understanding system deteriorates when used in a realistic classroom environment."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1068,
      "original_url": "https://aclanthology.org/2023.acl-short.157/",
      "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives",
      "pdf_url": "https://aclanthology.org/2023.acl-short.157.pdf",
      "authors": [
        "Xinpeng Wang",
        "Leonie Weissweiler",
        "Hinrich Schütze",
        "Barbara Plank"
      ],
      "abstract": "Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1069,
      "original_url": "https://aclanthology.org/2023.acl-short.158/",
      "title": "ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.acl-short.158.pdf",
      "authors": [
        "Anastasiia Sedova",
        "Benjamin Roth"
      ],
      "abstract": "Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotator show an improvement of 7% points when using ACTC in the challenging setting with an annotation budget of only 10 tuples, and an average improvement of 4% points over different budgets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1070,
      "original_url": "https://aclanthology.org/2023.acl-short.159/",
      "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-short.159.pdf",
      "authors": [
        "Hao Cheng",
        "Hao Fang",
        "Xiaodong Liu",
        "Jianfeng Gao"
      ],
      "abstract": "Given its effectiveness on knowledge-intensive natural language processing tasks, dense retrieval models have become increasingly popular. Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders. Further, recent studies show that such dense retrievers underperform BM25 in various settings. We thus propose a new architecture, Task-Aware Specialization for dEnse Retrieval (TASER), which enables parameter sharing by interleaving shared and specialized blocks in a single encoder. Our experiments on five question answering datasets show that TASER can achieve superior accuracy, surpassing BM25, while using about 60% of the parameters as bi-encoder dense retrievers. In out-of-domain evaluations, TASER is also empirically more robust than bi-encoder dense retrievers. Our code is available at https://github.com/microsoft/taser."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1071,
      "original_url": "https://aclanthology.org/2023.acl-short.160/",
      "title": "Linear Classifier: An Often-Forgotten Baseline for Text Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-short.160.pdf",
      "authors": [
        "Yu-Chen Lin",
        "Si-An Chen",
        "Jie-Jyun Liu",
        "Chih-Jen Lin"
      ],
      "abstract": "Large-scale pre-trained language models such as BERT are popular solutions for text classification. Due to the superior performance of these advanced methods, nowadays, people often directly train them for a few epochs and deploy the obtained model. In this opinion paper, we point out that this way may only sometimes get satisfactory results. We argue the importance of running a simple baseline like linear classifiers on bag-of-words features along with advanced methods. First, for many text data, linear methods show competitive performance, high efficiency, and robustness. Second, advanced models such as BERT may only achieve the best results if properly applied. Simple baselines help to confirm whether the results of advanced models are acceptable. Our experimental results fully support these points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1072,
      "original_url": "https://aclanthology.org/2023.acl-short.161/",
      "title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-short.161.pdf",
      "authors": [
        "Anian Ruoss",
        "Grégoire Delétang",
        "Tim Genewein",
        "Jordi Grau-Moya",
        "Róbert Csordás",
        "Mehdi Bennani",
        "Shane Legg",
        "Joel Veness"
      ],
      "abstract": "Transformers have impressive generalization capabilities on tasks with a fixed context length. However, they fail to generalize to sequences of arbitrary length, even for seemingly simple tasks such as duplicating a string. Moreover, simply training on longer sequences is inefficient due to the quadratic computation complexity of the global attention mechanism. In this work, we demonstrate that this failure mode is linked to positional encodings being out-of-distribution for longer sequences (even for relative encodings) and introduce a novel family of positional encodings that can overcome this problem. Concretely, our randomized positional encoding scheme simulates the positions of longer sequences and randomly selects an ordered subset to fit the sequence’s length. Our large-scale empirical evaluation of 6000 models across 15 algorithmic reasoning tasks shows that our method allows Transformers to generalize to sequences of unseen length (increasing test accuracy by 12.0% on average)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1073,
      "original_url": "https://aclanthology.org/2023.acl-short.162/",
      "title": "Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-short.162.pdf",
      "authors": [
        "Hidetaka Kamigaito",
        "Katsuhiko Hayashi",
        "Taro Watanabe"
      ],
      "abstract": "In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V & L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V & L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image related tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1074,
      "original_url": "https://aclanthology.org/2023.acl-short.163/",
      "title": "Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints",
      "pdf_url": "https://aclanthology.org/2023.acl-short.163.pdf",
      "authors": [
        "Chao Lou",
        "Kewei Tu"
      ],
      "abstract": "Neural QCFG is a grammar-based sequence-to-sequence model with strong inductive biases on hierarchical structures. It excels in interpretability and generalization but suffers from expensive inference. In this paper, we study two low-rank variants of Neural QCFG for faster inference with different trade-offs between efficiency and expressiveness. Furthermore, utilizing the symbolic interface provided by the grammar, we introduce two soft constraints over tree hierarchy and source coverage. We experiment with various datasets and find that our models outperform vanilla Neural QCFG in most settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1075,
      "original_url": "https://aclanthology.org/2023.acl-short.164/",
      "title": "TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-short.164.pdf",
      "authors": [
        "Yiming Ai",
        "Zhiwei He",
        "Kai Yu",
        "Rui Wang"
      ],
      "abstract": "Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model’s mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a corresponding benchmark, tense prediction accuracy. With the tense test set and the benchmark, researchers are able to measure the tense consistency performance of machine translation systems for the first time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1076,
      "original_url": "https://aclanthology.org/2023.acl-demo.1/",
      "title": "Human-in-the-loop Schema Induction",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.1.pdf",
      "authors": [
        "Tianyi Zhang",
        "Isaac Tham",
        "Zhaoyi Hou",
        "Jiaxuan Ren",
        "Leon Zhou",
        "Hainiu Xu",
        "Li Zhang",
        "Lara Martin",
        "Rotem Dror",
        "Sha Li",
        "Heng Ji",
        "Martha Palmer",
        "Susan Windisch Brown",
        "Reece Suchocki",
        "Chris Callison-Burch"
      ],
      "abstract": "Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1077,
      "original_url": "https://aclanthology.org/2023.acl-demo.2/",
      "title": "PersLEARN: Research Training through the Lens of Perspective Cultivation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.2.pdf",
      "authors": [
        "Yu-Zhe Shi",
        "Shiqian Li",
        "Xinyi Niu",
        "Qiao Xu",
        "Jiawen Liu",
        "Yifan Xu",
        "Shiyu Gu",
        "Bingru He",
        "Xinyang Li",
        "Xinyu Zhao",
        "Zijian Zhao",
        "Yidong Lyu",
        "Zhen Li",
        "Sijia Liu",
        "Lin Qiu",
        "Jinhao Ji",
        "Lecheng Ruan",
        "Yuxi Ma",
        "Wenjuan Han",
        "Yixin Zhu"
      ],
      "abstract": "Scientific research is inherently shaped by its authors’ perspectives, influenced by various factorssuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce PersLEARN , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework. By interacting with a prompt-based model, researchers can develop their perspectives explicitly. Our humanstudy reveals that scientific perspectives developed by students using PersLEARN exhibit a superior level of logical coherence and depth compared to those that did not. Furthermore, our pipeline outperforms baseline approaches across multiple domains of literature from various perspectives. These results suggest that PersLEARN could help foster a greater appreciation of diversity in scientific perspectives as an essential component of research training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1078,
      "original_url": "https://aclanthology.org/2023.acl-demo.3/",
      "title": "LAVIS: A One-stop Library for Language-Vision Intelligence",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.3.pdf",
      "authors": [
        "Dongxu Li",
        "Junnan Li",
        "Hung Le",
        "Guangsen Wang",
        "Silvio Savarese",
        "Steven C.H. Hoi"
      ],
      "abstract": "We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1079,
      "original_url": "https://aclanthology.org/2023.acl-demo.4/",
      "title": "Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.4.pdf",
      "authors": [
        "Bum Chul Kwon",
        "Nandana Mihindukulasooriya"
      ],
      "abstract": "Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. To address this issue, we propose Finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. The goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. Finspector is available at https://github.com/IBM/finspector."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1080,
      "original_url": "https://aclanthology.org/2023.acl-demo.5/",
      "title": "PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.5.pdf",
      "authors": [
        "Avi Sil",
        "Jaydeep Sen",
        "Bhavani Iyer",
        "Martin Franz",
        "Kshitij Fadnis",
        "Mihaela Bornea",
        "Sara Rosenthal",
        "Scott McCarley",
        "Rong Zhang",
        "Vishwajeet Kumar",
        "Yulong Li",
        "Md Arafat Sultan",
        "Riyaz Bhat",
        "Juergen Bross",
        "Radu Florian",
        "Salim Roukos"
      ],
      "abstract": "The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce PrimeQA: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods. PrimeQA supports core QA functionalities like retrieval and reading comprehension as well as auxiliary capabilities such as question generation. It has been designed as an end-to-end toolkit for various use cases: building front-end applications, replicating SOTA methods on public benchmarks, and expanding pre-existing methods. PrimeQA is available at: https://github.com/primeqa."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1081,
      "original_url": "https://aclanthology.org/2023.acl-demo.6/",
      "title": "Lingxi: A Diversity-aware Chinese Modern Poetry Generation System",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.6.pdf",
      "authors": [
        "Xinran Zhang",
        "Maosong Sun",
        "Jiafeng Liu",
        "Xiaobing Li"
      ],
      "abstract": "Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose a novel framework that incorporates CWS in the tokenization process. The proposed method can achieve a high vocabulary coverage rate with a reasonable vocabulary size. For the decoding method and the diversity issue, we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry. Empirical results show that even when the top 60% of cumulative probability mass of the predicted distribution is flattened, our method achieves comparable or even better performance than baseline sampling methods. Our system is available at http://lingxi.website."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1082,
      "original_url": "https://aclanthology.org/2023.acl-demo.7/",
      "title": "Autodive: An Integrated Onsite Scientific Literature Annotation Tool",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.7.pdf",
      "authors": [
        "Yi Du",
        "Ludi Wang",
        "Mengyi Huang",
        "Dongze Song",
        "Wenjuan Cui",
        "Yuanchun Zhou"
      ],
      "abstract": "Scientific literature is always available in Adobe’s Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, we present Autodive, an integrated onsite scientific literature annotation tool for natural scientists and Natural Language Processing (NLP) researchers. This tool provides six core functions of annotation that support the whole lifecycle of corpus generation including i)annotation project management, ii)resource management, iii)ontology management, iv)manual annotation, v)onsite auto annotation, and vi)annotation task statistic. Two experiments are carried out to verify efficiency of the presented tool. A live demo of Autodive is available at http://autodive.sciwiki.cn. The source code is available at https://github.com/Autodive."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1083,
      "original_url": "https://aclanthology.org/2023.acl-demo.8/",
      "title": "A Practical Toolkit for Multilingual Question and Answer Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.8.pdf",
      "authors": [
        "Asahi Ushio",
        "Fernando Alva-Manchego",
        "Jose Camacho-Collados"
      ],
      "abstract": "Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e. a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small number of publicly accessible QAG models. In this paper, we introduce AutoQG, an online service for multilingual QAG along with lmqg, an all-in-one python package for model fine-tuning, generation, and evaluation. We also release QAG models in eight languages fine-tuned on a few variants of pre-trained encoder-decoder language models, which can be used online via AutoQG or locally via lmqg. With these resources, practitioners of any level can benefit from a toolkit that includes a web interface for end users, and easy-to-use code for developers who require custom models or fine-grained controls for generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1084,
      "original_url": "https://aclanthology.org/2023.acl-demo.9/",
      "title": "OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.9.pdf",
      "authors": [
        "Libo Qin",
        "Qiguang Chen",
        "Xiao Xu",
        "Yunlong Feng",
        "Wanxiang Che"
      ],
      "abstract": "Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is highly modularized and extensible by decomposing the model architecture, inference, and learning process into reusable modules, which allows researchers to quickly set up SLU experiments with highly flexible configurations. OpenSLU is implemented based on PyTorch, and released at https://github.com/LightChen233/OpenSLU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1085,
      "original_url": "https://aclanthology.org/2023.acl-demo.10/",
      "title": "SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.10.pdf",
      "authors": [
        "Jivnesh Sandhan",
        "Anshul Agarwal",
        "Laxmidhar Behera",
        "Tushar Sandhan",
        "Pawan Goyal"
      ],
      "abstract": "We present a neural Sanskrit Natural Language Processing (NLP) toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings. So far as we know, this is the first neural-based Sanskrit NLP toolkit that has a web-based interface and a number of NLP modules. We are sure that the people who are willing to work with Sanskrit will find it useful for pedagogical and annotative purposes. SanskritShala is available at: https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be accessed at: https://youtu.be/x0X31Y9k0mw4."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1086,
      "original_url": "https://aclanthology.org/2023.acl-demo.11/",
      "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.11.pdf",
      "authors": [
        "Victor Dibia"
      ],
      "abstract": "Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. Code and demo are available at this url - https://microsoft.github.io/lida/"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1087,
      "original_url": "https://aclanthology.org/2023.acl-demo.12/",
      "title": "MetaPro Online: A Computational Metaphor Processing Online System",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.12.pdf",
      "authors": [
        "Rui Mao",
        "Xiao Li",
        "Kai He",
        "Mengshi Ge",
        "Erik Cambria"
      ],
      "abstract": "Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept mappings, attracting great attention from cognitive science and psychology communities. Thus, we aim to develop a computational metaphor processing online system, termed MetaPro Online, that allows users without a coding background, e.g., language learners and linguists, to easily query metaphoricity labels, metaphor paraphrases, and concept mappings for non-domain-specific text. The outputs of MetaPro can be directly used by language learners and natural language processing downstream tasks because MetaPro is an end-to-end system."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1088,
      "original_url": "https://aclanthology.org/2023.acl-demo.13/",
      "title": "DIAGRAPH: An Open-Source Graphic Interface for Dialog Flow Design",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.13.pdf",
      "authors": [
        "Dirk Väth",
        "Lindsey Vanderlyn",
        "Ngoc Thang Vu"
      ],
      "abstract": "In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. Our goal for this tool is threefold: 1) To support subject-experts to intuitively create complex and flexible dialog systems,2) To support rapid prototyping of dialog system behavior, e.g., for research, and 3) To provide a hands-on test bed for students learning about dialog systems. To facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating dialog systems without requiring any coding knowledge. Once a dialog graph has been created, it is automatically turned into a dialog system using state of the art language models. This allows for rapid prototyping and testing. Dialog designers can then distribute a link to their finished dialog system or embed it into a website.Additionally, to support scientific experiments and data collection, dialog designers can access chat logs. Finally, to verify the usability of DIAGRAPH, we performed evaluation with subject-experts who extensively worked with the tool and users testing it for the first time, receiving above average System Usability Scale (SUS) scores from both (82 out 100 and 75 out of 100, respectively).In this way, we hope DIAGRAPH helps reduce the barrier to entry for creating dialog interactions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1089,
      "original_url": "https://aclanthology.org/2023.acl-demo.14/",
      "title": "disco: a toolkit for Distributional Control of Generative Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.14.pdf",
      "authors": [
        "Germán Kruszewski",
        "Jos Rozen",
        "Marc Dymetman"
      ],
      "abstract": "Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researchers have introduced distributional control techniques. These techniques, not limited to language, allow controlling the prevalence (i.e. expectations) of any features of interest in the model’s outputs. Despite their potential, the widespread adoption of these techniques has been hindered by the difficulty in adapting the complex, disconnected code. Here, we present disco, an open-source Python library that brings these techniques to the broader public"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1090,
      "original_url": "https://aclanthology.org/2023.acl-demo.15/",
      "title": "A Hyperparameter Optimization Toolkit for Neural Machine Translation Research",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.15.pdf",
      "authors": [
        "Xuan Zhang",
        "Kevin Duh",
        "Paul McNamee"
      ],
      "abstract": "Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort. Code: https://github.com/kevinduh/sockeye-recipes3Video demo: https://cs.jhu.edu/kevinduh/j/demo.mp4"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1091,
      "original_url": "https://aclanthology.org/2023.acl-demo.16/",
      "title": "Japanese-to-English Simultaneous Dubbing Prototype",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.16.pdf",
      "authors": [
        "Xiaolin Wang",
        "Masao Utiyama",
        "Eiichiro Sumita"
      ],
      "abstract": "Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces the original speech of a live video stream in a simultaneous manner. Tests on a collection of 90 public videos show that our system achieves a low average latency of 11.90 seconds for smooth playback. Our method is general and can be extended to other language pairs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1092,
      "original_url": "https://aclanthology.org/2023.acl-demo.17/",
      "title": "VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.17.pdf",
      "authors": [
        "Zijun Yao",
        "Yuanyong Chen",
        "Xin Lv",
        "Shulin Cao",
        "Amy Xin",
        "Jifan Yu",
        "Hailong Jin",
        "Jianjun Xu",
        "Peng Zhang",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "We present Visual Knowledge oriented Programming platform (VisKoP), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as ”dragging” to add knowledge operators and ”slot filling” to designate operator arguments. Moreover, VisKoP provides auto-completion for its knowledge base schema and users can easily debug the KoPL program by checking its intermediate results. To facilitate the practical KBQA on a million-entity-level KB, we design a highly efficient KoPL execution engine for the back-end. Experiment results show that VisKoP is highly efficient and user interaction can fix a large portion of wrong KoPL programs to acquire the correct answer. The VisKoP online demo, highly efficient KoPL engine, and screencast video are now publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1093,
      "original_url": "https://aclanthology.org/2023.acl-demo.18/",
      "title": "PEEP-Talk: A Situational Dialogue-based Chatbot for English Education",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.18.pdf",
      "authors": [
        "Seungjun Lee",
        "Yoonna Jang",
        "Chanjun Park",
        "Jungseob Lee",
        "Jaehyung Seo",
        "Hyeonseok Moon",
        "Sugyeong Eo",
        "Seounghoon Lee",
        "Bernardo Yahya",
        "Heuiseok Lim"
      ],
      "abstract": "English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-world situational dialogue-based chatbot designed for English education. It also naturally switches to a new topic or situation in response to out-of-topic utterances, which are common among English beginners. Furthermore, PEEP-Talk provides feedback score on conversation and grammar error correction. We performed automatic and user evaluations to validate performance and education efficiency of our system. The results show that PEEP-Talk generates appropriate responses in various real-life situations while providing accurate feedback to learners. Moreover, we demonstrate a positive impact on English-speaking, grammar, and English learning anxiety, implying that PEEP-Talk can lower the barrier to learning natural conversation in effective ways."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1094,
      "original_url": "https://aclanthology.org/2023.acl-demo.19/",
      "title": "OpenTIPE: An Open-source Translation Framework for Interactive Post-Editing Research",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.19.pdf",
      "authors": [
        "Fabian Landwehr",
        "Thomas Steinmann",
        "Laura Mascarell"
      ],
      "abstract": "Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, current approaches do not consider the human interactions that occur in real post-editing scenarios. To address this issue, we present OpenTIPE, a flexible and extensible framework that aims at supporting research on interactive post-editing. Specifically, the interactive environment of OpenTIPE allows researchers to explore human-centered approaches for the post-editing task. We release the OpenTIPE source code and showcase its main functionalities with a demonstration video and an online live demo."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1095,
      "original_url": "https://aclanthology.org/2023.acl-demo.20/",
      "title": "TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.20.pdf",
      "authors": [
        "Zhe Zhao",
        "Yudong Li",
        "Cheng Hou",
        "Jing Zhao",
        "Rong Tian",
        "Weijie Liu",
        "Yiren Chen",
        "Ningyuan Sun",
        "Haoyan Liu",
        "Weiquan Mao",
        "Han Guo",
        "Weigang Gou",
        "Taiqiang Wu",
        "Tao Zhu",
        "Wenhang Shi",
        "Chen Chen",
        "Shan Huang",
        "Sihong Chen",
        "Liqun Liu",
        "Feifei Li",
        "Xiaoshuai Chen",
        "Xingwu Sun",
        "Zhanhui Kang",
        "Xiaoyong Du",
        "Linlin Shen",
        "Kimmo Yan"
      ],
      "abstract": "Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the modular design. The toolkit uniformly divides pre-training models into 5 components: embedding, encoder, target embedding, decoder, and target. As almost all of common modules are provided in each component, users can choose the desired modules from different components to build a complete pre-training model. The modular design enables users to efficiently reproduce existing pre-training models or build brand-new one. We test the toolkit on text, vision, and audio benchmarks and show that it can match the performance of the original implementations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1096,
      "original_url": "https://aclanthology.org/2023.acl-demo.21/",
      "title": "NeuroX Library for Neuron Analysis of Deep NLP Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.21.pdf",
      "authors": [
        "Fahim Dalvi",
        "Hassan Sajjad",
        "Nadir Durrani"
      ],
      "abstract": "Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.Demo Video available at: https://youtu.be/mLhs2YMx4u8"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1097,
      "original_url": "https://aclanthology.org/2023.acl-demo.22/",
      "title": "SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.22.pdf",
      "authors": [
        "Nianlong Gu",
        "Richard H.R. Hahnloser"
      ],
      "abstract": "Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1098,
      "original_url": "https://aclanthology.org/2023.acl-demo.23/",
      "title": "Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.23.pdf",
      "authors": [
        "Chris Jenkins",
        "Shantanu Agarwal",
        "Joel Barry",
        "Steven Fincke",
        "Elizabeth Boschee"
      ],
      "abstract": "In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. “cholera outbreaks in Iran”) or structured queries (e.g. find all events of type Disease-Outbreak with agent “cholera” and location “Iran”)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1099,
      "original_url": "https://aclanthology.org/2023.acl-demo.24/",
      "title": "YANMTT: Yet Another Neural Machine Translation Toolkit",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.24.pdf",
      "authors": [
        "Raj Dabre",
        "Diptesh Kanojia",
        "Chinmay Sawant",
        "Eiichiro Sumita"
      ],
      "abstract": "In this paper, we present our open-source neural machine translation (NMT) toolkit called “Yet Another Neural Machine Translation Toolkit” abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1100,
      "original_url": "https://aclanthology.org/2023.acl-demo.25/",
      "title": "XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.25.pdf",
      "authors": [
        "Dong-Ho Lee",
        "Akshen Kadakia",
        "Brihi Joshi",
        "Aaron Chan",
        "Ziyi Liu",
        "Kiran Narahari",
        "Takashi Shibuya",
        "Ryosuke Mitani",
        "Toshiyuki Sekiya",
        "Jay Pujara",
        "Xiang Ren"
      ],
      "abstract": "NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, thenusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose XMD: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,users can flexibly provide various forms of feedback via an intuitive, web-based UI. After receiving user feedback, XMD automatically updates the model in real time, by regularizing the model so that its explanationsalign with the user feedback. The new model can then be easily deployed into real-world applications via Hugging Face. Using XMD, we can improve the model’s OOD performance on text classification tasks by up to 18%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1101,
      "original_url": "https://aclanthology.org/2023.acl-demo.26/",
      "title": "OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.26.pdf",
      "authors": [
        "Shengding Hu",
        "Ning Ding",
        "Weilin Zhao",
        "Xingtai Lv",
        "Zhen Zhang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as “delta tuning” in Ding et al. (2022), which updates only a small subset of parameters, known as “delta modules”, while keeping the backbone model’s parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs’ code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large PTMs efficiently."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1102,
      "original_url": "https://aclanthology.org/2023.acl-demo.27/",
      "title": "Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.27.pdf",
      "authors": [
        "Itay Yair",
        "Hillel Taub-Tabib",
        "Yoav Goldberg"
      ],
      "abstract": "Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what’s available, and achance to dive deeper on some aspects. The sys-tem works by grouping similar items together,and arranging the remaining items into a hierar-chical navigable DAG structure. We apply themethod to medical information extraction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1103,
      "original_url": "https://aclanthology.org/2023.acl-demo.28/",
      "title": "CARE: Collaborative AI-Assisted Reading Environment",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.28.pdf",
      "authors": [
        "Dennis Zyska",
        "Nils Dycke",
        "Jan Buchmann",
        "Ilia Kuznetsov",
        "Iryna Gurevych"
      ],
      "abstract": "Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as text classification, generation or question answering. The extensible behavioral logging allows unique insights into the reading and commenting behavior, and flexible configuration makes the platform easy to deploy in new scenarios. To evaluate CARE in action, we apply the platform in a user study dedicated to scholarly peer review. CARE facilitates the data collection and study of inline commentary in NLP, extrinsic evaluation of NLP assistance, and application prototyping. We invite the community to explore and build upon the open source implementation of CARE.Github Repository: https://github.com/UKPLab/CAREPublic Live Demo: https://care.ukp.informatik.tu-darmstadt.de"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1104,
      "original_url": "https://aclanthology.org/2023.acl-demo.29/",
      "title": "The ROOTS Search Tool: Data Transparency for LLMs",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.29.pdf",
      "authors": [
        "Aleksandra Piktus",
        "Christopher Akiki",
        "Paulo Villegas",
        "Hugo Laurençon",
        "Gérard Dupont",
        "Sasha Luccioni",
        "Yacine Jernite",
        "Anna Rogers"
      ],
      "abstract": "ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces: https://huggingface.co/spaces/bigscience-data/roots-search. We describe our implementation and the possible use cases of our tool."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1105,
      "original_url": "https://aclanthology.org/2023.acl-demo.30/",
      "title": "The OPUS-MT Dashboard – A Toolkit for a Systematic Evaluation of Open Machine Translation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.30.pdf",
      "authors": [
        "Jörg Tiedemann",
        "Ona de Gibert"
      ],
      "abstract": "The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS-MT and Tatoeba models as well as external models from the Huggingface repository and user-contributed translations. The functionalities of the evaluation tool include summaries of benchmarks for over 2,300 models covering 4,560 language directions and 294 languages, as well as the inspection of predicted translations against their human reference. We focus on centralization, reproducibility and coverage of MT evaluation combined with scalability. The dashboard can be accessed live at https://opus.nlpl.eu/dashboard/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1106,
      "original_url": "https://aclanthology.org/2023.acl-demo.31/",
      "title": "The D-WISE Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.31.pdf",
      "authors": [
        "Florian Schneider",
        "Tim Fischer",
        "Fynn Petersen-Frey",
        "Isabel Eiser",
        "Gertraud Koch",
        "Chris Biemann"
      ],
      "abstract": "This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in which the discourses of contemporary societies are encoded. To provide meaningful insights from such data, our system leverages and combines state-of-the-art machine learning technologies from Natural Language Processing and Com-puter Vision. Further, the DWTS is conceived and developed by an interdisciplinary team ofcultural anthropologists and computer scientists to ensure the tool’s usability for modernDH research. Central features of the DWTS are: a) import of multi-modal data like text, image, audio, and video b) preprocessing pipelines for automatic annotations c) lexical and semantic search of documents d) manual span, bounding box, time-span, and frame annotations e) documentation of the research process."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1107,
      "original_url": "https://aclanthology.org/2023.acl-demo.32/",
      "title": "OpenRT: An Open-source Framework for Reasoning Over Tabular Data",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.32.pdf",
      "authors": [
        "Yilun Zhao",
        "Boyu Mi",
        "Zhenting Qi",
        "Linyong Nan",
        "Minghao Guo",
        "Arman Cohan",
        "Dragomir Radev"
      ],
      "abstract": "There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1108,
      "original_url": "https://aclanthology.org/2023.acl-demo.33/",
      "title": "UINAUIL: A Unified Benchmark for Italian Natural Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.33.pdf",
      "authors": [
        "Valerio Basile",
        "Livio Bioglio",
        "Alessio Bosca",
        "Cristina Bosco",
        "Viviana Patti"
      ],
      "abstract": "This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available Italian and multilingual language models on UINAUIL, providing an updated picture of the current state of the art in Italian NLU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1109,
      "original_url": "https://aclanthology.org/2023.acl-demo.34/",
      "title": "Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.34.pdf",
      "authors": [
        "Gabriele Picco",
        "Marcos Martinez Galindo",
        "Alberto Purpura",
        "Leopold Fuchs",
        "Vanessa Lopez",
        "Thanh Lam Hoang"
      ],
      "abstract": "The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models. In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1110,
      "original_url": "https://aclanthology.org/2023.acl-demo.35/",
      "title": "BiSync: A Bilingual Editor for Synchronized Monolingual Texts",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.35.pdf",
      "authors": [
        "Josep Crego",
        "Jitao Xu",
        "François Yvon"
      ],
      "abstract": "In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process. In addition, CAT systems assume that the source sentence is fixed and also restrict the possible changes on the target side. In order to make the writing process smoother, we present BiSync, a bilingual writing assistant that allows users to freely compose text in two languages, while maintaining the two monolingual texts synchronized. We also include additional functionalities, such as the display of alternative prefix translations and paraphrases, which are intended to facilitate the authoring of texts. We detail the model architecture used for synchronization and evaluate the resulting tool, showing that high accuracy can be attained with limited computational resources. The interface and models are publicly available at https://github.com/jmcrego/BiSync and a demonstration video can be watched on YouTube https://youtu.be/_l-ugDHfNgU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1111,
      "original_url": "https://aclanthology.org/2023.acl-demo.36/",
      "title": "Riveter: Measuring Power and Social Dynamics Between Entities",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.36.pdf",
      "authors": [
        "Maria Antoniak",
        "Anjalie Field",
        "Jimin Mun",
        "Melanie Walsh",
        "Lauren Klein",
        "Maarten Sap"
      ],
      "abstract": "Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1112,
      "original_url": "https://aclanthology.org/2023.acl-demo.37/",
      "title": "Fast Whitespace Correction with Encoder-Only Transformers",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.37.pdf",
      "authors": [
        "Hannah Bast",
        "Matthias Hertel",
        "Sebastian Walter"
      ],
      "abstract": "The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text “whi te space correctio nwithTransf or mers”, produce “whitespace correction with Transformers”. We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F1-score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1113,
      "original_url": "https://aclanthology.org/2023.acl-demo.38/",
      "title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.38.pdf",
      "authors": [
        "Brian Yan",
        "Jiatong Shi",
        "Yun Tang",
        "Hirofumi Inaguma",
        "Yifan Peng",
        "Siddharth Dalmia",
        "Peter Polák",
        "Patrick Fernandes",
        "Dan Berrebbi",
        "Tomoki Hayashi",
        "Xiaohui Zhang",
        "Zhaoheng Ni",
        "Moto Hira",
        "Soumi Maiti",
        "Juan Pino",
        "Shinji Watanabe"
      ],
      "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1114,
      "original_url": "https://aclanthology.org/2023.acl-demo.39/",
      "title": "CB2: Collaborative Natural Language Interaction Research Platform",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.39.pdf",
      "authors": [
        "Jacob Sharf",
        "Mustafa Omer Gul",
        "Yoav Artzi"
      ],
      "abstract": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1115,
      "original_url": "https://aclanthology.org/2023.acl-demo.40/",
      "title": "Inseq: An Interpretability Toolkit for Sequence Generation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.40.pdf",
      "authors": [
        "Gabriele Sarti",
        "Nils Feldhus",
        "Ludwig Sickert",
        "Oskar van der Wal"
      ],
      "abstract": "Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models’ internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1116,
      "original_url": "https://aclanthology.org/2023.acl-demo.41/",
      "title": "Pipeline for modeling causal beliefs from natural language",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.41.pdf",
      "authors": [
        "John Priniski",
        "Ishaan Verma",
        "Fred Morstatter"
      ],
      "abstract": "We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1117,
      "original_url": "https://aclanthology.org/2023.acl-demo.42/",
      "title": "TabGenie: A Toolkit for Table-to-Text Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.42.pdf",
      "authors": [
        "Zdeněk Kasner",
        "Ekaterina Garanina",
        "Ondrej Platek",
        "Ondrej Dusek"
      ],
      "abstract": "Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie – a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In TabGenie, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text generation, facilitates side-by-side comparison of generated system outputs, and allows easy exports for manual analysis. Furthermore, TabGenie is equipped with command line processing tools and Python bindings for unified dataset loading and processing. We release TabGenie as a PyPI package and provide its open-source code and a live demo at https://github.com/kasnerz/tabgenie."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1118,
      "original_url": "https://aclanthology.org/2023.acl-demo.43/",
      "title": "An Efficient Conversational Smart Compose System",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.43.pdf",
      "authors": [
        "Yun Zhu",
        "Xiayu Chen",
        "Lei Shu",
        "Bowen Tan",
        "Xinying Song",
        "Lijuan Liu",
        "Maria Wang",
        "Jindong Chen",
        "Ning Ruan"
      ],
      "abstract": "Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time. This paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. Heuristics from different perspectives are designed to achieve the best trade-off between quality and latency. From the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. Besides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. Additionally, the caching mechanism is applied to the serving framework. The demo video of the system is available at https://youtu.be/U1KXkaqr60g.We open-sourced our phrase tokenizer in https://github.com/tensorflow/text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1119,
      "original_url": "https://aclanthology.org/2023.acl-demo.44/",
      "title": "Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.44.pdf",
      "authors": [
        "Robin Chan",
        "Afra Amini",
        "Mennatallah El-Assady"
      ],
      "abstract": "We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. The dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. Additionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback. Through our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models’ robustness by creating adversarial test suites."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1120,
      "original_url": "https://aclanthology.org/2023.acl-demo.45/",
      "title": "LaTeX2Solver: a Hierarchical Semantic Parsing of LaTeX Document into Code for an Assistive Optimization Modeling Application",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.45.pdf",
      "authors": [
        "Rindra Ramamonjison",
        "Timothy Yu",
        "Linzi Xing",
        "Mahdi Mostajabdaveh",
        "Xiaorui Li",
        "Xiaojin Fu",
        "Xiongwei Han",
        "Yuanzhe Chen",
        "Ren Li",
        "Kun Mao",
        "Yong Zhang"
      ],
      "abstract": "We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an in-depth understanding of the problem description and a technical expertise to produce the modeling code. Thus, our proposed system TeX2Solver helps partially automate this conversion and help the users build optimization models more efficiently. In this paper, we describe its interface and the components of the hierarchical parsing system. A video demo walk-through is available online at http://bit.ly/3kuOm3x"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1121,
      "original_url": "https://aclanthology.org/2023.acl-demo.46/",
      "title": "Alfred: A System for Prompted Weak Supervision",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.46.pdf",
      "authors": [
        "Peilin Yu",
        "Stephen Bach"
      ],
      "abstract": "Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms. We find that this optimization improves query throughput by 2.9x versus a naive approach. We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available at https://github.com/BatsResearch/alfred."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1122,
      "original_url": "https://aclanthology.org/2023.acl-demo.47/",
      "title": "OpenICL: An Open-Source Framework for In-context Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.47.pdf",
      "authors": [
        "Zhenyu Wu",
        "Yaoxiang Wang",
        "Jiacheng Ye",
        "Zhiyong Wu",
        "Jiangtao Feng",
        "Jingjing Xu",
        "Yu Qiao"
      ],
      "abstract": "In recent years, In-context Learning (ICL) has gained increasing attentionand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1123,
      "original_url": "https://aclanthology.org/2023.acl-demo.48/",
      "title": "Self-Supervised Sentence Polishing by Adding Engaging Modifiers",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.48.pdf",
      "authors": [
        "Zhexin Zhang",
        "Jian Guan",
        "Xin Cui",
        "Yu Ran",
        "Bo Liu",
        "Minlie Huang"
      ],
      "abstract": "Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at http://coai.cs.tsinghua.edu.cn/static/polishSent/. A demo video is available at https://youtu.be/Y6gFHOgSv8Y."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1124,
      "original_url": "https://aclanthology.org/2023.acl-demo.49/",
      "title": "Effidit: An Assistant for Improving Writing Efficiency",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.49.pdf",
      "authors": [
        "Shuming Shi",
        "Enbo Zhao",
        "Wei Bi",
        "Deng Cai",
        "Leyang Cui",
        "Xinting Huang",
        "Haiyun Jiang",
        "Duyu Tang",
        "Kaiqiang Song",
        "Longyue Wang",
        "Chenyan Huang",
        "Guoping Huang",
        "Yan Wang",
        "Piji Li"
      ],
      "abstract": "Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (Efficient and Intelligent Editing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) technologies. We significantly expand the capacities of a writing assistantby providing functions in three modules: text completion, hint recommendation, and writing refinement. Based on the above efforts, Effidit can efficiently assist users in creating their own text. Effidit has been deployed to several Tencent products and publicly released at https://effidit.qq.com/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1125,
      "original_url": "https://aclanthology.org/2023.acl-demo.50/",
      "title": "WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.50.pdf",
      "authors": [
        "Zijie J. Wang",
        "Fred Hohman",
        "Duen Horng Chau"
      ],
      "abstract": "Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, WizMap enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as WebGL and Web Workers, WizMap scales to millions of embedding points directly in users’ web browsers and computational notebooks without the need for dedicated backend servers. WizMap is open-source and available at the following public demo link: https://poloclub.github.io/wizmap."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1126,
      "original_url": "https://aclanthology.org/2023.acl-demo.51/",
      "title": "A System for Answering Simple Questions in Multiple Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.51.pdf",
      "authors": [
        "Anton Razzhigaev",
        "Mikhail Salnikov",
        "Valentin Malykh",
        "Pavel Braslavski",
        "Alexander Panchenko"
      ],
      "abstract": "Our research focuses on the most prevalent type of queries— simple questions —exemplified by questions like “What is the capital of France?”. These questions reference an entity such as “France”, which is directly connected (one hop) to the answer entity “Paris” in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question’s text embeddings and the answer’s graph embeddings. A system incorporating this novel method is also described in our work. Through comprehensive experimentation using various English and multilingual datasets and two KGs — Freebase and Wikidata — we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1127,
      "original_url": "https://aclanthology.org/2023.acl-demo.52/",
      "title": "KWJA: A Unified Japanese Analyzer Based on Foundation Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.52.pdf",
      "authors": [
        "Nobuhiro Ueda",
        "Kazumasa Omura",
        "Takashi Kodama",
        "Hirokazu Kiyomaru",
        "Yugo Murawaki",
        "Daisuke Kawahara",
        "Sadao Kurohashi"
      ],
      "abstract": "We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.KWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS analysis, bridging reference resolution, coreference resolution, and discourse relation analysis, making it the most versatile among existing Japanese text analyzers.KWJA solves these tasks in a multi-task manner but still achieves competitive or better performance compared to existing analyzers specialized for each task.KWJA is publicly available under the MIT license at https://github.com/ku-nlp/kwja."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1128,
      "original_url": "https://aclanthology.org/2023.acl-demo.53/",
      "title": "Disease Network Constructor: a Pathway Extraction and Visualization",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.53.pdf",
      "authors": [
        "Mohammad Golam Sohrab",
        "Khoa Duong",
        "Goran Topić",
        "Masami Ikeda",
        "Nozomi Nagano",
        "Yayoi Natsume-Kitatani",
        "Masakata Kuroda",
        "Mari Itoh",
        "Hiroya Takamura"
      ],
      "abstract": "We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC’s back-endimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from https://youtu.be/rFhWwAgcXE8. We release an online system for end users and the source code is available at https://github.com/aistairc/PRISM-APIs/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1129,
      "original_url": "https://aclanthology.org/2023.acl-demo.54/",
      "title": "Petals: Collaborative Inference and Fine-tuning of Large Models",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.54.pdf",
      "authors": [
        "Alexander Borzunov",
        "Dmitry Baranchuk",
        "Tim Dettmers",
        "Maksim Riabinin",
        "Younes Belkada",
        "Artem Chumachenko",
        "Pavel Samygin",
        "Colin Raffel"
      ],
      "abstract": "Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with ≈1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.mlVideo (2 min): https://youtu.be/F4muLI-0hTE"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1130,
      "original_url": "https://aclanthology.org/2023.acl-demo.55/",
      "title": "UKP-SQuARE v3: A Platform for Multi-Agent QA Research",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.55.pdf",
      "authors": [
        "Haritz Puerto",
        "Tim Baumgärtner",
        "Rachneet Sachdeva",
        "Haishuo Fang",
        "Hao Zhang",
        "Sewin Tariverdian",
        "Kexin Wang",
        "Iryna Gurevych"
      ],
      "abstract": "The continuous development of Question Answering (QA) datasets has drawn the research community’s attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1131,
      "original_url": "https://aclanthology.org/2023.acl-demo.56/",
      "title": "Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.56.pdf",
      "authors": [
        "Mete Sertkan",
        "Sophia Althammer",
        "Sebastian Hofstätter"
      ],
      "abstract": "In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger’s benefits."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1132,
      "original_url": "https://aclanthology.org/2023.acl-demo.57/",
      "title": "GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.57.pdf",
      "authors": [
        "Aleksandra Piktus",
        "Odunayo Ogundepo",
        "Christopher Akiki",
        "Akintunde Oladipo",
        "Xinyu Zhang",
        "Hailey Schoelkopf",
        "Stella Biderman",
        "Martin Potthast",
        "Jimmy Lin"
      ],
      "abstract": "Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility. We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia. We then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1133,
      "original_url": "https://aclanthology.org/2023.acl-demo.58/",
      "title": "DeepPavlov Dream: Platform for Building Generative AI Assistants",
      "pdf_url": "https://aclanthology.org/2023.acl-demo.58.pdf",
      "authors": [
        "Diliara Zharikova",
        "Daniel Kornev",
        "Fedor Ignatov",
        "Maxim Talimanchuk",
        "Dmitry Evseev",
        "Ksenya Petukhova",
        "Veronika Smilga",
        "Dmitry Karpov",
        "Yana Shishkina",
        "Dmitry Kosenko",
        "Mikhail Burtsev"
      ],
      "abstract": "An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering, handle multiple prompts during the same dialog session and create simple multimodal assistants."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1134,
      "original_url": "https://aclanthology.org/2023.acl-srw.1/",
      "title": "ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.1.pdf",
      "authors": [
        "Dongqi Pu",
        "Vera Demberg"
      ],
      "abstract": "Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT’s performance in two controllable generation tasks, with respect to ChatGPT’s ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model’s performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1135,
      "original_url": "https://aclanthology.org/2023.acl-srw.2/",
      "title": "Multi-Dialectal Representation Learning of Sinitic Phonology",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.2.pdf",
      "authors": [
        "Zhibai Jia"
      ],
      "abstract": "Machine learning techniques have shown their competence for representing and reasoning in symbolic systems such as language and phonology. In Sinitic Historical Phonology, notable tasks that could benefit from machine learning include the comparison of dialects and reconstruction of proto-languages systems. Motivated by this, this paper provides an approach for obtaining multi-dialectal representations of Sinitic syllables, by constructing a knowledge graph from structured phonological data ,then applying the BoxE technique from knowledge base learning. We applied unsupervised clustering techniques to the obtained representations to observe that the representations capture phonemic contrast from the input dialects. Furthermore, we trained classifiers to perform inference of unobserved Middle Chinese labels, showing the representations’ potential for indicating archaic, proto-language features. The representations can be used for performing completion of fragmented Sinitic phonological knowledge bases, estimating divergences between different characters, or aiding the exploration and reconstruction of archaic features."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1136,
      "original_url": "https://aclanthology.org/2023.acl-srw.4/",
      "title": "Prompt-based Zero-shot Text Classification with Conceptual Knowledge",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.4.pdf",
      "authors": [
        "Yuqi Wang",
        "Wei Wang",
        "Qi Chen",
        "Kaizhu Huang",
        "Anh Nguyen",
        "Suparna De"
      ],
      "abstract": "In recent years, pre-trained language models have garnered significant attention due to their effectiveness, which stems from the rich knowledge acquired during pre-training. To mitigate the inconsistency issues between pre-training tasks and downstream tasks and to facilitate the resolution of language-related issues, prompt-based approaches have been introduced, which are particularly useful in low-resource scenarios. However, existing approaches mostly rely on verbalizers to translate the predicted vocabulary to task-specific labels. The major limitations of this approach are the ignorance of potentially relevant domain-specific words and being biased by the pre-training data. To address these limitations, we propose a framework that incorporates conceptual knowledge for text classification in the extreme zero-shot setting. The framework includes prompt-based keyword extraction, weight assignment to each prompt keyword, and final representation estimation in the knowledge graph embedding space. We evaluated the method on four widely-used datasets for sentiment analysis and topic detection, demonstrating that it consistently outperforms recently-developed prompt-based approaches in the same experimental settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1137,
      "original_url": "https://aclanthology.org/2023.acl-srw.5/",
      "title": "How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.5.pdf",
      "authors": [
        "Takuro Fujii",
        "Koki Shibata",
        "Atsuki Yamaguchi",
        "Terufumi Morishita",
        "Yasuhiro Sogawa"
      ],
      "abstract": "This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1138,
      "original_url": "https://aclanthology.org/2023.acl-srw.7/",
      "title": "Semantic-Aware Dynamic Retrospective-Prospective Reasoning for Event-Level Video Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.7.pdf",
      "authors": [
        "Chenyang Lyu",
        "Tianbo Ji",
        "Yvette Graham",
        "Jennifer Foster"
      ],
      "abstract": "Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code is publicly available at https://github.com/lyuchenyang/Semantic-aware-VideoQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1139,
      "original_url": "https://aclanthology.org/2023.acl-srw.8/",
      "title": "Jamp: Controlled Japanese Temporal Inference Dataset for Evaluating Generalization Capacity of Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.8.pdf",
      "authors": [
        "Tomoki Sugimoto",
        "Yasumasa Onoe",
        "Hitomi Yanaka"
      ],
      "abstract": "Natural Language Inference (NLI) tasks involving temporal inference remain challenging for pre-trained language models (LMs). Although various datasets have been created for this task, they primarily focus on English and do not address the need for resources in other languages. It is unclear whether current LMs realize the generalization capacity for temporal inference across languages. In this paper, we present Jamp, a Japanese NLI benchmark focused on temporal inference. Our dataset includes a range of temporal inference patterns, which enables us to conduct fine-grained analysis. To begin the data annotation process, we create diverse inference templates based on the formal semantics test suites. We then automatically generate diverse NLI examples by using the Japanese case frame dictionary and well-designed templates while controlling the distribution of inference patterns and gold labels. We evaluate the generalization capacities of monolingual/multilingual LMs by splitting our dataset based on tense fragments (i.e., temporal inference patterns). Our findings demonstrate that LMs struggle with specific linguistic phenomena, such as habituality, indicating that there is potential for the development of more effective NLI models across languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1140,
      "original_url": "https://aclanthology.org/2023.acl-srw.10/",
      "title": "Constructing Multilingual Code Search Dataset Using Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.10.pdf",
      "authors": [
        "Ryo Sekizawa",
        "Nan Duan",
        "Shuai Lu",
        "Hitomi Yanaka"
      ],
      "abstract": "Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate them on multiple code search test sets. Our results show that the model pre-trained with all natural and programming language data has performed best in most cases. By applying back-translation data filtering to our dataset, we demonstrate that the translation quality affects the model’s performance to a certain extent, but the data size matters more."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1141,
      "original_url": "https://aclanthology.org/2023.acl-srw.12/",
      "title": "Multimodal Neural Machine Translation Using Synthetic Images Transformed by Latent Diffusion Model",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.12.pdf",
      "authors": [
        "Ryoya Yuasa",
        "Akihiro Tamura",
        "Tomoyuki Kajiwara",
        "Takashi Ninomiya",
        "Tsuneo Kato"
      ],
      "abstract": "This study proposes a new multimodal neural machine translation (MNMT) model using synthetic images transformed by a latent diffusion model. MNMT translates a source language sentence based on its related image, but the image usually contains noisy information that are not relevant to the source language sentence. Our proposed method first generates a synthetic image corresponding to the content of the source language sentence by using a latent diffusion model and then performs translation based on the synthetic image. The experiments on the English-German translation tasks using the Multi30k dataset demonstrate the effectiveness of the proposed method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1142,
      "original_url": "https://aclanthology.org/2023.acl-srw.15/",
      "title": "Enhancing Ancient Chinese Understanding with Derived Noisy Syntax Trees",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.15.pdf",
      "authors": [
        "Ping Wang",
        "Shitou Zhang",
        "Zuchao Li",
        "Jingrui Hou"
      ],
      "abstract": "Despite the rapid development of neural-based models, syntax still plays a crucial role in modern natural language processing. However, few studies have incorporated syntactic information into ancient Chinese understanding tasks due to the lack of syntactic annotation. This paper explores the role of syntax in ancient Chinese understanding based on the noisy syntax trees from unsupervised derivation and modern Chinese syntax parsers. On top of that, we propose a novel syntax encoding component – confidence-based syntax encoding network (cSEN) to alleviate the side effects from the existing noise caused by unsupervised syntax derivation and the incompatibility between ancient and modern Chinese. Experiments on two typical ancient Chinese understanding tasks, ancient poetry theme classification and ancient-modern Chinese translation, demonstrate that syntactic information can effectively enhance the understanding of ancient Chinese over strong baselines, and that the proposed cSEN plays an important role in noisy scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1143,
      "original_url": "https://aclanthology.org/2023.acl-srw.17/",
      "title": "The Turing Quest: Can Transformers Make Good NPCs?",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.17.pdf",
      "authors": [
        "Qi Chen Gao",
        "Ali Emami"
      ],
      "abstract": "In this paper, we study the viability of the deployment of language models towards non-playable character (NPC) scripts, by introducing a novel pipeline for the automatic construction of NPC scripts using Transformer-based believable scripts for a variety of game genres and specifications. In addition, we propose a self-diagnosis method inspired by previous work to develop language models, tailored specifically to desirable NPC qualities such as coherency, believability, and degree of repetition. Finally, we propose a new benchmark, called The Turing Quest, which we use to show that the pipeline, when applied to GPT-3, can generate for a variety of game genres and contexts, NPC scripts that can fool judges in thinking they have been written by humans. We believe that these findings can greatly benefit both the gaming industry and its global community of users, since many current games continue to base their NPCs on manually-curated scripts that are resource-demanding and may curb the immersiveness and enjoyment of the user."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1144,
      "original_url": "https://aclanthology.org/2023.acl-srw.18/",
      "title": "Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.18.pdf",
      "authors": [
        "Hongyi Zheng",
        "Yixin Zhu",
        "Lavender Jiang",
        "Kyunghyun Cho",
        "Eric Oermann"
      ],
      "abstract": "Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1145,
      "original_url": "https://aclanthology.org/2023.acl-srw.19/",
      "title": "Intriguing Effect of the Correlation Prior on ICD-9 Code Assignment",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.19.pdf",
      "authors": [
        "Zihao Yang",
        "Chenkang Zhang",
        "Muru Wu",
        "Xujin Liu",
        "Lavender Jiang",
        "Kyunghyun Cho",
        "Eric Oermann"
      ],
      "abstract": "The Ninth Revision of the International Classification of Diseases (ICD-9) is a standardized coding system used to classify health conditions. It is used for billing, tracking individual patient conditions, and for epidemiology. The highly detailed and technical nature of the codes and their associated medical conditions make it difficult for humans to accurately record them. Researchers have explored the use of neural networks, particularly language models, for automated ICD-9 code assignment. However, the imbalanced distribution of ICD-9 codes leads to poor performance. One solution is to use domain knowledge to incorporate a useful prior. This paper evaluates the usefulness of the correlation bias: we hypothesize that correlations between ICD-9 codes and other medical codes could help improve language models’ performance. We showed that while the correlation bias worsens the overall performance, the effect on individual class can be negative or positive. Performance on classes that are more imbalanced and less correlated with other codes is more sensitive to incorporating the correlation bias. This suggests that while the correlation bias has potential to improve ICD-9 code assignment in certain cases, the applicability criteria need to be more carefully studied."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1146,
      "original_url": "https://aclanthology.org/2023.acl-srw.20/",
      "title": "Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.20.pdf",
      "authors": [
        "Mateusz Baran",
        "Joanna Baran",
        "Mateusz Wójcik",
        "Maciej Zięba",
        "Adam Gonczarek"
      ],
      "abstract": "State-of-the-art models can perform well in controlled environments, but they often struggle when presented with out-of-distribution (OOD) examples, making OOD detection a critical component of NLP systems. In this paper, we focus on highlighting the limitations of existing approaches to OOD detection in NLP. Specifically, we evaluated eight OOD detection methods that are easily integrable into existing NLP systems and require no additional OOD data or model modifications. One of our contributions is providing a well-structured research environment that allows for full reproducibility of the results. Additionally, our analysis shows that existing OOD detection methods for NLP tasks are not yet sufficiently sensitive to capture all samples characterized by various types of distributional shifts. Particularly challenging testing scenarios arise in cases of background shift and randomly shuffled word order within in domain texts. This highlights the need for future work to develop more effective OOD detection approaches for the NLP problems, and our work provides a well-defined foundation for further research in this area."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1147,
      "original_url": "https://aclanthology.org/2023.acl-srw.22/",
      "title": "Can LMs Store and Retrieve 1-to-N Relational Knowledge?",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.22.pdf",
      "authors": [
        "Haruki Nagasawa",
        "Benjamin Heinzerling",
        "Kazuma Kokuta",
        "Kentaro Inui"
      ],
      "abstract": "It has been suggested that pretrained language models can be viewed as knowledge bases. One of the prerequisites for using language models as knowledge bases is how accurately they can store and retrieve world knowledge. It is already revealed that language models can store much 1-to-1 relational knowledge, such as ”country and its capital,” with high memorization accuracy. On the other hand, world knowledge includes not only 1-to-1 but also 1-to-N relational knowledge, such as ”parent and children.”However, it is not clear how accurately language models can handle 1-to-N relational knowledge. To investigate language models’ abilities toward 1-to-N relational knowledge, we start by designing the problem settings. Specifically, we organize the character of 1-to-N relational knowledge and define two essential skills: (i) memorizing multiple objects individually and (ii) retrieving multiple stored objects without excesses or deficiencies at once. We inspect LMs’ ability to handle 1-to-N relational knowledge on the controlled synthesized data. As a result, we report that it is possible to memorize multiple objects with high accuracy, but generalizing the retrieval ability (expressly, enumeration) is challenging."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1148,
      "original_url": "https://aclanthology.org/2023.acl-srw.24/",
      "title": "Theoretical Linguistics Rivals Embeddings in Language Clustering for Multilingual Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.24.pdf",
      "authors": [
        "Sakura Imai",
        "Daisuke Kawahara",
        "Naho Orita",
        "Hiromune Oda"
      ],
      "abstract": "While embedding-based methods have been dominant in language clustering for multilingual tasks, clustering based on linguistic features has not yet been explored much, as it remains baselines (Tan et al., 2019; Shaffer, 2021). This study investigates whether and how theoretical linguistics improves language clustering for multilingual named entity recognition (NER). We propose two types of language groupings: one based on morpho-syntactic features in a nominal domain and one based on a head parameter. Our NER experiments show that the proposed methods largely outperform a state-of-the-art embedding-based model, suggesting that theoretical linguistics plays a significant role in multilingual learning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1149,
      "original_url": "https://aclanthology.org/2023.acl-srw.26/",
      "title": "Native Language Prediction from Gaze: a Reproducibility Study",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.26.pdf",
      "authors": [
        "Lina Skerath",
        "Paulina Toborek",
        "Anita Zielińska",
        "Maria Barrett",
        "Rob Van Der Goot"
      ],
      "abstract": "Numerous studies found that the linguistic properties of a person’s native language affect the cognitive processing of other languages. However, only one study has shown that it was possible to identify the native language based on eye-tracking records of natural L2 reading using machine learning. A new corpus allows us to replicate these results on a more interrelated and larger set of native languages. Our results show that comparable classification performance is maintained despite using less data. However, analysis shows that the correlation between L2 eye movements and native language similarity may be more complex than the original study found."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1150,
      "original_url": "https://aclanthology.org/2023.acl-srw.27/",
      "title": "MedTem2.0: Prompt-based Temporal Classification of Treatment Events from Discharge Summaries",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.27.pdf",
      "authors": [
        "Yang Cui",
        "Lifeng Han",
        "Goran Nenadic"
      ],
      "abstract": "Discharge summaries are comprehensive medical records that encompass vital information about a patient’s hospital stay. A crucial aspect of discharge summaries is the temporal information of treatments administered throughout the patient’s illness. With an extensive volume of clinical documents, manually extracting and compiling a patient’s medication list can be laborious, time-consuming, and susceptible to errors. The objective of this paper is to build upon the recent development on clinical NLP by temporally classifying treatments in clinical texts, specifically determining whether a treatment was administered between the time of admission and discharge from the hospital. State-of-the-art NLP methods including prompt-based learning on Generative Pre-trained Transformers (GPTs) models and fine-tuning on pre-trained language models (PLMs) such as BERT were employed to classify temporal relations between treatments and hospitalisation periods in discharge summaries. Fine-tuning with the BERT model achieved an F1 score of 92.45% and a balanced accuracy of 77.56%, while prompt learning using the T5 model and mixed templates resulted in an F1 score of 90.89% and a balanced accuracy of 72.07%.Our codes and data are available at https://github.com/HECTA-UoM/MedTem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1151,
      "original_url": "https://aclanthology.org/2023.acl-srw.28/",
      "title": "Sudden Semantic Shifts in Swedish NATO discourse",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.28.pdf",
      "authors": [
        "Brian Bonafilia",
        "Bastiaan Bruinsma",
        "Denitsa Saynova",
        "Moa Johansson"
      ],
      "abstract": "In this paper, we investigate a type of semantic shift that occurs when a sudden event radically changes public opinion on a topic. Looking at Sweden’s decision to apply for NATO membership in 2022, we use word embeddings to study how the associations users on Twitter have regarding NATO evolve. We identify several changes that we successfully validate against real-world events. However, the low engagement of the public with the issue often made it challenging to distinguish true signals from noise. We thus find that domain knowledge and data selection are of prime importance when using word embeddings to study semantic shifts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1152,
      "original_url": "https://aclanthology.org/2023.acl-srw.29/",
      "title": "Building a Buzzer-quiz Answering System",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.29.pdf",
      "authors": [
        "Naoya Sugiura",
        "Kosuke Yamada",
        "Ryohei Sasano",
        "Koichi Takeda",
        "Katsuhiko Toyama"
      ],
      "abstract": "A buzzer quiz is a genre of quiz in which multiple players simultaneously listen to a quiz being read aloud and respond it by buzzing in as soon as they can predict the answer. Because incorrect answers often result in penalties, a buzzer-quiz answering system must not only predict the answer from only part of a question but also estimate the predicted answer’s accuracy. In this paper, we introduce two types of buzzer-quiz answering systems: (1) a system that directly generates an answer from part of a question by using an autoregressive language model; and (2) a system that first reconstructs the entire question by using an autoregressive language model and then determines the answer according to the reconstructed question. We then propose a method to estimate the accuracy of the answers for each system by using the internal scores of each model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1153,
      "original_url": "https://aclanthology.org/2023.acl-srw.30/",
      "title": "Probing for Hyperbole in Pre-Trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.30.pdf",
      "authors": [
        "Nina Schneidermann",
        "Daniel Hershcovich",
        "Bolette Pedersen"
      ],
      "abstract": "Hyperbole is a common figure of speech, which is under-explored in NLP research. In this study, we conduct edge and minimal description length (MDL) probing experiments on three pre-trained language models (PLMs) in an attempt to explore the extent to which hyperbolic information is encoded in these models. We use both word-in-context and sentence-level representations as model inputs as a basis for comparison. We also annotate 63 hyperbole sentences from the HYPO dataset according to an operational taxonomy to conduct an error analysis to explore the encoding of different hyperbole categories. Our results show that hyperbole is to a limited extent encoded in PLMs, and mostly in the final layers. They also indicate that hyperbolic information may be better encoded by the sentence-level representations, which, due to the pragmatic nature of hyperbole, may therefore provide a more accurate and informative representation in PLMs. Finally, the inter-annotator agreement for our annotations, a Cohen’s Kappa of 0.339, suggest that the taxonomy categories may not be intuitive and need revision or simplification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1154,
      "original_url": "https://aclanthology.org/2023.acl-srw.31/",
      "title": "Towards Efficient Dialogue Processing in the Emergency Response Domain",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.31.pdf",
      "authors": [
        "Tatiana Anikina"
      ],
      "abstract": "In this paper we describe the task of adapting NLP models to dialogue processing in the emergency response domain. Our goal is to provide a recipe for building a system that performs dialogue act classification and domain-specific slot tagging while being efficient, flexible and robust. We show that adapter models Pfeiffer et al. (2020) perform well in the emergency response domain and benefit from additional dialogue context and speaker information. Comparing adapters to standard fine-tuned Transformer models we show that they achieve competitive results and can easily accommodate new tasks without significant memory increase since the base model can be shared between the adapters specializing on different tasks. We also address the problem of scarce annotations in the emergency response domain and evaluate different data augmentation techniques in a low-resource setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1155,
      "original_url": "https://aclanthology.org/2023.acl-srw.33/",
      "title": "I already said that! Degenerating redundant questions in open-domain dialogue systems.",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.33.pdf",
      "authors": [
        "Long Mai",
        "Julie Carson-berndsen"
      ],
      "abstract": "Neural text generation models have achieved remarkable success in carrying on short open-domain conversations. However, their performance degrades significantly in the long term, especially in their ability to ask coherent questions. A significant issue is the generation of redundant questions where the answer has already been provided by the user. We adapt and evaluate different methods, including negative training, decoding, and classification, to mitigate the redundancy problem. We also propose a simple yet effective method for generating training data without the need for crowdsourcing human-human or human-bot conversations. Experiments with the BlenderBot model show that our combined method significantly reduces the rate of redundant questions from 27.2% to 8.7%, while improving the quality of the original model. The code, dataset, and trained models can be found at our repository."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1156,
      "original_url": "https://aclanthology.org/2023.acl-srw.34/",
      "title": "Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.34.pdf",
      "authors": [
        "Takashi Kodama",
        "Hirokazu Kiyomaru",
        "Yin Jou Huang",
        "Taro Okahisa",
        "Sadao Kurohashi"
      ],
      "abstract": "Currently, most knowledge-grounded dialogue response generation models focus on reflecting given external knowledge. However, even when conveying external knowledge, humans integrate their own knowledge, experiences, and opinions with external knowledge to make their utterances engaging. In this study, we analyze such human behavior by annotating the utterances in an existing knowledge-grounded dialogue corpus. Each entity in the corpus is annotated with its information source, either derived from external knowledge (database-derived) or the speaker’s own knowledge, experiences, and opinions (speaker-derived). Our analysis shows that the presence of speaker-derived information in the utterance improves dialogue engagingness. We also confirm that responses generated by an existing model, which is trained to reflect the given knowledge, cannot include speaker-derived information in responses as often as humans do."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1157,
      "original_url": "https://aclanthology.org/2023.acl-srw.35/",
      "title": "Choosing What to Mask: More Informed Masking for Multimodal Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.35.pdf",
      "authors": [
        "Julia Sato",
        "Helena Caseli",
        "Lucia Specia"
      ],
      "abstract": "Pre-trained language models have achieved remarkable results on several NLP tasks. Most of them adopt masked language modeling to learn representations by randomly masking tokens and predicting them based on their context. However, this random selection of tokens to be masked is inefficient to learn some language patterns as it may not consider linguistic information that can be helpful for many NLP tasks, such as multimodal machine translation (MMT). Hence, we propose three novel masking strategies for cross-lingual visual pre-training - more informed visual masking, more informed textual masking, and more informed visual and textual masking - each one focusing on learning different linguistic patterns. We apply them to Vision Translation Language Modelling for video subtitles (Sato et al., 2022) and conduct extensive experiments on the Portuguese-English MMT task. The results show that our masking approaches yield significant improvements over the original random masking strategy for downstream MMT performance. Our models outperform the MMT baseline and we achieve state-of-the-art accuracy (52.70 in terms of BLEU score) on the How2 dataset, indicating that more informed masking helps in acquiring an understanding of specific language structures and has great potential for language understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1158,
      "original_url": "https://aclanthology.org/2023.acl-srw.36/",
      "title": "Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.36.pdf",
      "authors": [
        "Chong Shen",
        "Carina Silberer"
      ],
      "abstract": "Procedural knowledge understanding (PKU) underlies the ability to infer goal-step relations. The task of Visual Goal–Step Inference addresses this ability in the multimodal domain. It requires to identify images that represent the steps towards achieving a textually expressed goal. The best existing methods encode texts and images either with independent encoders, or with object-level multimodal encoders using blackbox transformers. This stands in contrast to early, linguistically inspired methods for event representations, which focus on capturing the most crucial information, namely actions and the participants, to learn stereotypical event sequences and hence procedural knowledge. In this work, we study various methods and their effects on PKU of injecting the early shallow event representations to nowadays multimodal deep learning-based models. We find that the early, linguistically inspired methods for representing event knowledge does contribute to understand procedures in combination with modern vision-and-language models. In the future, we are going to explore more complex structure of events and study how to exploit it on top of large language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1159,
      "original_url": "https://aclanthology.org/2023.acl-srw.37/",
      "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.37.pdf",
      "authors": [
        "Stephanie Schoch",
        "Ritwick Mishra",
        "Yangfeng Ji"
      ],
      "abstract": "Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the entire training set, and 2) a value transfer method that leverages value information extracted from a simple classifier trained using representations from the target language model. Our experiments applying TS-DShapley to select data for fine-tuning BERT-based language models on benchmark natural language understanding (NLU) datasets show that TS-DShapley outperforms existing data selection methods. Further, TS-DShapley can filter fine-tuning data to increase language model performance compared to training with the full fine-tuning dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1160,
      "original_url": "https://aclanthology.org/2023.acl-srw.38/",
      "title": "Distractor Generation for Fill-in-the-Blank Exercises by Question Type",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.38.pdf",
      "authors": [
        "Nana Yoshimi",
        "Tomoyuki Kajiwara",
        "Satoru Uchida",
        "Yuki Arase",
        "Takashi Ninomiya"
      ],
      "abstract": "This study addresses the automatic generation of distractors for English fill-in-the-blank exercises in the entrance examinations for Japanese universities. While previous studies applied the same method to all questions, actual entrance examinations have multiple question types that reflect the purpose of the questions. Therefore, we define three types of questions (grammar, function word, and context) and propose a method to generate distractors according to the characteristics of each question type. Experimental results on 500 actual questions show the effectiveness of the proposed method for both automatic and manual evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1161,
      "original_url": "https://aclanthology.org/2023.acl-srw.40/",
      "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.40.pdf",
      "authors": [
        "Gabriel Simmons"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This work investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This work explores this hypothesis in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, this work shows that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1162,
      "original_url": "https://aclanthology.org/2023.acl-srw.43/",
      "title": "LECO: Improving Early Exiting via Learned Exits and Comparison-based Exiting Mechanism",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.43.pdf",
      "authors": [
        "Jingfan Zhang",
        "Ming Tan",
        "Pengyu Dai",
        "Wei Zhu"
      ],
      "abstract": "Recently, dynamic early exiting has attracted much attention since it can accelerate the inference speed of pre-trained models (PTMs). However, previous work on early exiting has neglected the intermediate exits’ architectural designs. In this work, we propose a novel framework, Learned Exits and COmparison-based early exiting (LECO) to improve PTMs’ early exiting performances. First, to fully uncover the potentials of multi-exit BERT, we design a novel search space for intermediate exits and employ the idea of differentiable neural architecture search (DNAS) to design proper exit architectures for different intermediate layers automatically. Second, we propose a simple-yet-effective comparison-based early exiting mechanism (COBEE), which can help PTMs achieve better performance and speedup tradeoffs. Extensive experiments show that our LECO achieves the SOTA performances for multi-exit BERT training and dynamic early exiting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1163,
      "original_url": "https://aclanthology.org/2023.acl-srw.44/",
      "title": "Authorship Attribution of Late 19th Century Novels using GAN-BERT",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.44.pdf",
      "authors": [
        "Kanishka Silva",
        "Burcu Can",
        "Frédéric Blain",
        "Raheem Sarwar",
        "Laura Ugolini",
        "Ruslan Mitkov"
      ],
      "abstract": "Authorship attribution aims to identify the author of an anonymous text. The task becomes even more worthwhile when it comes to literary works. For example, pen names were commonly used by female authors in the 19th century resulting in some literary works being incorrectly attributed or claimed. With this motivation, we collated a dataset of late 19th century novels in English. Due to the imbalance in the dataset and the unavailability of enough data per author, we employed the GANBERT model along with data sampling strategies to fine-tune a transformer-based model for authorship attribution. Differently from the earlier studies on the GAN-BERT model, we conducted transfer learning on comparatively smaller author subsets to train more focused author-specific models yielding performance over 0.88 accuracy and F1 scores. Furthermore, we observed that increasing the sample size has a negative impact on the model’s performance. Our research mainly contributes to the ongoing authorship attribution research using GAN-BERT architecture, especially in attributing disputed novelists in the late 19th century."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1164,
      "original_url": "https://aclanthology.org/2023.acl-srw.46/",
      "title": "How-to Guides for Specific Audiences: A Corpus and Initial Findings",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.46.pdf",
      "authors": [
        "Nicola Fanton",
        "Agnieszka Falenska",
        "Michael Roth"
      ],
      "abstract": "Instructional texts for specific target groups should ideally take into account the prior knowledge and needs of the readers in order to guide them efficiently to their desired goals. However, targeting specific groups also carries the risk of reflecting disparate social norms and subtle stereotypes. In this paper, we investigate the extent to which how-to guides from one particular platform, wikiHow, differ in practice depending on the intended audience. We conduct two case studies in which we examine qualitative features of texts written for specific audiences. In a generalization study, we investigate which differences can also be systematically demonstrated using computational methods. The results of our studies show that guides from wikiHow, like other text genres, are subject to subtle biases. We aim to raise awareness of these inequalities as a first step to addressing them in future work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1165,
      "original_url": "https://aclanthology.org/2023.acl-srw.47/",
      "title": "“When Words Fail, Emojis Prevail”: A Novel Architecture for Generating Sarcastic Sentences With Emoji Using Valence Reversal and Semantic Incongruity",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.47.pdf",
      "authors": [
        "Faria Binte Kader",
        "Nafisa Hossain Nujat",
        "Tasmia Binte Sogir",
        "Mohsinul Kabir",
        "Hasan Mahmud",
        "Md Kamrul Hasan"
      ],
      "abstract": "Sarcasm is a form of figurative language that serves as a humorous tool for mockery and ridicule. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence in English. We divide the generation task into two sub tasks: one for generating textual sarcasm and another for collecting emojis associated with those sarcastic sentences. Two key elements of sarcasm are incorporated into the textual sarcasm generation task: valence reversal and semantic incongruity with context, where the context may involve shared commonsense or general knowledge between the speaker and their audience. The majority of existing sarcasm generation works have focused on this textual form. However, in the real world, when written texts fall short of effectively capturing the emotional cues of spoken and face-to-face communication, people often opt for emojis to accurately express their emotions. Due to the wide range of applications of emojis, incorporating appropriate emojis to generate textual sarcastic sentences helps advance sarcasm generation. We conclude our study by evaluating the generated sarcastic sentences using human judgement. All the codes and data used in this study has been made publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1166,
      "original_url": "https://aclanthology.org/2023.acl-srw.48/",
      "title": "Semantic Accuracy in Natural Language Generation: A Thesis Proposal",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.48.pdf",
      "authors": [
        "Patricia Schmidtova"
      ],
      "abstract": "With the fast-growing popularity of current large pre-trained language models (LLMs), it is necessary to dedicate efforts to making them more reliable. In this thesis proposal, we aim to improve the reliability of natural language generation systems (NLG) by researching the semantic accuracy of their outputs. We look at this problem from the outside (evaluation) and from the inside (interpretability). We propose a novel method for evaluating semantic accuracy and discuss the importance of working towards a unified and objective benchmark for NLG metrics. We also review interpretability approaches which could help us pinpoint the sources of inaccuracies within the models and explore potential mitigation strategies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1167,
      "original_url": "https://aclanthology.org/2023.acl-srw.49/",
      "title": "Math Word Problem Solving by Generating Linguistic Variants of Problem Statements",
      "pdf_url": "https://aclanthology.org/2023.acl-srw.49.pdf",
      "authors": [
        "Syed Rifat Raiyan",
        "Md Nafis Faiyaz",
        "Shah Md. Jawad Kabir",
        "Mohsinul Kabir",
        "Hasan Mahmud",
        "Md Kamrul Hasan"
      ],
      "abstract": "The art of mathematical reasoning stands as a fundamental pillar of intellectual progress and is a central catalyst in cultivating human ingenuity. Researchers have recently published a plethora of works centered around the task of solving Math Word Problems (MWP) — a crucial stride towards general AI. These existing models are susceptible to dependency on shallow heuristics and spurious correlations to derive the solution expressions. In order to ameliorate this issue, in this paper, we propose a framework for MWP solvers based on the generation of linguistic variants of the problem text. The approach involves solving each of the variant problems and electing the predicted expression with the majority of the votes. We use DeBERTa (Decoding-enhanced BERT with disentangled attention) as the encoder to leverage its rich textual representations and enhanced mask decoder to construct the solution expressions. Furthermore, we introduce a challenging dataset, ParaMAWPS, consisting of paraphrased, adversarial, and inverse variants of selectively sampled MWPs from the benchmark Mawps dataset. We extensively experiment on this dataset along with other benchmark datasets using some baseline MWP solver models. We show that training on linguistic variants of problem statements and voting on candidate predictions improve the mathematical reasoning and robustness of the model. We make our code and data publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1168,
      "original_url": "https://aclanthology.org/2023.acl-industry.1/",
      "title": "CWSeg: An Efficient and General Approach to Chinese Word Segmentation",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.1.pdf",
      "authors": [
        "Dedong Li",
        "Rui Zhao",
        "Fei Tan"
      ],
      "abstract": "In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1169,
      "original_url": "https://aclanthology.org/2023.acl-industry.2/",
      "title": "“Knowledge is Power”: Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.2.pdf",
      "authors": [
        "Kaveri Kale",
        "Pushpak Bhattacharyya",
        "Aditya Shetty",
        "Milind Gune",
        "Kush Shrivastava",
        "Rustom Lawyer",
        "Spriha Biswas"
      ],
      "abstract": "In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists’ dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our work is the set of knowledge graphs (KGs) of ten abdominal organs- Liver, Kidney, Gallbladder, Uterus, Urinary bladder, Ovary, Pancreas, Prostate, Biliary Tree, and Bowel. Our method for constructing these KGs relies on extracting entity1-relation-entity2 triplets from a large collection (about 10,000) of free-text radiology reports. The quality and coverage of the KGs are verified by two experienced radiologists (practicing for the last 30 years and 8 years, respectively). The dictation of the radiologist is automatically converted to what is called a pathological description which is the clinical description of the findings of the radiologist during ultrasonography (USG). Our knowledge-enhanced deep learning model improves the reported BLEU-3, ROUGE-L, METEOR, and CIDEr scores of the pathological description generation by 2%, 4%, 2% and 2% respectively. To the best of our knowledge, this is the first attempt at representing the abdominal organs in the form of knowledge graphs and utilising these graphs for the automatic generation of USG reports. A Minimum Viable Product (MVP) has been made available to the beta users, i.e., radiologists of reputed hospitals, for testing and evaluation. Our solution guarantees report generation within 30 seconds of running a scan."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1170,
      "original_url": "https://aclanthology.org/2023.acl-industry.3/",
      "title": "Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.3.pdf",
      "authors": [
        "Chikara Hashimoto",
        "Gautam Kumar",
        "Shuichiro Hashimoto",
        "Jun Suzuki"
      ],
      "abstract": "Patent applicants write patent specificationsthat describe embodiments of inventions. Some embodiments are claimed for a patent,while others may be unclaimeddue to strategic considerations. Unclaimed embodiments may be extracted byapplicants later and claimed incontinuing applications togain advantages over competitors. Despite being essential for corporate intellectual property (IP) strategies,unclaimed embodiment extraction is conducted manually,and little research has been conducted on its automation. This paper presents a novel task ofunclaimed embodiment extraction (UEE)and a novel dataset for the task. Our experiments with Transformer-based modelsdemonstratedthat the task was challenging as it requiredconducting natural language inference onpatent specifications, which consisted oftechnical, long, syntactically and semanticallyinvolved sentences. We release the dataset and code to foster this new area of research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1171,
      "original_url": "https://aclanthology.org/2023.acl-industry.4/",
      "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.4.pdf",
      "authors": [
        "Shima Imani",
        "Liang Du",
        "Harsh Shrivastava"
      ],
      "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1172,
      "original_url": "https://aclanthology.org/2023.acl-industry.5/",
      "title": "Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.5.pdf",
      "authors": [
        "Mohammad Kachuee",
        "Sungjin Lee"
      ],
      "abstract": "Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined constraints. For example, we may want to ensure fewer policy deviations in business-critical domains such as shopping, while allocating more exploration budget to domains such as music. We present a novel meta-gradient learning approach that is scalable and practical to address this problem. The proposed method adjusts constraint violation penalty terms adaptively through a meta objective that encourages balanced constraint satisfaction across domains. We conducted extensive experiments on a real-world conversational AI and using a set of realistic constraint benchmarks. The proposed approach has been deployed in production for a large-scale commercial assistant, enabling the best balance between the policy value and constraint satisfaction rate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1173,
      "original_url": "https://aclanthology.org/2023.acl-industry.6/",
      "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.6.pdf",
      "authors": [
        "Francesco Fusco",
        "Damian Pascual",
        "Peter Staar",
        "Diego Antognini"
      ],
      "abstract": "Large pre-trained language models based on transformer architectureƒhave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1174,
      "original_url": "https://aclanthology.org/2023.acl-industry.7/",
      "title": "Extracting Text Representations for Terms and Phrases in Technical Domains",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.7.pdf",
      "authors": [
        "Francesco Fusco",
        "Diego Antognini"
      ],
      "abstract": "Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 times faster, even on high-end GPUs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1175,
      "original_url": "https://aclanthology.org/2023.acl-industry.8/",
      "title": "CocaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.8.pdf",
      "authors": [
        "Jiapeng Wang",
        "Chengyu Wang",
        "Xiaodan Wang",
        "Jun Huang",
        "Lianwen Jin"
      ],
      "abstract": "Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial indexing and inference time and the large consumption of computational resources. Although knowledge distillation techniques have been widely utilized for uni-modal model compression, how to expand them to the situation when the numbers of modalities and teachers/students are doubled has been rarely studied. In this paper, we conduct comprehensive experiments on this topic and propose the fully-Connected knowledge interaction graph (Coca) technique for cross-modal pre-training distillation. Based on our findings, the resulting CocaCLIP achieves SOTA performances on the widely-used Flickr30K and MSCOCO benchmarks under the lightweight setting. An industry application of our method on an e-commercial platform further demonstrates the significant effectiveness of CocaCLIP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1176,
      "original_url": "https://aclanthology.org/2023.acl-industry.9/",
      "title": "KG-FLIP: Knowledge-guided Fashion-domain Language-Image Pre-training for E-commerce",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.9.pdf",
      "authors": [
        "Qinjin Jia",
        "Yang Liu",
        "Daoping Wu",
        "Shaoyuan Xu",
        "Huidong Liu",
        "Jinmiao Fu",
        "Roland Vollgraf",
        "Bryan Wang"
      ],
      "abstract": "Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without leveraging external knowledge. Adapting these models to downstream applications in specific domains like fashion requires fine-grained in-domain image-text corpus, which are usually less semantically aligned and in small scale that requires efficient pre-training strategies. In this paper, we propose a knowledge-guided fashion-domain language-image pre-training (FLIP) framework that focuses on learning fine-grained representations in e-commerce domain and utilizes external knowledge (i.e., product attribute schema), to improve the pre-training efficiency. Experiments demonstrate that FLIP outperforms previous state-of-the-art VLP models on Amazon data and on the Fashion-Gen dataset by large margins. FLIP has been successfully deployed in the Amazon catalog system to backfill missing attributes and improve the customer shopping experience."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1177,
      "original_url": "https://aclanthology.org/2023.acl-industry.10/",
      "title": "Domain-specific transformer models for query translation",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.10.pdf",
      "authors": [
        "Mandar Kulkarni",
        "Nikesh Garera",
        "Anusua Trivedi"
      ],
      "abstract": "Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products. Due to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to English to perform the search. However, the problem becomes non-trivial when the brand names themselves have Hinglish names and possibly have a literal English translation. In such queries, only the context (non-brand name) Hinglish words needs to be translated. In this paper, we propose a simple yet effective modification to the transformer training to preserve/correct Grocery brand names in the output while selectively translating the context words. To achieve this, we use an additional dataset of popular Grocery brand names. Brand names are added as tokens to the model vocabulary, and the token embeddings are randomly initialized. Further, we introduce a Brand loss in training the translation model. Brand loss is a cross entropy loss computed using a denoising auto-encoder objective with brand name data. We warm-start the training from a public pre-trained checkpoint (such as BART/T5) and further adapt it for query translation using the domain data. The proposed model is generic and can be used with English as well as code-mix Hinglish queries alleviating the need for language detection. To reduce the latency of the model for the production deployment, we use knowledge distillation and quantization. Experimental evaluation indicates that the proposed approach improves translation results by preserving/correcting English/Hinglish brand names. After positive results with A/B testing, the model is currently deployed in production."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1178,
      "original_url": "https://aclanthology.org/2023.acl-industry.11/",
      "title": "Label efficient semi-supervised conversational intent classification",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.11.pdf",
      "authors": [
        "Mandar Kulkarni",
        "Kyung Kim",
        "Nikesh Garera",
        "Anusua Trivedi"
      ],
      "abstract": "To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user’s intention from the query text. Due to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. This paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. Experimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1179,
      "original_url": "https://aclanthology.org/2023.acl-industry.12/",
      "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.12.pdf",
      "authors": [
        "Xiaoyu Shen",
        "Akari Asai",
        "Bill Byrne",
        "Adria De Gispert"
      ],
      "abstract": "Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers’ questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate. We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1180,
      "original_url": "https://aclanthology.org/2023.acl-industry.13/",
      "title": "Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.13.pdf",
      "authors": [
        "Beizhe Hu",
        "Qiang Sheng",
        "Juan Cao",
        "Yongchun Zhu",
        "Danding Wang",
        "Zhengjia Wang",
        "Zhiwei Jin"
      ],
      "abstract": "Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on past data and testing on future data. In this paper, we observe that the appearances of news events on the same topic may display discernible patterns over time, and posit that such patterns can assist in selecting training instances that could make the model adapt better to future data. Specifically, we design an effective framework FTT (Forecasting Temporal Trends), which could forecast the temporal distribution patterns of news data and then guide the detector to fast adapt to future distribution. Experiments on the real-world temporally split dataset demonstrate the superiority of our proposed framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1181,
      "original_url": "https://aclanthology.org/2023.acl-industry.14/",
      "title": "AVEN-GR: Attribute Value Extraction and Normalization using product GRaphs",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.14.pdf",
      "authors": [
        "Thomas Ricatte",
        "Donato Crisostomi"
      ],
      "abstract": "Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient and able to capture early emerging market trends. Query Attribute Understanding (QAU) is a sub-component of QU that involves extracting named attributes from user queries and linking them to existing e-commerce entities such as brand, material, color, etc. While extracting named entities from text has been extensively explored in the literature, QAU requires specific attention due to the nature of the queries, which are often short, noisy, ambiguous, and constantly evolving. This paper makes three contributions to QAU. First, we propose a novel end-to-end approach that jointly solves Named Entity Recognition (NER) and Entity Linking (NEL) and enables open-world reasoning for QAU. Second, we introduce a novel method for utilizing product graphs to enhance the representation of query entities. Finally, we present a new dataset constructed from public sources that can be used to evaluate the performance of future QAU systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1182,
      "original_url": "https://aclanthology.org/2023.acl-industry.15/",
      "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.15.pdf",
      "authors": [
        "Shicheng Tan",
        "Weng Lam Tam",
        "Yuanchun Wang",
        "Wenwen Gong",
        "Shu Zhao",
        "Peng Zhang",
        "Jie Tang"
      ],
      "abstract": "Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1183,
      "original_url": "https://aclanthology.org/2023.acl-industry.16/",
      "title": "FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.16.pdf",
      "authors": [
        "Xiaodan Wang",
        "Chengyu Wang",
        "Lei Li",
        "Zhixu Li",
        "Ben Chen",
        "Linbo Jin",
        "Jun Huang",
        "Yanghua Xiao",
        "Ming Gao"
      ],
      "abstract": "Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1184,
      "original_url": "https://aclanthology.org/2023.acl-industry.17/",
      "title": "Entity Contrastive Learning in a Large-Scale Virtual Assistant System",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.17.pdf",
      "authors": [
        "Jonathan Rubin",
        "Jason Crowley",
        "George Leung",
        "Morteza Ziyadi",
        "Maria Minakova"
      ],
      "abstract": "Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a production baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as live online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against production baselines. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionality-reduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces well-formed embedding clusters in representation space."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1185,
      "original_url": "https://aclanthology.org/2023.acl-industry.18/",
      "title": "Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.18.pdf",
      "authors": [
        "Kewei Cheng",
        "Xian Li",
        "Zhengyang Wang",
        "Chenwei Zhang",
        "Binxuan Huang",
        "Yifan Ethan Xu",
        "Xin Luna Dong",
        "Yizhou Sun"
      ],
      "abstract": "Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present Tab-Cleaner, a model designed to handle error detection over text-rich tabular data following a pre-training / fine-tuning paradigm. We train Tab-Cleaner on a real-world Amazon Product Catalog table w.r.t millions of products and show improvements over state-of-the-art methods by 16\\% on PR AUC over attribute applicability classification task and by 11\\% on PR AUC over attribute value validation task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1186,
      "original_url": "https://aclanthology.org/2023.acl-industry.19/",
      "title": "Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.19.pdf",
      "authors": [
        "Abishek Komma",
        "Nagesh Panyam Chandrasekarasastry",
        "Timothy Leffel",
        "Anuj Goyal",
        "Angeliki Metallinou",
        "Spyros Matsoukas",
        "Aram Galstyan"
      ],
      "abstract": "Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1187,
      "original_url": "https://aclanthology.org/2023.acl-industry.20/",
      "title": "Tab-CQA: A Tabular Conversational Question Answering Dataset on Financial Reports",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.20.pdf",
      "authors": [
        "Chuang Liu",
        "Junzhuo Li",
        "Deyi Xiong"
      ],
      "abstract": "Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally, we select 4,578 tables, from which 4,578 conversations with 73,595 QA pairs are automatically created via a template-based method. With the manually- and automatically-generated conversations, Tab-CQA contains answerable and unanswerable questions. For the answerable questions, we further diversify them to cover a wide range of skills, e.g., table retrieval, fact checking, numerical reasoning, so as to accommodate real-world scenarios. We further propose two different tabular CQA models, a text-based model and an operation-based model, and evaluate them on Tab-CQA. Experiment results show that Tab-CQA is a very challenging dataset, where a huge performance gap exists between human and neural models. We will publicly release Tab-CQA as a benchmark testbed to promote further research on Chinese tabular CQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1188,
      "original_url": "https://aclanthology.org/2023.acl-industry.21/",
      "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.21.pdf",
      "authors": [
        "Hwaran Lee",
        "Seokhee Hong",
        "Joonsuk Park",
        "Takyoung Kim",
        "Gunhee Kim",
        "Jung-woo Ha"
      ],
      "abstract": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1189,
      "original_url": "https://aclanthology.org/2023.acl-industry.22/",
      "title": "Improving Knowledge Production Efficiency With Question Answering on Conversation",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.22.pdf",
      "authors": [
        "Changlin Yang",
        "Siye Liu",
        "Sen Hu",
        "Wangshu Zhang",
        "Teng Xu",
        "Jing Zheng"
      ],
      "abstract": "Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in Chinese language. The challenges of conversation-based QA include: 1) answers may be scattered among multiple dialogue turns; 2) understanding complex dialogue contexts is more complicated than documents. To address these challenges, we propose a multi-span extraction model on this task and introduce continual pre-training and multi-task learning schemes to further improve model performance. To validate our approach, we construct two Chinese datasets using dialogues as the knowledge source, namely cs-qaconv and kd-qaconv, respectively. Experimental results demonstrate that the proposed model outperforms the baseline on both datasets. The online application also verifies the effectiveness of our method. The dataset kd-qaconv will be released publicly for research purposes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1190,
      "original_url": "https://aclanthology.org/2023.acl-industry.23/",
      "title": "Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.23.pdf",
      "authors": [
        "Donato Crisostomi",
        "Andrea Caciolai",
        "Alessandro Pedrani",
        "Kay Rottmann",
        "Alessandro Manzotti",
        "Enrico Palumbo",
        "Davide Bernardi"
      ],
      "abstract": "Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times. This paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets. Our approach eliminates duplicates at the batch level, without altering the data distribution observed by the model, making it model-agnostic and easy to implement as a plug-and-play module. We also provide a mathematical expression to estimate the reduction in training time that our approach provides. Through empirical evidence, we show that our approach significantly reduces training times on various models across datasets with varying redundancy factors, without impacting their performance on the Named Entity Recognition task, both on publicly available datasets and in real industrial settings. In the latter, the approach speeds training by up to 87%, and by 46% on average, with a drop in model performance of 0.2% relative at worst. We finally release a modular and reusable codebase to further advance research in this area."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1191,
      "original_url": "https://aclanthology.org/2023.acl-industry.24/",
      "title": "The economic trade-offs of large language models: A case study",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.24.pdf",
      "authors": [
        "Kristen Howell",
        "Gwen Christian",
        "Pavel Fomitchov",
        "Gitit Kehat",
        "Julianne Marzulla",
        "Leanne Rolston",
        "Jadin Tredup",
        "Ilana Zimmerman",
        "Ethan Selfridge",
        "Joseph Bradley"
      ],
      "abstract": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model’s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM — prompt engineering, fine-tuning, and knowledge distillation — using feedback from the brand’s customer service agents. We find that the usability of a model’s responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1192,
      "original_url": "https://aclanthology.org/2023.acl-industry.25/",
      "title": "Application-Agnostic Language Modeling for On-Device ASR",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.25.pdf",
      "authors": [
        "Markus Nussbaum-thom",
        "Lyan Verwimp",
        "Youssef Oualil"
      ],
      "abstract": "On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an increase in memory. Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model. We propose two novel feed-forward architectures that find an optimal trade off between different on-device constraints. In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1193,
      "original_url": "https://aclanthology.org/2023.acl-industry.26/",
      "title": "Building Accurate Low Latency ASR for Streaming Voice Search in E-commerce",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.26.pdf",
      "authors": [
        "Abhinav Goyal",
        "Nikesh Garera"
      ],
      "abstract": "Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish (blend of Hindi and English) Voice Search. We evaluate how various modifications in vanilla LSTM training improve the system’s accuracy while preserving the streaming capabilities. We also discuss a simple integration of end-of-speech (EOS) detection with CTC models, which helps reduce the overall search latency. Our model achieves a word error rate (WER) of 3.69% without EOS and 4.78% with EOS, with ~1300 ms (~46.64%) reduction in latency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1194,
      "original_url": "https://aclanthology.org/2023.acl-industry.27/",
      "title": "PLAtE: A Large-scale Dataset for List Page Web Extraction",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.27.pdf",
      "authors": [
        "Aidan San",
        "Yuan Zhuang",
        "Jan Bakus",
        "Colin Lockard",
        "David Ciemiewicz",
        "Sandeep Atluri",
        "Kevin Small",
        "Yangfeng Ji",
        "Heba Elfardy"
      ],
      "abstract": "Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1195,
      "original_url": "https://aclanthology.org/2023.acl-industry.28/",
      "title": "Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.28.pdf",
      "authors": [
        "Bingyan Liu",
        "Weifeng Lin",
        "Zhongjie Duan",
        "Chengyu Wang",
        "Wu Ziheng",
        "Zhang Zipeng",
        "Kui Jia",
        "Lianwen Jin",
        "Cen Chen",
        "Jun Huang"
      ],
      "abstract": "Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the open-source community cannot support industrial domain-specific applications due to the lack of entity knowledge and low inference speed. In this paper, we propose Rapid Diffusion, a novel framework for training and deploying super-resolution, text-to-image latent diffusion models with rich entity knowledge injected and optimized networks. Furthermore, we employ BladeDISC, an end-to-end Artificial Intelligence (AI) compiler, and FlashAttention techniques to optimize computational graphs of the generated models for online deployment. Experiments verify the effectiveness of our approach in terms of image quality and inference speed. In addition, we present industrial use cases and integrate Rapid Diffusion to an AI platform to show its practical values."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1196,
      "original_url": "https://aclanthology.org/2023.acl-industry.29/",
      "title": "Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.29.pdf",
      "authors": [
        "Anant Khandelwal",
        "Happy Mittal",
        "Shreyas Kulkarni",
        "Deepak Gupta"
      ],
      "abstract": "E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don’t label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using MXT, that consists of three key components: (i) MAG (Multimodal Adaptation Gate), (ii) Xception network, and (iii) T5 encoder-decoder. Our system consists of a generative model that generates attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our system is capable of handling zero-shot attribute prediction (when attribute value is not seen in training data) and value-absent prediction (when attribute value is not mentioned in the text) which are missing in traditional classification-based and NER-based models respectively. We have trained our models using distant supervision, removing dependency on human labeling, thus making them practical for real-world applications. With this framework, we are able to train a single model for 1000s of (product-type, attribute) pairs, thus reducing the overhead of training and maintaining separate models. Extensive experiments on two real world datasets (total 57 attributes) show that our framework improves the absolute recall@90P by 10.16% and 6.9 from the existing state of the art models. In a popular e-commerce store, we have productionized our models that cater to 12K (product-type, attribute) pairs, and have extracted 150MM attribute values."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1197,
      "original_url": "https://aclanthology.org/2023.acl-industry.30/",
      "title": "Consistent Text Categorization using Data Augmentation in e-Commerce",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.30.pdf",
      "authors": [
        "Noa Avigdor",
        "Guy Horowitz",
        "Ariel Raviv",
        "Stav Yanovsky Daye"
      ],
      "abstract": "The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model’s output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience. To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model’s consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1198,
      "original_url": "https://aclanthology.org/2023.acl-industry.31/",
      "title": "An efficient method for Natural Language Querying on Structured Data",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.31.pdf",
      "authors": [
        "Hanoz Bhathena",
        "Aviral Joshi",
        "Prateek Singh"
      ],
      "abstract": "We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following “bread and butter” NLP tasks: (a) Domain classification, for choosing which DB table to query, whether the question is out-of-scope (b) Multi-head slot/entity extraction (SE) to extract the field criteria and other attributes such as its role (filter, sort etc) from the raw text and (c) Slot value disambiguation (SVD) to resolve/normalize raw spans from SE to format suitable to query a DB. This is a general purpose, DB language agnostic approach and the output can be used to query any DB and return results to the user. Also each of these tasks is extremely well studied, mature, easier to collect data for and enables better error analysis by tracing problems to specific components when something goes wrong."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1199,
      "original_url": "https://aclanthology.org/2023.acl-industry.32/",
      "title": "Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.32.pdf",
      "authors": [
        "Zekai Chen",
        "Mariann Micsinai Balan",
        "Kevin Brown"
      ],
      "abstract": "Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients’ clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1200,
      "original_url": "https://aclanthology.org/2023.acl-industry.33/",
      "title": "EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.33.pdf",
      "authors": [
        "Kamer Yüksel",
        "Ahmet Gunduz",
        "Mohamed Al-badrashiny",
        "Hassan Sawaf"
      ],
      "abstract": "This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method’s online learning capability enables it to adapt to changes in the domain or MT engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To the best of our knowledge, EvolveMT is the first MT system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1201,
      "original_url": "https://aclanthology.org/2023.acl-industry.34/",
      "title": "A Static Evaluation of Code Completion by Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.34.pdf",
      "authors": [
        "Hantian Ding",
        "Varun Kumar",
        "Yuchen Tian",
        "Zijian Wang",
        "Rob Kwiatkowski",
        "Xiaopeng Li",
        "Murali Krishna Ramanathan",
        "Baishakhi Ray",
        "Parminder Bhatia",
        "Sudipta Sengupta"
      ],
      "abstract": "Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven’t been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1202,
      "original_url": "https://aclanthology.org/2023.acl-industry.35/",
      "title": "Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.35.pdf",
      "authors": [
        "Sarthak Ahuja",
        "Mohammad Kachuee",
        "Fatemeh Sheikholeslami",
        "Weiqing Liu",
        "Jaeyoung Do"
      ],
      "abstract": "Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1203,
      "original_url": "https://aclanthology.org/2023.acl-industry.36/",
      "title": "MobileNMT: Enabling Translation in 15MB and 30ms",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.36.pdf",
      "authors": [
        "Ye Lin",
        "Xiaohui Wang",
        "Zhexi Zhang",
        "Mingxuan Wang",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "abstract": "Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5% of memory with only 11.6% loss of BLEU. Our code will be publicly available after the anonymity period."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1204,
      "original_url": "https://aclanthology.org/2023.acl-industry.37/",
      "title": "Multi-doc Hybrid Summarization via Salient Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.37.pdf",
      "authors": [
        "Min Xiao"
      ],
      "abstract": "Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint evidence extraction and summary generation. In order to train this model, we conducted multi-task learning to optimize a composited loss, constructed over extractive and abstractive sub-components in a hierarchical way. We implemented the system based on a ubiquiotously adopted transformer architecture and conducted experimental studies on multiple datasets across two domains, achieving superior performance over the baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1205,
      "original_url": "https://aclanthology.org/2023.acl-industry.38/",
      "title": "SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.38.pdf",
      "authors": [
        "Zhenting Qi",
        "Xiaoyu Tan",
        "Chao Qu",
        "Yinghui Xu",
        "Yuan Qi"
      ],
      "abstract": "Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1206,
      "original_url": "https://aclanthology.org/2023.acl-industry.39/",
      "title": "Chemical Language Understanding Benchmark",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.39.pdf",
      "authors": [
        "Yunsoo Kim",
        "Hyuk Ko",
        "Jane Lee",
        "Hyun Young Heo",
        "Jinyoung Yang",
        "Sungsoo Lee",
        "Kyu-hwang Lee"
      ],
      "abstract": "In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1207,
      "original_url": "https://aclanthology.org/2023.acl-industry.40/",
      "title": "HyperT5: Towards Compute-Efficient Korean Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.40.pdf",
      "authors": [
        "Dongju Park",
        "Soonwon Ka",
        "Kang Min Yoo",
        "Gichang Lee",
        "Jaewook Kang"
      ],
      "abstract": "Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1208,
      "original_url": "https://aclanthology.org/2023.acl-industry.41/",
      "title": "Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.41.pdf",
      "authors": [
        "Jong Myoung Kim",
        "Young-jun Lee",
        "Sangkeun Jung",
        "Ho-jin Choi"
      ],
      "abstract": "Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design considering ambiguity is possible. We utilize similarity in a semantic space to detect ambiguity in service scenarios and training data. In addition, we apply task-specific embedding to improve performance. Our results demonstrate that ambiguities and resulting labeling errors in training data or scenarios can be detected. Additionally, we confirm that it can be used to debug services"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1209,
      "original_url": "https://aclanthology.org/2023.acl-industry.42/",
      "title": "Reliable and Interpretable Drift Detection in Streams of Short Texts",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.42.pdf",
      "authors": [
        "Ella Rabinovich",
        "Matan Vetzler",
        "Samuel Ackerman",
        "Ateret Anaby Tavor"
      ],
      "abstract": "Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards effective re-training of the model. In this study we propose an end-to-end framework for reliable model-agnostic change-point detection and interpretation in large task-oriented dialog systems, proven effective in multiple customer deployments. We evaluate our approach and demonstrate its benefits with a novel variant of intent classification training dataset, simulating customer requests to a dialog system. We make the data publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1210,
      "original_url": "https://aclanthology.org/2023.acl-industry.43/",
      "title": "Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.43.pdf",
      "authors": [
        "Jonathan Hueser",
        "Judith Gaspers",
        "Thomas Gueudre",
        "Chandana Prakash",
        "Jin Cao",
        "Daniil Sorokin",
        "Quynh Do",
        "Nicolas Anastassacos",
        "Tobias Falke",
        "Turan Gojayev"
      ],
      "abstract": "Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks. Larger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).We evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT encoders with language-, domain- and task-decoupled finetuning.Running inference with a larger shared encoder on GPU is latency neutral and reduces infrastructure cost compared to running inference for decoupled smaller encoders on CPU machines. The larger shared encoder reduces semantic error rates by 4.62% for test sets representing user requests to voice-controlled devices and 5.79% on the tail of the test sets on average across four languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1211,
      "original_url": "https://aclanthology.org/2023.acl-industry.44/",
      "title": "Annotating Research Infrastructure in Scientific Papers: An NLP-driven Approach",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.44.pdf",
      "authors": [
        "Seyed Amin Tabatabaei",
        "Georgios Cheirmpos",
        "Marius Doornenbal",
        "Alberto Zigoni",
        "Veronique Moore",
        "Georgios Tsatsaronis"
      ],
      "abstract": "In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such as evaluating the impact of RI investment, and supporting Open Science and research reproducibility. These links can also be used to establish a profile of the RI portfolio of each institution and associate each equipment with scientific output. The system we are describing here is already in production, and has been used to address real business use cases, some of which we discuss in this paper. The computational pipeline at the heart of the system comprises both supervised and unsupervised modules to detect the usage of research equipment by processing the full text of the articles. Additionally, we have created a knowledge graph of RI, which is utilized to annotate the articles with metadata. Finally, examples of the business value of the insights made possible by this NLP pipeline are illustrated."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1212,
      "original_url": "https://aclanthology.org/2023.acl-industry.45/",
      "title": "Event-Centric Query Expansion in Web Search",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.45.pdf",
      "authors": [
        "Yanan Zhang",
        "Weijie Cui",
        "Yangfan Zhang",
        "Xiaoling Bai",
        "Zhe Zhang",
        "Jin Ma",
        "Xiang Chen",
        "Tianhua Zhou"
      ],
      "abstract": "In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the QE system used in a famous Chinese search engine. EQE utilizes a novel event retrieval framework that consists of four stages, i.e., event collection, event reformulation, semantic retrieval and online ranking, which can select the best expansion from a significant amount of potential events rapidly and accurately. Specifically, we first collect and filter news headlines from websites. Then we propose a generation model that incorporates contrastive learning and prompt-tuning techniques to reformulate these headlines to concise candidates. Additionally, we fine-tune a dual-tower semantic model to serve as an encoder for event retrieval and explore a two-stage contrastive training approach to enhance the accuracy of event retrieval. Finally, we rank the retrieved events and select the optimal one as QE, which is then used to improve the retrieval of event-related documents. Through offline analysis and online A/B testing, we observed that the EQE system has significantly improved many indicators compared to the baseline. The system has been deployed in a real production environment and serves hundreds of millions of users."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1213,
      "original_url": "https://aclanthology.org/2023.acl-industry.46/",
      "title": "Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.46.pdf",
      "authors": [
        "Shansan Gong",
        "Zelin Zhou",
        "Shuo Wang",
        "Fengjiao Chen",
        "Xiujie Song",
        "Xuezhi Cao",
        "Yunsen Xian",
        "Kenny Zhu"
      ],
      "abstract": "As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. In order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. To further enhance domain transferability and better exploit cross-domain data, we design two plug-in modules: a heuristic mapping scorer and a pretrained contrastive ranking module with the help of meta concepts, which represent keyword knowledge shared across domains. Comprehensive offline experiments show that our method outperforms strong baselineson three dynamic multi-domain product categorization (DMPC) tasks,and online experiments reconfirm its efficacy with a5% increase on seasonal purchase revenue. Related datasets will be released."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1214,
      "original_url": "https://aclanthology.org/2023.acl-industry.47/",
      "title": "DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.47.pdf",
      "authors": [
        "Paul Darm",
        "Antonio Valerio Miceli Barone",
        "Shay B. Cohen",
        "Annalisa Riccardi"
      ],
      "abstract": "Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge Base (KB) databases are an effective way of storing and accessing such information to scale. In this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1215,
      "original_url": "https://aclanthology.org/2023.acl-industry.48/",
      "title": "BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.48.pdf",
      "authors": [
        "Wei Zhu",
        "Peng Wang",
        "Yuan Ni",
        "Guotong Xie",
        "Xiaoling Wang"
      ],
      "abstract": "Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs’ early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers’ exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\footnote{Code will be made publicly available to the research community upon acceptance.}"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1216,
      "original_url": "https://aclanthology.org/2023.acl-industry.49/",
      "title": "K-pop and fake facts: from texts to smart alerting for maritime security",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.49.pdf",
      "authors": [
        "Maxime Prieur",
        "Souhir Gahbiche",
        "Guillaume Gadek",
        "Sylvain Gatepaille",
        "Kilian Vasnier",
        "Valerian Justine"
      ],
      "abstract": "Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard and soft data (sensors and texts), either by tweaking their AIS emitters or by emitting false information on pseudo-newspapers. Many techniques exist to identify these pieces of false information, including using knowledge base population techniques to build a structured view of the information. This paper presents a use case for suspect data identification in a maritime setting. The proposed system UMBAR ingests data from sensors and texts, processing them through an information extraction step, in order to feed a Knowledge Base and finally perform coherence checks between the extracted facts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1217,
      "original_url": "https://aclanthology.org/2023.acl-industry.50/",
      "title": "Evaluating Embedding APIs for Information Retrieval",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.50.pdf",
      "authors": [
        "Ehsan Kamalloo",
        "Xinyu Zhang",
        "Odunayo Ogundepo",
        "Nandan Thakur",
        "David Alfonso-hermelo",
        "Mehdi Rezagholizadeh",
        "Jimmy Lin"
      ],
      "abstract": "The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1218,
      "original_url": "https://aclanthology.org/2023.acl-industry.51/",
      "title": "Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.51.pdf",
      "authors": [
        "Mateusz Wójcik",
        "Witold Kościukiewicz",
        "Mateusz Baran",
        "Tomasz Kajdanowicz",
        "Adam Gonczarek"
      ],
      "abstract": "Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. Particularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1219,
      "original_url": "https://aclanthology.org/2023.acl-industry.52/",
      "title": "Regression-Free Model Updates for Spoken Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.52.pdf",
      "authors": [
        "Andrea Caciolai",
        "Verena Weber",
        "Tobias Falke",
        "Alessandro Pedrani",
        "Davide Bernardi"
      ],
      "abstract": "In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, focal distillation, to model updates in a goal-oriented dialog system and assess its usefulness in practice. In particular, we evaluate its effectiveness for key language understanding tasks, including sentence classification and sequence labeling tasks, we further assess its effect when applied to repeated model updates over time, and test its compatibility with mislabeled data. Our experiments on a public benchmark and data from a deployed dialog system demonstrate that focal distillation can substantially reduce regressions, at only minor drops in accuracy, and that it further outperforms naive supervised training in challenging mislabeled data and label expansion settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1220,
      "original_url": "https://aclanthology.org/2023.acl-industry.53/",
      "title": "Reducing cohort bias in natural language understanding systems with targeted self-training scheme",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.53.pdf",
      "authors": [
        "Dieu-thu Le",
        "Gabriela Hernandez",
        "Bei Chen",
        "Melanie Bradford"
      ],
      "abstract": "Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is observed that natural language understanding models often have lower performance when dealing with requests coming from new users rather than experienced users. To mitigate this problem, we propose a framework that consists of two phases (1) a fixing phase with four active learning strategies used to identify important samples coming from new users, and (2) a self training phase where a teacher model trained from the first phase is used to annotate semi-supervised samples to expand the training data with relevant cohort utterances. We explain practical strategies that involve an identification of representative cohort-based samples through density clustering as well as employing implicit customer feedbacks to improve new customers’ experience. We demonstrate the effectiveness of our approach in a real world large scale voice assistant system for two languages, German and French through both offline experiments as well as A/B testings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1221,
      "original_url": "https://aclanthology.org/2023.acl-industry.54/",
      "title": "Content Moderation for Evolving Policies using Binary Question Answering",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.54.pdf",
      "authors": [
        "Sankha Subhra Mullick",
        "Mohan Bhambhani",
        "Suhit Sinha",
        "Akshat Mathur",
        "Somya Gupta",
        "Jidnya Shah"
      ],
      "abstract": "Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging. We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes constituting a policy. A decision logic is applied on top to aggregate the theme-specific validations. This way the questions pass theme information to a transformer network as explicit policy prompts, that in turn enables explainability. This setting further allows for faster adaptation to policy updates by leveraging zero-shot capabilities of pre-trained transformers. We showcase improved recall for our proposed method at 95\\% precision on two proprietary datasets of social media posts and comments respectively annotated under curated Hate Speech and Commercial Spam policies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1222,
      "original_url": "https://aclanthology.org/2023.acl-industry.55/",
      "title": "Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.55.pdf",
      "authors": [
        "Tianqi Wang",
        "Lei Chen",
        "Xiaodan Zhu",
        "Younghun Lee",
        "Jing Gao"
      ],
      "abstract": "Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task’s long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL’s representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1223,
      "original_url": "https://aclanthology.org/2023.acl-industry.56/",
      "title": "Towards Building a Robust Toxicity Predictor",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.56.pdf",
      "authors": [
        "Dmitriy Bespalov",
        "Sourav Bhabesh",
        "Yi Xiang",
        "Liutong Zhou",
        "Yanjun Qi"
      ],
      "abstract": "Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\texttt{ToxicTrap}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\texttt{ToxicTrap} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\texttt{ToxicTrap} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1224,
      "original_url": "https://aclanthology.org/2023.acl-industry.57/",
      "title": "AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.57.pdf",
      "authors": [
        "Md Tahmid Rahman Laskar",
        "Cheng Chen",
        "Xue-yong Fu",
        "Mahsa Azizi",
        "Shashi Bhushan",
        "Simon Corston-oliver"
      ],
      "abstract": "In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present “AI Coach Assis”, which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors. The system was trained and evaluated on a large dataset collected from real-world contact centers and provides an efficient and effective way to determine which calls are most relevant for coaching purposes. Extensive experimental evaluation demonstrates the potential of AI Coach Assist to improve the coaching process, resulting in enhancing the performance of contact center agents."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1225,
      "original_url": "https://aclanthology.org/2023.acl-industry.58/",
      "title": "Unified Contextual Query Rewriting",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.58.pdf",
      "authors": [
        "Yingxue Zhou",
        "Jie Hao",
        "Mukund Rungta",
        "Yang Liu",
        "Eunah Cho",
        "Xing Fan",
        "Yanbin Lu",
        "Vishal Vasudevan",
        "Kellen Gillespie",
        "Zeynab Raeesy"
      ],
      "abstract": "Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To this end, we propose a unified contextual query rewriting model that unifies QR for both reducing friction and contextual carryover purpose. Moreover, we involve multiple auxiliary tasks such as trigger prediction and NLU interpretation tasks to boost the performance of the rewrite. We leverage the text-to-text unified framework which uses independent tasks with weighted loss to account for task importance. Then we propose new unified multitask learning strategies including a sequential model which outputs one sentence for multi-tasks, and a hybrid model where some tasks are independent and some tasks are sequentially generated. Our experimental results demonstrate the effectiveness of the proposed unified learning methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1226,
      "original_url": "https://aclanthology.org/2023.acl-industry.59/",
      "title": "Context-Aware Query Rewriting for Improving Users’ Search Experience on E-commerce Websites",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.59.pdf",
      "authors": [
        "Simiao Zuo",
        "Qingyu Yin",
        "Haoming Jiang",
        "Shaohui Xi",
        "Bing Yin",
        "Chao Zhang",
        "Tuo Zhao"
      ],
      "abstract": "E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users’ true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users’ history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent. We propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1227,
      "original_url": "https://aclanthology.org/2023.acl-industry.60/",
      "title": "Federated Learning of Gboard Language Models with Differential Privacy",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.60.pdf",
      "authors": [
        "Zheng Xu",
        "Yanxiang Zhang",
        "Galen Andrew",
        "Christopher Choquette",
        "Peter Kairouz",
        "Brendan Mcmahan",
        "Jesse Rosenstock",
        "Yuanbo Zhang"
      ],
      "abstract": "We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. With the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.3, 2)$, with one model additionally trained with secure aggregation. We summarize our experience and provide concrete suggestions on DP training for practitioners."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1228,
      "original_url": "https://aclanthology.org/2023.acl-industry.61/",
      "title": "RadLing: Towards Efficient Radiology Report Understanding",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.61.pdf",
      "authors": [
        "Rikhiya Ghosh",
        "Oladimeji Farri",
        "Sanjeev Kumar Karn",
        "Manuela Danu",
        "Ramya Vunikili",
        "Larisa Micu"
      ],
      "abstract": "Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1229,
      "original_url": "https://aclanthology.org/2023.acl-industry.62/",
      "title": "Predicting Customer Satisfaction with Soft Labels for Ordinal Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.62.pdf",
      "authors": [
        "Etienne Manderscheid",
        "Matthias Lee"
      ],
      "abstract": "In a typical call center, only up to 8% of callersleave a Customer Satisfaction (CSAT) surveyresponse at the end of the call, and these tend tobe customers with strongly positive or negativeexperiences. To manage this data sparsity andresponse bias, we outline a predictive CSATdeep learning algorithm that infers CSAT onthe 1-5 scale on inbound calls to the call centerwith minimal latency. The key metric to maximize is the precision for CSAT = 1 (lowestCSAT). We maximize this metric in two ways. First, reframing the problemas a binary class, rather than five-class problem during model fine-tuning, and then mapping binary outcomes back to five classes usingtemperature-scaled model probabilities. Second, using soft labels to represent the classes. Theresult is a production model able to support keycustomer workflows with high accuracy overmillions of calls a month."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1230,
      "original_url": "https://aclanthology.org/2023.acl-industry.63/",
      "title": "Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.63.pdf",
      "authors": [
        "Liang Wang",
        "Ivano Lauriola",
        "Alessandro Moschitti"
      ],
      "abstract": "Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models. Unfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback data collected from deployed QA systems. This data can be collected from tens of millions of user with no additional cost, for real-world QA services, e.g., Alexa, Google Home, and etc. The main drawback is the noise affecting feedback on individual examples. Recent literature on QA systems has shown the benefit of training models even with noisy feedback. However, these studies have multiple limitations: (i) they used uniform random noise to simulate feedback responses, which is typically an unrealistic approximation as noise follows specific patterns, depending on target examples and users; and (ii) they do not show how to aggregate feedback for improving training signals. In this paper, we first collect a large scale (16M) QA dataset with real feedback sampled from the QA traffic of a popular Virtual Assistant.Second, we use this data to develop two strategies for filtering unreliable users and thus de-noise feedback: (i) ranking users with an automatic classifier, and (ii) aggregating feedback over similar instances and comparing users between each other. Finally, we train QA models on our filtered feedback data, showing a significant improvement over the state of the art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1231,
      "original_url": "https://aclanthology.org/2023.acl-industry.64/",
      "title": "SPM: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.64.pdf",
      "authors": [
        "Sheng Jiang",
        "Su Zhu",
        "Ruisheng Cao",
        "Qingliang Miao",
        "Kai Yu"
      ],
      "abstract": "In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label classification, which cannot generalize to utterances with more intents than training samples. Meanwhile, it lacks the ability to assign slots to each corresponding intent. To overcome these problems, we propose a Split-Parsing Method (SPM) for joint multiple intent detection and slot filling, which is a two-stage method. It first splits an input sentence into multiple sub-sentences which contain a single-intent, and then a joint single intent detection and slot filling model is applied to parse each sub-sentence recurrently. Finally, we integrate the parsed results. The sub-sentence split task is also treated as a sequence labelling problem with only one entity-label, which can effectively generalize to a sentence with more intents unseen in the training set. Experimental results on three multi-intent datasets show that our method obtains substantial improvements over different baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1232,
      "original_url": "https://aclanthology.org/2023.acl-industry.65/",
      "title": "NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.65.pdf",
      "authors": [
        "Xinpeng Zhang",
        "Ming Tan",
        "Jingfan Zhang",
        "Wei Zhu"
      ],
      "abstract": "Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.\\footnote{Code will be publicly available to the research community upon acceptance.}"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1233,
      "original_url": "https://aclanthology.org/2023.acl-industry.66/",
      "title": "Search Query Spell Correction with Weak Supervision in E-commerce",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.66.pdf",
      "authors": [
        "Vishal Kakkar",
        "Chinmay Sharma",
        "Madhura Pande",
        "Surender Kumar"
      ],
      "abstract": "Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling patterns for a single correct query. In this paper, we first systematically analyze and group different spell errors into error classes and then leverage the state-of-the-art Transformer model for contextual spell correction. We overcome the constraint of limited human labelled data by proposing novel synthetic data generation techniques for voluminous generation of training pairs needed by data hungry Transformers, without any human intervention. We further utilize weakly supervised data coupled with curriculum learning strategies to improve on tough spell mistakes without regressing on the easier ones. We show significant improvements from our model on human labeled data and online A/B experiments against multiple state-of-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1234,
      "original_url": "https://aclanthology.org/2023.acl-industry.67/",
      "title": "“Let’s not Quote out of Context”: Unified Vision-Language Pretraining for Context Assisted Image Captioning",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.67.pdf",
      "authors": [
        "Abisek Rajakumar Kalarani",
        "Pushpak Bhattacharyya",
        "Niyati Chhaya",
        "Sumit Shekhar"
      ],
      "abstract": "Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach aims to overcome the context-independent (image and text are treated independently) nature of the existing approaches. We exploit context by pretraining our model with datasets of three tasks- news image captioning where the news article is the context, contextual visual entailment, and keyword extraction from the context. The second pretraining task is a new VL task, and we construct and release two datasets for the task with 1.1M and 2.2K data instances. Our system achieves state-of-the-art results with an improvement of up to 8.34 CIDEr score on the benchmark news image captioning datasets. To the best of our knowledge, ours is the first effort at incorporating contextual information in pretraining the models for the VL tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1235,
      "original_url": "https://aclanthology.org/2023.acl-industry.68/",
      "title": "What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.68.pdf",
      "authors": [
        "Deuksin Kwon",
        "Sunwoo Lee",
        "Ki Hyun Kim",
        "Seojin Lee",
        "Taeyoon Kim",
        "Eric Davis"
      ],
      "abstract": "This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1236,
      "original_url": "https://aclanthology.org/2023.acl-industry.69/",
      "title": "CUPID: Curriculum Learning Based Real-Time Prediction using Distillation",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.69.pdf",
      "authors": [
        "Arindam Bhattacharya",
        "Ankith Ms",
        "Ankit Gandhi",
        "Vijay Huddar",
        "Atul Saroop",
        "Rahul Bhagat"
      ],
      "abstract": "Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction in such a setting, its high evaluation latency makes it unsuitable for real-time predictions in which thousands of products must be evaluated against a user query within few milliseconds. To address this issue, we propose CUPID: a Curriculum learning based real-time Prediction using Distillation that utilizes knowledge distillation within a curriculum learning setting to learn a simpler architecture that can be evaluated within low latency budgets. In a bi-lingual relevance prediction task, our approach shows an 302 bps improvement on English and 676 bps improvement for low-resource Arabic, while maintaining the low evaluation latency on CPUs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1237,
      "original_url": "https://aclanthology.org/2023.acl-industry.70/",
      "title": "Answering Unanswered Questions through Semantic Reformulations in Spoken QA",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.70.pdf",
      "authors": [
        "Pedro Faustini",
        "Zhiyu Chen",
        "Besnik Fetahu",
        "Oleg Rokhlenko",
        "Shervin Malmasi"
      ],
      "abstract": "Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24% of previously unanswered questions obtain relevant answers (75%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relevance feedback shows high user satisfaction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1238,
      "original_url": "https://aclanthology.org/2023.acl-industry.71/",
      "title": "Exploring Zero and Few-shot Techniques for Intent Classification",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.71.pdf",
      "authors": [
        "Soham Parikh",
        "Mitul Tiwari",
        "Prashil Tumbade",
        "Quaizar Vohra"
      ],
      "abstract": "Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1239,
      "original_url": "https://aclanthology.org/2023.acl-industry.72/",
      "title": "Referring to Screen Texts with Voice Assistants",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.72.pdf",
      "authors": [
        "Shruti Bhargava",
        "Anand Dhoot",
        "Ing-marie Jonsson",
        "Hoang Long Nguyen",
        "Alkesh Patel",
        "Hong Yu",
        "Vincent Renkens"
      ],
      "abstract": "Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants have limited capacity to understand their users’ context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1240,
      "original_url": "https://aclanthology.org/2023.acl-industry.73/",
      "title": "Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.73.pdf",
      "authors": [
        "Zhiyu Chen",
        "Jason Choi",
        "Besnik Fetahu",
        "Oleg Rokhlenko",
        "Shervin Malmasi"
      ],
      "abstract": "Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user’s shopping experience poses challenges on deciding when and how to show FAQ results. Our proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. Offline evaluation demonstrates that our approach improves 12% in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These improvements are further validated by real user feedback, where more than 99% of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1241,
      "original_url": "https://aclanthology.org/2023.acl-industry.74/",
      "title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.74.pdf",
      "authors": [
        "Zhiwei Jia",
        "Pradyumna Narayana",
        "Arjun Akula",
        "Garima Pruthi",
        "Hao Su",
        "Sugato Basu",
        "Varun Jampani"
      ],
      "abstract": "Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1242,
      "original_url": "https://aclanthology.org/2023.acl-industry.75/",
      "title": "Weakly supervised hierarchical multi-task classification of customer questions",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.75.pdf",
      "authors": [
        "Jitenkumar Rana",
        "Promod Yenigalla",
        "Chetan Aggarwal",
        "Sandeep Sricharan Mukku",
        "Manan Soni",
        "Rashmi Patange"
      ],
      "abstract": "Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification Framework (HMCF) to identify topics from customer questions at various granularities. Complexity lies in creating a list of granular topics (taxonomy) for 1000s of product categories and building a scalable classification system. To this end, we introduce a clustering based Taxonomy Creation and Data Labeling (TCDL) module for creating taxonomy and labelled data with minimal supervision. Using TCDL module, taxonomy and labelled data creation task reduces to 2 hours as compared to 2 weeks of manual efforts by a subject matter expert. For classification, we propose a two level HMCF that performs multi-class classification to identify coarse level-1 topic and leverages NLI based label-aware approach to identify granular level-2 topic. We showcase that HMCF (based on BERT and NLI) a) achieves absolute improvement of 13% in Top-1 accuracy over single-task non-hierarchical baselines b) learns a generic domain invariant function that can adapt to constantly evolving taxonomy (open label set) without need of re-training. c) reduces model deployment efforts significantly since it needs only one model that caters to 1000s of product categories."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1243,
      "original_url": "https://aclanthology.org/2023.acl-industry.76/",
      "title": "Automated Digitization of Unstructured Medical Prescriptions",
      "pdf_url": "https://aclanthology.org/2023.acl-industry.76.pdf",
      "authors": [
        "Megha Sharma",
        "Tushar Vatsal",
        "Srujana Merugu",
        "Aruna Rajan"
      ],
      "abstract": "Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and rule-based components for (a) image to text extraction, (b) segmentation into blocks and medication items, (c) medication attribute extraction, (d) matching against medicine catalog, and (e) shopping cart building. Our approach efficiently utilizes multiple signals like layout, medical ontologies, and semantic embeddings via LayoutLMv2 model to yield substantial improvement relative to strong baselines on medication attribute extraction. Our pipeline achieves +5.9% gain in precision@3 and +5.6% in recall@3 over catalog-based fuzzy matching baseline for shopping cart building for printed prescriptions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1244,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.1/",
      "title": "Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.1.pdf",
      "authors": [
        "Yang Deng",
        "Wenqiang Lei",
        "Minlie Huang",
        "Tat-Seng Chua"
      ],
      "abstract": "Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target conversational goal and capable of leading the conversation towards the goal, which is a significant step towards higher-level intelligence and artificial consciousness. It can not only largely improve user engagement and service efficiency in the conversation, but also empower the system to handle more complicated conversation tasks that involve strategical and motivational interactions. In this tutorial, we will introduce the recent advances on the design of agent’s awareness of goals in a wide range of conversational systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1245,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.2/",
      "title": "Complex Reasoning in Natural Language",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.2.pdf",
      "authors": [
        "Wenting Zhao",
        "Mor Geva",
        "Bill Yuchen Lin",
        "Michihiro Yasunaga",
        "Aman Madaan",
        "Tao Yu"
      ],
      "abstract": "Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc. A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area. We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1246,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.3/",
      "title": "Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.3.pdf",
      "authors": [
        "Sunayana Sitaram",
        "Monojit Choudhury",
        "Barun Patra",
        "Vishrav Chaudhary",
        "Kabir Ahuja",
        "Kalika Bali"
      ],
      "abstract": "This tutorial will describe various aspects of scaling up language technologies to many of the world’s languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1247,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.4/",
      "title": "Generating Text from Language Models",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.4.pdf",
      "authors": [
        "Afra Amini",
        "Ryan Cotterell",
        "John Hewitt",
        "Luca Malagutti",
        "Clara Meister",
        "Tiago Pimentel"
      ],
      "abstract": "An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms—like top-p sampling or beam search—which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1248,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.5/",
      "title": "Indirectly Supervised Natural Language Processing",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.5.pdf",
      "authors": [
        "Wenpeng Yin",
        "Muhao Chen",
        "Ben Zhou",
        "Qiang Ning",
        "Kai-Wei Chang",
        "Dan Roth"
      ],
      "abstract": "This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a “related” task T′? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T ′ that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations—all having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1249,
      "original_url": "https://aclanthology.org/2023.acl-tutorials.6/",
      "title": "Retrieval-based Language Models and Applications",
      "pdf_url": "https://aclanthology.org/2023.acl-tutorials.6.pdf",
      "authors": [
        "Akari Asai",
        "Sewon Min",
        "Zexuan Zhong",
        "Danqi Chen"
      ],
      "abstract": "Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1250,
      "original_url": "https://aclanthology.org/2023.findings-acl.1/",
      "title": "Investigating Glyph-Phonetic Information for Chinese Spell Checking: What Works and What’s Next?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.1.pdf",
      "authors": [
        "Xiaotian Zhang",
        "Yanjun Zheng",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "abstract": "While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability of CSC models to distinguish misspelled characters, with good results at the accuracy level on public datasets. However, the generalization ability of these CSC models has not been well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1251,
      "original_url": "https://aclanthology.org/2023.findings-acl.2/",
      "title": "A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.2.pdf",
      "authors": [
        "Hwiyeol Jo"
      ],
      "abstract": "We investigate the representation of pretrained language models and humans, using the idea of word definition modeling–how well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences. We then present a simple method DefBERT that integrates pretrained models with word semantics in dictionaries. We show its benefits on newly-proposed tasks of definition ranking and definition sense disambiguation. Furthermore, we present the results on standard word similarity tasks and short text classification tasks where models are required to encode semantics with only a few words. The results demonstrate the effectiveness of integrating word definitions and pretrained language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1252,
      "original_url": "https://aclanthology.org/2023.findings-acl.3/",
      "title": "Conformal Nucleus Sampling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.3.pdf",
      "authors": [
        "Shauli Ravfogel",
        "Yoav Goldberg",
        "Jacob Goldberger"
      ],
      "abstract": "Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-p) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. In this work, we assess whether a top-p set is indeed aligned with its probabilistic meaning in various linguistic contexts.We employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter p as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1253,
      "original_url": "https://aclanthology.org/2023.findings-acl.4/",
      "title": "DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.4.pdf",
      "authors": [
        "Chunkit Chan",
        "Xin Liu",
        "Jiayang Cheng",
        "Zihan Li",
        "Yangqiu Song",
        "Ginny Wong",
        "Simon See"
      ],
      "abstract": "Implicit Discourse Relation Recognition (IDRR) is a sophisticated and challenging task to recognize the discourse relations between the arguments with the absence of discourse connectives. The sense labels for each discourse relation follow a hierarchical classification scheme in the annotation process (Prasad et al., 2008), forming a hierarchy structure. Most existing works do not well incorporate the hierarchy structure but focus on the syntax features and the prior knowledge of connectives in the manner of pure text classification. We argue that it is more effective to predict the paths inside the hierarchical tree (e.g., “Comparison -> Contrast -> however”) rather than flat labels (e.g., Contrast) or connectives (e.g., however). We propose a prompt-based path prediction method to utilize the interactive information and intrinsic senses among the hierarchy in IDRR. This is the first work that injects such structure information into pre-trained language models via prompt tuning, and the performance of our solution shows significant and consistent improvement against competitive baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1254,
      "original_url": "https://aclanthology.org/2023.findings-acl.5/",
      "title": "Modularized Zero-shot VQA with Pre-trained Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.5.pdf",
      "authors": [
        "Rui Cao",
        "Jing Jiang"
      ],
      "abstract": "Large-scale pre-trained models (PTMs) show great zero-shot capabilities. In this paper, we study how to leverage them for zero-shot visual question answering (VQA).Our approach is motivated by a few observations. First, VQA questions often require multiple steps of reasoning, which is still a capability that most PTMs lack. Second, different steps in VQA reasoning chains require different skills such as object detection and relational reasoning, but a single PTM may not possess all these skills. Third, recent work on zero-shot VQA does not explicitly consider multi-step reasoning chains, which makes them less interpretable compared with a decomposition-based approach. We propose a modularized zero-shot network that explicitly decomposes questions into sub reasoning steps and is highly interpretable. We convert sub reasoning tasks to acceptable objectives of PTMs and assign tasks to proper PTMs without any adaptation. Our experiments on two VQA benchmarks under the zero-shot setting demonstrate the effectiveness of our method and better interpretability compared with several baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1255,
      "original_url": "https://aclanthology.org/2023.findings-acl.6/",
      "title": "TimelineQA: A Benchmark for Question Answering over Timelines",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.6.pdf",
      "authors": [
        "Wang-Chiew Tan",
        "Jane Dwivedi-Yu",
        "Yuliang Li",
        "Lambert Mathias",
        "Marzieh Saeidi",
        "Jing Nathan Yan",
        "Alon Halevy"
      ],
      "abstract": "Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information. We create and publicly release TimelineQA, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA with several state-of-the-art QA models. Our experiments reveal that for atomic queries, an extractive QA system significantly out-performs a state-of-the-art retrieval-augmented QA system. For multi-hop queries involving aggregates, we show that the best result is obtained with a state-of-the-art table QA technique, assuming the ground truth set of episodes for deriving the answer is available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1256,
      "original_url": "https://aclanthology.org/2023.findings-acl.7/",
      "title": "Abstractive Text Summarization Using the BRIO Training Paradigm",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.7.pdf",
      "authors": [
        "Khang Lam",
        "Thieu Doan",
        "Khang Pham",
        "Jugal Kalita"
      ],
      "abstract": "Summary sentences produced by abstractive summarization models may be coherent and comprehensive, but they lack control and rely heavily on reference summaries. The BRIO training paradigm assumes a non-deterministic distribution to reduce the model’s dependence on reference summaries, and improve model performance during inference. This paper presents a straightforward but effective technique to improve abstractive summaries by fine-tuning pre-trained language models, and training them with the BRIO paradigm. We build a text summarization dataset for Vietnamese, called VieSum. We perform experiments with abstractive summarization models trained with the BRIO paradigm on the CNNDM and the VieSum datasets. The results show that the models, trained on basic hardware, outperform all existing abstractive summarization models, especially for Vietnamese."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1257,
      "original_url": "https://aclanthology.org/2023.findings-acl.8/",
      "title": "Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.8.pdf",
      "authors": [
        "Ting Wu",
        "Rui Zheng",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classification tasks, results demonstrate that Q-Diversity can consistently improve worst-case accuracy under different distributional shifts, outperforming state-of-the-art alternatives."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1258,
      "original_url": "https://aclanthology.org/2023.findings-acl.9/",
      "title": "Pre-training Language Model as a Multi-perspective Course Learner",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.9.pdf",
      "authors": [
        "Beiduo Chen",
        "Shaohan Huang",
        "Zihan Zhang",
        "Wu Guo",
        "Zhenhua Ling",
        "Haizhen Huang",
        "Furu Wei",
        "Weiwei Deng",
        "Qi Zhang"
      ],
      "abstract": "ELECTRA, the generator-discriminator pre-training framework, has achieved impressive semantic construction capability among various downstream tasks. Despite the convincing performance, ELECTRA still faces the challenges of monotonous training and deficient interaction. Generator with only masked language modeling (MLM) leads to biased learning and label imbalance for discriminator, decreasing learning efficiency; no explicit feedback loop from discriminator to generator results in the chasm between these two components, underutilizing the course learning. In this study, a multi-perspective course learning (MCL) method is proposed to fetch a many degrees and visual angles for sample-efficient pre-training, and to fully leverage the relationship between generator and discriminator. Concretely, three self-supervision courses are designed to alleviate inherent flaws of MLM and balance the label in a multi-perspective way. Besides, two self-correction courses are proposed to bridge the chasm between the two encoders by creating a “correction notebook” for secondary-supervision. Moreover, a course soups trial is conducted to solve the “tug-of-war” dynamics problem of MCL, evolving a stronger pre-trained model. Experimental results show that our method significantly improves ELECTRA’s average performance by 2.8% and 3.2% absolute points respectively on GLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style models under the same settings. The pre-trained MCL model is available at https://huggingface.co/McmanusChen/MCL-base."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1259,
      "original_url": "https://aclanthology.org/2023.findings-acl.10/",
      "title": "Layerwise universal adversarial attack on NLP models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.10.pdf",
      "authors": [
        "Olga Tsymboi",
        "Danil Malaev",
        "Andrei Petrovskii",
        "Ivan Oseledets"
      ],
      "abstract": "In this work, we examine the vulnerability of language models to universal adversarial triggers (UATs). We propose a new white-box approach to the construction of layerwise UATs (LUATs), which searches the triggers by perturbing hidden layers of a network. On the example of three transformer models and three datasets from the GLUE benchmark, we demonstrate that our method provides better transferability in a model-to-model setting with an average gain of 9.3% in the fooling rate over the baseline. Moreover, we investigate triggers transferability in the task-to-task setting. Using small subsets from the datasets similar to the target tasks for choosing a perturbed layer, we show that LUATs are more efficient than vanilla UATs by 7.1% in the fooling rate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1260,
      "original_url": "https://aclanthology.org/2023.findings-acl.11/",
      "title": "Scene-robust Natural Language Video Localization via Learning Domain-invariant Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.11.pdf",
      "authors": [
        "Zehan Wang",
        "Yang Zhao",
        "Haifeng Huang",
        "Yan Xia",
        "Zhou Zhao"
      ],
      "abstract": "Natural language video localization(NLVL) task involves the semantic matching of a text query with a moment from an untrimmed video. Previous methods primarily focus on improving performance with the assumption of independently identical data distribution while ignoring the out-of-distribution data. Therefore, these approaches often fail when handling the videos and queries in novel scenes, which is inevitable in real-world scenarios. In this paper, we, for the first time, formulate the scene-robust NLVL problem and propose a novel generalizable NLVL framework utilizing data in multiple available scenes to learn a robust model. Specifically, our model learns a group of generalizable domain-invariant representations by alignment and decomposition. First, we propose a comprehensive intra- and inter-sample distance metric for complex multi-modal feature space, and an asymmetric multi-modal alignment loss for different information densities of text and vision. Further, to alleviate the conflict between domain-invariant features for generalization and domain-specific information for reasoning, we introduce domain-specific and domain-agnostic predictors to decompose and refine the learned features by dynamically adjusting the weights of samples. Based on the original video tags, we conduct extensive experiments on three NLVL datasets with different-grained scene shifts to show the effectiveness of our proposed methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1261,
      "original_url": "https://aclanthology.org/2023.findings-acl.12/",
      "title": "Exploiting Pseudo Image Captions for Multimodal Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.12.pdf",
      "authors": [
        "Chaoya Jiang",
        "Rui Xie",
        "Wei Ye",
        "Jinan Sun",
        "Shikun Zhang"
      ],
      "abstract": "Multimodal summarization with multimodal output (MSMO) faces a challenging semantic gap between visual and textual modalities due to the lack of reference images for training. Our pilot investigation indicates that image captions, which naturally connect texts and images, can significantly benefit MSMO. However, exposure of image captions during training is inconsistent with MSMO’s task settings, where prior cross-modal alignment information is excluded to guarantee the generalization of cross-modal semantic modeling. To this end, we propose a novel coarse-to-fine image-text alignment mechanism to identify the most relevant sentence of each image in a document, resembling the role of image captions in capturing visual knowledge and bridging the cross-modal semantic gap. Equipped with this alignment mechanism, our method easily yet impressively sets up state-of-the-art performances on all intermodality and intramodality metrics (e.g., more than 10% relative improvement on image recommendation precision). Further experiments reveal the correlation between image captions and text summaries, and prove that the pseudo image captions we generated are even better than the original ones in terms of promoting multimodal summarization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1262,
      "original_url": "https://aclanthology.org/2023.findings-acl.13/",
      "title": "Cross-Lingual Transfer with Target Language-Ready Task Adapters",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.13.pdf",
      "authors": [
        "Marinela Parovic",
        "Alan Ansell",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "abstract": "Adapters have emerged as a modular and parameter-efficient approach to (zero-shot) cross-lingual transfer. The established MAD-X framework employs separate language and task adapters which can be arbitrarily combined to perform the transfer of any task to any target language. Subsequently, BAD-X, an extension of the MAD-X framework, achieves improved transfer at the cost of MAD-X’s modularity by creating ‘bilingual’ adapters specific to the source-target language pair. In this work, we aim to take the best of both worlds by (i) fine-tuning *task* adapters adapted to the target language(s) (so-called *‘target language-ready’ (TLR)* adapters) to maintain high transfer performance, but (ii) without sacrificing the highly modular design of MAD-X. The main idea of ‘target language-ready’ adapters is to resolve the training-vs-inference discrepancy of MAD-X: the task adapter ‘sees’ the target language adapter for the very first time during inference, and thus might not be fully compatible with it. We address this mismatch by exposing the task adapter to the target language adapter during training, and empirically validate several variants of the idea: in the simplest form, we alternate between using the source and target language adapters during task adapter training, which can be generalized to cycling over any set of language adapters. We evaluate different TLR-based transfer configurations with varying degrees of generality across a suite of standard cross-lingual benchmarks, and find that the most general (and thus most modular) configuration consistently outperforms MAD-X and BAD-X on most tasks and languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1263,
      "original_url": "https://aclanthology.org/2023.findings-acl.14/",
      "title": "DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.14.pdf",
      "authors": [
        "Nishant Balepur",
        "Shivam Agarwal",
        "Karthik Venkat Ramanan",
        "Susik Yoon",
        "Diyi Yang",
        "Jiawei Han"
      ],
      "abstract": "Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user’s needs. Further, the topic evolutions produced by DTMs tend to contain generic terms that are not indicative of their designated time steps. To address these issues, we propose the task of discriminative dynamic topic discovery. This task aims to discover topic evolutions from temporal corpora that distinctly align with a set of user-provided category names and uniquely capture topics at each time step. We solve this task by developing DynaMiTE, a framework that ensembles semantic similarity, category indicative, and time indicative scores to produce informative topic evolutions. Through experiments on three diverse datasets, including the use of a newly-designed human evaluation experiment, we demonstrate that DynaMiTE is a practical and efficient framework for helping users discover high-quality topic evolutions suited to their interests."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1264,
      "original_url": "https://aclanthology.org/2023.findings-acl.15/",
      "title": "Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.15.pdf",
      "authors": [
        "Chong Yu",
        "Tao Chen",
        "Zhongxue Gan"
      ],
      "abstract": "Along with the performance improvement in NLP domain, the sizes of transformer-based language models (TLM) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization characters. Especially, a dense TLM model is first pruned to meet the GPU’s acceleration constraint of sparse patterns with FP16 type, then it is further quantized into a fixed-point one by quantization-aware training, to provide an extra speedup for integer tensors on GPU. A mixed-strategy knowledge distillation of labels, logits and feature maps is used for best accuracy compensation during pruning and quantization process. Experiment results show GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and decoder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks. Moreover, GPUSQ-TLM can boost actual deployment performance by up to 4.08-4.25x latency and 6.18-6.79x throughput on A100 GPU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1265,
      "original_url": "https://aclanthology.org/2023.findings-acl.16/",
      "title": "RMSSinger: Realistic-Music-Score based Singing Voice Synthesis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.16.pdf",
      "authors": [
        "Jinzheng He",
        "Jinglin Liu",
        "Zhenhui Ye",
        "Rongjie Huang",
        "Chenye Cui",
        "Huadai Liu",
        "Zhou Zhao"
      ],
      "abstract": "We are interested in a challenging task, Realistic-Music-Score based Singing Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices given realistic music scores with different note types (grace, slur, rest, etc.). Though significant progress has been achieved, recent singing voice synthesis (SVS) methods are limited to fine-grained music scores, which require a complicated data collection pipeline with time-consuming manual annotation to align music notes with phonemes. % Furthermore, existing approaches cannot synthesize rhythmic singing voices given realistic music scores due to the domain gap between fine-grained music scores and realistic music scores. Furthermore, these manual annotation destroys the regularity of note durations in music scores, making fine-grained music scores inconvenient for composing. To tackle these challenges, we propose RMSSinger, the first RMS-SVS method, which takes realistic music scores as input, eliminating most of the tedious manual annotation and avoiding the aforementioned inconvenience. Note that music scores are based on words rather than phonemes, in RMSSinger, we introduce word-level modeling to avoid the time-consuming phoneme duration annotation and the complicated phoneme-level mel-note alignment. Furthermore, we propose the first diffusion-based pitch modeling method, which ameliorates the naturalness of existing pitch-modeling methods. To achieve these, we collect a new dataset containing realistic music scores and singing voices according to these realistic music scores from professional singers. Extensive experiments on the dataset demonstrate the effectiveness of our methods. Audio samples are available at https://rmssinger.github.io/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1266,
      "original_url": "https://aclanthology.org/2023.findings-acl.17/",
      "title": "Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.17.pdf",
      "authors": [
        "Hui-Chi Kuo",
        "Yun-Nung Chen"
      ],
      "abstract": "The current generation of intelligent assistants require explicit user requests to perform tasks or services, often leading to lengthy and complex conversations. In contrast, human assistants can infer multiple implicit intents from utterances via their commonsense knowledge, thereby simplifying interactions. To bridge this gap, this paper proposes a framework for multi-domain dialogue systems. This framework automatically infers implicit intents from user utterances, and prompts a large pre-trained language model to suggest suitable task-oriented bots. By leveraging commonsense knowledge, our framework recommends associated bots in a zero-shot manner, enhancing interaction efficiency and effectiveness. This approach substantially reduces interaction complexity, seamlessly integrates various domains and tasks, and represents a significant step towards creating more human-like intelligent assistants that can reason about implicit intents, offering a superior user experience."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1267,
      "original_url": "https://aclanthology.org/2023.findings-acl.18/",
      "title": "MTGP: Multi-turn Target-oriented Dialogue Guided by Generative Global Path with Flexible Turns",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.18.pdf",
      "authors": [
        "Anqi Liu",
        "Bo Wang",
        "Yue Tan",
        "Dongming Zhao",
        "Kun Huang",
        "Ruifang He",
        "Yuexian Hou"
      ],
      "abstract": "Target-oriented dialogue guides the dialogue to a target quickly and smoothly. The latest approaches focus on global planning, which plans toward the target before the conversation instead of adopting a greedy strategy during the conversation. However, the global plan in existing works is fixed to certain turns by generating paths with certain nodes, which limits the optimization of turns and coherence of the target-oriented process. Toward flexible global planning, we propose to generate a global path as a natural language sentence instead of a sequence of nodes. With this path, the dialog is guided to the target with flexible turns of dialog. For model training, we also extract targetoriented dialogues from the chit-chat corpus with a knowledge graph. We conduct experiments on three datasets and simulate scenarios with and without user participation. The results show that our method has fewer turns, more coherent semantics, and a higher success rate in reaching the target than baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1268,
      "original_url": "https://aclanthology.org/2023.findings-acl.19/",
      "title": "The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.19.pdf",
      "authors": [
        "Antonio Valerio Miceli Barone",
        "Fazl Barez",
        "Shay B. Cohen",
        "Ioannis Konstas"
      ],
      "abstract": "Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1269,
      "original_url": "https://aclanthology.org/2023.findings-acl.20/",
      "title": "Class Lifelong Learning for Intent Detection via Structure Consolidation Networks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.20.pdf",
      "authors": [
        "Qingbin Liu",
        "Yanchao Hao",
        "Xiaolong Liu",
        "Bo Li",
        "Dianbo Sui",
        "Shizhu He",
        "Kang Liu",
        "Jun Zhao",
        "Xi Chen",
        "Ningyu Zhang",
        "Jiaoyan Chen"
      ],
      "abstract": "Intent detection, which estimates diverse intents behind user utterances, is an essential component of task-oriented dialogue systems. Previous intent detection models are usually trained offline, which can only handle predefined intent classes. In the real world, new intents may keep challenging deployed models. For example, with the prevalence of the COVID-19 pandemic, users may pose various issues related to the pandemic to conversational systems, which brings many new intents. A general intent detection model should be intelligent enough to continually learn new data and recognize new arriving intent classes. Therefore, this work explores Class Lifelong Learning for Intent Detection (CLL-ID), where the model continually learns new intent classes from new data while avoiding catastrophic performance degradation on old data. To this end, we propose a novel lifelong learning method, called Structure Consolidation Networks (SCN), which consists of structure-based retrospection and contrastive knowledge distillation to handle the problems of expression diversity and class imbalance in the CLL-ID task. In addition to formulating the new task, we construct 3 benchmarks based on 8 intent detection datasets. Experimental results demonstrate the effectiveness of SCN, which significantly outperforms previous lifelong learning methods on the three benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1270,
      "original_url": "https://aclanthology.org/2023.findings-acl.21/",
      "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.21.pdf",
      "authors": [
        "Aniket Vashishtha",
        "Kabir Ahuja",
        "Sunayana Sitaram"
      ],
      "abstract": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1271,
      "original_url": "https://aclanthology.org/2023.findings-acl.22/",
      "title": "Rethinking Round-Trip Translation for Machine Translation Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.22.pdf",
      "authors": [
        "Terry Yue Zhuo",
        "Qiongkai Xu",
        "Xuanli He",
        "Trevor Cohn"
      ],
      "abstract": "Automatic evaluation methods for translation often require model training, and thus the availability of parallel corpora limits their applicability to low-resource settings. Round-trip translation is a potential workaround, which can reframe bilingual evaluation into a much simpler monolingual task. Early results from the era of statistical machine translation (SMT) raised fundamental concerns about the utility of this approach, based on poor correlation with human translation quality judgments. In this paper, we revisit this technique with modern neural translation (NMT) and show that round-trip translation does allow for accurate automatic evaluation without the need for reference translations. These opposite findings can be explained through the copy mechanism in SMT that is absent in NMT. We demonstrate that round-trip translation benefits multiple machine translation evaluation tasks: i) predicting forward translation scores; ii) improving the performance of a quality estimation model; and iii) identifying adversarial competitors in shared tasks via cross-system verification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1272,
      "original_url": "https://aclanthology.org/2023.findings-acl.23/",
      "title": "G3R: A Graph-Guided Generate-and-Rerank Framework for Complex and Cross-domain Text-to-SQL Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.23.pdf",
      "authors": [
        "Yanzheng Xiang",
        "Qian-Wen Zhang",
        "Xu Zhang",
        "Zejie Liu",
        "Yunbo Cao",
        "Deyu Zhou"
      ],
      "abstract": "We present a framework called G3R for complex and cross-domain Text-to-SQL generation. G3R aims to address two limitations of current approaches: (1) The structure of the abstract syntax tree (AST) is not fully explored during the decoding process which is crucial for complex SQL generation; (2) Domain knowledge is not incorporated to enhance their ability to generalise to unseen domains. G3R consists of a graph-guided SQL generator and a knowledge-enhanced re-ranking mechanism. Firstly, during the decoding process, An AST-Grammar bipartite graph is constructed for both the AST and corresponding grammar rules of the generated partial SQL query. The graph-guided SQL generator captures its structural information and fuses heterogeneous information to predict the action sequence which can construct the AST for the corresponding SQL query uniquely. Then, in the inference stage, a knowledge-enhanced re-ranking mechanism is proposed to introduce domain knowledge to re-rank candidate SQL queries from the beam output and choose the final answer. The SQL ranker is based on pre-trained language models (PLM) and contrastive learning with hybrid prompt tuning is incorporated to stimulate the knowledge of PLMs and make it more discriminative. The proposed approach achieves state-of-the-art results on the Spider and Spider-DK benchmarks, which are challenging complex and cross-domain benchmarks for Text-to-SQL semantic analysis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1273,
      "original_url": "https://aclanthology.org/2023.findings-acl.24/",
      "title": "A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.24.pdf",
      "authors": [
        "Ruiqing Ding",
        "Xiao Han",
        "Leye Wang"
      ],
      "abstract": "By focusing the pre-training process on domain-specific corpora, some domain-specific pre-trained language models (PLMs) have achieved state-of-the-art results. However, it is under-investigated to design a unified paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose KnowledgeDA, a unified domain language model development service to enhance the task-specific training procedure with domain knowledge graphs. Given domain-specific task texts input, KnowledgeDA can automatically generate a domain-specific language model following three steps: (i) localize domain knowledge entities in texts via an embedding-similarity approach; (ii) generate augmented samples by retrieving replaceable domain entity pairs from two views of both knowledge graph and training data; (iii) select high-quality augmented samples for fine-tuning via confidence-based assessment. We implement a prototype of KnowledgeDA to learn language models for two domains, healthcare and software development. Experiments on domain-specific text classification and QA tasks verify the effectiveness and generalizability of KnowledgeDA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1274,
      "original_url": "https://aclanthology.org/2023.findings-acl.25/",
      "title": "Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.25.pdf",
      "authors": [
        "Jian Wang",
        "Dongding Lin",
        "Wenjie Li"
      ],
      "abstract": "Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1275,
      "original_url": "https://aclanthology.org/2023.findings-acl.26/",
      "title": "A Match Made in Heaven: A Multi-task Framework for Hyperbole and Metaphor Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.26.pdf",
      "authors": [
        "Naveen Badathala",
        "Abisek Rajakumar Kalarani",
        "Tejpalsingh Siledar",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Hyperbole and metaphor are common in day-to-day communication (e.g., “I am in deep trouble”: how does trouble have depth?), which makes their detection important, especially in a conversational AI setting. Existing approaches to automatically detect metaphor and hyperbole have studied these language phenomena independently, but their relationship has hardly, if ever, been explored computationally. In this paper, we propose a multi-task deep learning framework to detect hyperbole and metaphor simultaneously. We hypothesize that metaphors help in hyperbole detection, and vice-versa. To test this hypothesis, we annotate two hyperbole datasets- HYPO and HYPO-L- with metaphor labels. Simultaneously, we annotate two metaphor datasets- TroFi and LCC- with hyperbole labels. Experiments using these datasets give an improvement of the state of the art of hyperbole detection by 12%. Additionally, our multi-task learning (MTL) approach shows an improvement of up to 17% over single-task learning (STL) for both hyperbole and metaphor detection, supporting our hypothesis. To the best of our knowledge, ours is the first demonstration of computational leveraging of linguistic intimacy between metaphor and hyperbole, leading to showing the superiority of MTL over STL for hyperbole and metaphor detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1276,
      "original_url": "https://aclanthology.org/2023.findings-acl.27/",
      "title": "Prompt Tuning for Unified Multimodal Pretrained Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.27.pdf",
      "authors": [
        "Hao Yang",
        "Junyang Lin",
        "An Yang",
        "Peng Wang",
        "Chang Zhou"
      ],
      "abstract": "Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. The parameter-efficient prompt tuning methods that optimize soft embeddings while keeping the pretrained model frozen demonstrate advantages in low computation costs and almost lossless performance. In this work, we explore the transfer of prompt tuning to multimodal pretrained models. Specifically, we implement prompt tuning to a unified sequence-to-sequence pretrained model by adding a sequence of learnable embeddings to each layer and finetuning the pretrained model on downstream task with only the learnable embeddings being optimized. Experimental results on a series of multimodal understanding and generation tasks demonstrate that our method OFA-PT can achieve comparable performance with finetuning across a series of multimodal generation and understanding tasks. Additionally, it significantly outperforms the unified multimodal pretrained model with other parameter-efficient tuning methods, e.g., Adapter, BitFit. etc. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1277,
      "original_url": "https://aclanthology.org/2023.findings-acl.28/",
      "title": "Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.28.pdf",
      "authors": [
        "Yifu Gao",
        "Yongquan He",
        "Zhigang Kan",
        "Yi Han",
        "Linbo Qiao",
        "Dongsheng Li"
      ],
      "abstract": "Temporal knowledge graph completion that predicts missing links for incomplete temporal knowledge graphs (TKG) is gaining increasing attention. Most existing works have achieved good results by incorporating time information into static knowledge graph embedding methods. However, they ignore the contextual nature of the TKG structure, i.e., query-specific subgraph contains both structural and temporal neighboring facts. This paper presents the SToKE, a novel method that employs the pre-trained language model (PLM) to learn joint Structural and Temporal Contextualized Knowledge Embeddings.Specifically, we first construct an event evolution tree (EET) for each query to enable PLMs to handle the TKG, which can be seen as a structured event sequence recording query-relevant structural and temporal contexts. We then propose a novel temporal embedding and structural matrix to learn the time information and structural dependencies of facts in EET.Finally, we formulate TKG completion as a mask prediction problem by masking the missing entity of the query to fine-tune pre-trained language models. Experimental results on three widely used datasets show the superiority of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1278,
      "original_url": "https://aclanthology.org/2023.findings-acl.29/",
      "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.29.pdf",
      "authors": [
        "Md Tahmid Rahman Laskar",
        "M Saiful Bari",
        "Mizanur Rahman",
        "Md Amran Hossen Bhuiyan",
        "Shafiq Joty",
        "Jimmy Huang"
      ],
      "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT’s performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT’s performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1279,
      "original_url": "https://aclanthology.org/2023.findings-acl.30/",
      "title": "Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.30.pdf",
      "authors": [
        "Jianxing Yu",
        "Shiqi Wang",
        "Libin Zheng",
        "Qinliang Su",
        "Wei Liu",
        "Baoquan Zhao",
        "Jian Yin"
      ],
      "abstract": "This paper proposes a new task of commonsense question generation, which aims to yield deep-level and to-the-point questions from the text. Their answers need to reason over disjoint relevant contexts and external commonsense knowledge, such as encyclopedic facts and causality. The knowledge may not be explicitly mentioned in the text but is used by most humans for problem-shooting. Such complex reasoning with hidden contexts involves deep semantic understanding. Thus, this task has great application value, such as making high-quality quizzes in advanced exams. Due to the lack of modeling complexity, existing methods may produce shallow questions that can be answered by simple word matching. To address these challenges, we propose a new QG model by simultaneously considering asking contents, expressive ways, and answering complexity. We first retrieve text-related commonsense context. Then we disentangle the key factors that control questions in terms of reasoning content and verbalized way. Independence priors and constraints are imposed to facilitate disentanglement. We further develop a discriminator to promote the deep results by considering their answering complexity. Through adversarial inference, we learn the latent factors from data. By sampling the expressive factor from the data distributions, diverse questions can be yielded. Evaluations of two typical data sets show the effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1280,
      "original_url": "https://aclanthology.org/2023.findings-acl.31/",
      "title": "TADA: Efficient Task-Agnostic Domain Adaptation for Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.31.pdf",
      "authors": [
        "Chia-Chien Hung",
        "Lukas Lange",
        "Jannik Strötgen"
      ],
      "abstract": "Intermediate training of pre-trained transformer-based language models on domain-specific data leads to substantial gains for downstream tasks. To increase efficiency and prevent catastrophic forgetting alleviated from full domain-adaptive pre-training, approaches such as adapters have been developed. However, these require additional parameters for each layer, and are criticized for their limited expressiveness. In this work, we introduce TADA, a novel task-agnostic domain adaptation method which is modular, parameter-efficient, and thus, data-efficient. Within TADA, we retrain the embeddings to learn domain-aware input representations and tokenizers for the transformer encoder, while freezing all other parameters of the model. Then, task-specific fine-tuning is performed. We further conduct experiments with meta-embeddings and newly introduced meta-tokenizers, resulting in one model per task in multi-domain use cases. Our broad evaluation in 4 downstream tasks for 14 domains across single- and multi-domain setups and high- and low-resource scenarios reveals that TADA is an effective and efficient alternative to full domain-adaptive pre-training and adapters for domain adaptation, while not introducing additional parameters or complex training steps."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1281,
      "original_url": "https://aclanthology.org/2023.findings-acl.32/",
      "title": "Robust Natural Language Understanding with Residual Attention Debiasing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.32.pdf",
      "authors": [
        "Fei Wang",
        "James Y. Huang",
        "Tianyi Yan",
        "Wenxuan Zhou",
        "Muhao Chen"
      ],
      "abstract": "Natural language understanding (NLU) models often suffer from unintended dataset biases. Among bias mitigation methods, ensemble-based debiasing methods, especially product-of-experts (PoE), have stood out for their impressive empirical success. However, previous ensemble-based debiasing methods typically apply debiasing on top-level logits without directly addressing biased attention patterns. Attention serves as the main media of feature interaction and aggregation in PLMs and plays a crucial role in providing robust prediction. In this paper, we propose REsidual Attention Debiasing (READ), an end-to-end debiasing method that mitigates unintended biases from attention. Experiments on three NLU benchmarks show that READ significantly improves the OOD performance of BERT-based models, including +12.9% accuracy on HANS, +11.0% accuracy on FEVER-Symmetric, and +2.7% F1 on PAWS. Detailed analyses demonstrate the crucial role of unbiased attention in robust NLU models and that READ effectively mitigates biases in attention."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1282,
      "original_url": "https://aclanthology.org/2023.findings-acl.33/",
      "title": "MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.33.pdf",
      "authors": [
        "Haoning Zhang",
        "Junwei Bao",
        "Haipeng Sun",
        "Youzheng Wu",
        "Wenye Li",
        "Shuguang Cui",
        "Xiaodong He"
      ],
      "abstract": "Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizes all history information, the dialogue state in the previous turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the previous turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the model’s ability to update and correct slot values. Furthermore, a contrastive contextmatching framework is designed to narrow the representation distance between a state and itscorresponding noised variant, which reduces the impact of noised state and makes the model better understand the dialogue history. Experimental results on MultiWOZ datasets show that MoNET outperforms previous DST methods. Ablations and analysis verify the effectiveness of MoNET in alleviating state momentum issues and improving the anti-noise ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1283,
      "original_url": "https://aclanthology.org/2023.findings-acl.34/",
      "title": "PAL: Persona-Augmented Emotional Support Conversation Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.34.pdf",
      "authors": [
        "Jiale Cheng",
        "Sahand Sabour",
        "Hao Sun",
        "Zhuang Chen",
        "Minlie Huang"
      ],
      "abstract": "Due to the lack of human resources for mental health support, there is an increasing demand for employing conversational agents for support. Recent work has demonstrated the effectiveness of dialogue models in providing emotional support. As previous studies have demonstrated that seekers’ persona is an important factor for effective support, we investigate whether there are benefits to modeling such information in dialogue models for support. In this paper, our empirical analysis verifies that persona has an important impact on emotional support. Therefore, we propose a framework for dynamically inferring and modeling seekers’ persona. We first train a model for inferring the seeker’s persona from the conversation history. Accordingly, we propose PAL, a model that leverages persona information and, in conjunction with our strategy-based controllable generation method, provides personalized emotional support. Automatic and manual evaluations demonstrate that PAL achieves state-of-the-art results, outperforming the baselines on the studied benchmark. Our code and data are publicly available at https://github.com/chengjl19/PAL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1284,
      "original_url": "https://aclanthology.org/2023.findings-acl.35/",
      "title": "Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.35.pdf",
      "authors": [
        "Xiao Wang",
        "Weikang Zhou",
        "Qi Zhang",
        "Jie Zhou",
        "SongYang Gao",
        "Junzhe Wang",
        "Menghan Zhang",
        "Xiang Gao",
        "Yun Wen Chen",
        "Tao Gui"
      ],
      "abstract": "Pretrained language models have achieved remarkable success in various natural language processing tasks. However, pretraining has recently shifted toward larger models and larger data, which has resulted in significant computational and energy costs. In this paper, we propose Influence Subset Selection (ISS) for language model, which explicitly utilizes end-task knowledge to select a tiny subset of the pretraining corpus. Specifically, the ISS selects the samples that will provide the most positive influence on the performance of the end task. Furthermore, we design a gradient matching-based influence estimation method, which can drastically reduce the computation time of influence. With only 0.45% of the data and a three-orders-of-magnitude lower computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight datasets covering four domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1285,
      "original_url": "https://aclanthology.org/2023.findings-acl.36/",
      "title": "Exclusive Supermask Subnetwork Training for Continual Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.36.pdf",
      "authors": [
        "Prateek Yadav",
        "Mohit Bansal"
      ],
      "abstract": "Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNetwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT) module that utilizes previously acquired knowledge to learn new tasks better and faster. We demonstrate that ExSSNeT outperforms strong previous methods on both NLP and Vision domains while preventing forgetting. Moreover, ExSSNeT is particularly advantageous for sparse masks that activate 2-10% of the model parameters, resulting in an average improvement of 8.3% over SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1286,
      "original_url": "https://aclanthology.org/2023.findings-acl.37/",
      "title": "Transferring General Multimodal Pretrained Models to Text Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.37.pdf",
      "authors": [
        "Junyang Lin",
        "Xuancheng Ren",
        "Yichang Zhang",
        "Gao Liu",
        "Peng Wang",
        "An Yang",
        "Chang Zhou"
      ],
      "abstract": "This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark. Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1287,
      "original_url": "https://aclanthology.org/2023.findings-acl.38/",
      "title": "A Formal Perspective on Byte-Pair Encoding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.38.pdf",
      "authors": [
        "Vilém Zouhar",
        "Clara Meister",
        "Juan Gastaldi",
        "Li Du",
        "Tim Vieira",
        "Mrinmaya Sachan",
        "Ryan Cotterell"
      ],
      "abstract": "Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a 1/sigma*(1-e(-sigma))-approximation of an optimal merge sequence, where sigma is the total backward curvature with respect to the optimal merge sequence. Empirically the lower bound of the approximation is approx0.37.We provide a faster implementation of BPE which improves the runtime complexity from O(NM) to O(N log M), where N is the sequence length and M is the merge count. Finally, we optimize the brute-force algorithm for optimal BPE using memoization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1288,
      "original_url": "https://aclanthology.org/2023.findings-acl.39/",
      "title": "Automatic Named Entity Obfuscation in Speech",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.39.pdf",
      "authors": [
        "Judita Preiss"
      ],
      "abstract": "Sharing data containing personal information often requires its anonymization, even when consent for sharing was obtained from the data originator. While approaches exist for automated anonymization of text, the area is not as thoroughly explored in speech. This work focuses on identifying, replacing and inserting replacement named entities synthesized using voice cloning into original audio thereby retaining prosodic information while reducing the likelihood of deanonymization. The approach employs a novel named entity recognition (NER) system built directly on speech by training HuBERT (Hsu et al, 2021) using the English speech NER dataset (Yadav et al, 2020). Name substitutes are found using a masked language model and are synthesized using text to speech voice cloning (Eren and team, 2021), upon which the substitute named entities are re-inserted into the original text. The approach is prototyped on a sample of the LibriSpeech corpus (Panyatov et al, 2015) with each step evaluated individually."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1289,
      "original_url": "https://aclanthology.org/2023.findings-acl.40/",
      "title": "Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.40.pdf",
      "authors": [
        "Soochan Lee",
        "Gunhee Kim"
      ],
      "abstract": "Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models’ (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs’ inference capability to solve problems, whose solution consists of hundreds of thousands of tokens."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1290,
      "original_url": "https://aclanthology.org/2023.findings-acl.41/",
      "title": "UniS-MMC: Multimodal Classification via Unimodality-supervised Multimodal Contrastive Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.41.pdf",
      "authors": [
        "Heqing Zou",
        "Meng Shen",
        "Chen Chen",
        "Yuchen Hu",
        "Deepu Rajan",
        "Eng Siong Chng"
      ],
      "abstract": "Multimodal learning aims to imitate human beings to acquire complementary information from multiple modalities for various downstream tasks. However, traditional aggregation-based multimodal fusion methods ignore the inter-modality relationship, treat each modality equally, suffer sensor noise, and thus reduce multimodal learning performance. In this work, we propose a novel multimodal contrastive method to explore more reliable multimodal representations under the weak supervision of unimodal predicting. Specifically, we first capture task-related unimodal representations and the unimodal predictions from the introduced unimodal predicting task. Then the unimodal representations are aligned with the more effective one by the designed multimodal contrastive method under the supervision of the unimodal predictions. Experimental results with fused features on two image-text classification benchmarks UPMC-Food-101 and N24News show that our proposed Unimodality-Supervised MultiModal Contrastive UniS-MMC learning method outperforms current state-of-the-art multimodal methods. The detailed ablation study and analysis further demonstrate the advantage of our proposed method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1291,
      "original_url": "https://aclanthology.org/2023.findings-acl.42/",
      "title": "Robustness-Aware Word Embedding Improves Certified Robustness to Adversarial Word Substitutions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.42.pdf",
      "authors": [
        "Yibin Wang",
        "Yichen Yang",
        "Di He",
        "Kun He"
      ],
      "abstract": "Natural Language Processing (NLP) models have gained great success on clean texts, but they are known to be vulnerable to adversarial examples typically crafted by synonym substitutions. In this paper, we target to solve this problem and find that word embedding is important to the certified robustness of NLP models. Given the findings, we propose the Embedding Interval Bound Constraint (EIBC) triplet loss to train robustness-aware word embeddings for better certified robustness. We optimize the EIBC triplet loss to reduce distances between synonyms in the embedding space, which is theoretically proven to make the verification boundary tighter. Meanwhile, we enlarge distances among non-synonyms, maintaining the semantic representation of word embeddings. Our method is conceptually simple and componentized. It can be easily combined with IBP training and improves the certified robust accuracy from 76.73% to 84.78% on the IMDB dataset. Experiments demonstrate that our method outperforms various state-of-the-art certified defense baselines and generalizes well to unseen substitutions. The code is available at https://github.com/JHL-HUST/EIBC-IBP/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1292,
      "original_url": "https://aclanthology.org/2023.findings-acl.43/",
      "title": "Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.43.pdf",
      "authors": [
        "Aiwei Liu",
        "Wei Liu",
        "Xuming Hu",
        "Shuang Li",
        "Fukun Ma",
        "Yawen Yang",
        "Lijie Wen"
      ],
      "abstract": "In the context-dependent Text-to-SQL task, the generated SQL statements are refined iteratively based on the user input utterance from each interaction. The input text from each interaction can be viewed as component modifications to the previous SQL statements, which could be further extracted as the modification patterns. Since these modification patterns could also be combined with other SQL statements, the models are supposed to have the compositional generalization to these novel combinations. This work is the first exploration of compositional generalization in context-dependent Text-to-SQL scenarios. To facilitate related studies, we constructed two challenging benchmarks named CoSQL-CG and SParC-CG by recombining the modification patterns and existing SQL statements. The following experiments show that almost all current models struggle on our proposed benchmarks. Furthermore, we found that better aligning the previous SQL statements with the input utterance could give models better combinatorial generalization ability. Based on these observations, we propose a method name p-align to improve the combinatorial generalization of Text-to-SQL models. Further experiments validate the effectiveness of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1293,
      "original_url": "https://aclanthology.org/2023.findings-acl.44/",
      "title": "Towards Generative Event Factuality Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.44.pdf",
      "authors": [
        "John Murzaku",
        "Tyler Osborne",
        "Amittai Aviram",
        "Owen Rambow"
      ],
      "abstract": "We present a novel end-to-end generative task and system for predicting event factuality holders, targets, and their associated factuality values. We perform the first experiments using all sources and targets of factuality statements from the FactBank corpus. We perform multi-task learning with other tasks and event-factuality corpora to improve on the FactBank source and target task. We argue that careful domain specific target text output format in generative systems is important and verify this with multiple experiments on target text output structure. We redo previous state-of-the-art author-only event factuality experiments and also offer insights towards a generative paradigm for the author-only event factuality prediction task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1294,
      "original_url": "https://aclanthology.org/2023.findings-acl.45/",
      "title": "Can Language Models Be Specific? How?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.45.pdf",
      "authors": [
        "Jie Huang",
        "Kevin Chen-Chuan Chang",
        "Jinjun Xiong",
        "Wen-mei Hwu"
      ],
      "abstract": "“He is a person”, “Paris is located on the earth”. Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given “Toronto is located in [MASK].”, we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research community to further explore this important but understudied problem."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1295,
      "original_url": "https://aclanthology.org/2023.findings-acl.46/",
      "title": "The Web Can Be Your Oyster for Improving Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.46.pdf",
      "authors": [
        "Junyi Li",
        "Tianyi Tang",
        "Wayne Xin Zhao",
        "Jingyuan Wang",
        "Jian-Yun Nie",
        "Ji-Rong Wen"
      ],
      "abstract": "Pretrained language models (PLMs) encode a large amount of world knowledge. However, as such knowledge is frozen at the time of model training, the models become static and limited by the training data at that time. In order to further improve the capacity of PLMs for knowledge-intensive tasks, we consider augmenting PLMs with the large-scale web using search engine. Unlike previous augmentation sources (e.g., Wikipedia data dump), the web provides broader, more comprehensive and constantly updated information. In this paper, we present a web-augmented PLM – UniWeb, which is trained over 16 knowledge-intensive tasks in a unified text-to-text format. Instead of simply using the retrieved contents from web, our approach has made two major improvements. Firstly, we propose an adaptive search engine assisted learning method that can self-evaluate the confidence level of PLM’s predictions, and adaptively determine when to refer to the web for more data, which can avoid useless or noisy augmentation from web. Secondly, we design a pretraining task, i.e., continual knowledge learning, based on salient spans prediction, to reduce the discrepancy between the encoded and retrieved knowledge. Experiments on a wide range of knowledge-intensive tasks show that our model significantly outperforms previous retrieval-augmented methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1296,
      "original_url": "https://aclanthology.org/2023.findings-acl.47/",
      "title": "Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.47.pdf",
      "authors": [
        "Hwichan Kim",
        "Mamoru Komachi"
      ],
      "abstract": "Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent performance in the target language. However, if no labeled data in the target language are available, they need to be created through human annotations. In this study, we devise a metric to select annotation candidates from an unlabeled data pool that efficiently enhance accuracy for few-shot cross-lingual transfer. It is known that training a model with hard examples is important to improve the model’s performance. Therefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the MMLM.We then choose high peculiarity examples as annotation candidates and perform few-shot cross-lingual transfer. In comprehensive experiments with 20 languages and 6 tasks, we demonstrate that the high peculiarity examples improve the target language accuracy compared to other candidate selection methods proposed in previous studies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1297,
      "original_url": "https://aclanthology.org/2023.findings-acl.48/",
      "title": "Overcoming Catastrophic Forgetting in Massively Multilingual Continual Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.48.pdf",
      "authors": [
        "Genta Winata",
        "Lingjue Xie",
        "Karthik Radhakrishnan",
        "Shijie Wu",
        "Xisen Jin",
        "Pengxiang Cheng",
        "Mayank Kulkarni",
        "Daniel Preotiuc-Pietro"
      ],
      "abstract": "Real-life multilingual systems should be able to efficiently incorporate new languages as data distributions fed to the system evolve and shift over time. To do this, systems need to handle the issue of catastrophic forgetting, where the model performance drops for languages or tasks seen further in its past. In this paper, we study catastrophic forgetting, as well as methods to minimize this, in a massively multilingual continual learning framework involving up to 51 languages and covering both classification and sequence labeling tasks. We present LR ADJUST, a learning rate scheduling method that is simple, yet effective in preserving new information without strongly overwriting past knowledge. Furthermore, we show that this method is effective across multiple continual learning approaches. Finally, we provide further insights into the dynamics of catastrophic forgetting in this massively multilingual setup."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1298,
      "original_url": "https://aclanthology.org/2023.findings-acl.49/",
      "title": "UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.49.pdf",
      "authors": [
        "Rui Sun",
        "Zhecan Wang",
        "Haoxuan You",
        "Noel Codella",
        "Kai-Wei Chang",
        "Shih-Fu Chang"
      ],
      "abstract": "Vision-language tasks, such as VQA, SNLI-VE, and VCR are challenging because they require the model’s reasoning ability to understand the semantics of the visual world and natural language. Supervised methods working for vision-language tasks have been well-studied. However, solving these tasks in a zero-shot setting is less explored. Since Contrastive Language-Image Pre-training (CLIP) has shown remarkable zero-shot performance on image-text matching, previous works utilized its strong zero-shot ability by converting vision-language tasks into an image-text matching problem, and they mainly consider global-level matching (e.g., the whole image or sentence). However, we find visual and textual fine-grained information, e.g., keywords in the sentence and objects in the image, can be fairly informative for semantics understanding. Inspired by this, we propose a unified framework to take advantage of the fine-grained information for zero-shot vision-language learning, covering multiple tasks such as VQA, SNLI-VE, and VCR. Our experiments show that our framework outperforms former zero-shot methods on VQA and achieves substantial improvement on SNLI-VE and VCR. Furthermore, our ablation studies confirm the effectiveness and generalizability of our proposed method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1299,
      "original_url": "https://aclanthology.org/2023.findings-acl.50/",
      "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.50.pdf",
      "authors": [
        "Kai Zhang",
        "Bernal Jimenez Gutierrez",
        "Yu Su"
      ],
      "abstract": "Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE’s low incidence in instruction-tuning datasets, making up less than 1% of all tasks (Wang et al. 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1300,
      "original_url": "https://aclanthology.org/2023.findings-acl.51/",
      "title": "TADA : Task Agnostic Dialect Adapters for English",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.51.pdf",
      "authors": [
        "William Held",
        "Caleb Ziems",
        "Diyi Yang"
      ],
      "abstract": "Large Language Models, the dominant starting point for Natural Language Processing (NLP) applications, fail at a higher rate for speakers of English dialects other than Standard American English (SAE). Prior work addresses this using task specific data or synthetic data augmentation, both of which require intervention for each dialect and task pair. This poses a scalability issue that prevents the broad adoption of robust dialectal English NLP. We introduce a simple yet effective method for task-agnostic dialect adaptation by aligning non-SAE dialects using adapters and composing them with task-specific adapters from SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on 4 dialectal variants of the GLUE benchmark without task-specific supervision."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1301,
      "original_url": "https://aclanthology.org/2023.findings-acl.52/",
      "title": "Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.52.pdf",
      "authors": [
        "Xuefeng Li",
        "Liwen Wang",
        "Guanting Dong",
        "Keqing He",
        "Jinzheng Zhao",
        "Hao Lei",
        "Jiachi Liu",
        "Weiran Xu"
      ],
      "abstract": "Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt tuning strategy to boost higher performance only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44% F1) on the unseen slots."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1302,
      "original_url": "https://aclanthology.org/2023.findings-acl.53/",
      "title": "Re-appraising the Schema Linking for Text-to-SQL",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.53.pdf",
      "authors": [
        "Yujian Gan",
        "Xinyun Chen",
        "Matthew Purver"
      ],
      "abstract": "Most text-to-SQL models, even though based on the same grammar decoder, generate the SQL structure first and then fill in the SQL slots with the correct schema items. This second step depends on schema linking: aligning the entity references in the question with the schema columns or tables. This is generally approached via Exact Match based Schema Linking (EMSL) within a neural network-based schema linking module. EMSL has become standard in text-to-SQL: many state-of-the-art models employ EMSL, with performance dropping significantly when the EMSL component is removed. In this work, however, we show that EMSL reduces robustness, rendering models vulnerable to synonym substitution and typos. Instead of relying on EMSL to make up for deficiencies in question-schema encoding, we show that using a pre-trained language model as an encoder can improve performance without using EMSL, giving a more robust model. We also study the design choice of the schema linking module, finding that a suitable design benefits performance and interoperability. Finally, based on the above study of schema linking, we introduce the grammar linking to help model align grammar references in the question with the SQL keywords."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1303,
      "original_url": "https://aclanthology.org/2023.findings-acl.54/",
      "title": "Echoes from Alexandria: A Large Resource for Multilingual Book Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.54.pdf",
      "authors": [
        "Alessandro Scirè",
        "Simone Conia",
        "Simone Ciciliano",
        "Roberto Navigli"
      ],
      "abstract": "In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and availability in English only. To overcome these limitations, we present “Echoes from Alexandria”, or in shortened form, “Echoes”, a large resource for multilingual book summarization. Echoes featuresthree novel datasets: i) Echo-Wiki, for multilingual book summarization, ii) Echo-XSum, for extremely-compressive multilingual book summarization, and iii) Echo-FairySum, for extractive book summarization. To the best of our knowledge, Echoes – with its thousands of books and summaries – is the largest resource, and the first to be multilingual, featuring 5 languages and 25 language pairs. In addition to Echoes, we also introduce a new extractive-then-abstractive baseline, and, supported by our experimental results and manual analysis of the summaries generated, we argue that this baseline is more suitable for book summarization than purely-abstractive approaches. We release our resource and software at https://github.com/Babelscape/echoes-from-alexandria in the hope of fostering innovative research in multilingual booksummarization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1304,
      "original_url": "https://aclanthology.org/2023.findings-acl.55/",
      "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.55.pdf",
      "authors": [
        "Chengcheng Han",
        "Liqing Cui",
        "Renyu Zhu",
        "Jianing Wang",
        "Nuo Chen",
        "Qiushi Sun",
        "Xiang Li",
        "Ming Gao"
      ],
      "abstract": "Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner. Experimental results show that GDFO can achieve significant performance gains over previous state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1305,
      "original_url": "https://aclanthology.org/2023.findings-acl.56/",
      "title": "Align-then-Enhance: Multilingual Entailment Graph Enhancement with Soft Predicate Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.56.pdf",
      "authors": [
        "Yuting Wu",
        "Yutong Hu",
        "Yansong Feng",
        "Tianyi Li",
        "Mark Steedman",
        "Dongyan Zhao"
      ],
      "abstract": "Entailment graphs (EGs) with predicates as nodes and entailment relations as edges are typically incomplete, while EGs in different languages are often complementary to each other. In this paper, we propose a new task, multilingual entailment graph enhancement, which aims to utilize the entailment information from one EG to enhance another EG in a different language. The ultimate goal is to obtain an enhanced EG containing richer and more accurate entailment information. We present an align-then-enhance framework (ATE) to achieve accurate multilingual entailment graph enhancement, which first exploits a cross-graph guided interaction mechanism to automatically discover potential equivalent predicates between different EGs and then constructs more accurate enhanced entailment graphs based on soft predicate alignments. Extensive experiments show that ATE achieves better and more robust predicate alignment results between different EGs, and the enhanced entailment graphs generated by ATE outperform the original graphs for entailment detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1306,
      "original_url": "https://aclanthology.org/2023.findings-acl.57/",
      "title": "Few-shot Classification with Hypersphere Modeling of Prototypes",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.57.pdf",
      "authors": [
        "Ning Ding",
        "Yulin Chen",
        "Ganqu Cui",
        "Xiaobin Wang",
        "Haitao Zheng",
        "Zhiyuan Liu",
        "Pengjun Xie"
      ],
      "abstract": "Metric-based meta-learning is one of the de facto standards in few-shot learning. It composes of representation learning and metrics calculation designs. Previous works construct class representations in different ways, varying from mean output embedding to covariance and distributions. However, using embeddings in space lacks expressivity and cannot capture class information robustly, while statistical complex modeling poses difficulty to metric designs. In this work, we use tensor fields (“areas”) to model classes from the geometrical perspective for few-shot learning. We present a simple and effective method, dubbed as hypersphere prototypes (HyperProto), where class information is represented by hyperspheres with dynamic sizes with two sets of learnable parameters: the hypersphere’s center and the radius. Extending from points to areas, hyperspheres are much more expressive than embeddings. Moreover, it is more convenient to perform metric-based classification with hypersphere prototypes than statistical modeling, as we only need to calculate the distance from a data point to the surface of the hypersphere. Following this idea, we also develop two variants of prototypes under other measurements. Extensive experiments and analysis on few-shot NLP tasks and comparison with 20+ competitive baselines demonstrate the effectiveness of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1307,
      "original_url": "https://aclanthology.org/2023.findings-acl.58/",
      "title": "Structured Mean-Field Variational Inference for Higher-Order Span-Based Semantic Role Labeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.58.pdf",
      "authors": [
        "Wei Liu",
        "Songlin Yang",
        "Kewei Tu"
      ],
      "abstract": "In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling. To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2H), predicate-to-tail (P2T), and head-to-tail (H2T), where head/tail means the first/last word of the semantic argument span. As such, we use a CRF-based higher-order dependency parser and leverage Mean-Field Variational Inference (MFVI) for higher-order inference. Moreover, since semantic arguments of predicates are often constituents within a constituency parse tree, we can leverage such nice structural property by defining a TreeCRF distribution over all H2T edges, using the idea of partial marginalization to define structural training loss. We further leverage structured MFVI to enhance inference. We experiment on span-based SRL benchmarks, showing the effectiveness of both higher-order and structured modeling and the combination thereof. In addition, we show superior performance of structured MFVI against vanilla MFVI."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1308,
      "original_url": "https://aclanthology.org/2023.findings-acl.59/",
      "title": "AQE: Argument Quadruplet Extraction via a Quad-Tagging Augmented Generative Approach",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.59.pdf",
      "authors": [
        "Jia Guo",
        "Liying Cheng",
        "Wenxuan Zhang",
        "Stanley Kok",
        "Xin Li",
        "Lidong Bing"
      ],
      "abstract": "Argument mining involves multiple sub-tasks that automatically identify argumentative elements, such as claim detection, evidence extraction, stance classification, etc. However, each subtask alone is insufficient for a thorough understanding of the argumentative structure and reasoning process. To learn a complete view of an argument essay and capture the interdependence among argumentative components, we need to know what opinions people hold (i.e., claims), why those opinions are valid (i.e., supporting evidence), which source the evidence comes from (i.e., evidence type), and how those claims react to the debating topic (i.e., stance). In this work, we for the first time propose a challenging argument quadruplet extraction task (AQE), which can provide an all-in-one extraction of four argumentative components, i.e., claims, evidence, evidence types, and stances. To support this task, we construct a large-scale and challenging dataset. However, there is no existing method that can solve the argument quadruplet extraction. To fill this gap, we propose a novel quad-tagging augmented generative approach, which leverages a quadruplet tagging module to augment the training of the generative framework. The experimental results on our dataset demonstrate the empirical superiority of our proposed approach over several strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1309,
      "original_url": "https://aclanthology.org/2023.findings-acl.60/",
      "title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.60.pdf",
      "authors": [
        "Sabrina Chiesurin",
        "Dimitris Dimakopoulos",
        "Marco Antonio Sobrevilla Cabezudo",
        "Arash Eshghi",
        "Ioannis Papaioannou",
        "Verena Rieser",
        "Ioannis Konstas"
      ],
      "abstract": "Large language models are known to produce output which sounds fluent and convincing, but is also often wrong, e.g. “unfaithful” with respect to a rationale as retrieved from a knowledge base. In this paper, we show that task-based systems which exhibit certain advanced linguistic dialog behaviors, such as lexical alignment (repeating what the user said), are in fact preferred and trusted more, whereas other phenomena, such as pronouns and ellipsis are dis-preferred. We use open-domain question answering systems as our test-bed for task based dialog generation and compare several open- and closed-book models. Our results highlight the danger of systems that appear to be trustworthy by parroting user input while providing an unfaithful response."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1310,
      "original_url": "https://aclanthology.org/2023.findings-acl.61/",
      "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.61.pdf",
      "authors": [
        "Sukmin Cho",
        "Soyeong Jeong",
        "Jeong yeon Seo",
        "Jong Park"
      ],
      "abstract": "Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1311,
      "original_url": "https://aclanthology.org/2023.findings-acl.62/",
      "title": "Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.62.pdf",
      "authors": [
        "Kanishka Misra",
        "Cicero Nogueira dos Santos",
        "Siamak Shakeri"
      ],
      "abstract": "Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks over structured knowledge graphs. Specifically, we use soft-prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random-walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require multi-hop reasoning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1312,
      "original_url": "https://aclanthology.org/2023.findings-acl.63/",
      "title": "Multimedia Generative Script Learning for Task Planning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.63.pdf",
      "authors": [
        "Qingyun Wang",
        "Manling Li",
        "Hou Pong Chan",
        "Lifu Huang",
        "Julia Hockenmaier",
        "Girish Chowdhary",
        "Heng Ji"
      ],
      "abstract": "Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provides detailed information that is not covered by text and will guide subsequent steps. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 5,652 tasks and 79,089 multimedia steps. This task is challenging in three aspects: the multimedia challenge of capturing the visual states in images, the induction challenge of performing unseen tasks, and the diversity challenge of covering different information in individual steps. We propose to encode visual state changes through a selective multimedia encoder to address the multimedia challenge, transfer knowledge from previously observed tasks using a retrieval-augmented decoder to overcome the induction challenge, and further present distinct information at each step by optimizing a diversity-oriented contrastive learning objective. We define metrics to evaluate both generation and inductive quality. Experiment results demonstrate that our approach significantly outperforms strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1313,
      "original_url": "https://aclanthology.org/2023.findings-acl.64/",
      "title": "Label Agnostic Pre-training for Zero-shot Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.64.pdf",
      "authors": [
        "Christopher Clarke",
        "Yuzhao Heng",
        "Yiping Kang",
        "Krisztian Flautner",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "abstract": "Conventional approaches to text classification typically assume the existence of a fixed set of predefined labels to which a given text can be classified. However, in real-world applications, there exists an infinite label space for describing a given text. In addition, depending on the aspect (sentiment, topic, etc.) and domain of the text (finance, legal, etc.), the interpretation of the label can vary greatly. This makes the task of text classification, particularly in the zero-shot scenario, extremely challenging. In this paper, we investigate the task of zero-shot text classification with the aim of improving the ability of pre-trained language models (PLMs) to generalize to both seen and unseen data across varying aspects and domains. To solve this we introduce two new simple yet effective pre-training strategies, Implicit and Explicit pre-training. These methods inject aspect-level understanding into the model at train time with the goal of conditioning the model to build task-level understanding. To evaluate this, we construct and release UTCD, a new benchmark dataset for evaluating text classification in zero-shot settings. Experimental results on UTCD show that our approach achieves improved zero-shot generalization on a suite of challenging datasets across an array of zero-shot formalizations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1314,
      "original_url": "https://aclanthology.org/2023.findings-acl.65/",
      "title": "Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.65.pdf",
      "authors": [
        "Chujie Zheng",
        "Pei Ke",
        "Zheng Zhang",
        "Minlie Huang"
      ],
      "abstract": "It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Leo for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Leo outperforms strong baselines of controllable text generation and demonstrate the superiority of Leo’s sample construction strategy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1315,
      "original_url": "https://aclanthology.org/2023.findings-acl.66/",
      "title": "Improving Embedding-based Unsupervised Keyphrase Extraction by Incorporating Structural Information",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.66.pdf",
      "authors": [
        "Mingyang Song",
        "Huafeng Liu",
        "Yi Feng",
        "Liping Jing"
      ],
      "abstract": "Keyphrase extraction aims to extract a set of phrases with the central idea of the source document. In a structured document, there are certain locations (e.g., the title or the first sentence) where a keyphrase is most likely to appear. However, when extracting keyphrases from the document, most existing embedding-based unsupervised keyphrase extraction models ignore the indicative role of the highlights in certain locations, leading to wrong keyphrases extraction. In this paper, we propose a new Highlight-Guided Unsupervised Keyphrase Extraction model (HGUKE) to address the above issue. Specifically, HGUKE first models the phrase-document relevance via the highlights of the documents. Next, HGUKE calculates the cross-phrase relevance between all candidate phrases. Finally, HGUKE aggregates the above two relevance as the importance score of each candidate phrase to rank and extract keyphrases. The experimental results on three benchmarks demonstrate that HGUKE outperforms the state-of-the-art unsupervised keyphrase extraction baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1316,
      "original_url": "https://aclanthology.org/2023.findings-acl.67/",
      "title": "Towards Reasoning in Large Language Models: A Survey",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.67.pdf",
      "authors": [
        "Jie Huang",
        "Kevin Chen-Chuan Chang"
      ],
      "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1317,
      "original_url": "https://aclanthology.org/2023.findings-acl.68/",
      "title": "Transitioning from benchmarks to a real-world case of information-seeking in Scientific Publications",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.68.pdf",
      "authors": [
        "Chyrine Tahri",
        "Aurore Bochnakian",
        "Patrick Haouat",
        "Xavier Tannier"
      ],
      "abstract": "Although recent years have been marked by incredible advances in the whole development process of NLP systems, there are still blind spots in characterizing what is still hampering real-world adoption of models in knowledge-intensive settings. In this paper, we illustrate through a real-world zero-shot text search case for information seeking in scientific papers, the masked phenomena that the current process of measuring performance might not reflect, even when benchmarks are, in appearance, faithfully representative of the task at hand. In addition to experimenting with TREC-COVID and NFCorpus, we provide an industrial, expert-carried/annotated, case of studying vitamin B’s impact on health. We thus discuss the misalignment between solely focusing on single-metric performance as a criterion for model choice and relevancy as a subjective measure for meeting a user’s need."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1318,
      "original_url": "https://aclanthology.org/2023.findings-acl.69/",
      "title": "CLIPText: A New Paradigm for Zero-shot Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.69.pdf",
      "authors": [
        "Libo Qin",
        "Weiyun Wang",
        "Qiguang Chen",
        "Wanxiang Che"
      ],
      "abstract": "While CLIP models are useful for zero-shot vision-and-language (VL) tasks or computer vision tasks, little attention has been paid to the application of CLIP for language tasks. Intuitively, CLIP model have a rich representation pre-trained with natural language supervision, in which we argue that it is useful for language tasks. Hence, this work bridge this gap by investigating a CLIP model for zero-shot text classification. Specifically, we introduce CLIPText, a novel paradigm for zero-shot text classification, which reformulates zero-shot text classification into a text-image matching problem that CLIP can be applied to. In addition, we further incorporate prompt into CLIPText (Prompt-CLIPText) to better derive knowledge from CLIP. Experimental results on seven publicly available zero-shot text classification datasets show that both CLIPText and Prompt-CLIPText attain promising performance. Besides, extensive analysis further verifies that knowledge from CLIP can benefit zero-shot text classification task. We hope this work can attract more breakthroughs on applying VL pre-trained models for language tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1319,
      "original_url": "https://aclanthology.org/2023.findings-acl.70/",
      "title": "Rethinking Dictionaries and Glyphs for Chinese Language Pre-training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.70.pdf",
      "authors": [
        "Yuxuan Wang",
        "Jack Wang",
        "Dongyan Zhao",
        "Zilong Zheng"
      ],
      "abstract": "We introduce CDBert, a new learning paradigm that enhances the semantics understanding ability of the Chinese PLMs with dictionary knowledge and structure of Chinese characters. We name the two core modules of CDBert as Shuowen and Jiezi, where Shuowen refers to the process of retrieving the most appropriate meaning from Chinese dictionaries and Jiezi refers to the process of enhancing characters’ glyph representations with structure understanding. To facilitate dictionary understanding, we propose three pre-training tasks, i.e.„ Masked Entry Modeling, Contrastive Learning for Synonym and Antonym, and Example Learning. We evaluate our method on both modern Chinese understanding benchmark CLUE and ancient Chinese benchmark CCLUE. Moreover, we propose a new polysemy discrimination task PolyMRC based on the collected dictionary of ancient Chinese. Our paradigm demonstrates consistent improvements on previous Chinese PLMs across all tasks. Moreover, our approach yields significant boosting on few-shot setting of ancient Chinese understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1320,
      "original_url": "https://aclanthology.org/2023.findings-acl.71/",
      "title": "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.71.pdf",
      "authors": [
        "Hongjin Su",
        "Weijia Shi",
        "Jungo Kasai",
        "Yizhong Wang",
        "Yushi Hu",
        "Mari Ostendorf",
        "Wen-tau Yih",
        "Noah A. Smith",
        "Luke Zettlemoyer",
        "Tao Yu"
      ],
      "abstract": "We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https://instructor-embedding.github.io."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1321,
      "original_url": "https://aclanthology.org/2023.findings-acl.72/",
      "title": "Towards Speech Dialogue Translation Mediating Speakers of Different Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.72.pdf",
      "authors": [
        "Shuichiro Shimizu",
        "Chenhui Chu",
        "Sheng Li",
        "Sadao Kurohashi"
      ],
      "abstract": "We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context. We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1322,
      "original_url": "https://aclanthology.org/2023.findings-acl.73/",
      "title": "Adaptation Approaches for Nearest Neighbor Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.73.pdf",
      "authors": [
        "Rishabh Bhardwaj",
        "George Polovets",
        "Monica Sunkara"
      ],
      "abstract": "Semi-parametric Nearest Neighbor Language Models (kNN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting kNN-LMs — 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot (kNN-LM) baselines that construct datastores from the adaptation data. On average, we see perplexity improvements of 17.1% and 16% for these respective baselines, across domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1323,
      "original_url": "https://aclanthology.org/2023.findings-acl.74/",
      "title": "Language Models for German Text Simplification: Overcoming Parallel Data Scarcity through Style-specific Pre-training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.74.pdf",
      "authors": [
        "Miriam Anschütz",
        "Joshua Oehms",
        "Thomas Wimmer",
        "Bartłomiej Jezierski",
        "Georg Groh"
      ],
      "abstract": "Automatic text simplification systems help to reduce textual information barriers on the internet. However, for languages other than English, only few parallel data to train these systems exists. We propose a two-step approach to overcome this data scarcity issue. First, we fine-tuned language models on a corpus of German Easy Language, a specific style of German. Then, we used these models as decoders in a sequence-to-sequence simplification task. We show that the language models adapt to the style characteristics of Easy Language and output more accessible texts. Moreover, with the style-specific pre-training, we reduced the number of trainable parameters in text simplification models. Hence, less parallel data is sufficient for training. Our results indicate that pre-training on unaligned data can reduce the required parallel data while improving the performance on downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1324,
      "original_url": "https://aclanthology.org/2023.findings-acl.75/",
      "title": "Client-Customized Adaptation for Parameter-Efficient Federated Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.75.pdf",
      "authors": [
        "Yeachan Kim",
        "Junho Kim",
        "Wing-Lam Mok",
        "Jun-Hyung Park",
        "SangKeun Lee"
      ],
      "abstract": "Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be the use of parameter-efficient fine-tuning (PEFT) in the context of FL. However, we have observed that typical PEFT tends to severely suffer from heterogeneity among clients in FL scenarios, resulting in unstable and slow convergence. In this paper, we propose Client-Customized Adaptation (C2A), a novel hypernetwork-based FL framework that generates client-specific adapters by conditioning the client information. With the effectiveness of the hypernetworks in generating customized weights through learning to adopt the different characteristics of inputs, C2A can maximize the utility of shared model parameters while minimizing the divergence caused by client heterogeneity. To verify the efficacy of C2A, we perform extensive evaluations on FL scenarios involving heterogeneity in label and language distributions. Comprehensive evaluation results clearly support the superiority of C2A in terms of both efficiency and effectiveness in FL scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1325,
      "original_url": "https://aclanthology.org/2023.findings-acl.76/",
      "title": "FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.76.pdf",
      "authors": [
        "Changlong Yu",
        "Weiqi Wang",
        "Xin Liu",
        "Jiaxin Bai",
        "Yangqiu Song",
        "Zheng Li",
        "Yifan Gao",
        "Tianyu Cao",
        "Bing Yin"
      ],
      "abstract": "Understanding users’ intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework, to reveal the structure of humans’ minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models (LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and study demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1326,
      "original_url": "https://aclanthology.org/2023.findings-acl.77/",
      "title": "I am PsyAM: Modeling Happiness with Cognitive Appraisal Dimensions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.77.pdf",
      "authors": [
        "Xuan Liu",
        "Kokil Jaidka"
      ],
      "abstract": "This paper proposes and evaluates PsyAM (https://anonymous.4open.science/r/BERT-PsyAM-10B9), a framework that incorporates adaptor modules in a sequential multi-task learning setup to generate high-dimensional feature representations of hedonic well-being (momentary happiness) in terms of its psychological underpinnings. PsyAM models emotion in text through its cognitive antecedents through auxiliary models that achieve multi-task learning through novel feature fusion methods. We show that BERT-PsyAM has cross-task validity and cross-domain generalizability through experiments with emotion-related tasks – on new emotion tasks and new datasets, as well as against traditional methods and BERT baselines. We further probe the robustness of BERT-PsyAM through feature ablation studies, as well as discuss the qualitative inferences we can draw regarding the effectiveness of the framework for representing emotional states. We close with a discussion of a future agenda of psychology-inspired neural network architectures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1327,
      "original_url": "https://aclanthology.org/2023.findings-acl.78/",
      "title": "Value type: the bridge to a better DST model",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.78.pdf",
      "authors": [
        "Gao Qixiang",
        "Mingyang Sun",
        "Yutao Mou",
        "Chen Zeng",
        "Weiran Xu"
      ],
      "abstract": "Value type of the slots can provide lots of useful information for DST tasks. However, it has been ignored in most previous works. In this paper, we propose a new framework for DST task based on these value types. Firstly, we extract the type of token from each turn. Specifically, we divide the slots in the dataset into 9 categories according to the type of slot value, and then train a Ner model to extract the corresponding type-entity from each turn of conversation according to the token. Secondly, we improve the attention mode which is integrated into value type information between the slot and the conversation history to help each slot pay more attention to the turns that contain the same value type. Meanwhile, we introduce a sampling strategy to integrate these types into the attention formula, which decrease the error of Ner model. Finally, we conduct a comprehensive experiment on two multi-domain task-oriented conversation datasets, MultiWOZ 2.1 and MultiWOZ 2.4. The ablation experimental results show that our method is effective on both datasets, which verify the necessity of considering the type of slot value."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1328,
      "original_url": "https://aclanthology.org/2023.findings-acl.79/",
      "title": "Hypothetical Training for Robust Machine Reading Comprehension of Tabular Context",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.79.pdf",
      "authors": [
        "Moxin Li",
        "Wenjie Wang",
        "Fuli Feng",
        "Hanwang Zhang",
        "Qifan Wang",
        "Tat-Seng Chua"
      ],
      "abstract": "Machine Reading Comprehension (MRC) models easily learn spurious correlations from complex contexts such as tabular data. Counterfactual training—using the factual and counterfactual data by augmentation—has become a promising solution. However, it is costly to construct faithful counterfactual examples because it is tricky to maintain the consistency and dependency of the tabular data. In this paper, we take a more efficient fashion to ask hypothetical questions like “in which year would the net profit be larger if the revenue in 2019 were $38,298?”, whose effects on the answers are equivalent to those expensive counterfactual tables. We propose a hypothetical training framework that uses paired examples with different hypothetical questions to supervise the direction of model gradient towards the counterfactual answer change. The superior generalization results on tabular MRC datasets, including a newly constructed stress test and MultiHiertt, validate our effectiveness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1329,
      "original_url": "https://aclanthology.org/2023.findings-acl.80/",
      "title": "BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.80.pdf",
      "authors": [
        "Mohsinul Kabir",
        "Obayed Bin Mahfuz",
        "Syed Rifat Raiyan",
        "Hasan Mahmud",
        "Md Kamrul Hasan"
      ],
      "abstract": "The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining sentiment unigrams, which may provide insight into common classification errors in under-resourced languages like Bangla. Our codes and data are publicly available at https://github.com/mohsinulkabir14/BanglaBook."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1330,
      "original_url": "https://aclanthology.org/2023.findings-acl.81/",
      "title": "Risks and NLP Design: A Case Study on Procedural Document QA",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.81.pdf",
      "authors": [
        "Nikita Haduong",
        "Alice Gao",
        "Noah A. Smith"
      ],
      "abstract": "As NLP systems are increasingly deployed at scale, concerns about their potential negative impacts have attracted the attention of the research community, yet discussions of risk have mostly been at an abstract level and focused on generic AI or NLP applications. We argue that clearer assessments of risks and harms to users—and concrete strategies to mitigate them—will be possible when we specialize the analysis to more concrete applications and their plausible users. As an illustration, this paper is grounded in cooking recipe procedural document question answering (ProcDocQA), where there are well-defined risks to users such as injuries or allergic reactions. Our case study shows that an existing language model, applied in “zero-shot” mode, quantitatively answers real-world questions about recipes as well or better than the humans who have answered the questions on the web. Using a novel questionnaire informed by theoretical work on AI risk, we conduct a risk-oriented error analysis that could then inform the design of a future system to be deployed with lower risk of harm and better performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1331,
      "original_url": "https://aclanthology.org/2023.findings-acl.82/",
      "title": "The Diminishing Returns of Masked Language Models to Science",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.82.pdf",
      "authors": [
        "Zhi Hong",
        "Aswathy Ajith",
        "James Pauloski",
        "Eamon Duede",
        "Kyle Chard",
        "Ian Foster"
      ],
      "abstract": "Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770Mparameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model size, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if any, in scientific information extraction tasks. We offer possible explanations for this surprising result."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1332,
      "original_url": "https://aclanthology.org/2023.findings-acl.83/",
      "title": "Causal Matching with Text Embeddings: A Case Study in Estimating the Causal Effects of Peer Review Policies",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.83.pdf",
      "authors": [
        "Raymond Zhang",
        "Neha Nayak Kennard",
        "Daniel Smith",
        "Daniel McFarland",
        "Andrew McCallum",
        "Katherine Keith"
      ],
      "abstract": "A promising approach to estimate the causal effects of peer review policies is to analyze data from publication venues that shift policies from single-blind to double-blind from one year to the next. However, in these settings the content of the manuscript is a confounding variable—each year has a different distribution of scientific content which may naturally affect the distribution of reviewer scores. To address this textual confounding, we extend variable ratio nearest neighbor matching to incorporate text embeddings. We compare this matching method to a widely-used causal method of stratified propensity score matching and a baseline of randomly selected matches. For our case study of the ICLR conference shifting from single- to double-blind review from 2017 to 2018, we find human judges prefer manuscript matches from our method in 70% of cases. While the unadjusted estimate of the average causal effect of reviewers’ scores is -0.25, our method shifts the estimate to -0.17, a slightly smaller difference between the outcomes of single- and double-blind policies. We hope this case study enables exploration of additional text-based causal estimation methods and domains in the future."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1333,
      "original_url": "https://aclanthology.org/2023.findings-acl.84/",
      "title": "Learning to Generalize for Cross-domain QA",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.84.pdf",
      "authors": [
        "Yingjie Niu",
        "Linyi Yang",
        "Ruihai Dong",
        "Yue Zhang"
      ],
      "abstract": "There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing with fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1334,
      "original_url": "https://aclanthology.org/2023.findings-acl.85/",
      "title": "Enhanced Chart Understanding via Visual Language Pre-training on Plot Table Pairs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.85.pdf",
      "authors": [
        "Mingyang Zhou",
        "Yi Fung",
        "Long Chen",
        "Christopher Thomas",
        "Heng Ji",
        "Shih-Fu Chang"
      ],
      "abstract": "Building cross-model intelligence that can understand charts and communicate the salient information hidden behind them is an appealing challenge in the vision and language (V+L) community. The capability to uncover the underlined table data of chart figures is a critical key to automatic chart understanding. We introduce ChartT5, a V+L model that learns how to interpret table information from chart images via cross-modal pre-training on plot table pairs. Specifically, we propose two novel pre-training objectives: Masked Header Prediction (MHP) and Masked Value Prediction (MVP) to facilitate the model with different skills to interpret the table information. We have conducted extensive experiments on chart question answering and chart summarization to verify the effectiveness of the proposed pre-training strategies. In particular, on the ChartQA benchmark, our ChartT5 outperforms the state-of-the-art non-pretraining methods by over 8% performance gains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1335,
      "original_url": "https://aclanthology.org/2023.findings-acl.86/",
      "title": "Importance of Synthesizing High-quality Data for Text-to-SQL Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.86.pdf",
      "authors": [
        "Yiqun Hu",
        "Yiyun Zhao",
        "Jiarong Jiang",
        "Wuwei Lan",
        "Henghui Zhu",
        "Anuj Chauhan",
        "Alexander Hanbo Li",
        "Lin Pan",
        "Jun Wang",
        "Chung-Wei Hang",
        "Sheng Zhang",
        "Jiang Guo",
        "Mingwen Dong",
        "Joseph Lilien",
        "Patrick Ng",
        "Zhiguo Wang",
        "Vittorio Castelli",
        "Bing Xiang"
      ],
      "abstract": "There has been increasing interest in synthesizing data to improve downstream text-to-SQL tasks. In this paper, we examined the existing synthesized datasets and discovered that state-of-the-art text-to-SQL algorithms did not further improve on popular benchmarks when trained with augmented synthetic data. We observed three shortcomings: illogical synthetic SQL queries from independent column sampling, arbitrary table joins, and language gaps between the synthesized SQL and natural language question (NLQ) pair. To address these issues, we propose a novel synthesis framework that imposes strong typing constraints, incorporates key relationships from schema, and conducts schema-distance-weighted column sampling. We also adopt an intermediate representation (IR) for the SQL-to-text task to further improve the quality of the generated NLQ. When existing powerful text-to-SQL parsers are pretrained on our high-quality synthesized data, these models have significant accuracy boosts and achieve new state-of-the-art performance on Spider. We also demonstrate the effectiveness of our techniques with ablation studies"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1336,
      "original_url": "https://aclanthology.org/2023.findings-acl.87/",
      "title": "Exploring Schema Generalizability of Text-to-SQL",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.87.pdf",
      "authors": [
        "Jieyu Li",
        "Lu Chen",
        "Ruisheng Cao",
        "Su Zhu",
        "Hongshen Xu",
        "Zhi Chen",
        "Hanchong Zhang",
        "Kai Yu"
      ],
      "abstract": "Exploring the generalizability of a text-to-SQL parser is essential for a system to automatically adapt the real-world databases. Previous investigation works mostly focus on lexical diversity, including the influence of the synonym and perturbations in both natural language questions and databases. However, the structural variability of database schema (DS), as a widely seen real-world scenario, is yet underexplored. Specifically, confronted with the same input question, the target SQL may be represented in different ways when the DS comes to a different structure. In this work, we provide in-depth discussions about the schema generalizability challenge of text-to-SQL tasks. We observe that current datasets are too templated to study schema generalization. To collect suitable test data, we propose a framework to generate novel text-to-SQL data via automatic and synchronous (DS, SQL) pair altering. When evaluating state-of-the-art text-to-SQL models on the synthetic samples, performance is significantly degraded, which demonstrates the limitation of current research regarding schema generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1337,
      "original_url": "https://aclanthology.org/2023.findings-acl.88/",
      "title": "Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.88.pdf",
      "authors": [
        "Shuang Li",
        "Xuming Hu",
        "Aiwei Liu",
        "Yawen Yang",
        "Fukun Ma",
        "Philip S. Yu",
        "Lijie Wen"
      ],
      "abstract": "Cross-lingual natural language inference is a fundamental problem in cross-lingual language understanding. Many recent works have used prompt learning to address the lack of annotated parallel corpora in XNLI.However, these methods adopt discrete prompting by simply translating the templates to the target language and need external expert knowledge to design the templates. Besides, discrete prompts of human-designed template words are not trainable vectors and can not be migrated to target languages in the inference stage flexibly. In this paper, we propose a novel Soft prompt learning framework with the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs cloze-style question with soft prompts for the input sample. Then we leverage bilingual dictionaries to generate an augmented multilingual question for the original question. SoftMV adopts a multilingual verbalizer to align the representations of original and augmented multilingual questions into a unified semantic space with consistency regularization. Experimental results on XNLI demonstrate that SoftMV can achieve state-of-the-art performance and significantly outperform the previous methods under the few-shot and full-shot cross-lingual transfer settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1338,
      "original_url": "https://aclanthology.org/2023.findings-acl.89/",
      "title": "A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.89.pdf",
      "authors": [
        "Limao Xiong",
        "Jie Zhou",
        "Qunxi Zhu",
        "Xiao Wang",
        "Yuanbin Wu",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang",
        "Jin Ma",
        "Ying Shan"
      ],
      "abstract": "Existing models for named entity recognition (NER) are mainly based on large-scale labeled datasets, which always obtain using crowdsourcing. However, it is hard to obtain a unified and correct label via majority voting from multiple annotators for NER due to the large labeling space and complexity of this task. To address this problem, we aim to utilize the original multi-annotator labels directly. Particularly, we propose a CONfidence-based partial Label Learning (CONLL) method to integrate the prior confidence (given by annotators) and posterior confidences (learned by models) for crowd-annotated NER. This model learns a token- and content-dependent confidence via an Expectation–Maximization (EM) algorithm by minimizing empirical risk. The true posterior estimator and confidence estimator perform iteratively to update the true posterior and confidence respectively. We conduct extensive experimental results on both real-world and synthetic datasets, which show that our model can improve performance effectively compared with strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1339,
      "original_url": "https://aclanthology.org/2023.findings-acl.90/",
      "title": "Towards Zero-Shot Persona Dialogue Generation with In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.90.pdf",
      "authors": [
        "Xinchao Xu",
        "Zeyang Lei",
        "Wenquan Wu",
        "Zheng-Yu Niu",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "Much work has been done to improve persona consistency by finetuning a pretrained dialogue model on high-quality human-annoated persona datasets. However, these methods still face the challenges of high cost and poor scalability. To this end, we propose a simple-yet-effective approach to significantly improve zero-shot persona consistency via in-context learning. Specifically, we first pre-train a persona-augmented dialogue generation model and then utilize in-context prompting mechanism to realize zero-shot persona customization. Experimental results demonstrate that our method can dramatically improve persona consistency without compromising coherence and informativeness in zero-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1340,
      "original_url": "https://aclanthology.org/2023.findings-acl.91/",
      "title": "Grammar-based Decoding for Improved Compositional Generalization in Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.91.pdf",
      "authors": [
        "Jing Zheng",
        "Jyh-Herng Chow",
        "Zhongnan Shen",
        "Peng Xu"
      ],
      "abstract": "Sequence-to-sequence (seq2seq) models have achieved great success in semantic parsing tasks, but they tend to struggle on out-of-distribution (OOD) data. Despite recent progress, robust semantic parsing on large-scale tasks with combined challenges from both compositional generalization and natural language variations remains an unsolved problem. To promote research in this area, this work presents CUDON, a large-scale dialogue dataset in Chinese language, particularly designed for evaluating compositional generalization of semantic parsing. The dataset contains about ten thousand multi-turn complex queries, and provides multiple splits with different degrees of train-test distribution divergence. We have investigated improving compositional generalization with grammar-based decodering on this dataset. With specially designed grammars leveraging program schema, we are able to substantially improve accuracy of seq2seq semantic parsers on OOD splits: A LSTM-based parser using a Context-free Grammar (CFG) achieves over 25% higher accuracy than a standard seq2seq baseline; a parser using Tree-Substitution Grammar (TSG) improves parsing speed five to seven times over the CFG parser with only a small accuracy loss. The grammar-based LSTM parsers also outperforms BART- and T5-based seq2seq parsers on the OOD splits, despite having less than one tenth of parameters and no pretraining. We also verified our approach on the SMCalflow-CS dataset, particularly, on the zero-shot learning task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1341,
      "original_url": "https://aclanthology.org/2023.findings-acl.92/",
      "title": "Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.92.pdf",
      "authors": [
        "Chenyang Lyu",
        "Linyi Yang",
        "Yue Zhang",
        "Yvette Graham",
        "Jennifer Foster"
      ],
      "abstract": "User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architectureor do not make full use of user/product associations. The contribution of this work is twofold: i) a method to explicitly employ historical reviews belonging to the same user/product in initializing representations, and ii) efficient incorporation of textual associations between users and products via a user-product cross-context module. Experiments on the IMDb, Yelp-2013 and Yelp-2014 English benchmarks with BERT, SpanBERT and Longformer pretrained language models show that our approach substantially outperforms previous state-of-the-art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1342,
      "original_url": "https://aclanthology.org/2023.findings-acl.93/",
      "title": "Efficient Out-of-Domain Detection for Sequence to Sequence Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.93.pdf",
      "authors": [
        "Artem Vazhentsev",
        "Akim Tsvigun",
        "Roman Vashurin",
        "Sergey Petrakov",
        "Daniil Vasilev",
        "Maxim Panov",
        "Alexander Panchenko",
        "Artem Shelmanov"
      ],
      "abstract": "Sequence-to-sequence (seq2seq) models based on the Transformer architecture have become a ubiquitous tool applicable not only to classical text generation tasks such as machine translation and summarization but also to any other task where an answer can be represented in a form of a finite text fragment (e.g., question answering). However, when deploying a model in practice, we need not only high performance but also an ability to determine cases where the model is not applicable. Uncertainty estimation (UE) techniques provide a tool for identifying out-of-domain (OOD) input where the model is susceptible to errors. State-of-the-art UE methods for seq2seq models rely on computationally heavyweight and impractical deep ensembles. In this work, we perform an empirical investigation of various novel UE methods for large pre-trained seq2seq models T5 and BART on three tasks: machine translation, text summarization, and question answering. We apply computationally lightweight density-based UE methods to seq2seq models and show that they often outperform heavyweight deep ensembles on the task of OOD detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1343,
      "original_url": "https://aclanthology.org/2023.findings-acl.94/",
      "title": "Emotion Cause Extraction on Social Media without Human Annotation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.94.pdf",
      "authors": [
        "Debin Xiao",
        "Rui Xia",
        "Jianfei Yu"
      ],
      "abstract": "In social media, there is a vast amount of information pertaining to people’s emotions and the corresponding causes. The emotion cause extraction (ECE) from social media data is an important research area that has not been thoroughly explored due to the lack of fine-grained annotations. Early studies referred to either unsupervised rule-based methods or supervised machine learning methods using a number of manually annotated data in specific domains. However, the former suffers from limitations in extraction performance, while the latter is constrained by the availability of fine-grained annotations and struggles to generalize to diverse domains. To address these issues, this paper proposes a new ECE framework on Chinese social media that achieves high extraction performance and generalizability without relying on human annotation. Specifically, we design a more dedicated rule-based system based on constituency parsing tree to discover causal patterns in social media. This system enables us to acquire large amounts of fine-grained annotated data. Next, we train a neural model on the rule-annotated dataset with a specific training strategy to further improve the model’s generalizability. Extensive experiments demonstrate the superiority of our approach over other methods in unsupervised and weakly-supervised settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1344,
      "original_url": "https://aclanthology.org/2023.findings-acl.95/",
      "title": "Pseudo Outlier Exposure for Out-of-Distribution Detection using Pretrained Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.95.pdf",
      "authors": [
        "Jaeyoung Kim",
        "Kyuheon Jung",
        "Dongbin Na",
        "Sion Jang",
        "Eunbin Park",
        "Sungchul Choi"
      ],
      "abstract": "For real-world language applications, detecting an out-of-distribution (OOD) sample is helpful to alert users or reject such unreliable samples. However, modern over-parameterized language models often produce overconfident predictions for both in-distribution (ID) and OOD samples. In particular, language models suffer from OOD samples with a similar semantic representation to ID samples since these OOD samples lie near the ID manifold.A rejection network can be trained with ID and diverse outlier samples to detect test OOD samples, but explicitly collecting auxiliary OOD datasets brings an additional burden for data collection. In this paper, we propose a simple but effective method called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD dataset by sequentially masking tokens related to ID classes. The surrogate OOD sample introduced by POE shows a similar representation to ID data, which is most effective in training a rejection network. Our method does not require any external OOD data and can be easily implemented within off-the-shelf Transformers.A comprehensive comparison with state-of-the-art algorithms demonstrates POE’s competitiveness on several text classification benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1345,
      "original_url": "https://aclanthology.org/2023.findings-acl.96/",
      "title": "Adversarial Multi-task Learning for End-to-end Metaphor Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.96.pdf",
      "authors": [
        "Shenglong Zhang",
        "Ying Liu"
      ],
      "abstract": "Metaphor detection (MD) suffers from limited training data. In this paper, we started with a linguistic rule called Metaphor Identification Procedure and then proposed a novel multi-task learning framework to transfer knowledge in basic sense discrimination (BSD) to MD. BSD is constructed from word sense disambiguation (WSD), which has copious amounts of data. We leverage adversarial training to align the data distributions of MD and BSD in the same feature space, so task-invariant representations can be learned. To capture fine-grained alignment patterns, we utilize the multi-mode structures of MD and BSD. Our method is totally end-to-end and can mitigate the data scarcity problem in MD. Competitive results are reported on four public datasets. Our code and datasets are available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1346,
      "original_url": "https://aclanthology.org/2023.findings-acl.97/",
      "title": "SERENGETI: Massively Multilingual Language Models for Africa",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.97.pdf",
      "authors": [
        "Ife Adebara",
        "AbdelRahim Elmadany",
        "Muhammad Abdul-Mageed",
        "Alcides Alcoba Inciarte"
      ],
      "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a set of massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research. Anonymous link"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1347,
      "original_url": "https://aclanthology.org/2023.findings-acl.98/",
      "title": "Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.98.pdf",
      "authors": [
        "Heejin Do",
        "Yunsu Kim",
        "Gary Geunbae Lee"
      ],
      "abstract": "Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining challenge of AES. In this paper, we propose a robust model: prompt- and trait relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay representation by essay-prompt attention and utilizing the topic-coherence feature extracted by the topic-modeling mechanism without access to labeled data; therefore, our model considers the prompt adherence of an essay, even in a cross-prompt setting. To facilitate multi-trait scoring, we design trait-similarity loss that encapsulates the correlations of traits. Experiments prove the efficacy of our model, showing state-of-the-art results for all prompts and traits. Significant improvements in low-resource-prompt and inferior traits further indicate our model’s strength."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1348,
      "original_url": "https://aclanthology.org/2023.findings-acl.99/",
      "title": "AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.99.pdf",
      "authors": [
        "Chujie Zheng",
        "Sahand Sabour",
        "Jiaxin Wen",
        "Zheng Zhang",
        "Minlie Huang"
      ],
      "abstract": "Crowdsourced dialogue corpora are usually limited in scale and topic coverage due to the expensive cost of data curation. This would hinder the generalization of downstream dialogue models to open-domain topics. In this work, we leverage large language models for dialogue augmentation in the task of emotional support conversation (ESC). By treating dialogue augmentation as a dialogue completion task, we prompt a fine-tuned language model to complete full dialogues from available dialogue posts of various topics, which are then postprocessed based on heuristics. Applying this approach, we construct AugESC, an augmented dataset for the ESC task, which largely extends the scale and topic coverage of the crowdsourced ESConv corpus. Through comprehensive human evaluation, we demonstrate that our approach is superior to strong baselines of dialogue augmentation and that AugESC has comparable dialogue quality to the crowdsourced corpus. We also conduct human interactive evaluation and prove that post-training on AugESC improves downstream dialogue models’ generalization ability to open-domain topics. These results suggest the utility of AugESC and highlight the potential of large language models in improving data-scarce dialogue generation tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1349,
      "original_url": "https://aclanthology.org/2023.findings-acl.100/",
      "title": "2*n is better than n2: Decomposing Event Coreference Resolution into Two Tractable Problems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.100.pdf",
      "authors": [
        "Shafiuddin Rehan Ahmed",
        "Abhijnan Nath",
        "James H. Martin",
        "Nikhil Krishnaswamy"
      ],
      "abstract": "Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in which they appear. Existing methods for training coreference systems sample from a largely skewed distribution, making it difficult for the algorithm to learn coreference beyond surface matching. Additionally, these methods are intractable because of the quadratic operations needed. To address these challenges, we break the problem of ECR into two parts: a) a heuristic to efficiently filter out a large number of non-coreferent pairs, and b) a training approach on a balanced set of coreferent and non-coreferent mention pairs. By following this approach, we show that we get comparable results to the state of the art on two popular ECR datasets while significantly reducing compute requirements. We also analyze the mention pairs that are “hard” to accurately classify as coreferent or non-coreferentcode repo: \\mathtt{github.com/ahmeshaf/lemma\\_ce\\_coref}."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1350,
      "original_url": "https://aclanthology.org/2023.findings-acl.101/",
      "title": "SCCS: Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.101.pdf",
      "authors": [
        "Jielin Qiu",
        "Jiacheng Zhu",
        "Mengdi Xu",
        "Franck Dernoncourt",
        "Trung Bui",
        "Zhaowen Wang",
        "Bo Li",
        "Ding Zhao",
        "Hailin Jin"
      ],
      "abstract": "Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However, existing methods extract features from the whole video and article and use fusion methods to select the representative one, thus usually ignoring the critical structure and varying semantics with video/document. In this work, we propose a Semantics-Consistent Cross-domain Summarization (SCCS) model based on optimal transport alignment with visual and textual segmentation. Our method first decomposes both videos and articles into segments in order to capture the structural semantics, and then follows a cross-domain alignment objective with optimal transport distance, which leverages multimodal interaction to match and select the visual and textual summary. We evaluated our method on three MSMO datasets, and achieved performance improvement by 8% & 6% of textual and 6.6% &5.7% of video summarization, respectively, which demonstrated the effectiveness of our method in producing high-quality multimodal summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1351,
      "original_url": "https://aclanthology.org/2023.findings-acl.102/",
      "title": "General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.102.pdf",
      "authors": [
        "Rui Meng",
        "Tong Wang",
        "Xingdi Yuan",
        "Yingbo Zhou",
        "Daqing He"
      ],
      "abstract": "Training keyphrase generation (KPG) models require a large amount of annotated data, which can be prohibitively expensive and often limited to specific domains. In this study, we first demonstrate that large distribution shifts among different domains severely hinder the transferability of KPG models. We then propose a three-stage pipeline, which gradually guides KPG models’ learning focus from general syntactical features to domain-related semantics, in a data-efficient manner. With domain-general phrase pre-training, we pre-train Sequence-to-Sequence models with generic phrase annotations that are widely available on the web, which enables the models to generate phrases in a wide range of domains. The resulting model is then applied in the Transfer Labeling stage to produce domain-specific pseudo keyphrases, which help adapt models to a new domain. Finally, we fine-tune the model with limited data with true labels to fully adapt it to the target domain. Our experiment results show that the proposed process can produce good quality keyphrases in new domains and achieve consistent improvements after adaptation with limited in-domain annotated data. All code and datasets are available at https://github.com/memray/OpenNMT-kpg-release."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1352,
      "original_url": "https://aclanthology.org/2023.findings-acl.103/",
      "title": "E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.103.pdf",
      "authors": [
        "Zhen Zhang",
        "Mengting Hu",
        "Shiwan Zhao",
        "Minlie Huang",
        "Haotian Wang",
        "Lemao Liu",
        "Zhirui Zhang",
        "Zhe Liu",
        "Bingzhe Wu"
      ],
      "abstract": "Most named entity recognition (NER) systems focus on improving model performance, ignoring the need to quantify model uncertainty, which is critical to the reliability of NER systems in open environments. Evidential deep learning (EDL) has recently been proposed as a promising solution to explicitly model predictive uncertainty for classification tasks. However, directly applying EDL to NER applications faces two challenges, i.e., the problems of sparse entities and OOV/OOD entities in NER tasks. To address these challenges, we propose a trustworthy NER framework named E-NER by introducing two uncertainty-guided loss terms to the conventional EDL, along with a series of uncertainty-guided training strategies. Experiments show that E-NER can be applied to multiple NER paradigms to obtain accurate uncertainty estimation. Furthermore, compared to state-of-the-art baselines, the proposed method achieves a better OOV/OOD detection performance and better generalization ability on OOV entities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1353,
      "original_url": "https://aclanthology.org/2023.findings-acl.104/",
      "title": "LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.104.pdf",
      "authors": [
        "Rita Ramos",
        "Bruno Martins",
        "Desmond Elliott"
      ],
      "abstract": "Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCap first retrieves the captions of similar images using a multilingual CLIP encoder. These captions are then combined into a prompt for an XGLM decoder, in order to generate captions in the desired language. In other words, the generation model does not directly process the image, instead it processes retrieved captions. Experiments on the XM3600 dataset of geographically diverse images show that our model is competitive with fully-supervised multilingual captioning models, without requiring any supervised training on any captioning data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1354,
      "original_url": "https://aclanthology.org/2023.findings-acl.105/",
      "title": "Boosting Text Augmentation via Hybrid Instance Filtering Framework",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.105.pdf",
      "authors": [
        "Heng Yang",
        "Ke Li"
      ],
      "abstract": "Text augmentation is an effective technique for addressing the problem of insufficient data in natural language processing. However, existing text augmentation methods tend to focus on few-shot scenarios and usually perform poorly on large public datasets. Our research indicates that existing augmentation methods often generate instances with shifted feature spaces, which leads to a drop in performance on the augmented data (for example, EDA generally loses approximately 2% in aspect-based sentiment classification). To address this problem, we propose a hybrid instance-filtering framework (BoostAug) based on pre-trained language models that can maintain a similar feature space with natural datasets. BoostAug is transferable to existing text augmentation methods (such as synonym substitution and back translation) and significantly improves the augmentation performance by 2-3% in classification accuracy. Our experimental results on three classification tasks and nine public datasets show that BoostAug addresses the performance drop problem and outperforms state-of-the-art text augmentation methods. Additionally, we release the code to help improve existing augmentation methods on large datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1355,
      "original_url": "https://aclanthology.org/2023.findings-acl.106/",
      "title": "Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.106.pdf",
      "authors": [
        "Thong Nguyen",
        "Xiaobao Wu",
        "Xinshuai Dong",
        "Cong-Duy Nguyen",
        "Zhen Hai",
        "Lidong Bing",
        "Anh Tuan Luu"
      ],
      "abstract": "Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise objective, which works on review pairs, may not completely capture the MRHP goal to produce the ranking for the entire review list, and possibly induces low generalization during testing. To address these issues, we propose a listwise attention network that clearly captures the MRHP ranking context and a listwise optimization objective that enhances model generalization. We further propose gradient-boosted decision tree as the score predictor to efficaciously partition product reviews’ representations. Extensive experiments demonstrate that our method achieves state-of-the-art results and polished generalization performance on two large-scale MRHP benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1356,
      "original_url": "https://aclanthology.org/2023.findings-acl.107/",
      "title": "Extract and Attend: Improving Entity Translation in Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.107.pdf",
      "authors": [
        "Zixin Zeng",
        "Rui Wang",
        "Yichong Leng",
        "Junliang Guo",
        "Shufang Xie",
        "Xu Tan",
        "Tao Qin",
        "Tie-Yan Liu"
      ],
      "abstract": "While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we usually first look up in a dictionary and then organize the entity translation together with the translations of other parts to form a smooth target sentence. Inspired by this translation process, we propose an Extract-and-Attend approach to enhance entity translation in NMT, where the translation candidates of source entities are first extracted from a dictionary and then attended to by the NMT model to generate the target sentence. Specifically, the translation candidates are extracted by first detecting the entities in a source sentence and then translating the entities through looking up in a dictionary. Then, the extracted candidates are added as a prefix of the decoder input to be attended to by the decoder when generating the target sentence through self-attention. Experiments conducted on En-Zh and En-Ru demonstrate that the proposed method is effective on improving both the translation accuracy of entities and the overall translation quality, with up to 35% reduction on entity error rate and 0.85 gain on BLEU and 13.8 gain on COMET."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1357,
      "original_url": "https://aclanthology.org/2023.findings-acl.108/",
      "title": "Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.108.pdf",
      "authors": [
        "Hao Zheng",
        "Mirella Lapata"
      ],
      "abstract": "Compositional generalization is a basic mechanism in human language learning, which current neural networks struggle with. A recently proposed Disentangled sequence-to-sequence model (Dangle) shows promising generalization capability by learning specialized encodings for each decoding step. We introduce two key modifications to this model which encourage more disentangled representations and improve its compute and memory efficiency, allowing us to tackle compositional generalization in a more realistic setting. Specifically, instead of adaptively re-encoding source keys and values at each time step, we disentangle their representations and only re-encode keys periodically, at some interval. Our new architecture leads to better generalization performance across existing tasks and datasets, and a new machine translation benchmark which we create by detecting naturally occurring compositional patterns in relation to a training set. We show this methodology better emulates real-world requirements than artificial challenges."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1358,
      "original_url": "https://aclanthology.org/2023.findings-acl.109/",
      "title": "Cross-lingual AMR Aligner: Paying Attention to Cross-Attention",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.109.pdf",
      "authors": [
        "Abelardo Carlos Martínez Lorenzo",
        "Pere Lluís Huguet Cabot",
        "Roberto Navigli"
      ],
      "abstract": "This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner’s ability to obtain them across multiple languages. Our code will be available at [https://www.github.com/babelscape/AMR-alignment](https://www.github.com/babelscape/AMR-alignment)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1359,
      "original_url": "https://aclanthology.org/2023.findings-acl.110/",
      "title": "Zero-Shot Text Classification via Self-Supervised Tuning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.110.pdf",
      "authors": [
        "Chaoqun Liu",
        "Wenxuan Zhang",
        "Guizhen Chen",
        "Xiaobao Wu",
        "Anh Tuan Luu",
        "Chip Hong Chang",
        "Lidong Bing"
      ],
      "abstract": "Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1360,
      "original_url": "https://aclanthology.org/2023.findings-acl.111/",
      "title": "Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.111.pdf",
      "authors": [
        "Borui Wang",
        "Qiuyuan Huang",
        "Budhaditya Deb",
        "Aaron Halfaker",
        "Liqun Shao",
        "Daniel McDuff",
        "Ahmed Hassan Awadallah",
        "Dragomir Radev",
        "Jianfeng Gao"
      ],
      "abstract": "Natural language contains rich logical structures and logical information, and correctly detecting and accurately understanding these logical structures and information underlying natural language texts is very crucial for NLP models’ performance on many important NLU and NLG tasks. Existing pre-trained language models based on the transformer architecture mostly adopt a classical design for constructing their input embeddings that ignores the logical structures underlying natural language texts, thus limiting their ability to better capture and encode key logical information in the input sequences. To overcome such limitations, in this paper we first propose a novel approach to construct logic-aware input embeddings for transformer language models through a combination of logic detection, logic mapping and hierarchical logical projections, and then develop a corresponding new modeling paradigm that can upgrade existing transformer language models into logical transformers to boost their performance on different NLU and NLG tasks. Our empirical experiments on four important and challenging NLU and NLG tasks demonstrate that our proposed logical transformer language models can achieve superior performance over their baseline transformer models through a deeper understanding of the logical structures of texts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1361,
      "original_url": "https://aclanthology.org/2023.findings-acl.112/",
      "title": "Large Language Models with Controllable Working Memory",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.112.pdf",
      "authors": [
        "Daliang Li",
        "Ankit Singh Rawat",
        "Manzil Zaheer",
        "Xin Wang",
        "Michal Lukasik",
        "Andreas Veit",
        "Felix Yu",
        "Sanjiv Kumar"
      ],
      "abstract": "Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), partly owing to the massive amounts of world knowledge they memorize during pretraining. While many downstream applications provide the model with an informational context to aid its underlying task, how the model’s world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model’s memorized knowledge. This enables model predictions to be grounded in the context, which then facilitates updating specific model predictions without frequently retraining the model. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM models (both pretrained and finetuned) could exhibit low controllability and robustness that does not improve with increasing the model size. As a solution, we propose a simple yet effective method – knowledge aware finetuning (KAFT) – to strengthen both controllability and robustness by injecting counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1362,
      "original_url": "https://aclanthology.org/2023.findings-acl.113/",
      "title": "A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.113.pdf",
      "authors": [
        "Neeraj Varshney",
        "Himanshu Gupta",
        "Eric Robertson",
        "Bing Liu",
        "Chitta Baral"
      ],
      "abstract": "State-of-the-art natural language processing models have been shown to achieve remarkable performance in ‘closed-world’ settings where all the labels in the evaluation set are known at training time. However, in real-world settings, ‘novel’ instances that do not belong to any known class are often observed. This renders the ability to deal with novelties crucial. To initiate a systematic research in this important area of ‘dealing with novelties’, we introduce NoveltyTask, a multi-stage task to evaluate a system’s performance on pipelined novelty ‘detection’ and ‘accommodation’ tasks. We provide mathematical formulation of NoveltyTask and instantiate it with the authorship attribution task that pertains to identifying the correct author of a given text. We use amazon reviews corpus and compile a large dataset (consisting of 250k instances across 200 authors/labels) for NoveltyTask. We conduct comprehensive experiments and explore several baseline methods for the task. Our results show that the methods achieve considerably low performance making the task challenging and leaving sufficient room for improvement. Finally, we believe our work will encourage research in this underexplored area of dealing with novelties, an important step en route to developing robust systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1363,
      "original_url": "https://aclanthology.org/2023.findings-acl.114/",
      "title": "CDA: A Contrastive Data Augmentation Method for Alzheimer’s Disease Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.114.pdf",
      "authors": [
        "Junwen Duan",
        "Fangyuan Wei",
        "Jin Liu",
        "Hongdong Li",
        "Tianming Liu",
        "Jianxin Wang"
      ],
      "abstract": "Alzheimer’s Disease (AD) is a neurodegenerative disorder that significantly impacts a patient’s ability to communicate and organize language. Traditional methods for detecting AD, such as physical screening or neurological testing, can be challenging and time-consuming. Recent research has explored the use of deep learning techniques to distinguish AD patients from non-AD patients by analysing the spontaneous speech. These models, however, are limited by the availability of data. To address this, we propose a novel contrastive data augmentation method, which simulates the cognitive impairment of a patient by randomly deleting a proportion of text from the transcript to create negative samples. The corrupted samples are expected to be in worse conditions than the original by a margin. Experimental results on the benchmark ADReSS Challenge dataset demonstrate that our model achieves the best performance among language-based models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1364,
      "original_url": "https://aclanthology.org/2023.findings-acl.115/",
      "title": "Disentangling Aspect and Stance via a Siamese Autoencoder for Aspect Clustering of Vaccination Opinions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.115.pdf",
      "authors": [
        "Lixing Zhu",
        "Runcong Zhao",
        "Gabriele Pergola",
        "Yulan He"
      ],
      "abstract": "Mining public opinions about vaccines from social media has been increasingly relevant to analyse trends in public debates and to provide quick insights to policy-makers. However, the application of existing models has been hindered by the wide variety of users’ attitudes and the new aspects continuously arising in the public debate. Existing approaches, frequently framed via well-known tasks, such as aspect classification or text span detection, make direct usage of the supervision information constraining the models to predefined aspect classes, while still not distinguishing those aspects from users’ stances. As a result, this has significantly hindered the dynamic integration of new aspects. We thus propose a model, namely Disentangled Opinion Clustering (DOC), for vaccination opinion mining from social media. DOC is able to disentangle users’ stances from opinions via a disentangling attention mechanism and a Swapping-Autoencoder, and is designed to process unseen aspect categories via a clustering approach, leveraging clustering-friendly representations induced by out-of-the-box Sentence-BERT encodings and disentangling mechanisms. We conduct a thorough experimental assessment demonstrating the benefit of the disentangling mechanisms and cluster-based approach on both the quality of aspect clusters and the generalization across new aspect categories, outperforming existing methodologies on aspect-based opinion mining."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1365,
      "original_url": "https://aclanthology.org/2023.findings-acl.116/",
      "title": "Temporal Relation Classification using Boolean Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.116.pdf",
      "authors": [
        "Omer Cohen",
        "Kfir Bar"
      ],
      "abstract": "Classifying temporal relations between a pair of events is crucial to natural language understanding and a well-known natural language processing task. Given a document and two event mentions, the task is aimed at finding which one started first. We propose an efficient approach for temporal relation classification (TRC) using a boolean question answering (QA) model which we fine-tune on questions that we carefully design based on the TRC annotation guidelines, thereby mimicking the way human annotators approach the task. Our new QA-based TRC model outperforms previous state-of-the-art results by 2.4%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1366,
      "original_url": "https://aclanthology.org/2023.findings-acl.117/",
      "title": "Are Synonym Substitution Attacks Really Synonym Substitution Attacks?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.117.pdf",
      "authors": [
        "Cheng-Han Chiang",
        "Hung-yi Lee"
      ],
      "abstract": "In this paper, we explore the following question: Are synonym substitution attacks really synonym substitution attacks (SSAs)?We approach this question by examining how SSAs replace words in the original sentence and show that there are still unresolved obstacles that make current SSAs generate invalid adversarial samples. We reveal that four widely used word substitution methods generate a large fraction of invalid substitution words that are ungrammatical or do not preserve the original sentence’s semantics. Next, we show that the semantic and grammatical constraints used in SSAs for detecting invalid word replacements are highly insufficient in detecting invalid adversarial samples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1367,
      "original_url": "https://aclanthology.org/2023.findings-acl.118/",
      "title": "DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.118.pdf",
      "authors": [
        "Venkatesh E",
        "Kaushal Maurya",
        "Deepak Kumar",
        "Maunendra Sankar Desarkar"
      ],
      "abstract": "Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called DIVHSK. It has two components:KEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. We have also created a high-quality multi-reference headline dataset from news articles."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1368,
      "original_url": "https://aclanthology.org/2023.findings-acl.119/",
      "title": "Similarity-Based Content Scoring - A more Classroom-Suitable Alternative to Instance-Based Scoring?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.119.pdf",
      "authors": [
        "Marie Bexte",
        "Andrea Horbach",
        "Torsten Zesch"
      ],
      "abstract": "Automatically scoring student answers is an important task that is usually solved using instance-based supervised learning. Recently, similarity-based scoring has been proposed as an alternative approach yielding similar perfor- mance. It has hypothetical advantages such as a lower need for annotated training data and better zero-shot performance, both of which are properties that would be highly beneficial when applying content scoring in a realistic classroom setting. In this paper we take a closer look at these alleged advantages by comparing different instance-based and similarity-based methods on multiple data sets in a number of learning curve experiments. We find that both the demand on data and cross-prompt performance is similar, thus not confirming the former two suggested advantages. The by default more straightforward possibility to give feedback based on a similarity-based approach may thus tip the scales in favor of it, although future work is needed to explore this advantage in practice."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1369,
      "original_url": "https://aclanthology.org/2023.findings-acl.120/",
      "title": "Pragmatic Inference with a CLIP Listener for Contrastive Captioning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.120.pdf",
      "authors": [
        "Jiefu Ou",
        "Benno Krojer",
        "Daniel Fried"
      ],
      "abstract": "We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision-language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity — outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1370,
      "original_url": "https://aclanthology.org/2023.findings-acl.121/",
      "title": "A Statistical Exploration of Text Partition Into Constituents: The Case of the Priestly Source in the Books of Genesis and Exodus",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.121.pdf",
      "authors": [
        "Gideon Yoffe",
        "Axel Bühler",
        "Nachum Dershowitz",
        "Thomas Romer",
        "Eli Piasetzky",
        "Israel Finkelstein",
        "Barak Sober"
      ],
      "abstract": "We present a pipeline for a statistical stylometric exploration of a hypothesized partition of a text. Given a parameterization of the text, our pipeline: (1) detects literary features yielding the optimal overlap between the hypothesized and unsupervised partitions, (2) performs a hypothesis-testing analysis to quantify the statistical significance of the optimal overlap, while conserving implicit correlations between units of text that are more likely to be grouped, and (3) extracts and quantifies the importance of features most responsible for the classification, estimates their statistical stability and cluster-wise abundance. We apply our pipeline to the first two books in the Bible, where one stylistic component stands out in the eyes of biblical scholars, namely, the Priestly component. We identify and explore statistically significant stylistic differences between the Priestly and non-Priestly components."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1371,
      "original_url": "https://aclanthology.org/2023.findings-acl.122/",
      "title": "A Language-First Approach for Procedure Planning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.122.pdf",
      "authors": [
        "Jiateng Liu",
        "Sha Li",
        "Zhenhailong Wang",
        "Manling Li",
        "Heng Ji"
      ],
      "abstract": "Procedure planning, or the ability to predict a series of steps that can achieve a given goal conditioned on the current observation, is critical for building intelligent embodied agents that can assist users in everyday tasks. Encouraged by the recent success of language models (LMs) for zero-shot and few-shot planning, we hypothesize that LMs may be equipped with stronger priors for planning compared to their visual counterparts. To this end, we propose a language-first procedure planning framework with a modularized design: we first align the current and goal observations with corresponding steps and then use a pre-trained LM to predict the intermediate steps. Under this framework, we find that using an image captioning model for alignment can already match state-of-the-art performance and by designing a double retrieval model conditioned over current and goal observations jointly, we can achieve large improvements (19.2%-98.9% relatively higher success rate than state-of-the-art) on both COIN and CrossTask benchmarks. Our work verifies the planning ability of LMs and demonstrates how LMs can serve as a powerful “reasoning engine” even when the input is provided in another modality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1372,
      "original_url": "https://aclanthology.org/2023.findings-acl.123/",
      "title": "An Empirical Analysis of Leveraging Knowledge for Low-Resource Task-Oriented Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.123.pdf",
      "authors": [
        "Mayank Kulkarni",
        "Aoxiao Zhong",
        "Nicolas Guenon des mesnards",
        "Sahar Movaghati",
        "Mukund Sridhar",
        "He Xie",
        "Jianhua Lu"
      ],
      "abstract": "Task-oriented semantic parsing has drawn a lot of interest from the NLP community, and especially the voice assistant industry as it enables representing the meaning of user requests with arbitrarily nested semantics, including multiple intents and compound entities. SOTA models are large seq2seq transformers and require hundreds of thousands of annotated examples to be trained. However annotating such data to bootstrap new domains or languages is expensive and error-prone, especially for requests made of nested semantics. In addition large models easily break the tight latency constraints imposed in a user-facing production environment. As part of this work we explore leveraging external knowledge to improve model accuracy in low-resource and low-compute settings. We demonstrate that using knowledge-enhanced encoders inside seq2seq models does not result in performance gains by itself, but jointly learning to uncover entities in addition to the parse generation is a simple yet effective way of improving performance across the board. We show this is especially true in the low-compute scarce-data setting and for entity-rich domains, with relative gains up to 74.48% on the TOPv2 dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1373,
      "original_url": "https://aclanthology.org/2023.findings-acl.124/",
      "title": "TempLM: Distilling Language Models into Template-Based Generators",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.124.pdf",
      "authors": [
        "Tianyi Zhang",
        "Mina Lee",
        "Xiang Lisa Li",
        "Ende Shen",
        "Tatsunori Hashimoto"
      ],
      "abstract": "While pretrained language models (PLMs) have greatly improved text generation, they have also been known to produce unfaithful or inappropriate content. In contrast, classic template-based systems provide strong guarantees of faithfulness at the cost of fluency. We propose TempLM, which achieves the best of both worlds by distilling a PLM into a template-based generator. On the E2E and SynthBio data-to-text datasets, we show that TempLM is more faithful than the original PLM and is more fluent than prior template systems. Notably, on an out-of-domain evaluation, TempLM reduces a finetuned BART model’s unfaithfulness rate from 83% to 0%. In a human study, we find that TempLM’s templates substantially improve upon human-written ones in BERTScore."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1374,
      "original_url": "https://aclanthology.org/2023.findings-acl.125/",
      "title": "Incorporating Graph Information in Transformer-based AMR Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.125.pdf",
      "authors": [
        "Pavlo Vasylenko",
        "Pere Lluís Huguet Cabot",
        "Abelardo Carlos Martínez Lorenzo",
        "Roberto Navigli"
      ],
      "abstract": "Abstract Meaning Representation (AMR) is a Semantic Parsing formalism that aims at providing a semantic graph abstraction representing a given text. Current approaches are based on autoregressive language models such as BART or T5, fine-tuned through Teacher Forcing to obtain a linearized version of the AMR graph from a sentence. In this paper, we present LeakDistill, a model and method that explores a modification to the Transformer architecture, using structural adapters to explicitly incorporate graph information into the learned representations and improve AMR parsing performance. Our experiments show how, by employing word-to-node alignment to embed graph structural information into the encoder at training time, we can obtain state-of-the-art AMR parsing through self-knowledge distillation, even without the use of additional data. We release the code at [http://www.github.com/sapienzanlp/LeakDistill](http://www.github.com/sapienzanlp/LeakDistill)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1375,
      "original_url": "https://aclanthology.org/2023.findings-acl.126/",
      "title": "Rethinking the Word-level Quality Estimation for Machine Translation from Human Judgement",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.126.pdf",
      "authors": [
        "Zhen Yang",
        "Fandong Meng",
        "Yuanmeng Yan",
        "Jie Zhou"
      ],
      "abstract": "Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing effort, where the word labels in the dataset, i.e., OK or BAD, are automatically generated by comparing words between MT sentences and the post-edited sentences through a Translation Error Rate (TER) toolkit. While the post-editing effort can be used to measure the translation quality to some extent, we find it usually conflicts with human judgment on whether the word is well or poorly translated. To investigate this conflict, we first create a golden benchmark dataset, namely HJQE (Human Judgement on Quality Estimation), where the source and MT sentences are identical to the original TER-based dataset and the expert translators directly annotate the poorly translated words on their judgments. Based on our analysis, we further propose two tag-correcting strategies which can make the TER-based artificial QE corpus closer to HJQE. We conduct substantial experiments based on the publicly available WMT En-De and En-Zh corpora. The results not only show our proposed dataset is more consistent with human judgment but also confirm the effectiveness of the proposed tag-correcting strategies.For reviewers, the corpora and codes can be found in the attached files."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1376,
      "original_url": "https://aclanthology.org/2023.findings-acl.127/",
      "title": "PV2TEA: Patching Visual Modality to Textual-Established Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.127.pdf",
      "authors": [
        "Hejie Cui",
        "Rongmei Lin",
        "Nasser Zalmout",
        "Chenwei Zhang",
        "Jingbo Shang",
        "Carl Yang",
        "Xian Li"
      ],
      "abstract": "Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute in- formation extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established ex- tractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-pruning that adaptively distinguishes the visual foreground; (S3) Two-level neighborhood regularization that mitigates the label textual bias via reliability estimation. Empirical results on real-world e-Commerce datasets1 demonstrate up to 11.74% absolute (20.97% relatively) F1 increase over unimodal baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1377,
      "original_url": "https://aclanthology.org/2023.findings-acl.128/",
      "title": "Structural Contrastive Pretraining for Cross-Lingual Comprehension",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.128.pdf",
      "authors": [
        "Nuo Chen",
        "Linjun Shou",
        "Tengtao Song",
        "Ming Gong",
        "Jian Pei",
        "Jianhui Chang",
        "Daxin Jiang",
        "Jia Li"
      ],
      "abstract": "To present, multilingual language models trained using various pre-training tasks like mask language modeling (MLM) have yielded encouraging results on a wide range of downstream tasks. Despite the promising performances, structural knowledge in cross-lingual corpus is less explored in current works, leading to the semantic misalignment. In this paper, we propose a new pre-training task named Structural Contrast Pretraining (SCP) to align the structural words in a parallel sentence, enhancing the models’ ability to comprehend cross-lingual representations. Concretely, each structural word in source and target languages is regarded as a positive pair in SCP. Since contrastive learning compares positive and negative pairs, an increase in the frequency of negative pairings could enhance the performance of the resulting model. Therefore, we further propose Cross-lingual Momentum Contrast (CL-MoCo) to increase the number of negative pairs by maintaining a large size of the queue. CL-MoCo extends the original Moco approach into cross-lingual training and jointly optimizes the source-to-target language and target-to-source language representations, resulting in a more suitable encoder for cross-lingual transfer. We conduct extensive experiments to validate the proposed approach on three cross-lingual tasks across five datasets such as MLQA, WikiAnn, etc, and results prove the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1378,
      "original_url": "https://aclanthology.org/2023.findings-acl.129/",
      "title": "Reducing Sensitivity on Speaker Names for Text Generation from Dialogues",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.129.pdf",
      "authors": [
        "Qi Jia",
        "Haifeng Tang",
        "Kenny Zhu"
      ],
      "abstract": "Changing speaker names consistently throughout a dialogue should not affect its meaning and corresponding outputs for text generation from dialogues. However, pre-trained language models, serving as the backbone for dialogue-processing tasks, have shown to be sensitive to nuances. This may result in unfairness in real-world applications. No comprehensive analysis of this problem has been done in the past. In this work, we propose to quantitatively measure a model’s sensitivity on speaker names, and comprehensively evaluate a number of known methods for reducing speaker name sensitivity, including a novel approach of our own. Extensive experiments on multiple datasets provide a benchmark for this problem and show the favorable performance of our approach in sensitivity reduction and quality of generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1379,
      "original_url": "https://aclanthology.org/2023.findings-acl.130/",
      "title": "Topic and Style-aware Transformer for Multimodal Emotion Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.130.pdf",
      "authors": [
        "Shuwen Qiu",
        "Nitesh Sekhar",
        "Prateek Singhal"
      ],
      "abstract": "Understanding emotion expressions in multimodal signals is key for machines to have a better understanding of human communication. While language, visual and acoustic modalities can provide clues from different perspectives, the visual modality is shown to make minimal contribution to the performance in the emotion recognition field due to its high dimensionality. Therefore, we first leverage the strong multimodality backbone VATT to project the visual signal to the common space with language and acoustic signals. Also, we propose content-oriented features Topic and Speaking style on top of it to approach the subjectivity issues. Experiments conducted on the benchmark dataset MOSEI show our model can outperform SOTA results and effectively incorporate visual signals and handle subjectivity issues by serving as content “normalization”."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1380,
      "original_url": "https://aclanthology.org/2023.findings-acl.131/",
      "title": "Exploiting Abstract Meaning Representation for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.131.pdf",
      "authors": [
        "Cunxiang Wang",
        "Zhikun Xu",
        "Qipeng Guo",
        "Xiangkun Hu",
        "Xuefeng Bai",
        "Zheng Zhang",
        "Yue Zhang"
      ],
      "abstract": "The Open-Domain Question Answering (ODQA) task involves retrieving and subsequently generating answers from fine-grained relevant passages within a database. Current systems leverage Pretrained Language Models (PLMs) to model the relationship between questions and passages. However, the diversity in surface form expressions can hinder the model’s ability to capture accurate correlations, especially within complex contexts. Therefore, we utilize Abstract Meaning Representation (AMR) graphs to assist the model in understanding complex semantic information. We introduce a method known as Graph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural Questions (NQ) and TriviaQA (TQ) demonstrate that our GST method can significantly improve performance, resulting in up to 2.44/3.17 Exact Match score improvements on NQ/TQ respectively. Furthermore, our method enhances robustness and outperforms alternative Graph Neural Network (GNN) methods for integrating AMRs. To the best of our knowledge, we are the first to employ semantic graphs in ODQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1381,
      "original_url": "https://aclanthology.org/2023.findings-acl.132/",
      "title": "Nonparametric Masked Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.132.pdf",
      "authors": [
        "Sewon Min",
        "Weijia Shi",
        "Mike Lewis",
        "Xilun Chen",
        "Wen-tau Yih",
        "Hannaneh Hajishirzi",
        "Luke Zettlemoyer"
      ],
      "abstract": "Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1382,
      "original_url": "https://aclanthology.org/2023.findings-acl.133/",
      "title": "Pay More Attention to Relation Exploration for Knowledge Base Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.133.pdf",
      "authors": [
        "Yong Cao",
        "Xianzhi Li",
        "Huiwen Liu",
        "Wen Dai",
        "Shuai Chen",
        "Bin Wang",
        "Min Chen",
        "Daniel Hershcovich"
      ],
      "abstract": "Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the effectiveness and superiority of our framework, improving the F1 score by 5.8% from 40.5 to 46.3 on CWQ and 5.7% from 62.8 to 68.5 on WebQSP, better or on par with state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1383,
      "original_url": "https://aclanthology.org/2023.findings-acl.134/",
      "title": "Speaking Multiple Languages Affects the Moral Bias of Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.134.pdf",
      "authors": [
        "Katharina Haemmerl",
        "Bjoern Deiseroth",
        "Patrick Schramowski",
        "Jindřich Libovický",
        "Constantin Rothkopf",
        "Alexander Fraser",
        "Kristian Kersting"
      ],
      "abstract": "Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MORALDIRECTION framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experiments demonstrate that, indeed, PMLMs encode differing moral biases, but these do not necessarily correspond to cultural differences or commonalities in human opinions. We release our code and models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1384,
      "original_url": "https://aclanthology.org/2023.findings-acl.135/",
      "title": "Retrieving Relevant Context to Align Representations for Cross-lingual Event Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.135.pdf",
      "authors": [
        "Chien Nguyen",
        "Linh Ngo",
        "Thien Nguyen"
      ],
      "abstract": "We study the problem of cross-lingual transfer learning for event detection (ED) where models trained on a source language are expected to perform well on data for a new target language. Among a few recent works for this problem, the main approaches involve representation matching (e.g., adversarial training) that aims to eliminate language-specific features from the representations to achieve the language-invariant representations. However, due to the mix of language-specific features with event-discriminative context, representation matching methods might also remove important features for event prediction, thus hindering the performance for ED. To address this issue, we introduce a novel approach for cross-lingual ED where representations are augmented with additional context (i.e., not eliminating) to bridge the gap between languages while enriching the contextual information to facilitate ED. At the core of our method involves a retrieval model that retrieves relevant sentences in the target language for an input sentence to compute augmentation representations. Experiments on three languages demonstrate the state-of-the-art performance of our model for cross-lingual ED."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1385,
      "original_url": "https://aclanthology.org/2023.findings-acl.136/",
      "title": "NormNet: Normalize Noun Phrases for More Robust NLP",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.136.pdf",
      "authors": [
        "Minlong Peng",
        "Mingming Sun"
      ],
      "abstract": "A critical limitation of deep NLP models is their over-fitting over spurious features. Previous work has proposed several approaches to debunk such features and reduce their impact on the learned models. In this work, a normalization strategy is proposed to eliminate the false features caused by the textual surfaces of noun phrases. The motivation for this strategy is that noun phrases often play the role of slots in textual expressions and their exact forms are often not that important for performing the final task. As an intuitive example, consider the expression ”x like eating y\". There are a huge number of suitable instantiations for x and y in the locale. However, humans can already infer the sentiment polarity of x toward y without knowing their exact forms.Based on this intuition, we introduce NormNet, a pretrained language model based network, to implement the normalization strategy. NormNet learns to replace as many noun phrases in the input sentence as possible with pre-defined base forms. The output of NormNet is then fed as input to a prompt-based learning model to perform label prediction. To evaluate the effectiveness of our strategy, we conducted experimental studies on several tasks, including aspect sentiment classification (ASC), semantic text similarity (STS), and natural language inference (NLI). The experimental results confirm the effectiveness of our strategy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1386,
      "original_url": "https://aclanthology.org/2023.findings-acl.137/",
      "title": "Cross Encoding as Augmentation: Towards Effective Educational Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.137.pdf",
      "authors": [
        "Hyun Seung Lee",
        "Seungtaek Choi",
        "Yunsung Lee",
        "Hyeongdon Moon",
        "Shinhyeok Oh",
        "Myeongho Jeong",
        "Hyojun Go",
        "Christian Wallraven"
      ],
      "abstract": "Text classification in education, usually called auto-tagging, is the automated process of assigning relevant tags to educational content, such as questions and textbooks. However, auto-tagging suffers from a data scarcity problem, which stems from two major challenges: 1) it possesses a large tag space and 2) it is multi-label. Though a retrieval approach is reportedly good at low-resource scenarios, there have been fewer efforts to directly address the data scarcity problem. To mitigate these issues, here we propose a novel retrieval approach CEAA that provides effective learning in educational text classification. Our main contributions are as follows: 1) we leverage transfer learning from question-answering datasets, and 2) we propose a simple but effective data augmentation method introducing cross-encoder style texts to a bi-encoder architecture for more efficient inference. An extensive set of experiments shows that our proposed method is effective in multi-label scenarios and low-resource tags compared to state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1387,
      "original_url": "https://aclanthology.org/2023.findings-acl.138/",
      "title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.138.pdf",
      "authors": [
        "Venkata Prabhakara Sarath Nookala",
        "Gaurav Verma",
        "Subhabrata Mukherjee",
        "Srijan Kumar"
      ],
      "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for prompt-based FSL and (ii) multiple prompts flip the trend – the few-shot learning approaches demonstrate a lesser drop in task performance than fully fine-tuned models. We further demonstrate that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla FSL methods. Broadly, our work sheds light on the adversarial robustness evaluation of prompt-based FSL methods for NLU tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1388,
      "original_url": "https://aclanthology.org/2023.findings-acl.139/",
      "title": "This prompt is measuring <mask>: evaluating bias evaluation in language models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.139.pdf",
      "authors": [
        "Seraphina Goldfarb-Tarrant",
        "Eddie Ungless",
        "Esma Balkir",
        "Su Lin Blodgett"
      ],
      "abstract": "Bias research in NLP seeks to analyse models for social biases, thus helping NLP practitioners uncover, measure, and mitigate social harms. We analyse the body of work that uses prompts and templates to assess bias in language models. We draw on a measurement modelling framework to create a taxonomy of attributes that capture what a bias test aims to measure and how that measurement is carried out. By applying this taxonomy to 90 bias tests, we illustrate qualitatively and quantitatively that core aspects of bias test conceptualisations and operationalisations are frequently unstated or ambiguous, carry implicit assumptions, or be mismatched. Our analysis illuminates the scope of possible bias types the field is able to measure, and reveals types that are as yet under-researched. We offer guidance to enable the community to explore a wider section of the possible bias space, and to better close the gap between desired outcomes and experimental design, both for bias and for evaluating language models more broadly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1389,
      "original_url": "https://aclanthology.org/2023.findings-acl.140/",
      "title": "Towards Open Environment Intent Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.140.pdf",
      "authors": [
        "Yunhua Zhou",
        "Jiawei Hong",
        "Xipeng Qiu"
      ],
      "abstract": "Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not further differentiate detected OOD intents in fine granularity. Discovering focuses on how to cluster unlabeled samples according to their semantic representation, which relies heavily on prior knowledge and can not provide label information for the formed clusters. To be closer to the real user-facing scenarios, we introduce a task paradigm to extend Classification with Discovering referred as Open Environment Intent Prediction, which is to make a further fine-grained discovery of OOD based on OOD Intent Classification. Using various widely-used generative models as an archetype, we propose a general scheme for Open Environment Intent Prediction. In a nutshell, we first perform intent detection to identify the In-domain (IND) samples and then generate labels for those identified as OOD. With these generated labels, we can discover new general intents and provide label information for them. We develop a suite of benchmarks on the existing intent datasets and present a simple yet effective implementation. Extensive experiments demonstrate that our method establishes substantial improvement compared to the baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1390,
      "original_url": "https://aclanthology.org/2023.findings-acl.141/",
      "title": "Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.141.pdf",
      "authors": [
        "Minqian Liu",
        "Lifu Huang"
      ],
      "abstract": "Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&O which further take the logits of previously learned classes from old sessions or a constant logit of an Other class as constraint to the learning of new classifiers. Extensive experiments and analysis on 6 class-incremental information extraction tasks demonstrate that our solutions, especially ICE-O, consistently show significant improvement over the previous state-of-the-art approaches with up to 44.7% absolute F-score gain, providing a strong baseline and insights for future research on class-incremental learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1391,
      "original_url": "https://aclanthology.org/2023.findings-acl.142/",
      "title": "C-XNLI: Croatian Extension of XNLI Dataset",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.142.pdf",
      "authors": [
        "Leo Obadić",
        "Andrej Jertec",
        "Marko Rajnović",
        "Branimir Dropuljić"
      ],
      "abstract": "Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Facebook’s 1.2B parameter m2m_100 model. We thoroughly analyze the Croatian train set and compare its quality with the existing machine-translated German set. The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT). Our findings reveal that a less-resourced language like Croatian is still lacking in translation quality of longer sentences compared to German. However, both sets have a substantial amount of poor quality translations, which should be considered in translation-based training or evaluation setups."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1392,
      "original_url": "https://aclanthology.org/2023.findings-acl.143/",
      "title": "AVATAR: A Parallel Corpus for Java-Python Program Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.143.pdf",
      "authors": [
        "Wasi Uddin Ahmad",
        "Md Golam Rahman Tushar",
        "Saikat Chakraborty",
        "Kai-Wei Chang"
      ],
      "abstract": "Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment results show that the models lack in generating functionally accurate code."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1393,
      "original_url": "https://aclanthology.org/2023.findings-acl.144/",
      "title": "On Dataset Transferability in Active Learning for Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.144.pdf",
      "authors": [
        "Fran Jelenić",
        "Josip Jukić",
        "Nina Drobac",
        "Jan Snajder"
      ],
      "abstract": "Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1394,
      "original_url": "https://aclanthology.org/2023.findings-acl.145/",
      "title": "Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.145.pdf",
      "authors": [
        "Florian Weber",
        "Thiemo Wambsganss",
        "Seyed Parsa Neshaei",
        "Matthias Soellner"
      ],
      "abstract": "We present an annotation approach for capturing structured components and arguments inlegal case solutions of German students. Based on the appraisal style, which dictates the structured way of persuasive writing in German law, we propose an annotation scheme with annotation guidelines that identify structured writing in legal case solutions. We conducted an annotation study with two annotators and annotated legal case solutions to capture the structures of a persuasive legal text. Based on our dataset, we trained three transformer-based models to show that the annotated components can be successfully predicted, e.g. to provide users with writing assistance for legal texts. We evaluated a writing support system in which our models were integrated in an online experiment with law students and found positive learning success and users’ perceptions. Finally, we present our freely available corpus of 413 law student case studies to support the development of intelligent writing support systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1395,
      "original_url": "https://aclanthology.org/2023.findings-acl.146/",
      "title": "Characterizing the Impacts of Instances on Robustness",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.146.pdf",
      "authors": [
        "Rui Zheng",
        "Zhiheng Xi",
        "Qin Liu",
        "Wenbin Lai",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang",
        "Jin Ma",
        "Ying Shan",
        "Weifeng Ge"
      ],
      "abstract": "Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with robust patterns carring innate robustness. In this paper, we show that robust and non-robust instances in the training dataset, though are both important for test performance, have contrary impacts on robustness, which makes it possible to build a highly robust model by leveraging the training dataset in a more effective way. We propose a new method that can distinguish between robust instances from non-robust ones according to the model’s sensitivity to perturbations on individual instances during training. Surprisingly, we find that the model under standard training easily overfits the robust instances by relying on their simple patterns before the model completely learns their robust features. Finally, we propose a new mitigation algorithm to further release the potential of robust instances. Experimental results show that proper use of robust instances in the original dataset is a new line to achieve highly robust models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1396,
      "original_url": "https://aclanthology.org/2023.findings-acl.147/",
      "title": "Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.147.pdf",
      "authors": [
        "Xingyu Fu",
        "Sheng Zhang",
        "Gukyeong Kwon",
        "Pramuditha Perera",
        "Henghui Zhu",
        "Yuhao Zhang",
        "Alexander Hanbo Li",
        "William Yang Wang",
        "Zhiguo Wang",
        "Vittorio Castelli",
        "Patrick Ng",
        "Dan Roth",
        "Bing Xiang"
      ],
      "abstract": "The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias – the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality – only models using GPT-3 can achieve the best result. To address the aforementioned challenges, we propose RASO: a new VQA pipeline that deploys a generate-then-select strategy guided by world knowledge for the first time. Rather than following the de facto standard to train a multi-modal model that directly generates the VQA answer, {pasted macro ‘MODEL’}name first adopts PLM to generate all the possible answers, and then trains a lightweight answer selection model for the correct answer. As proved in our analysis, RASO expands the knowledge coverage from in-domain training data by a large margin. We provide extensive experimentation and show the effectiveness of our pipeline by advancing the state-of-the-art by 4.1% on OK-VQA, without additional computation cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1397,
      "original_url": "https://aclanthology.org/2023.findings-acl.148/",
      "title": "Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.148.pdf",
      "authors": [
        "Yongkang Wu",
        "Meng Han",
        "Yutao Zhu",
        "Lei Li",
        "Xinyu Zhang",
        "Ruofei Lai",
        "Xiaoguang Li",
        "Yuanhang Ren",
        "Zhicheng Dou",
        "Zhao Cao"
      ],
      "abstract": "Syllogistic reasoning, a typical form of deductive reasoning, is a critical capability widely required in natural language understanding tasks, such as text entailment and question answering. To better facilitate research on syllogistic reasoning, we develop a benchmark called SylloBase that differs from existing syllogistic datasets in three aspects: (1) Covering a complete taxonomy of syllogism reasoning patterns; (2) Containing both automatically and manually constructed samples; and (3) Involving both the generation and understanding tasks. We automatically construct 50k template-based syllogism samples by mining syllogism patterns from Wikidata and ConceptNet. To improve our dataset’s naturalness and challenge, we apply GPT-3 to paraphrase the template-based data and further manually rewrite 1,000 samples as the test set. State-of-the-art pre-trained language models can achieve the best generation ROUGE-L of 38.72 by T5 and the best multi-choice accuracy of 72.77% by RoBERTa on SylloBase, which indicates the great challenge of learning diverse syllogistic reasoning types on SylloBase. Our datasets are released at https://github.com/casually-PYlearner/SYLLOBASE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1398,
      "original_url": "https://aclanthology.org/2023.findings-acl.149/",
      "title": "Categorial grammar induction from raw data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.149.pdf",
      "authors": [
        "Christian Clark",
        "William Schuler"
      ],
      "abstract": "Grammar induction, the task of learning a set of grammatical rules from raw or minimally labeled text data, can provide clues about what kinds of syntactic structures are learnable without prior knowledge. Recent work (e.g., Kim et al., 2019; Zhu et al., 2020; Jin et al., 2021a) has achieved advances in unsupervised induction of probabilistic context-free grammars (PCFGs). However, categorial grammar induction has received less recent attention, despite allowing inducers to support a larger set of syntactic categories—due to restrictions on how categories can combine—and providing a transparent interface with compositional semantics, opening up possibilities for models that jointly learn form and meaning. Motivated by this, we propose a new model for inducing a basic (Ajdukiewicz, 1935; Bar-Hillel, 1953) categorial grammar. In contrast to earlier categorial grammar induction systems (e.g., Bisk and Hockenmaier, 2012), our model learns from raw data without any part-of-speech information. Experiments on child-directed speech show that our model attains a recall-homogeneity of 0.33 on average, which dramatically increases to 0.59 when a bias toward forward function application is added to the model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1399,
      "original_url": "https://aclanthology.org/2023.findings-acl.150/",
      "title": "Attribute Controlled Dialogue Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.150.pdf",
      "authors": [
        "Runcheng Liu",
        "Ahmad Rashid",
        "Ivan Kobyzev",
        "Mehdi Rezagholizadeh",
        "Pascal Poupart"
      ],
      "abstract": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1400,
      "original_url": "https://aclanthology.org/2023.findings-acl.151/",
      "title": "Open-World Factually Consistent Question Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.151.pdf",
      "authors": [
        "Himanshu Maheshwari",
        "Sumit Shekhar",
        "Apoorv Saxena",
        "Niyati Chhaya"
      ],
      "abstract": "Question generation methods based on pre-trained language models often suffer from factual inconsistencies and incorrect entities and are not answerable from the input paragraph. Domain shift – where the test data is from a different domain than the training data - further exacerbates the problem of hallucination. This is a critical issue for any natural language application doing question generation. In this work, we propose an effective data processing technique based on de-lexicalization for consistent question generation across domains. Unlike existing approaches for remedying hallucination, the proposed approach does not filter training data and is generic across question-generation models. Experimental results across six benchmark datasets show that our model is robust to domain shift and produces entity-level factually consistent questions without significant impact on traditional metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1401,
      "original_url": "https://aclanthology.org/2023.findings-acl.152/",
      "title": "Contrastive Learning of Sociopragmatic Meaning in Social Media",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.152.pdf",
      "authors": [
        "Chiyu Zhang",
        "Muhammad Abdul-Mageed",
        "Ganesh Jawahar"
      ],
      "abstract": "Recent progress in representation and contrastive learning in NLP has not widely considered the class of sociopragmatic meaning (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our model obtains an improvement of 11.66 average F1 on 16 datasets when fine-tuned on only 20 training samples per dataset. We also show that our framework improves uniformity and preserves the semantic structure of representations. Our code is available at: https://github.com/UBC-NLP/infodcl"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1402,
      "original_url": "https://aclanthology.org/2023.findings-acl.153/",
      "title": "Noisy Positive-Unlabeled Learning with Self-Training for Speculative Knowledge Graph Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.153.pdf",
      "authors": [
        "Ruijie Wang",
        "Baoyu Li",
        "Yichen Lu",
        "Dachun Sun",
        "Jinning Li",
        "Yuchen Yan",
        "Shengzhong Liu",
        "Hanghang Tong",
        "Tarek Abdelzaher"
      ],
      "abstract": "This paper studies speculative reasoning task on real-world knowledge graphs (KG) that contain both false negative issue (i.e., potential true facts being excluded) and false positive issue (i.e., unreliable or outdated facts being included). State-of-the-art methods fall short in the speculative reasoning ability, as they assume the correctness of a fact is solely determined by its presence in KG, making them vulnerable to false negative/positive issues. The new reasoning task is formulated as a noisy Positive-Unlabeled learning problem. We propose a variational framework, namely nPUGraph, that jointly estimates the correctness of both collected and uncollected facts (which we call label posterior) and updates model parameters during training. The label posterior estimation facilitates speculative reasoning from two perspectives. First, it improves the robustness of a label posterior-aware graph encoder against false positive links. Second, it identifies missing facts to provide high-quality grounds of reasoning. They are unified in a simple yet effective self-training procedure. Empirically, extensive experiments on three benchmark KG and one Twitter dataset with various degrees of false negative/positive cases demonstrate the effectiveness of nPUGraph."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1403,
      "original_url": "https://aclanthology.org/2023.findings-acl.154/",
      "title": "ACROSS: An Alignment-based Framework for Low-Resource Many-to-One Cross-Lingual Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.154.pdf",
      "authors": [
        "Peiyao Li",
        "Zhengkun Zhang",
        "Jun Wang",
        "Liang Li",
        "Adam Jatowt",
        "Zhenglu Yang"
      ],
      "abstract": "This research addresses the challenges of Cross-Lingual Summarization (CLS) in low-resource scenarios and over imbalanced multilingual data. Existing CLS studies mostly resort to pipeline frameworks or multi-task methods in bilingual settings. However, they ignore the data imbalance in multilingual scenarios and do not utilize the high-resource monolingual summarization data. In this paper, we propose the Aligned CROSs-lingual Summarization (ACROSS) model to tackle these issues. Our framework aligns low-resource cross-lingual data with high-resource monolingual data via contrastive and consistency loss, which help enrich low-resource information for high-quality summaries. In addition, we introduce a data augmentation method that can select informative monolingual sentences, which facilitates a deep exploration of high-resource information and introduce new information for low-resource languages. Experiments on the CrossSum dataset show that ACROSS outperforms baseline models and obtains consistently dominant performance on 45 language pairs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1404,
      "original_url": "https://aclanthology.org/2023.findings-acl.155/",
      "title": "RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.155.pdf",
      "authors": [
        "Cunxiang Wang",
        "Haofei Yu",
        "Yue Zhang"
      ],
      "abstract": "Open-Domain Question Answering (ODQA) systems necessitate a reader model capable of generating answers by simultaneously referring to multiple passages. Although representative models like Fusion-in-Decoder (FiD) have been proposed to address this challenge, these systems can inadvertently rely on spurious features instead of genuine causal relationships between the question and the passages to generate answers. To counter this problem, we introduce the Rational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of FiD to differentiate between causal relationships and spurious features, subsequently guiding the decoder to generate answers informed by this discernment. Experimental results on two ODQA datasets, Natural Questions (NQ) and TriviaQA (TQ), demonstrate that our model surpasses previous methods, achieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and exhibits an enhanced ability to identify causal relationships."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1405,
      "original_url": "https://aclanthology.org/2023.findings-acl.156/",
      "title": "Unsupervised Keyphrase Extraction by Learning Neural Keyphrase Set Function",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.156.pdf",
      "authors": [
        "Mingyang Song",
        "Haiyun Jiang",
        "Lemao Liu",
        "Shuming Shi",
        "Liping Jing"
      ],
      "abstract": "We create a paradigm shift concerning building unsupervised keyphrase extraction systems in this paper. Instead of modeling the relevance between an individual candidate phrase and the document as in the commonly used framework, we formulate the unsupervised keyphrase extraction task as a document-set matching problem from a set-wise perspective, in which the document and the candidate set are globally matched in the semantic space to particularly take into account the interactions among all candidate phrases. Since it is intractable to exactly extract the keyphrase set by the matching function during the inference, we propose an approximate approach, which obtains the candidate subsets via a set extractor agent learned by reinforcement learning. Exhaustive experimental results demonstrate the effectiveness of our model, which outperforms the recent state-of-the-art unsupervised keyphrase extraction baselines by a large margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1406,
      "original_url": "https://aclanthology.org/2023.findings-acl.157/",
      "title": "Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.157.pdf",
      "authors": [
        "Zhiyuan Zhang",
        "Deli Chen",
        "Hao Zhou",
        "Fandong Meng",
        "Jie Zhou",
        "Xu Sun"
      ],
      "abstract": "Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. According to the relationship between parameter drifts and Hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. To the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. Experimental results validate the effectiveness of Fine-purifying even with a small clean dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1407,
      "original_url": "https://aclanthology.org/2023.findings-acl.158/",
      "title": "Retrieving Multimodal Prompts for Generative Visual Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.158.pdf",
      "authors": [
        "Timothy Ossowski",
        "Junjie Hu"
      ],
      "abstract": "Recent years have witnessed impressive results of pre-trained vision-language models on knowledge-intensive tasks such as visual question answering (VQA). Despite the recent advances in VQA, existing methods mainly adopt a discriminative formulation that predicts answers within a pre-defined label set, leading to easy overfitting on low-resource domains (e.g., medicine) and poor generalization under domain shift to another dataset. To tackle this limitation, we propose a novel generative model enhanced by multimodal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text. Our generative model enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets. Our experiments on medical VQA tasks show that MPR outperforms its non-retrieval counterpart by up to 30% accuracy points in a few-shot domain adaptation setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1408,
      "original_url": "https://aclanthology.org/2023.findings-acl.159/",
      "title": "InfoSync: Information Synchronization across Multilingual Semi-structured Tables",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.159.pdf",
      "authors": [
        "Siddharth Khincha",
        "Chelsi Jain",
        "Vivek Gupta",
        "Tushar Kataria",
        "Shuo Zhang"
      ],
      "abstract": "Information Synchronization of semi-structured data across languages is challenging. For example, Wikipedia tables in one language need to be synchronized with others. To address this problem, we introduce a new dataset InfoSync and a two-step method for tabular synchronization. InfoSync contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages, of which a subset (~3.5K pairs) are manually annotated. The proposed method includes 1) Information Alignment to map rows and 2) Information Update for updating missing/outdated information for aligned tables across multilingual tables. When evaluated on InfoSync, information alignment achieves an F1 score of 87.91 (en <-> non-en). To evaluate information updation, we perform human-assisted Wikipedia edits on Infoboxes for 532 table pairs. Our approach obtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of the proposed method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1409,
      "original_url": "https://aclanthology.org/2023.findings-acl.160/",
      "title": "T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.160.pdf",
      "authors": [
        "Jialu Wang",
        "Xinyue Liu",
        "Zonglin Di",
        "Yang Liu",
        "Xin Wang"
      ],
      "abstract": "*Warning: This paper contains several contents that may be toxic, harmful, or offensive.*In the last few years, text-to-image generative models have gained remarkable success in generating images with unprecedented quality accompanied by a breakthrough of inference speed. Despite their rapid progress, human biases that manifest in the training examples, particularly with regard to common stereotypical biases, like gender and skin tone, still have been found in these generative models. In this work, we seek to measure more complex human biases exist in the task of text-to-image generations. Inspired by the well-known Implicit Association Test (IAT) from social psychology, we propose a novel Text-to-Image Association Test (T2IAT) framework that quantifies the implicit stereotypes between concepts and valence, and those in the images. We replicate the previously documented bias tests on generative models, including morally neutral tests on flowers and insects as well as demographic stereotypical tests on diverse social attributes. The results of these experiments demonstrate the presence of complex stereotypical behaviors in image generations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1410,
      "original_url": "https://aclanthology.org/2023.findings-acl.161/",
      "title": "An Investigation of Evaluation Methods in Automatic Medical Note Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.161.pdf",
      "authors": [
        "Asma Ben Abacha",
        "Wen-wai Yim",
        "George Michalopoulos",
        "Thomas Lin"
      ],
      "abstract": "Recent studies on automatic note generation have shown that doctors can save significant amounts of time when using automatic clinical note generation (Knoll et al., 2022). Summarization models have been used for this task to generate clinical notes as summaries of doctor-patient conversations (Krishna et al., 2021; Cai et al., 2022). However, assessing which model would best serve clinicians in their daily practice is still a challenging task due to the large set of possible correct summaries, and the potential limitations of automatic evaluation metrics. In this paper we study evaluation methods and metrics for the automatic generation of clinical notes from medical conversation. In particular, we propose new task-specific metrics and we compare them to SOTA evaluation metrics in text summarization and generation, including: (i) knowledge-graph embedding-based metrics, (ii) customized model-based metrics with domain-specific weights, (iii) domain-adapted/fine-tuned metrics, and (iv) ensemble metrics. To study the correlation between the automatic metrics and manual judgments, we evaluate automatic notes/summaries by comparing the system and reference facts and computing the factual correctness, and the hallucination and omission rates for critical medical facts. This study relied on seven datasets manually annotated by domain experts. Our experiments show that automatic evaluation metrics can have substantially different behaviors on different types of clinical notes datasets. However, the results highlight one stable subset of metrics as the most correlated with human judgments with a relevant aggregation of different evaluation criteria."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1411,
      "original_url": "https://aclanthology.org/2023.findings-acl.162/",
      "title": "Rethinking Translation Memory Augmented Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.162.pdf",
      "authors": [
        "Hongkun Hao",
        "Guoping Huang",
        "Lemao Liu",
        "Zhirui Zhang",
        "Shuming Shi",
        "Rui Wang"
      ],
      "abstract": "This paper rethinks translation memory augmented neural machine translation (TM-augmented NMT) from two perspectives, i.e., a probabilistic view of retrieval and the variance-bias decomposition principle. The finding demonstrates that TM-augmented NMT is good at the ability of fitting data (i.e., lower bias) but is more sensitive to the fluctuations in the training data (i.e., higher variance), which provides an explanation to a recently reported contradictory phenomenon on the same translation task: TM-augmented NMT substantially advances NMT without TM under the high resource scenario whereas it fails under the low resource scenario. Then this paper proposes a simple yet effective TM-augmented NMT model to promote the variance and address the contradictory phenomenon. Extensive experiments show that the proposed TM-augmented NMT achieves consistent gains over both conventional NMT and existing TM-augmented NMT under two variance-preferable (low resource and plug-and-play) scenarios as well as the high resource scenario."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1412,
      "original_url": "https://aclanthology.org/2023.findings-acl.163/",
      "title": "Controlling Styles in Neural Machine Translation with Activation Prompt",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.163.pdf",
      "authors": [
        "Yifan Wang",
        "Zewei Sun",
        "Shanbo Cheng",
        "Weiguo Zheng",
        "Mingxuan Wang"
      ],
      "abstract": "Controlling styles in neural machine translation (NMT) has attracted wide attention, as it is crucial for enhancing user experience. Earlier studies on this topic typically concentrate on regulating the level of formality and achieve some progress in this area. However, they still encounter two major challenges. The first is the difficulty in style evaluation. The style comprises various aspects such as lexis, syntax, and others that provide abundant information. Nevertheless, only formality has been thoroughly investigated. The second challenge involves excessive dependence on incremental adjustments, particularly when new styles are necessary. To address both challenges, this paper presents a new benchmark and approach. A multiway stylized machine translation (MSMT) benchmark is introduced, incorporating diverse categories of styles across four linguistic domains. Then, we propose a method named style activation prompt (StyleAP) by retrieving prompts from stylized monolingual corpus, which does not require extra fine-tuning. Experiments show that StyleAP could effectively control the style of translation and achieve remarkable performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1413,
      "original_url": "https://aclanthology.org/2023.findings-acl.164/",
      "title": "Focusing, Bridging and Prompting for Few-shot Nested Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.164.pdf",
      "authors": [
        "Yuanyuan Xu",
        "Zeng Yang",
        "Linhai Zhang",
        "Deyu Zhou",
        "Tiandeng Wu",
        "Rong Zhou"
      ],
      "abstract": "Few-shot named entity recognition (NER), identifying named entities with a small number of labeled data, has attracted much attention. Frequently, entities are nested within each other. However, most of the existing work on few-shot NER addresses flat entities instead of nested entities. To tackle nested NER in a few-shot setting, it is crucial to utilize the limited labeled data to mine unique features of nested entities, such as the relationship between inner and outer entities and contextual position information. Therefore, in this work, we propose a novel method based on focusing, bridging and prompting for few-shot nested NER without using source domain data. Both focusing and bridging components provide accurate candidate spans for the prompting component. The prompting component leverages the unique features of nested entities to classify spans based on soft prompts and contrastive learning. Experimental results show that the proposed approach achieves state-of-the-art performance consistently on the four benchmark datasets (ACE2004, ACE2005, GENIA and KBP2017) and outperforms several competing baseline models on F1-score by 9.33% on ACE2004, 6.17% on ACE2005, 9.40% on GENIA and 5.12% on KBP2017 on the 5-shot setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1414,
      "original_url": "https://aclanthology.org/2023.findings-acl.165/",
      "title": "Together We Make Sense–Learning Meta-Sense Embeddings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.165.pdf",
      "authors": [
        "Haochen Luo",
        "Yi Zhou",
        "Danushka Bollegala"
      ],
      "abstract": "Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses. For this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning methods. However, not all existing sense embeddings cover all senses of ambiguous words equally well due to the discrepancies in their training resources. To address this problem, we propose the first-ever meta-sense embedding method – Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings by combining multiple independently trained source sense embeddings such that the sense neighbourhoods computed from the source embeddings are preserved in the meta-embedding space. Our proposed method can combine source sense embeddings that cover different sets of word senses. Experimental results on Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the proposed meta-sense embedding method consistently outperforms several competitive baselines. An anonymised version of the source code implementation for our proposed method is submitted to reviewing system. Both source code and the learnt meta-sense embeddings will be publicly released upon paper acceptance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1415,
      "original_url": "https://aclanthology.org/2023.findings-acl.166/",
      "title": "Multimodal Prompt Learning for Product Title Generation with Extremely Limited Labels",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.166.pdf",
      "authors": [
        "Bang Yang",
        "Fenglin Liu",
        "Zheng Li",
        "Qingyu Yin",
        "Chenyu You",
        "Bing Yin",
        "Yuexian Zou"
      ],
      "abstract": "Generating an informative and attractive title for the product is a crucial task for e-commerce. Most existing works follow the standard multimodal natural language generation approaches, e.g., image captioning, and employ the large scale of human-labelled datasets to train desirable models. However, for novel products, especially in a different domain, there are few existing labelled data. In this paper, we propose a prompt-based approach, i.e., the Multimodal Prompt Learning framework, to accurately and efficiently generate titles for novel products with limited labels. We observe that the core challenges of novel product title generation are the understanding of novel product characteristics and the generation of titles in a novel writing style. To this end, we build a set of multimodal prompts from different modalities to preserve the corresponding characteristics and writing styles of novel products. As a result, with extremely limited labels for training, the proposed method can retrieve the multimodal prompts to generate desirable titles for novel products. The experiments and analyses are conducted on five novel product categories under both the in-domain and out-of-domain experimental settings. The results show that, with only 1% of downstream labelled data for training, our proposed approach achieves the best few-shot results and even achieves competitive results with fully-supervised methods trained on 100% of training data; With the full labelled data for training, our method achieves state-of-the-art results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1416,
      "original_url": "https://aclanthology.org/2023.findings-acl.167/",
      "title": "Large Language Models are Built-in Autoregressive Search Engines",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.167.pdf",
      "authors": [
        "Noah Ziems",
        "Wenhao Yu",
        "Zhihan Zhang",
        "Meng Jiang"
      ],
      "abstract": "Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval. Surprisingly, when providing a few Query-URL pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers. Experiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings. The code for this work can be found at https://github.com/Ziems/llm-url."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1417,
      "original_url": "https://aclanthology.org/2023.findings-acl.168/",
      "title": "Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.168.pdf",
      "authors": [
        "Yaoming Zhu",
        "Zewei Sun",
        "Shanbo Cheng",
        "Luyang Huang",
        "Liwei Wu",
        "Mingxuan Wang"
      ],
      "abstract": "Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems focus on better access and use of visual information and tend to validate their methods on image-related datasets. However, these studies face two challenges. First, they can only utilize a limited amount of data that is composed of bilingual texts and images (referred to as “triple data”), which is scarce. Second, current benchmarks for MMT are restricted and do not correspond to realistic scenarios. Therefore, this paper correspondingly establishes new methods and a new dataset for MMT. We propose a novel framework for MMT that addresses these challenges by utilizing large-scale non-triple data, such as monolingual image-text and parallel text-only data. Additionally, we construct a new e-commercial multimodal translation dataset, named EMMT, of which the test set is specifically designed to include ambiguous words that require visual context for accurate translation. Experiments show that our method is well-suited for real-world scenarios and can significantly improve translation performance with more non-triple data. In addition, our model also rivals or surpasses various SOTA models in conventional multimodal translation benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1418,
      "original_url": "https://aclanthology.org/2023.findings-acl.169/",
      "title": "From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.169.pdf",
      "authors": [
        "Albert Coil",
        "Vered Shwartz"
      ],
      "abstract": "Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3’s performance is not perfect, it is better than that of humans—likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the outputs from GPT-3 often have significant overlap with a large web corpus, but that the parroting strategy is less beneficial for novel noun compounds."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1419,
      "original_url": "https://aclanthology.org/2023.findings-acl.170/",
      "title": "Measuring Intersectional Biases in Historical Documents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.170.pdf",
      "authors": [
        "Nadav Borenstein",
        "Karolina Stanczak",
        "Thea Rolskov",
        "Natacha Klein Käfer",
        "Natália da Silva Perez",
        "Isabelle Augenstein"
      ],
      "abstract": "Data-driven analyses of biases in historical texts can help illuminate the origin and development of biases prevailing in modern society. However, digitised historical documents pose a challenge for NLP practitioners as these corpora suffer from errors introduced by optical character recognition (OCR) and are written in an archaic language. In this paper, we investigate the continuities and transformations of bias in historical newspapers published in the Caribbean during the colonial era (18th to 19th centuries). Our analyses are performed along the axes of gender, race, and their intersection. We examine these biases by conducting a temporal study in which we measure the development of lexical associations using distributional semantics models and word embeddings. Further, we evaluate the effectiveness of techniques designed to process OCR-generated data and assess their stability when trained on and applied to the noisy historical newspapers. We find that there is a trade-off between the stability of the word embeddings and their compatibility with the historical dataset. We provide evidence that gender and racial biases are interdependent, and their intersection triggers distinct effects. These findings align with the theory of intersectionality, which stresses that biases affecting people with multiple marginalised identities compound to more than the sum of their constituents."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1420,
      "original_url": "https://aclanthology.org/2023.findings-acl.171/",
      "title": "Incomplete Utterance Rewriting by A Two-Phase Locate-and-Fill Regime",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.171.pdf",
      "authors": [
        "Zitong Li",
        "Jiawei Li",
        "Haifeng Tang",
        "Kenny Zhu",
        "Ruolan Yang"
      ],
      "abstract": "Rewriting incomplete and ambiguous utterances can improve dialogue models’ understanding of the context and help them generate better results. However, the existing end-to-end models will have the problem of too large search space, resulting in poor quality of rewriting results. We propose a 2-phase rewriting framework which first predicts the empty slots in the utterance that need to be completed, and then generate the part to be filled into each positions. Our framework is simple to implement, fast to run, and achieves the state-of-the-art results on several public rewriting datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1421,
      "original_url": "https://aclanthology.org/2023.findings-acl.172/",
      "title": "Exploring Variation of Results from Different Experimental Conditions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.172.pdf",
      "authors": [
        "Maja Popović",
        "Mohammad Arvan",
        "Natalie Parde",
        "Anya Belz"
      ],
      "abstract": "It might reasonably be expected that running multiple experiments for the same task using the same data and model would yield very similar results. Recent research has, however, shown this not to be the case for many NLP experiments. In this paper, we report extensive coordinated work by two NLP groups to run the training and testing pipeline for three neural text simplification models under varying experimental conditions, including different random seeds, run-time environments, and dependency versions, yielding a large number of results for each of the three models using the same data and train/dev/test set splits. From one perspective, these results can be interpreted as shedding light on the reproducibility of evaluation results for the three NTS models, and we present an in-depth analysis of the variation observed for different combinations of experimental conditions. From another perspective, the results raise the question of whether the averaged score should be considered the ‘true’ result for each model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1422,
      "original_url": "https://aclanthology.org/2023.findings-acl.173/",
      "title": "Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.173.pdf",
      "authors": [
        "Nicolás Benjamín Ocampo",
        "Elena Cabrio",
        "Serena Villata"
      ],
      "abstract": "Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages in complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a “build it, break it, fix it”, training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models’ performances on implicit HS benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1423,
      "original_url": "https://aclanthology.org/2023.findings-acl.174/",
      "title": "X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.174.pdf",
      "authors": [
        "Mehrad Moradshahi",
        "Tianhao Shen",
        "Kalika Bali",
        "Monojit Choudhury",
        "Gael de Chalendar",
        "Anmol Goel",
        "Sungkyun Kim",
        "Prashant Kodali",
        "Ponnurangam Kumaraguru",
        "Nasredine Semmar",
        "Sina Semnani",
        "Jiwon Seo",
        "Vivek Seshadri",
        "Manish Shrivastava",
        "Michael Sun",
        "Aditya Yadavalli",
        "Chaobin You",
        "Deyi Xiong",
        "Monica Lam"
      ],
      "abstract": "Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. To reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language.X-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language, and unlike most multilingual prior work, is an end-to-end dataset for building fully-functioning agents. The many difficulties we encountered in creating X-RiSAWOZ led us to develop a toolset to accelerate the post-editing of a new language dataset after translation. This toolset improves machine translation with a hybrid entity alignment technique that combines neural with dictionary-based methods, along with many automated and semi-automated validation checks. We establish strong baselines for X-RiSAWOZ by training dialogue agents in the zero- and few-shot settings where limited gold data is available in the target language. Our results suggest that our translation and post-editing methodology and toolset can be used to create new high-quality multilingual dialogue agents cost-effectively. Our dataset, code, and toolkit are released open-source."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1424,
      "original_url": "https://aclanthology.org/2023.findings-acl.175/",
      "title": "Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.175.pdf",
      "authors": [
        "Francois Meyer",
        "Jan Buys"
      ],
      "abstract": "Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1425,
      "original_url": "https://aclanthology.org/2023.findings-acl.176/",
      "title": "Measuring and Mitigating Local Instability in Deep Neural Networks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.176.pdf",
      "authors": [
        "Arghya Datta",
        "Subhrangshu Nandi",
        "Jingcheng Xu",
        "Greg Ver Steeg",
        "He Xie",
        "Anoop Kumar",
        "Aram Galstyan"
      ],
      "abstract": "Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample “label entropy” across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization methods to improve stability and propose new data-centric methods that exploit our local stability estimates. We find that our localized data-specific mitigation strategy dramatically outperforms data-agnostic methods, and comes within 90% of the gold standard, achieved by ensembling, at a fraction of the computational cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1426,
      "original_url": "https://aclanthology.org/2023.findings-acl.177/",
      "title": "What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.177.pdf",
      "authors": [
        "Wenhao Zhu",
        "Shujian Huang",
        "Yunzhe Lv",
        "Xin Zheng",
        "Jiajun Chen"
      ],
      "abstract": "kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single entry and for a given neighborhood. Empirical study shows that our investigation successfully finds the conditions where the NMT model could easily fail and need related knowledge. Experiments on six diverse target domains and two language-pairs show that pruning according to local correctness brings a light and more explainable memory for kNN-MT domain adaptation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1427,
      "original_url": "https://aclanthology.org/2023.findings-acl.178/",
      "title": "Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.178.pdf",
      "authors": [
        "Ting Xu",
        "Huiyun Yang",
        "Zhen Wu",
        "Jiaze Chen",
        "Fei Zhao",
        "Xinyu Dai"
      ],
      "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is widely used in various applications. However, existing ASTE datasets are limited in their ability to represent real-world scenarios, hindering the advancement of research in this area. In this paper, we introduce a new dataset, named DMASTE, which is manually annotated to better fit real-world scenarios by providing more diverse and realistic reviews for the task. The dataset includes various lengths, diverse expressions, more aspect types, and more domains than existing datasets. We conduct extensive experiments on DMASTE in multiple settings to evaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is a more challenging ASTE dataset. Further analyses of in-domain and cross-domain settings provide some promising directions for future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1428,
      "original_url": "https://aclanthology.org/2023.findings-acl.179/",
      "title": "Grounding the Lexical Substitution Task in Entailment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.179.pdf",
      "authors": [
        "Talgat Omarov",
        "Grzegorz Kondrak"
      ],
      "abstract": "Existing definitions of lexical substitutes are often vague or inconsistent with the gold annotations. We propose a new definition which is grounded in the relation of entailment; namely, that the sentence that results from the substitution should be in the relation of mutual entailment with the original sentence. We argue that the new definition is well-founded and supported by previous work on lexical entailment. We empirically validate our definition by verifying that it covers the majority of gold substitutes in existing datasets. Based on this definition, we create a new dataset from existing semantic resources. Finally, we propose a novel context augmentation method motivated by the definition, which relates the substitutes to the sense of the target word by incorporating glosses and synonyms directly into the context. Experimental results demonstrate that our augmentation approach improves the performance of lexical substitution systems on the existing benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1429,
      "original_url": "https://aclanthology.org/2023.findings-acl.180/",
      "title": "Operator Selection and Ordering in a Pipeline Approach to Efficiency Optimizations for Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.180.pdf",
      "authors": [
        "Ji Xin",
        "Raphael Tang",
        "Zhiying Jiang",
        "Yaoliang Yu",
        "Jimmy Lin"
      ],
      "abstract": "There exists a wide variety of efficiency methods for natural language processing (NLP) tasks, such as pruning, distillation, dynamic inference, quantization, etc. From a different perspective, we can consider an efficiency method as an operator applied on a model. Naturally, we may construct a pipeline of operators, i.e., to apply multiple efficiency methods on the model sequentially. In this paper, we study the plausibility of this idea, and more importantly, the commutativity and cumulativeness of efficiency operators. We make two interesting observations from our experiments: (1) The operators are commutative—the order of efficiency methods within the pipeline has little impact on the final results; (2) The operators are also cumulative—the final results of combining several efficiency methods can be estimated by combining the results of individual methods. These observations deepen our understanding of efficiency operators and provide useful guidelines for building them in real-world applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1430,
      "original_url": "https://aclanthology.org/2023.findings-acl.181/",
      "title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.181.pdf",
      "authors": [
        "Asaad Alghamdi",
        "Xinyu Duan",
        "Wei Jiang",
        "Zhenhai Wang",
        "Yimeng Wu",
        "Qingrong Xia",
        "Zhefeng Wang",
        "Yi Zheng",
        "Mehdi Rezagholizadeh",
        "Baoxing Huai",
        "Peilun Cheng",
        "Abbas Ghaddar"
      ],
      "abstract": "Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1431,
      "original_url": "https://aclanthology.org/2023.findings-acl.182/",
      "title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.182.pdf",
      "authors": [
        "Julia White",
        "Arushi Raghuvanshi",
        "Yada Pruksachatkun"
      ],
      "abstract": "Task-oriented dialogues often require agents to enact complex, multi-step procedures in order to meet user requests. While large language models have found success automating these dialogues in constrained environments, their widespread deployment is limited by the substantial quantities of task-specific data required for training. The following paper presents a data-efficient solution to constructing dialogue systems, leveraging explicit instructions derived from agent guidelines, such as company policies or customer service manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a large language model with a knowledge retrieval module that pulls documents outlining relevant procedures from a predefined set of policies, given a user-agent interaction. To train this system, we introduce a semi-supervised pre-training scheme that employs dialogue-document matching and action-oriented masked language modeling with partial parameter freezing. We evaluate the effectiveness of our approach on prominent task-oriented dialogue datasets, Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue tasks: action state tracking and workflow discovery. Our results demonstrate that procedural knowledge augmentation improves accuracy predicting in- and out-of-distribution actions while preserving high performance in settings with low or sparse data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1432,
      "original_url": "https://aclanthology.org/2023.findings-acl.183/",
      "title": "Quantifying Train-Evaluation Overlap with Nearest Neighbors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.183.pdf",
      "authors": [
        "Gauri Kambhatla",
        "Thuy Nguyen",
        "Eunsol Choi"
      ],
      "abstract": "Characterizing benchmark datasets is crucial to interpreting model performance. In this work, we study train-evaluation overlap as a measure of an individual dataset’s adequacy to evaluate model generalization over a wide range of datasets. We quantify the overlap with a simple novel metric based on a nearest neighbors approach between the training and evaluation sets. We identify nearest training examples for each evaluation example by mapping instances with generic and task-specific embedding methods. Our study on eleven classification and extractive QA tasks reveals a wide range of train-evaluation overlap, and we show that the data collection method of the dataset and the difficulty of the task may play a role in the amount of overlap. Lastly, we use our nearest neighbor analysis to identify challenging or potentially mislabeled examples. Our analysis quantifies train-evaluation overlap, providing insights for constructing datasets to study generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1433,
      "original_url": "https://aclanthology.org/2023.findings-acl.184/",
      "title": "Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.184.pdf",
      "authors": [
        "Aviv Weinstein",
        "Yoav Goldberg"
      ],
      "abstract": "Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism—based on contextualized word representations—which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applied without modification but with high accuracy also to the nominal constructions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1434,
      "original_url": "https://aclanthology.org/2023.findings-acl.185/",
      "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.185.pdf",
      "authors": [
        "Genta Winata",
        "Alham Fikri Aji",
        "Zheng Xin Yong",
        "Thamar Solorio"
      ],
      "abstract": "Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1435,
      "original_url": "https://aclanthology.org/2023.findings-acl.186/",
      "title": "Learning to Predict Persona Information for Dialogue Personalization without Explicit Persona Description",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.186.pdf",
      "authors": [
        "Wangchunshu Zhou",
        "Qifei Li",
        "Chenle Li"
      ],
      "abstract": "Personalizing dialogue agents is important for dialogue systems to generate more specific,consistent, and engaging responses. However, most current dialogue personalization approaches rely on explicit persona descriptions during inference, which severely restricts its application. In this paper, we propose a novel approach that learns to predict persona information based on the dialogue history to personalize the dialogue agent without relying on any explicit persona descriptions during inference. Experimental results on the PersonaChat dataset show that the proposed method can improve the consistency of generated responses when conditioning on the predicted profile of the dialogue agent (i.e. “self persona”), and improve the engagingness of the generated responses when conditioning on the predicted persona of the dialogue partner (i.e. “their persona”). We also find that a trained persona prediction model can be successfully transferred to other datasets and help generate more relevant responses."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1436,
      "original_url": "https://aclanthology.org/2023.findings-acl.187/",
      "title": "Automated Refugee Case Analysis: A NLP Pipeline for Supporting Legal Practitioners",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.187.pdf",
      "authors": [
        "Claire Barale",
        "Michael Rovatsos",
        "Nehal Bhuta"
      ],
      "abstract": "In this paper, we introduce an end-to-end pipeline for retrieving, processing, and extracting targeted information from legal cases. We investigate an under-studied legal domain with a case study on refugee law Canada. Searching case law for past similar cases is a key part of legal work for both lawyers and judges, the potential end-users of our prototype. While traditional named-entity recognition labels such as dates are meaningful information in law, we propose to extend existing models and retrieve a total of 19 categories of items from refugee cases. After creating a novel data set of cases, we perform information extraction based on state-of-the-art neural named-entity recognition (NER). We test different architectures including two transformer models, using contextual and non-contextual embeddings, and compare general purpose versus domain-specific pre-training. The results demonstrate that models pre-trained on legal data perform best despite their smaller size, suggesting that domain-matching had a larger effect than network architecture. We achieve a F1- score superior to 90% on five of the targeted categories and superior to 80% on an additional 4 categories."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1437,
      "original_url": "https://aclanthology.org/2023.findings-acl.188/",
      "title": "Recurrent Attention Networks for Long-text Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.188.pdf",
      "authors": [
        "Xianming Li",
        "Zongxi Li",
        "Xiaotian Luo",
        "Haoran Xie",
        "Xing Lee",
        "Yingbin Zhao",
        "Fu Lee Wang",
        "Qing Li"
      ],
      "abstract": "Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1438,
      "original_url": "https://aclanthology.org/2023.findings-acl.189/",
      "title": "Exploring the Relationship between Alignment and Cross-lingual Transfer in Multilingual Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.189.pdf",
      "authors": [
        "Felix Gaschi",
        "Patricio Cerda",
        "Parisa Rastin",
        "Yannick Toussaint"
      ],
      "abstract": "Without any explicit cross-lingual training data, multilingual language models can achieve cross-lingual transfer. One common way to improve this transfer is to perform realignment steps before fine-tuning, i.e., to train the model to build similar representations for pairs of words from translated sentences. But such realignment methods were found to not always improve results across languages and tasks, which raises the question of whether aligned representations are truly beneficial for cross-lingual transfer. We provide evidence that alignment is actually significantly correlated with cross-lingual transfer across languages, models and random seeds. We show that fine-tuning can have a significant impact on alignment, depending mainly on the downstream task and the model. Finally, we show that realignment can, in some instances, improve cross-lingual transfer, and we identify conditions in which realignment methods provide significant improvements. Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. For example, for POS-tagging, between English and Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even outperforming XLM-R Large by 1.7. We thus advocate for further research on realignment methods for smaller multilingual models as an alternative to scaling."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1439,
      "original_url": "https://aclanthology.org/2023.findings-acl.190/",
      "title": "Aerial Vision-and-Dialog Navigation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.190.pdf",
      "authors": [
        "Yue Fan",
        "Winson Chen",
        "Tongzhou Jiang",
        "Chun Zhou",
        "Yi Zhang",
        "Xin Wang"
      ],
      "abstract": "The ability to converse with humans and follow natural language commands is crucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can relieve people’s burden of holding a controller all the time, allow multitasking, and make drone control more accessible for people with disabilities or with their hands occupied. To this end, we introduce Aerial Vision-and-Dialog Navigation (AVDN), to navigate a drone via natural language conversation. We build a drone simulator with a continuous photorealistic environment and collect a new AVDN dataset of over 3k recorded navigation trajectories with asynchronous human-human dialogs between commanders and followers. The commander provides initial navigation instruction and further guidance by request, while the follower navigates the drone in the simulator and asks questions when needed. During data collection, followers’ attention on the drone’s visual observation is also recorded. Based on the AVDN dataset, we study the tasks of aerial navigation from (full) dialog history and propose an effective Human Attention Aided Transformer model (HAA-Transformer), which learns to predict both navigation waypoints and human attention."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1440,
      "original_url": "https://aclanthology.org/2023.findings-acl.191/",
      "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.191.pdf",
      "authors": [
        "Hanlin Zhang",
        "Jiani Huang",
        "Ziyang Li",
        "Mayur Naik",
        "Eric Xing"
      ],
      "abstract": "Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1441,
      "original_url": "https://aclanthology.org/2023.findings-acl.192/",
      "title": "B2T Connection: Serving Stability and Performance in Deep Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.192.pdf",
      "authors": [
        "Sho Takase",
        "Shun Kiyono",
        "Sosuke Kobayashi",
        "Jun Suzuki"
      ],
      "abstract": "In the perspective of a layer normalization (LN) position, the architecture of Transformers can be categorized into two types: Post-LN and Pre-LN.Recent Transformers prefer to select Pre-LN because the training in Post-LN with deep Transformers, e.g., ten or more layers, often becomes unstable, resulting in useless models. However, in contrast, Post-LN has also consistently achieved better performance than Pre-LN in relatively shallow Transformers, e.g., six or fewer layers. This study first investigates the reason for these discrepant observations empirically and theoretically and discovers 1, the LN in Post-LN is the source of the vanishing gradient problem that mainly leads the unstable training whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation that may lead an effective training. Exploiting the new findings, we propose a method that can equip both higher stability and effective training by a simple modification from Post-LN.We conduct experiments on a wide range of text generation tasks and demonstrate that our method outperforms Pre-LN, and stable training regardless of the shallow or deep layer settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1442,
      "original_url": "https://aclanthology.org/2023.findings-acl.193/",
      "title": "Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.193.pdf",
      "authors": [
        "Robert Litschko",
        "Ekaterina Artemova",
        "Barbara Plank"
      ],
      "abstract": "Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distant languages (up to 2x absolute gain). We further show that our approach is robust towards the ratio of code-switched tokens and also extends to unseen languages. Our results demonstrate that training on code-switched data is a cheap and effective way of generalizing zero-shot rankers for cross-lingual and multilingual retrieval."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1443,
      "original_url": "https://aclanthology.org/2023.findings-acl.194/",
      "title": "Domain-specific Attention with Distributional Signatures for Multi-Domain End-to-end Task-Oriented Dialogue",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.194.pdf",
      "authors": [
        "Xing Ma",
        "Peng Zhang",
        "Feifei Zhao"
      ],
      "abstract": "The end-to-end task-oriented dialogue system has achieved great success in recent years. Most of these dialogue systems need to accommodate multi-domain dialogue in real-world scenarios. However, due to the high cost of dialogue data annotation and the scarcity of labeled dialogue data, existing methods are difficult to extend to new domains. Therefore, it is important to use limited data to construct multi-domain dialogue systems. To solve this problem, we propose a novel domain attention module. It use the distributional signatures to construct a multi-domain dialogue system effectively with limited data, which has strong extensibility. We also define a adjacent n-gram pattern to explore potential patterns for dialogue entities. Experimental results show that our approach outperforms the baseline models on most metrics. In the few-shot scenario, we show our method get a great improvement compared with previous methods while keeping smaller model scale."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1444,
      "original_url": "https://aclanthology.org/2023.findings-acl.195/",
      "title": "CKDST: Comprehensively and Effectively Distill Knowledge from Machine Translation to End-to-End Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.195.pdf",
      "authors": [
        "Yikun Lei",
        "Zhengshan Xue",
        "Xiaohu Zhao",
        "Haoran Sun",
        "Shaolin Zhu",
        "Xiaodong Lin",
        "Deyi Xiong"
      ],
      "abstract": "Distilling knowledge from a high-resource task, e.g., machine translation, is an effective way to alleviate the data scarcity problem of end-to-end speech translation. However, previous works simply use the classical knowledge distillation that does not allow for adequate transfer of knowledge from machine translation. In this paper, we propose a comprehensive knowledge distillation framework for speech translation, CKDST, which is capable of comprehensively and effectively distilling knowledge from machine translation to speech translation from two perspectives: cross-modal contrastive representation distillation and simultaneous decoupled knowledge distillation. In the former, we leverage a contrastive learning objective to optmize the mutual information between speech and text representations for representation distillation in the encoder. In the later, we decouple the non-target class knowledge from target class knowledge for logits distillation in the decoder. Experiments on the MuST-C benchmark dataset demonstrate that our CKDST substantially improves the baseline by 1.2 BLEU on average in all translation directions, and outperforms previous state-of-the-art end-to-end and cascaded speech translation models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1445,
      "original_url": "https://aclanthology.org/2023.findings-acl.196/",
      "title": "Follow the leader(board) with confidence: Estimating p-values from a single test set with item and response variance",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.196.pdf",
      "authors": [
        "Shira Wein",
        "Christopher Homan",
        "Lora Aroyo",
        "Chris Welty"
      ],
      "abstract": "Among the problems with leaderboard culture in NLP has been the widespread lack of confidence estimation in reported results. In this work, we present a framework and simulator for estimating p-values for comparisons between the results of two systems, in order to understand the confidence that one is actually better (i.e. ranked higher) than the other. What has made this difficult in the past is that each system must itself be evaluated by comparison to a gold standard. We define a null hypothesis that each system’s metric scores are drawn from the same distribution, using variance found naturally (though rarely reported) in test set items and individual labels on an item (responses) to produce the metric distributions. We create a test set that evenly mixes the responses of the two systems under the assumption the null hypothesis is true. Exploring how to best estimate the true p-value from a single test set under different metrics, tests, and sampling methods, we find that the presence of response variance (from multiple raters or multiple model versions) has a profound impact on p-value estimates for model comparison, and that choice of metric and sampling method is critical to providing statistical guarantees on model comparisons."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1446,
      "original_url": "https://aclanthology.org/2023.findings-acl.197/",
      "title": "Parallel Data Helps Neural Entity Coreference Resolution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.197.pdf",
      "authors": [
        "Gongbo Tang",
        "Christian Hardmeier"
      ],
      "abstract": "Coreference resolution is the task of finding expressions that refer to the same entity in a text. Coreference models are generally trained on monolingual annotated data but annotating coreference is expensive and challenging. Hardmeier et al. (2013) have shown that parallel data contains latent anaphoric knowledge, but it has not been explored in end-to-end neural models yet. In this paper, we propose a simple yet effective model to exploit coreference knowledge from parallel data. In addition to the conventional modules learning coreference from annotations, we introduce an unsupervised module to capture cross-lingual coreference knowledge. Our proposed cross-lingual model achieves consistent improvements, up to 1.74 percentage points, on the OntoNotes 5.0 English dataset using 9 different synthetic parallel datasets. These experimental results confirm that parallel data can provide additional coreference knowledge which is beneficial to coreference resolution tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1447,
      "original_url": "https://aclanthology.org/2023.findings-acl.198/",
      "title": "Towards Open-Domain Twitter User Profile Inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.198.pdf",
      "authors": [
        "Haoyang Wen",
        "Zhenxin Xiao",
        "Eduard Hovy",
        "Alexander Hauptmann"
      ],
      "abstract": "Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users’ privacy. Therefore, it is important for researchers to determine the extent of profiling in a safe environment to facilitate proper use and make the public aware of the potential risks. Contrary to existing approaches on limited attributes, we explore open-domain Twitter user profile inference. We conduct a case study where we collect publicly available WikiData public figure profiles and use diverse WikiData predicates for profile inference. After removing sensitive attributes, our data contains over 150K public figure profiles from WikiData, over 50 different attribute predicates, and over 700K attribute values. We further propose a prompt-based generation method, which can infer values that are implicitly mentioned in the Twitter information. Experimental results show that the generation-based approach can infer more comprehensive user profiles than baseline extraction-based methods, but limitations still remain to be applied for real-world use. We also enclose a detailed ethical statement for our data, potential benefits and risks from this work, and our efforts to mitigate the risks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1448,
      "original_url": "https://aclanthology.org/2023.findings-acl.199/",
      "title": "Eliciting Affective Events from Language Models by Multiple View Co-prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.199.pdf",
      "authors": [
        "Yuan Zhuang",
        "Ellen Riloff"
      ],
      "abstract": "Prior research on affective event classification showed that exploiting weakly labeled data for training can improve model performance. In this work, we propose a simpler and more effective approach for generating training data by automatically acquiring and labeling affective events with Multiple View Co-prompting, which leverages two language model prompts that provide independent views of an event. The approach starts with a modest amount of gold data and prompts pre-trained language models to generate new events. Next, information about the probable affective polarity of each event is collected from two complementary language model prompts and jointly used to assign polarity labels. Experimental results on two datasets show that the newly acquired events improve a state-of-the-art affective event classifier. We also present analyses which show that using multiple views produces polarity labels of higher quality than either view on its own."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1449,
      "original_url": "https://aclanthology.org/2023.findings-acl.200/",
      "title": "ZeroAE: Pre-trained Language Model based Autoencoder for Transductive Zero-shot Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.200.pdf",
      "authors": [
        "Kaihao Guo",
        "Hang Yu",
        "Cong Liao",
        "Jianguo Li",
        "Haipeng Zhang"
      ],
      "abstract": "Many text classification tasks require handling unseen domains with plenty of unlabeled data, thus giving rise to the self-adaption or the so-called transductive zero-shot learning (TZSL) problem. However, current methods based solely on encoders or decoders overlook the possibility that these two modules may promote each other. As a first effort to bridge this gap, we propose an autoencoder named ZeroAE. Specifically, the text is encoded with two separate BERT-based encoders into two disentangled spaces, i.e., label-relevant (for classification) and label-irrelevant respectively. The two latent spaces are then decoded by prompting GPT-2 to recover the text as well as to further generate text with labels in the unseen domains to train the encoder in turn. To better exploit the unlabeled data, a novel indirect uncertainty-aware sampling (IUAS) approach is proposed to train ZeroAE. Extensive experiments show that ZeroAE largely surpasses the SOTA methods by 15.93% and 8.70% on average respectively in the label-partially-unseen and label-fully-unseen scenario. Notably, the label-fully-unseen ZeroAE even possesses superior performance to the label-partially-unseen SOTA methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1450,
      "original_url": "https://aclanthology.org/2023.findings-acl.201/",
      "title": "PRAM: An End-to-end Prototype-based Representation Alignment Model for Zero-resource Cross-lingual Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.201.pdf",
      "authors": [
        "Yucheng Huang",
        "Wenqiang Liu",
        "Xianli Zhang",
        "Jun Lang",
        "Tieliang Gong",
        "Chen Li"
      ],
      "abstract": "Zero-resource cross-lingual named entity recognition (ZRCL-NER) aims to leverage rich labeled source language data to address the NER problem in the zero-resource target language. Existing methods are built either based on data transfer or representation transfer. However, the former usually leads to additional computation costs, and the latter lacks explicit optimization specific to the NER task. To overcome the above limitations, we propose a novel prototype-based representation alignment model (PRAM) for the challenging ZRCL-NER task. PRAM models the cross-lingual (CL) NER task and transfers knowledge from source languages to target languages in a unified neural network, and performs end-to-end training, avoiding additional computation costs. Moreover, PRAM borrows the CL inference ability of multilingual language models and enhances it with a novel training objective—attribution-prediction consistency (APC)—for explicitly enforcing the entity-level alignment between entity representations and predictions, as well as that across languages using prototypes as bridges. The experimental results show that PRAM significantly outperforms existing state-of-the-art methods, especially in some challenging scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1451,
      "original_url": "https://aclanthology.org/2023.findings-acl.202/",
      "title": "It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.202.pdf",
      "authors": [
        "Arjun Subramonian",
        "Xingdi Yuan",
        "Hal Daumé III",
        "Su Lin Blodgett"
      ],
      "abstract": "Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1452,
      "original_url": "https://aclanthology.org/2023.findings-acl.203/",
      "title": "Task-adaptive Label Dependency Transfer for Few-shot Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.203.pdf",
      "authors": [
        "Shan Zhang",
        "Bin Cao",
        "Tianming Zhang",
        "Yuqi Liu",
        "Jing Fan"
      ],
      "abstract": "Named Entity Recognition (NER), as a crucial subtask in natural language processing (NLP), suffers from limited labeled samples (a.k.a. few-shot). Meta-learning methods are widely used for few-shot NER, but these existing methods overlook the importance of label dependency for NER, resulting in suboptimal performance. However, applying meta-learning methods to label dependency learning faces a special challenge, that is, due to the discrepancy of label sets in different domains, the label dependencies can not be transferred across domains. In this paper, we propose the Task-adaptive Label Dependency Transfer (TLDT) method to make label dependency transferable and effectively adapt to new tasks by a few samples. TLDT improves the existing optimization-based meta-learning methods by learning general initialization and individual parameter update rule for label dependency. Extensive experiments show that TLDT achieves significant improvement over the state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1453,
      "original_url": "https://aclanthology.org/2023.findings-acl.204/",
      "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.204.pdf",
      "authors": [
        "Bo Zhou",
        "Qianglong Chen",
        "Tianyu Wang",
        "Xiaomi Zhong",
        "Yin Zhang"
      ],
      "abstract": "To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The field of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingual, however, there has been a lack of attention given to the area of classical Chinese, also known as \"wen yan wen (文言文)\", which has a rich history spanning thousands of years and holds significant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classification, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1454,
      "original_url": "https://aclanthology.org/2023.findings-acl.205/",
      "title": "A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.205.pdf",
      "authors": [
        "Jianheng Tang",
        "Kangfei Zhao",
        "Jia Li"
      ],
      "abstract": "Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint optimization framework. To address the computational challenges associated with optimizing FGW, we devise a three-stage progressive optimization algorithm. It starts with a basic semantic embedding matching, proceeds to approximate cross-KG structural and relational similarity matching based on iterative updates of high-confidence entity links, and ultimately culminates in a global structural comparison between KGs. We perform extensive experiments on four entity alignment datasets covering 14 distinct KGs across five languages. Without any supervision or hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including cutting-edge supervised entity alignment methods. Our code is available at https://github.com/squareRoot3/FusedGW-Entity-Alignment."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1455,
      "original_url": "https://aclanthology.org/2023.findings-acl.206/",
      "title": "Two Examples are Better than One: Context Regularization for Gradient-based Prompt Tuning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.206.pdf",
      "authors": [
        "Hyeonmin Ha",
        "Soyoung Jung",
        "Jinsol Park",
        "Minjoon Seo",
        "Seung-won Hwang",
        "Byung-Gon Chun"
      ],
      "abstract": "Prompting has gained tremendous attention as an efficient method for the adaptation of large-scale language models. However, prompts often act against human intuition and report unstable performances, which has motivated methods that automatically find effective prompts. One popular approach is gradient-based search, which iteratively updates a (randomly) initialized prompt towards the optimal one with the guide of gradients. We propose a novel regularization method, CoRe, for gradient-based prompt tuning techniques, which guides a prompt to produce a task context properly. CoRe realizes two regularization effects — context attuning and context filtering — that improve prediction performance in a zero-shot in-context learning setting where a model makes inferences only with the prompt tuned by CoRe, without any demonstration examples for in-context learning. Context attuning guides the context generated by the input and the tuned prompt toward embedding the appropriate context for the task. In our theoretical analysis, regularizing the context extends to improving zero-shot in-context learning performance. Context filtering steers the prompt to select only the task-related context so that context attuning solely focuses on creating and sending the right task context. We evaluate CoRe on natural language understanding datasets and two large language models, GPT2-XL and GPT-J.Our training scheme shows performance improvements up to 11.9% on GPT2-XL, and up to 6.3% on GPT-J in zero-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1456,
      "original_url": "https://aclanthology.org/2023.findings-acl.207/",
      "title": "An Investigation of Noise in Morphological Inflection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.207.pdf",
      "authors": [
        "Adam Wiemerslage",
        "Changbing Yang",
        "Garrett Nicolai",
        "Miikka Silfverberg",
        "Katharina Kann"
      ],
      "abstract": "With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the- art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models’ resistance to noise. Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1457,
      "original_url": "https://aclanthology.org/2023.findings-acl.208/",
      "title": "Graph Reasoning for Question Answering with Triplet Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.208.pdf",
      "authors": [
        "Shiyang Li",
        "Yifan Gao",
        "Haoming Jiang",
        "Qingyu Yin",
        "Zheng Li",
        "Xifeng Yan",
        "Chao Zhang",
        "Bing Yin"
      ],
      "abstract": "Answering complex questions often requires reasoning over knowledge graphs (KGs). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into KG encoder, e.g. graph neural networks (GNNs), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in KGs that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from KGs and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both CommonsenseQA and OpenbookQA datasets show that our method can outperform state-of-the-art up to 4.6% absolute accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1458,
      "original_url": "https://aclanthology.org/2023.findings-acl.209/",
      "title": "End-to-End Argument Mining over Varying Rhetorical Structures",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.209.pdf",
      "authors": [
        "Elena Chistova"
      ],
      "abstract": "Rhetorical Structure Theory implies no single discourse interpretation of a text, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically similar texts with varying rhetorical structures. In this work, the differences between paraphrases within the same argument scheme are evaluated from a rhetorical perspective. The study proposes a deep dependency parsing model to assess the connection between rhetorical and argument structures. The model utilizes rhetorical relations; RST structures of paraphrases serve as training data augmentations. The method allows for end-to-end argumentation analysis using a rhetorical tree instead of a word sequence. It is evaluated on the bilingual Microtexts corpus, and the first results on fully-fledged argument parsing for the Russian version of the corpus are reported. The results suggest that argument mining can benefit from multiple variants of discourse structure."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1459,
      "original_url": "https://aclanthology.org/2023.findings-acl.210/",
      "title": "Unsupervised Task Graph Generation from Instructional Video Transcripts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.210.pdf",
      "authors": [
        "Lajanugen Logeswaran",
        "Sungryull Sohn",
        "Yunseok Jang",
        "Moontae Lee",
        "Honglak Lee"
      ],
      "abstract": "This work explores the problem of generating task graphs of real-world activities. Different from prior formulations, we consider a setting where text transcripts of instructional videos performing a real-world activity (e.g., making coffee) are provided and the goal is to identify the key steps relevant to the task as well as the dependency relationship between these key steps. We propose a novel task graph generation approach that combines the reasoning capabilities of instruction-tuned language models along with clustering and ranking components to generate accurate task graphs in a completely unsupervised manner. We show that the proposed approach generates more accurate task graphs compared to a supervised learning approach on tasks from the ProceL and CrossTask datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1460,
      "original_url": "https://aclanthology.org/2023.findings-acl.211/",
      "title": "Exploiting Hierarchically Structured Categories in Fine-grained Chinese Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.211.pdf",
      "authors": [
        "Jiuding Yang",
        "Jinwen Luo",
        "Weidong Guo",
        "Di Niu",
        "Yu Xu"
      ],
      "abstract": "Chinese Named Entity Recognition (CNER) is a widely used technology in various applications. While recent studies have focused on utilizing additional information of the Chinese language and characters to enhance CNER performance, this paper focuses on a specific aspect of CNER known as fine-grained CNER (FG-CNER). FG-CNER involves the use of hierarchical, fine-grained categories (e.g. Person-MovieStar) to label named entities. To promote research in this area, we introduce the FiNE dataset, a dataset for FG-CNER consisting of 30,000 sentences from various domains and containing 67,651 entities in 54 fine-grained flattened hierarchical categories. Additionally, we propose SoftFiNE, a novel approach for FG-CNER that utilizes a custom-designed relevance scoring function based on label structures to learn the potential relevance between different flattened hierarchical labels. Our experimental results demonstrate that the proposed SoftFiNE method outperforms the state-of-the-art baselines on the FiNE dataset. Furthermore, we conduct extensive experiments on three other datasets, including OntoNotes 4.0, Weibo, and Resume, where SoftFiNE achieved state-of-the-art performance on all three datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1461,
      "original_url": "https://aclanthology.org/2023.findings-acl.212/",
      "title": "Adversarial Textual Robustness on Visual Dialog",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.212.pdf",
      "authors": [
        "Lu Yu",
        "Verena Rieser"
      ],
      "abstract": "Adversarial robustness evaluates the worst-case performance scenario of a machine learning model to ensure its safety and reliability. For example, cases where the user input contains a minimal change, e.g. a synonym, which causes the previously correct model to return a wrong answer. Using this scenario, this study is the first to investigate the robustness of visually grounded dialog models towards textual attacks. We first aim to understand how multimodal input components contribute to model robustness. Our results show that models which encode dialog history are more robust by providing redundant information. This is in contrast to prior work which finds that dialog history is negligible for model performance on this task. We also evaluate how to generate adversarial test examples which successfully fool the model but remain undetected by the user/software designer. Our analysis shows that the textual, as well as the visual context are important to generate plausible attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1462,
      "original_url": "https://aclanthology.org/2023.findings-acl.213/",
      "title": "Language Model Analysis for Ontology Subsumption Inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.213.pdf",
      "authors": [
        "Yuan He",
        "Jiaoyan Chen",
        "Ernesto Jimenez-Ruiz",
        "Hang Dong",
        "Ian Horrocks"
      ],
      "abstract": "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM’s knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1463,
      "original_url": "https://aclanthology.org/2023.findings-acl.214/",
      "title": "Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.214.pdf",
      "authors": [
        "Wanyun Cui",
        "Xingran Chen"
      ],
      "abstract": "Previous research has demonstrated that natural language explanations provide valuable inductive biases that guide models, thereby improving the generalization ability and data efficiency. In this paper, we undertake a systematic examination of the effectiveness of these explanations. Remarkably, we find that corrupted explanations with diminished inductive biases can achieve competitive or superior performance compared to the original explanations. Our findings furnish novel insights into the characteristics of natural language explanations in the following ways: (1) the impact of explanations varies across different training styles and datasets, with previously believed improvements primarily observed in frozen language models. (2) While previous research has attributed the effect of explanations solely to their inductive biases, our study shows that the effect persists even when the explanations are completely corrupted. We propose that the main effect is due to the provision of additional context space. (3) Utilizing the proposed automatic perturbed context, we were able to attain comparable results to annotated explanations, but with a significant increase in computational efficiency, 20-30 times faster."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1464,
      "original_url": "https://aclanthology.org/2023.findings-acl.215/",
      "title": "Varta: A Large-Scale Headline-Generation Dataset for Indic Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.215.pdf",
      "authors": [
        "Rahul Aralikatte",
        "Ziling Cheng",
        "Sumanth Doddapaneni",
        "Jackie Chi Kit Cheung"
      ],
      "abstract": "We present Varta, a large-scale multilingual dataset for headline generation in Indic languages. This dataset includes more than 41 million pairs of headlines and articles in 14 different Indic languages (and English), which come from a variety of high-quality news sources. To the best of our knowledge, this is the largest collection of curated news articles for Indic languages currently available. We use the collected data in a series of experiments to answer important questions related to Indic NLP and multilinguality research in general. We show that the dataset is challenging even for state-of-the-art abstractive models and that they perform only slightly better than extractive baselines. Owing to its size, we also show that the dataset can be used to pre-train strong language models that outperform competitive baselines in both NLU and NLG benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1465,
      "original_url": "https://aclanthology.org/2023.findings-acl.216/",
      "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.216.pdf",
      "authors": [
        "Xingchen Wan",
        "Ruoxi Sun",
        "Hanjun Dai",
        "Sercan Arik",
        "Tomas Pfister"
      ],
      "abstract": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few- and zero-shot abilities – they can effectively learn from a handful of handcrafted, completed responses (“in-context examples”), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of the examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria combining consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP significantly improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines at a range of reasoning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1466,
      "original_url": "https://aclanthology.org/2023.findings-acl.217/",
      "title": "Multimodal Recommendation Dialog with Subjective Preference: A New Challenge and Benchmark",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.217.pdf",
      "authors": [
        "Yuxing Long",
        "Binyuan Hui",
        "Caixia Yuan",
        "Fei Huang",
        "Yongbin Li",
        "Xiaojie Wang"
      ],
      "abstract": "Existing multimodal task-oriented dialog data fails to demonstrate the diverse expressions of user subjective preferences and recommendation acts in the real-life shopping scenario. This paper introduces a new dataset SURE (Multimodal Recommendation Dialog with Subjective Preference), which contains 12K shopping dialogs in complex store scenes. The data is built in two phases with human annotations to ensure quality and diversity. SURE is well-annotated with subjective preferences and recommendation acts proposed by sales experts. A comprehensive analysis is given to reveal the distinguishing features of SURE. Three benchmark tasks are then proposed on the data to evaluate the capability of multimodal recommendation agents. Basing on the SURE, we propose a baseline model, powered by a state-of-the-art multimodal model, for these tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1467,
      "original_url": "https://aclanthology.org/2023.findings-acl.218/",
      "title": "ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.218.pdf",
      "authors": [
        "Thilini Wijesiriwardene",
        "Ruwan Wickramarachchi",
        "Bimal Gajera",
        "Shreeyash Gowaikar",
        "Chandan Gupta",
        "Aman Chadha",
        "Aishwarya Naresh Reganti",
        "Amit Sheth",
        "Amitava Das"
      ],
      "abstract": "Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity – (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space. Our evaluation finds that it is increasingly challenging for LLMs to identify analogies when going up the analogy taxonomy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1468,
      "original_url": "https://aclanthology.org/2023.findings-acl.219/",
      "title": "Financial Numeric Extreme Labelling: A dataset and benchmarking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.219.pdf",
      "authors": [
        "Soumya Sharma",
        "Subhendu Khatuya",
        "Manjunath Hegde",
        "Afreen Shaikh",
        "Koustuv Dasgupta",
        "Pawan Goyal",
        "Niloy Ganguly"
      ],
      "abstract": "The U.S. Securities and Exchange Commission (SEC) mandates all public companies to file periodic financial statements that should contain numerals annotated with a particular label from a taxonomy. In this paper, we formulate the task of automating the assignment of a label to a particular numeral span in a sentence from an extremely large label set. Towards this task, we release a dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794 labels. We benchmark the performance of the FNXL dataset by formulating the task as (a) a sequence labelling problem and (b) a pipeline with span extraction followed by Extreme Classification. Although the two approaches perform comparably, the pipeline solution provides a slight edge for the least frequent labels."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1469,
      "original_url": "https://aclanthology.org/2023.findings-acl.220/",
      "title": "Multilingual Summarization with Factual Consistency Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.220.pdf",
      "authors": [
        "Roee Aharoni",
        "Shashi Narayan",
        "Joshua Maynez",
        "Jonathan Herzig",
        "Elizabeth Clark",
        "Mirella Lapata"
      ],
      "abstract": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation. We release models and human judgements of summaries to foster progress towards more factually consistent multilingual summarization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1470,
      "original_url": "https://aclanthology.org/2023.findings-acl.221/",
      "title": "Enhancing Out-of-Vocabulary Estimation with Subword Attention",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.221.pdf",
      "authors": [
        "Raj Patel",
        "Carlotta Domeniconi"
      ],
      "abstract": "Word embedding methods like word2vec and GloVe have been shown to learn strong representations of words. However, these methods only learn representations for words in the training corpus and therefore struggle to handle unknown and new words, known as out-of-vocabulary (OOV) words. As a result, there have been multiple attempts to learn OOV word representations in a similar fashion to how humans learn new words, using word roots/subwords and/or surrounding words. However, while most of these approaches use advanced architectures like attention on the context of the OOV word, they tend to use simple structures like ngram addition or character based convolutional neural networks (CNN) to handle processing subword information. In response to this, we propose SubAtt, a transformer based OOV estimation model that uses attention mechanisms on both the context and the subwords. In addition to attention, we also show that pretraining subword representations also leads to improvement in OOV estimation. We show SubAtt outperforms current state-of-the-art OOV estimation models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1471,
      "original_url": "https://aclanthology.org/2023.findings-acl.222/",
      "title": "Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.222.pdf",
      "authors": [
        "Sufeng Duan",
        "Hai Zhao"
      ],
      "abstract": "Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs. In this paper, we propose a new monolingual PLM-sponsored NMT model to let both encoder and decoder enjoy PLM enhancement to alleviate such obvious inconvenience. Especially, incorporating a newly proposed frequency-weighted embedding transformation algorithm, PLM embeddings can be effectively exploited in terms of the representations of the NMT decoder. We evaluate our model on IWSLT14 En-De, De-En, WMT14 En-De, and En-Fr tasks, and the results show that our proposed PLM enhancement gives significant improvement and even helps achieve new state-of-the-art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1472,
      "original_url": "https://aclanthology.org/2023.findings-acl.223/",
      "title": "TransGEC: Improving Grammatical Error Correction with Translationese",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.223.pdf",
      "authors": [
        "Tao Fang",
        "Xuebo Liu",
        "Derek F. Wong",
        "Runzhe Zhan",
        "Liang Ding",
        "Lidia S. Chao",
        "Dacheng Tao",
        "Min Zhang"
      ],
      "abstract": "Data augmentation is an effective way to improve model performance of grammatical error correction (GEC). This paper identifies a critical side-effect of GEC data augmentation, which is due to the style discrepancy between the data used in GEC tasks (i.e., texts produced by non-native speakers) and data augmentation (i.e., native texts). To alleviate this issue, we propose to use an alternative data source, translationese (i.e., human-translated texts), as input for GEC data augmentation, which 1) is easier to obtain and usually has better quality than non-native texts, and 2) has a more similar style to non-native texts. Experimental results on the CoNLL14 and BEA19 English, NLPCC18 Chinese, Falko-MERLIN German, and RULEC-GEC Russian GEC benchmarks show that our approach consistently improves correction accuracy over strong baselines. Further analyses reveal that our approach is helpful for overcoming mainstream correction difficulties such as the corrections of frequent words, missing words, and substitution errors. Data, code, models and scripts are freely available at https://github.com/NLP2CT/TransGEC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1473,
      "original_url": "https://aclanthology.org/2023.findings-acl.224/",
      "title": "NewsDialogues: Towards Proactive News Grounded Conversation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.224.pdf",
      "authors": [
        "Siheng Li",
        "Yichun Yin",
        "Cheng Yang",
        "Wangjie Jiang",
        "Yiwei Li",
        "Zesen Cheng",
        "Lifeng Shang",
        "Xin Jiang",
        "Qun Liu",
        "Yujiu Yang"
      ],
      "abstract": "Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset NewsDialogues, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a generator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple responses to alleviate the exposure bias. We conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and further present several key findings and challenges to prompt future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1474,
      "original_url": "https://aclanthology.org/2023.findings-acl.225/",
      "title": "Task-aware Retrieval with Instructions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.225.pdf",
      "authors": [
        "Akari Asai",
        "Timo Schick",
        "Patrick Lewis",
        "Xilun Chen",
        "Gautier Izacard",
        "Sebastian Riedel",
        "Hannaneh Hajishirzi",
        "Wen-tau Yih"
      ],
      "abstract": "We study the problem of retrieval with instructions, where users provide explicit descriptions of their intent along with their queries to guide a retrieval system. Our solution is a general-purpose task-aware retrieval system, trained using multi-task instruction tuning and can follow human-written instructions to find relevant documents to a given query. We introduce the first large-scale collection of 37 retrieval datasets with instructions, BERRI, and present TART, a single multi-task retrieval system trained on BERRI with instructions that can adapt to a new task without any parameter updates. TART advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X2-Retrieval, to better reflect real-world scenarios in which diverse domains and tasks are pooled. TART significantly outperforms competitive baselines in this setup, further highlighting the effectiveness of guiding retrieval with instructions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1475,
      "original_url": "https://aclanthology.org/2023.findings-acl.226/",
      "title": "Non-Repeatable Experiments and Non-Reproducible Results: The Reproducibility Crisis in Human Evaluation in NLP",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.226.pdf",
      "authors": [
        "Anya Belz",
        "Craig Thomson",
        "Ehud Reiter",
        "Simon Mille"
      ],
      "abstract": "Human evaluation is widely regarded as the litmus test of quality in NLP. A basic requirementof all evaluations, but in particular where they are used for meta-evaluation, is that they should support the same conclusions if repeated. However, the reproducibility of human evaluations is virtually never queried, let alone formally tested, in NLP which means that their repeatability and the reproducibility of their results is currently an open question. This focused contribution reports our review of human evaluation experiments reported in NLP papers over the past five years which we assessed in terms oftheir ability to be rerun. Overall, we estimatethat just 5% of human evaluations are repeatable in the sense that (i) there are no prohibitivebarriers to repetition, and (ii) sufficient information about experimental design is publicly available for rerunning them. Our estimate goesup to about 20% when author help is sought. We complement this investigation with a survey of results concerning the reproducibilityof human evaluations where those are repeatable in the first place. Here we find worryinglylow degrees of reproducibility, both in terms ofsimilarity of scores and of findings supportedby them. We summarise what insights can begleaned so far regarding how to make humanevaluations in NLP more repeatable and morereproducible."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1476,
      "original_url": "https://aclanthology.org/2023.findings-acl.227/",
      "title": "Define, Evaluate, and Improve Task-Oriented Cognitive Capabilities for Instruction Generation Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.227.pdf",
      "authors": [
        "Lingjun Zhao",
        "Khanh Nguyen",
        "Hal Daumé III"
      ],
      "abstract": "Recent work studies the cognitive capabilities of language models through psychological tests designed for humans. While these studies are helpful for understanding the general capabilities of these models, there is no guarantee that a model possessing sufficient capabilities to pass those tests would actually use those capabilities in performing real-life tasks. In this work, we formulate task-oriented cognitive capabilities, which are human-like cognitive capabilities that language models leverage to perform tasks. These capabilities are (i) the ability to quickly generate good candidate utterances (the search capability) (ii) the ability to predict how a listener interprets those utterances and choose the most appropriate one (the pragmatic capability). We design an evaluation scheme for comparing these capabilities of a language model with those of a human. Applying this scheme to examine various models in a navigation instruction generation problem, we find that their pragmatic capability is severely lacking. This insight leads us to augment them with better models of the listener and obtain a significant boost of 11% in success rate in guiding real humans. Our work advocates for having a principled procedure for aligning language models with humans that involves (i) formulating task-oriented capabilities, (ii) devising a method to quantify their deficiency, and (iii) iteratively improving them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1477,
      "original_url": "https://aclanthology.org/2023.findings-acl.228/",
      "title": "Robustness of Multi-Source MT to Transcription Errors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.228.pdf",
      "authors": [
        "Dominik Macháček",
        "Peter Polák",
        "Ondřej Bojar",
        "Raj Dabre"
      ],
      "abstract": "Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation quality if the sources complement one another in terms of correct information they contain. To this end, we first show that on a 10-hour ESIC corpus, the ASR errors in the original English speech and its simultaneous interpreting into German and Czech are mutually independent. We then use two sources, English and German, in a multi-source setting for translation into Czech to establish its robustness to ASR errors. Furthermore, we observe this robustness when translating both noisy sources together in a simultaneous translation setting. Our results show that multi-source neural machine translation has the potential to be useful in a real-time simultaneous translation setting, thereby motivating further investigation in this area."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1478,
      "original_url": "https://aclanthology.org/2023.findings-acl.229/",
      "title": "Not The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description Mappings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.229.pdf",
      "authors": [
        "Xin Liu",
        "Yuan Tan",
        "Zhenghang Xiao",
        "Jianwei Zhuge",
        "Rui Zhou"
      ],
      "abstract": "As the number of vulnerabilities increases day by day, security management requires more and more structured data. In addition to textual descriptions of vulnerabilities, security engineers must classify and assess vulnerabilities and clarify their associated techniques. Vulnerability Description Mapping (VDM) refers to mapping vulnerabilities to Common Weakness Enumeration (CWE), Common Attack Pattern Enumeration and Classification, ATT&CK Techniques, and other classifications. Accurate VDM is necessary to reduce the pressure of security management and improve the speed of security emergency response. ChatGPT is the latest state-of-the-art closed-source conversational large language model (LLM), which performs excellently on many tasks. This paper explores the application of closed-source LLMs to real-world security management scenarios by evaluating ChatGPT’s performance on VDM tasks. The results show that although ChatGPT may be close to the level of human experts on some tasks, it still cannot replace the critical role of professional security engineers in vulnerability analysis. In a word, closed-source LLM is not the end of story."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1479,
      "original_url": "https://aclanthology.org/2023.findings-acl.230/",
      "title": "Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.230.pdf",
      "authors": [
        "Nikita Moghe",
        "Evgeniia Razumovskaia",
        "Liane Guillou",
        "Ivan Vulić",
        "Anna Korhonen",
        "Alexandra Birch"
      ],
      "abstract": "Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support. These systems are typically constructed for a single domain or language and do not generalise well beyond this. To support work on Natural Language Understanding (NLU) in ToD across multiple languages and domains simultaneously, we constructed Multi3NLU++, a multilingual, multi-intent, multi-domain dataset. Multi3NLU++ extends the English-only NLU++ dataset to include manual translations into a range of high, medium, and low resource languages (Spanish, Marathi, Turkish and Amharic), in two domains (banking and hotels). Because of its multi-intent property, Multi3NLU++ represents complex and natural user goals, and therefore allows us to measure the realistic performance of ToD systems in a varied set of the world’s languages. We use Multi3NLU++ to benchmark state-of-the-art multilingual models for the NLU tasks of intent detection and slot labeling for ToD systems in the multilingual setting. The results demonstrate the challenging nature of the dataset, particularly in the low-resource language setting, offering ample room for future experimentation in multi-domain multilingual ToD setups."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1480,
      "original_url": "https://aclanthology.org/2023.findings-acl.231/",
      "title": "A Robust Information-Masking Approach for Domain Counterfactual Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.231.pdf",
      "authors": [
        "Pengfei Hong",
        "Rishabh Bhardwaj",
        "Navonil Majumder",
        "Somak Aditya",
        "Soujanya Poria"
      ],
      "abstract": "Domain shift is a big challenge in NLP. Many approaches, thus, resort to learning domain-invariant features to mitigate the hurdles of domain shift during inference. Such methods, however, inexorably fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation has recently been proposed that aims to transform a text from the source domain to a given target domain. To achieve this, the existing method uses a frequency-based approach to identify and mask the source-domain-specific tokens in a text. A pretrained LM is then prompted to fill the masks with target-domain-specific tokens. We, however, have observed that, due to limitations of the available data, such a frequency-based method may either miss some domain-token associations or lead to some spurious domain-token associations. To this end, we additionally employ attention norm-based scores to identify additional token-domain associations from a domain classifier. To minimize spurious associations, we also devise an iterative unmasking heuristic that unmasks the masked tokens to minimize the confidence of a domain classifier in the source domain. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer across various classification tasks. The proposed approach outperforms the baselines on 10 out of 12 domain-counterfactual classification settings with an average of 1.7% improvement in accuracy metric."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1481,
      "original_url": "https://aclanthology.org/2023.findings-acl.232/",
      "title": "Misleading Relation Classifiers by Substituting Words in Texts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.232.pdf",
      "authors": [
        "Tian Jiang",
        "Yunqi Liu",
        "Yan Feng",
        "Yuqing Li",
        "Xiaohui Cui"
      ],
      "abstract": "Relation classification is to determine the semantic relationship between two entities in a given sentence. However, many relation classifiers are vulnerable to adversarial attacks, which is using adversarial examples to lead victim models to output wrong results. In this paper, we propose a simple but effective method for misleading relation classifiers. We first analyze the most important parts of speech (POSs) from the syntax and morphology perspectives, then we substitute words labeled with these POS tags in original samples with synonyms or hyponyms. Experimental results show that our method can generate adversarial texts of high quality, and most of the relationships between entities can be correctly identified in the process of human evaluation. Furthermore, the adversarial examples generated by our method possess promising transferability, and they are also helpful for improving the robustness of victim models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1482,
      "original_url": "https://aclanthology.org/2023.findings-acl.233/",
      "title": "Automatic Table Union Search with Tabular Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.233.pdf",
      "authors": [
        "Xuming Hu",
        "Shen Wang",
        "Xiao Qin",
        "Chuan Lei",
        "Zhengyuan Shen",
        "Christos Faloutsos",
        "Asterios Katsifodimos",
        "George Karypis",
        "Lijie Wen",
        "Philip S. Yu"
      ],
      "abstract": "Given a data lake of tabular data as well as a query table, how can we retrieve all the tables in the data lake that can be unioned with the query table? Table union search constitutes an essential task in data discovery and preparation as it enables data scientists to navigate massive open data repositories. Existing methods identify uniability based on column representations (word surface forms or token embeddings) and column relation represented by column representation similarity. However, the semantic similarity obtained between column representations is often insufficient to reveal latent relational features to describe the column relation between pair of columns and not robust to the table noise. To address these issues, in this paper, we propose a multi-stage self-supervised table union search framework called AutoTUS, which represents column relation as a vector– column relational representation and learn column relational representation in a multi-stage manner that can better describe column relation for unionability prediction. In particular, the large language model powered contextualized column relation encoder is updated by adaptive clustering and pseudo label classification iteratively so that the better column relational representation can be learned. Moreover, to improve the robustness of the model against table noises, we propose table noise generator to add table noise to the training table data. Experiments on real-world datasets as well as synthetic test set augmented with table noise show that AutoTUS achieves 5.2% performance gain over the SOTA baseline."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1483,
      "original_url": "https://aclanthology.org/2023.findings-acl.234/",
      "title": "Bidirectional Transformer Reranker for Grammatical Error Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.234.pdf",
      "authors": [
        "Ying Zhang",
        "Hidetaka Kamigaito",
        "Manabu Okumura"
      ],
      "abstract": "Pre-trained seq2seq models have achieved state-of-the-art results in the grammatical error correction task. However, these models still suffer from a prediction bias due to their unidirectional decoding. Thus, we propose a bidirectional Transformer reranker (BTR), that re-estimates the probability of each candidate sentence generated by the pre-trained seq2seq model. The BTR preserves the seq2seq-style Transformer architecture but utilizes a BERT-style self-attention mechanism in the decoder to compute the probability of each target token by using masked language modeling to capture bidirectional representations from the target context. For guiding the reranking, the BTR adopts negative sampling in the objective function to minimize the unlikelihood. During inference, the BTR gives final results after comparing the reranked top-1 results with the original ones by an acceptance threshold. Experimental results show that, in reranking candidates from a pre-trained seq2seq model, T5-base, the BTR on top of T5-base could yield 65.47 and 71.27 F0.5 scores on the CoNLL-14 and BEA test sets, respectively, and yield 59.52 GLEU score on the JFLEG corpus, with improvements of 0.36, 0.76 and 0.48 points compared with the original T5-base. Furthermore, when reranking candidates from T5-large, the BTR on top of T5-base improved the original T5-large by 0.26 points on the BEA test set."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1484,
      "original_url": "https://aclanthology.org/2023.findings-acl.235/",
      "title": "Not Enough Data to Pre-train Your Language Model? MT to the Rescue!",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.235.pdf",
      "authors": [
        "Gorka Urbizu",
        "Iñaki San Vicente",
        "Xabier Saralegi",
        "Ander Corral"
      ],
      "abstract": "In recent years, pre-trained transformer-based language models (LM) have become a key resource for implementing most NLP tasks. However, pre-training such models demands large text collections not available in most languages. In this paper, we study the use of machine-translated corpora for pre-training LMs. We answer the following research questions: RQ1: Is MT-based data an alternative to real data for learning a LM?; RQ2: Can real data be complemented with translated data and improve the resulting LM? In order to validate these two questions, several BERT models for Basque have been trained, combining real data and synthetic data translated from Spanish.The evaluation carried out on 9 NLU tasks indicates that models trained exclusively on translated data offer competitive results. Furthermore, models trained with real data can be improved with synthetic data, although further research is needed on the matter."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1485,
      "original_url": "https://aclanthology.org/2023.findings-acl.236/",
      "title": "UMSE: Unified Multi-scenario Summarization Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.236.pdf",
      "authors": [
        "Shen Gao",
        "Zhitao Yao",
        "Chongyang Tao",
        "Xiuying Chen",
        "Pengjie Ren",
        "Zhaochun Ren",
        "Zhumin Chen"
      ],
      "abstract": "Summarization quality evaluation is a non-trivial task in text summarization. Contemporary methods can be mainly categorized into two scenarios: (1) reference-based: evaluating with human-labeled reference summary; (2) reference-free: evaluating the summary consistency of the document. Recent studies mainly focus on one of these scenarios and explore training neural models built on PLMs to align with human criteria. However, the models from different scenarios are optimized individually, which may result in sub-optimal performance since they neglect the shared knowledge across different scenarios. Besides, designing individual models for each scenario caused inconvenience to the user. Inspired by this, we propose Unified Multi-scenario Summarization Evaluation Model (UMSE). More specifically, we propose a perturbed prefix tuning method to share cross-scenario knowledge between scenarios and use a self-supervised training paradigm to optimize the model without extra human labeling. Our UMSE is the first unified summarization evaluation framework engaged with the ability to be used in three evaluation scenarios. Experimental results across three typical scenarios on the benchmark dataset SummEval indicate that our UMSE can achieve comparable performance with several existing strong methods which are specifically designed for each scenario."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1486,
      "original_url": "https://aclanthology.org/2023.findings-acl.237/",
      "title": "Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.237.pdf",
      "authors": [
        "Zhengxiao Liu",
        "Bowen Shen",
        "Zheng Lin",
        "Fali Wang",
        "Weiping Wang"
      ],
      "abstract": "Pre-trained language model (PLM) can be stealthily misled to target outputs by backdoor attacks when encountering poisoned samples, without performance degradation on clean samples. The stealthiness of backdoor attacks is commonly attained through minimal cross-entropy loss fine-tuning on a union of poisoned and clean samples. Existing defense paradigms provide a workaround by detecting and removing poisoned samples at pre-training or inference time. On the contrary, we provide a new perspective where the backdoor attack is directly reversed. Specifically, maximum entropy loss is incorporated in training to neutralize the minimal cross-entropy loss fine-tuning on poisoned data. We defend against a range of backdoor attacks on classification tasks and significantly lower the attack success rate. In extension, we explore the relationship between intended backdoor attacks and unintended dataset bias, and demonstrate the feasibility of the maximum entropy principle in de-biasing."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1487,
      "original_url": "https://aclanthology.org/2023.findings-acl.238/",
      "title": "Improving Named Entity Recognition via Bridge-based Domain Adaptation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.238.pdf",
      "authors": [
        "Jingyun Xu",
        "Changmeng Zheng",
        "Yi Cai",
        "Tat-Seng Chua"
      ],
      "abstract": "Recent studies have shown remarkable success in cross-domain named entity recognition (cross-domain NER). Despite the promising results, existing methods mainly utilize pre-training language models like BERT to represent words. As such, the original chaotic representations may challenge them to distinguish entity types of entities, leading to entity type misclassification. To this end, we attempt to utilize contrastive learning to refine the original representations and propose a model-agnostic framework named MoCL for cross-domain NER. Additionally, we respectively combine MoCL with two distinctive cross-domain NER methods and two pre-training language models to explore its generalization ability. Empirical results on seven domains show the effectiveness and good generalization ability of MoCL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1488,
      "original_url": "https://aclanthology.org/2023.findings-acl.239/",
      "title": "SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.239.pdf",
      "authors": [
        "Shuzheng Si",
        "Zefan Cai",
        "Shuang Zeng",
        "Guoqiang Feng",
        "Jiaxing Lin",
        "Baobao Chang"
      ],
      "abstract": "Distantly-Supervised Named Entity Recognition effectively alleviates the burden of time-consuming and expensive annotation in the supervised setting. But the context-free matching process and the limited coverage of knowledge bases introduce inaccurate and incomplete annotation noise respectively. Previous studies either considered only incomplete one or indiscriminately handle two types of noise with the same strategy. In this paper, we argue that the different causes of two types of noise bring up the requirement of different strategies in model architecture. Therefore, we propose the SANTA to handle these two types of noise separately with (1) Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate decision boundary shifting problem caused by incomplete annotation and a noise-tolerant loss to improve the model’s robustness. Benefiting from our separate tailored strategies, we confirm in the experiment that the two types of noise are well mitigated.SANTA also achieves a new state-of-the-art on five public datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1489,
      "original_url": "https://aclanthology.org/2023.findings-acl.240/",
      "title": "The State of Profanity Obfuscation in Natural Language Processing Scientific Publications",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.240.pdf",
      "authors": [
        "Debora Nozza",
        "Dirk Hovy"
      ],
      "abstract": "Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers and increases its internet frequency. While maintaining publications’ professional appearance, obfuscating profanities makes it challenging to evaluate the content, especially for non-native speakers. Surveying 150 ACL papers, we discovered that obfuscation is usually used for English but not other languages, and even then, quite unevenly. We discuss the problems with obfuscation and suggest a multilingual community resource called PrOf with a Python module to standardize profanity obfuscation processes. We believe PrOf can help scientific publication policies to make hate speech work accessible and comparable, irrespective of language."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1490,
      "original_url": "https://aclanthology.org/2023.findings-acl.241/",
      "title": "Teacher and Student Models of Offensive Language in Social Media",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.241.pdf",
      "authors": [
        "Tharindu Ranasinghe",
        "Marcos Zampieri"
      ],
      "abstract": "State-of-the-art approaches to identifying offensive language online make use of large pre-trained transformer models. However, the inference time, disk, and memory requirements of these transformer models present challenges for their wide usage in the real world. Even the distilled transformer models remain prohibitively large for many usage scenarios. To cope with these challenges, in this paper, we propose transferring knowledge from transformer models to much smaller neural models to make predictions at the token- and at the post-level. We show that this approach leads to lightweight offensive language identification models that perform on par with large transformers but with 100 times fewer parameters and much less memory usage"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1491,
      "original_url": "https://aclanthology.org/2023.findings-acl.242/",
      "title": "A Simple Yet Strong Domain-Agnostic De-bias Method for Zero-Shot Sentiment Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.242.pdf",
      "authors": [
        "Yang Zhao",
        "Tetsuya Nasukawa",
        "Masayasu Muraoka",
        "Bishwaranjan Bhattacharjee"
      ],
      "abstract": "Zero-shot prompt-based learning has made much progress in sentiment analysis, and considerable effort has been dedicated to designing high-performing prompt templates. However, two problems exist; First, large language models are often biased to their pre-training data, leading to poor performance in prompt templates that models have rarely seen. Second, in order to adapt to different domains, re-designing prompt templates is usually required, which is time-consuming and inefficient. To remedy both shortcomings, we propose a simple yet strong data construction method to de-bias a given prompt template, yielding a large performance improvement in sentiment analysis tasks across different domains, pre-trained language models, and prompt templates. Also, we demonstrate the advantage of using domain-agnostic generic responses over the in-domain ground-truth data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1492,
      "original_url": "https://aclanthology.org/2023.findings-acl.243/",
      "title": "Balancing the Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.243.pdf",
      "authors": [
        "Debarati Das",
        "David Ma",
        "Dongyeop Kang"
      ],
      "abstract": "Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the considered stylistic attributes, adding to the challenges of training a style transfer model. This paper explores the impact of training data input diversity on the quality of the generated text from the multi-style transfer model. We construct a pseudo-parallel dataset by devising heuristics to adjust the style distribution in the training samples. We balance our training dataset using marginal and joint distributions to train our style transfer models. We observe that a balanced dataset produces more effective control effects over multiple styles than an imbalanced or skewed one. Through quantitative analysis, we explore the impact of multiple style distributions in training data on style-transferred output. These findings will better inform the design of style-transfer datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1493,
      "original_url": "https://aclanthology.org/2023.findings-acl.244/",
      "title": "A Benchmark on Extremely Weakly Supervised Text Classification: Reconcile Seed Matching and Prompting Approaches",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.244.pdf",
      "authors": [
        "Zihan Wang",
        "Tianle Wang",
        "Dheeraj Mekala",
        "Jingbo Shang"
      ],
      "abstract": "Extremely Weakly Supervised Text Classification (XWS-TC) refers to text classification based on minimal high-level human guidance, such as a few label-indicative seed words or classification instructions. There are two mainstream approaches for XWS-TC, however, never being rigorously compared: (1) training classifiers based on pseudo-labels generated by (softly) matching seed words (Seed) and (2) prompting (and calibrating) language models using classification instruction (and raw texts) to decode label words (Prompt). This paper presents the first XWS-TC benchmark to compare the two approaches on fair grounds, where the datasets, supervisions, and hyperparameter choices are standardized across methods. Our benchmarking results suggest that (1) Both Seed and Prompt approaches are competitive and there is no clear winner; (2) Seed is empirically more tolerant than Prompt to human guidance (e.g., seed words, classification instructions, and label words) changes; (3) Seed is empirically more selective than Prompt to the pre-trained language models; (4) Recent Seed and Prompt methods have close connections and a clustering post-processing step based on raw in-domain texts is a strong performance booster to both. We hope this benchmark serves as a guideline in selecting XWS-TC methods in different scenarios and stimulate interest in developing guidance- and model-robust XWS-TC methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1494,
      "original_url": "https://aclanthology.org/2023.findings-acl.245/",
      "title": "Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.245.pdf",
      "authors": [
        "Zhu Liu",
        "Ying Liu"
      ],
      "abstract": "Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty estimation (UE) in the real-world setting, where the data is always noisy and out of distribution. This paper extensively studies UE on the benchmark designed for WSD. Specifically, we first compare four uncertainty scores for a state-of-the-art WSD model and verify that the conventional predictive probabilities obtained at the end of the model are inadequate to quantify uncertainty. Then, we examine the capability of capturing data and model uncertainties by the model with the selected UE score on well-designed test scenarios and discover that the model reflects data uncertainty satisfactorily but underestimates model uncertainty. Furthermore, we explore numerous lexical properties that intrinsically affect data uncertainty and provide a detailed analysis of four critical aspects: the syntactic category, morphology, sense granularity, and semantic relations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1495,
      "original_url": "https://aclanthology.org/2023.findings-acl.246/",
      "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.246.pdf",
      "authors": [
        "Zhenhailong Wang",
        "Xiaoman Pan",
        "Dian Yu",
        "Dong Yu",
        "Jianshu Chen",
        "Heng Ji"
      ],
      "abstract": "Although large language models have exhibited impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with retrieved related background knowledge, alleviate the need for storing everything into the model parameters. Although existing semi-parametric language models have demonstrated promising language modeling capabilities, it remains unclear whether they can exhibit competitive zero-shot abilities as their fully-parametric counterparts. In this work, we introduce Zemi, a semi-parametric language model for zero-shot task generalization. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train Zemi with semi-parametric multitask training, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, during both training and inference, Zemi is equipped with a retrieval system based on the unlabeled pretraining corpus of our backbone model. To address the unique challenges from large-scale retrieval, we further propose a novel retrieval-augmentation fusion module that can effectively incorporate noisy retrieved documents. Finally, we show detailed analysis and ablation studies on the key ingredients towards building effective zero-shot semi-parametric language models. Notably, our proposed Zemi_Large model outperforms T0-3B by 16% across seven diverse evaluation tasks while being 3.8x smaller in scale."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1496,
      "original_url": "https://aclanthology.org/2023.findings-acl.247/",
      "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.247.pdf",
      "authors": [
        "Damai Dai",
        "Yutao Sun",
        "Li Dong",
        "Yaru Hao",
        "Shuming Ma",
        "Zhifang Sui",
        "Furu Wei"
      ],
      "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at https://aka.ms/icl."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1497,
      "original_url": "https://aclanthology.org/2023.findings-acl.248/",
      "title": "Dramatic Conversation Disentanglement",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.248.pdf",
      "authors": [
        "Kent Chang",
        "Danica Chen",
        "David Bamman"
      ],
      "abstract": "We present a new dataset for studying conversation disentanglement in movies and TV series. While previous work has focused on conversation disentanglement in IRC chatroom dialogues, movies and TV shows provide a space for studying complex pragmatic patterns of floor and topic change in face-to-face multi-party interactions. In this work, we draw on theoretical research in sociolinguistics, sociology, and film studies to operationalize a conversational thread (including the notion of a floor change) in dramatic texts, and use that definition to annotate a dataset of 10,033 dialogue turns (comprising 2,209 threads) from 831 movies. We compare the performance of several disentanglement models on this dramatic dataset, and apply the best-performing model to disentangle 808 movies. We see that, contrary to expectation, average thread lengths do not decrease significantly over the past 40 years, and characters portrayed by actors who are women, while underrepresented, initiate more new conversational threads relative to their speaking time."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1498,
      "original_url": "https://aclanthology.org/2023.findings-acl.249/",
      "title": "Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.249.pdf",
      "authors": [
        "Yongil Kim",
        "Yerin Hwang",
        "Joongbo Shin",
        "Hyunkyung Bae",
        "Kyomin Jung"
      ],
      "abstract": "In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select and present to the user. Existing TOD systems handle this ambiguity by randomly selecting one or few results and presenting their names to the user. However, in a real scenario, users do not always accept a randomly recommended entity, and users should have access to more comprehensive information about the search results. To address this limitation, we propose a novel task called Comparison-Based database search Ambiguity handling (CBA), which handles ambiguity in database search results by comparing the properties of multiple entities to enable users to choose according to their preferences. Accordingly, we introduce a new framework for automatically collecting high-quality dialogue data along with the Disambiguating Schema-guided Dialogue (DSD) dataset, an augmented version of the SGD dataset. Experimental studies on the DSD dataset demonstrate that training baseline models with the dataset effectively address the CBA task. Our dataset and code will be publicized."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1499,
      "original_url": "https://aclanthology.org/2023.findings-acl.250/",
      "title": "Emergent Modularity in Pre-trained Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.250.pdf",
      "authors": [
        "Zhengyan Zhang",
        "Zhiyuan Zeng",
        "Yankai Lin",
        "Chaojun Xiao",
        "Xiaozhi Wang",
        "Xu Han",
        "Zhiyuan Liu",
        "Ruobing Xie",
        "Maosong Sun",
        "Jie Zhou"
      ],
      "abstract": "This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore to find a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformer first constructs the modular structure and then learns fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1500,
      "original_url": "https://aclanthology.org/2023.findings-acl.251/",
      "title": "Universal Information Extraction with Meta-Pretrained Self-Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.251.pdf",
      "authors": [
        "Xin Cong",
        "Bowen Yu",
        "Mengcheng Fang",
        "Tingwen Liu",
        "Haiyang Yu",
        "Zhongkai Hu",
        "Fei Huang",
        "Yongbin Li",
        "Bin Wang"
      ],
      "abstract": "Universal Information Extraction (Universal IE) aims to solve different extraction tasks in a uniform text-to-structure generation manner. Such a generation procedure tends to struggle when there exist complex information structures to be extracted. Retrieving knowledge from external knowledge bases may help models to overcome this problem but it is impossible to construct a knowledge base suitable for various IE tasks. Inspired by the fact that large amount of knowledge are stored in the pretrained language models (PLM) and can be retrieved explicitly, in this paper, we propose MetaRetriever to retrieve task-specific knowledge from PLMs to enhance universal IE. As different IE tasks need different knowledge, we further propose a Meta-Pretraining Algorithm which allows MetaRetriever to quicktly achieve maximum task-specific retrieval performance when fine-tuning on downstream IE tasks. Experimental results show that MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets under fully-supervised, low-resource and few-shot scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1501,
      "original_url": "https://aclanthology.org/2023.findings-acl.252/",
      "title": "SETI: Systematicity Evaluation of Textual Inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.252.pdf",
      "authors": [
        "Xiyan Fu",
        "Anette Frank"
      ],
      "abstract": "We propose SETI (Systematicity Evaluation of Textual Inference), a novel and comprehensive benchmark designed for evaluating pre-trained language models (PLMs) for their systematicity capabilities in the domain of textual inference. Specifically, SETI offers three different NLI tasks and corresponding datasets to evaluate various types of systematicity in reasoning processes. In order to solve these tasks, models are required to perform compositional inference based on known primitive constituents. We conduct experiments of SETI on six widely used PLMs. Results show that various PLMs are able to solve unseen compositional inferences when having encountered the knowledge of how to combine primitives, with good performance. However, they are considerably limited when this knowledge is unknown to the model (40-100 % points decrease). Furthermore, we find that PLMs are able to improve dramatically once exposed to crucial compositional knowledge in minimalistic shots. These findings position SETI as the first benchmark for measuring the future progress of PLMs in achieving systematicity generalization in the textual inference."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1502,
      "original_url": "https://aclanthology.org/2023.findings-acl.253/",
      "title": "Coarse-to-fine Few-shot Learning for Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.253.pdf",
      "authors": [
        "Ruotian Ma",
        "Zhang Lin",
        "Xuanting Chen",
        "Xin Zhou",
        "Junzhe Wang",
        "Tao Gui",
        "Qi Zhang",
        "Xiang Gao",
        "Yun Wen Chen"
      ],
      "abstract": "Recently, Few-shot Named Entity Recognition has received wide attention with the growing need for NER models to learn new classes with minimized annotation costs. However, one common yet understudied situation is to transfer a model trained with coarse-grained classes to recognize fine-grained classes, such as separating a product category into sub-classes. We find that existing few-shot NER solutions are not suitable for such a situation since they do not consider the sub-class discrimination during coarse training and various granularity of new classes during few-shot learning. In this work, we introduce the Coarse-to-fine Few-shot NER (C2FNER) task and propose an effective solution. Specifically, during coarse training, we propose a cluster-based prototype margin loss to learn group-wise discriminative representations, so as to benefit fine-grained learning. Targeting various granularity of new classes, we separate the coarse classes into extra-fine clusters and propose a novel prototype retrieval and bootstrapping algorithm to retrieve representative clusters for each fine class. We then adopt a mixture prototype loss to efficiently learn the representations of fine classes. We conduct experiments on both in-domain and cross-domain C2FNER settings with various target granularity, and the proposed method shows superior performance over the baseline methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1503,
      "original_url": "https://aclanthology.org/2023.findings-acl.254/",
      "title": "Self-Evolution Learning for Discriminative Language Model Pretraining",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.254.pdf",
      "authors": [
        "Qihuang Zhong",
        "Liang Ding",
        "Juhua Liu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "abstract": "Masked language modeling, widely used in discriminative language model (e.g., BERT) pretraining, commonly adopts a random masking strategy. However, random masking does not consider the importance of the different words in the sentence meaning, where some of them are more worthy to be predicted. Therefore, various masking strategies (e.g., entity-level masking) are proposed, but most of them require expensive prior knowledge and generally train from scratch without reusing existing model weights. In this paper, we present Self-Evolution learning (SE), a simple and effective token masking and learning method to fully and wisely exploit the knowledge from data. SE focuses on learning the informative yet under-explored tokens and adaptively regularizes the training by introducing a novel Token-specific Label Smoothing approach. Experiments on 10 tasks show that our SE brings consistent and significant improvements (+1.43 2.12 average scores) upon different PLMs. In-depth analyses demonstrate that SE improves linguistic knowledge learning and generalization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1504,
      "original_url": "https://aclanthology.org/2023.findings-acl.255/",
      "title": "QueryForm: A Simple Zero-shot Form Entity Query Framework",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.255.pdf",
      "authors": [
        "Zifeng Wang",
        "Zizhao Zhang",
        "Jacob Devlin",
        "Chen-Yu Lee",
        "Guolong Su",
        "Hao Zhang",
        "Jennifer Dy",
        "Vincent Perot",
        "Tomas Pfister"
      ],
      "abstract": "Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6% 10.1%) and the Payment (+3.2% 9.5%) zero-shot benchmark, with a smaller model size and no additional image input."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1505,
      "original_url": "https://aclanthology.org/2023.findings-acl.256/",
      "title": "Search-Oriented Conversational Query Editing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.256.pdf",
      "authors": [
        "Kelong Mao",
        "Zhicheng Dou",
        "Bang Liu",
        "Hongjin Qian",
        "Fengran Mo",
        "Xiangli Wu",
        "Xiaohua Cheng",
        "Zhao Cao"
      ],
      "abstract": "Conversational query rewriting (CQR) realizes conversational search by reformulating the search dialogue into a standalone rewrite. However, existing CQR models either are not learned toward improving the downstream search performance or inefficiently generate the rewrite token-by-token from scratch while neglecting the fact that the search dialogue often has a large overlap with the rewrite. In this paper, we propose EdiRCS, a new text editing-based CQR model tailored for conversational search. In EdiRCS, most of the rewrite tokens are selected from the dialogue in a non-autoregressive fashion and only a few new tokens are generated to supplement the final rewrite, which makes EdiRCS highly efficient. In particular, the learning of EdiRCS is augmented with two search-oriented objectives, including contrastive ranking augmentation and contextualization knowledge transfer, which effectively improve it to select and generate more useful tokens from the view of retrieval. We show that EdiRCS outperforms state-of-the-art CQR models on three conversational search benchmarks while having low rewriting latency, and is robust to out-of-domain search dialogues and long dialogue contexts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1506,
      "original_url": "https://aclanthology.org/2023.findings-acl.257/",
      "title": "TAPIR: Learning Adaptive Revision for Incremental Natural Language Understanding with a Two-Pass Model",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.257.pdf",
      "authors": [
        "Patrick Kahardipraja",
        "Brielen Madureira",
        "David Schlangen"
      ],
      "abstract": "Language is by its very nature incremental in how it is produced and processed. This property can be exploited by NLP systems to produce fast responses, which has been shown to be beneficial for real-time interactive applications. Recent neural network-based approaches for incremental processing mainly use RNNs or Transformers. RNNs are fast but monotonic (cannot correct earlier output, which can be necessary in incremental processing). Transformers, on the other hand, consume whole sequences, and hence are by nature non-incremental. A restart-incremental interface that repeatedly passes longer input prefixes can be used to obtain partial outputs, while providing the ability to revise. However, this method becomes costly as the sentence grows longer. In this work, we propose the Two-pass model for AdaPtIve Revision (TAPIR) and introduce a method to obtain an incremental supervision signal for learning an adaptive revision policy. Experimental results on sequence labelling show that our model has better incremental performance and faster inference speed compared to restart-incremental Transformers, while showing little degradation on full sequences."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1507,
      "original_url": "https://aclanthology.org/2023.findings-acl.258/",
      "title": "Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.258.pdf",
      "authors": [
        "Ece Takmaz",
        "Nicolo’ Brandizzi",
        "Mario Giulianelli",
        "Sandro Pezzelle",
        "Raquel Fernandez"
      ],
      "abstract": "Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this paper, we model a visually grounded referential game between a knowledgeable speaker and a listener with more limited visual and linguistic experience. Inspired by psycholinguistic theories, we endow our speaker with the ability to adapt its referring expressions via a simulation module that monitors the effectiveness of planned utterances from the listener’s perspective. We propose an adaptation mechanism building on plug-and-play approaches to controlled language generation, where utterance generation is steered on the fly by the simulator without finetuning the speaker’s underlying language model. Our results and analyses show that our approach is effective: the speaker’s utterances become closer to the listener’s domain of expertise, which leads to higher communicative success."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1508,
      "original_url": "https://aclanthology.org/2023.findings-acl.259/",
      "title": "A Semi-Autoregressive Graph Generative Model for Dependency Graph Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.259.pdf",
      "authors": [
        "Ye Ma",
        "Mingming Sun",
        "Ping Li"
      ],
      "abstract": "Recent years have witnessed the impressive progress in Neural Dependency Parsing. According to the different factorization approaches to the graph joint probabilities, existing parsers can be roughly divided into autoregressive and non-autoregressive patterns. The former means that the graph should be factorized into multiple sequentially dependent components, then it can be built up component by component. And the latter assumes these components to be independent so that they can be outputted in a one-shot manner. However, when treating the directed edge as an explicit dependency relationship, we discover that there is a mixture of independent and interdependent components in the dependency graph, signifying that both aforementioned models fail to precisely capture the explicit dependencies among nodes and edges. Based on this property, we design a Semi-Autoregressive Dependency Parser to generate dependency graphs via adding node groups and edge groups autoregressively while pouring out all group elements in parallel. The model gains a trade-off between non-autoregression and autoregression, which respectively suffer from the lack of target inter-dependencies and the uncertainty of graph generation orders. The experiments show the proposed parser outperforms strong baselines on Enhanced Universal Dependencies of multiple languages, especially achieving 4% average promotion at graph-level accuracy. Also, the performances of model variations show the importance of specific parts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1509,
      "original_url": "https://aclanthology.org/2023.findings-acl.260/",
      "title": "AMR-TST: Abstract Meaning Representation-based Text Style Transfer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.260.pdf",
      "authors": [
        "Kaize Shi",
        "Xueyao Sun",
        "Li He",
        "Dingxian Wang",
        "Qing Li",
        "Guandong Xu"
      ],
      "abstract": "Abstract Meaning Representation (AMR) is a semantic representation that can enhance natural language generation (NLG) by providing a logical semantic input. In this paper, we propose the AMR-TST, an AMR-based text style transfer (TST) technique. The AMR-TST converts the source text to an AMR graph and generates the transferred text based on the AMR graph modified by a TST policy named style rewriting. Our method combines both the explainability and diversity of explicit and implicit TST methods. The experiments show that the proposed method achieves state-of-the-art results compared with other baseline models in automatic and human evaluations. The generated transferred text in qualitative evaluation proves the AMR-TST have significant advantages in keeping semantic features and reducing hallucinations. To the best of our knowledge, this work is the first to apply the AMR method focusing on node-level features to the TST task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1510,
      "original_url": "https://aclanthology.org/2023.findings-acl.261/",
      "title": "Understanding the Cooking Process with English Recipe Text",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.261.pdf",
      "authors": [
        "Yi Fan",
        "Anthony Hunter"
      ],
      "abstract": "Translating procedural text, like recipes, into a graphical representation can be important for visualizing the text, and can offer a machine-readable formalism for use in software. There are proposals for translating recipes into a flow graph representation, where each node represents an ingredient, action, location, or equipment, and each arc between the nodes denotes the steps of the recipe. However, these proposals have had performance problems with both named entity recognition and relationship extraction. To address these problems, we propose a novel framework comprising two modules to construct a flow graph from the input recipe. The first module identifies the named entities in the input recipe text using BERT, Bi-LSTM and CRF, and the second module uses BERT to predict the relationships between the entities. We evaluate our framework on the English recipe flow graph corpus. Our framework can predict the edge label and achieve the overall F1 score of 92.2, while the baseline F1 score is 43.3 without the edge label predicted."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1511,
      "original_url": "https://aclanthology.org/2023.findings-acl.262/",
      "title": "Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.262.pdf",
      "authors": [
        "Mirac Suzgun",
        "Luke Melas-Kyriazi",
        "Dan Jurafsky"
      ],
      "abstract": "In open-ended natural-language generation, existing text decoding methods typically struggle to produce text which is both diverse and high-quality. Greedy and beam search are known to suffer from text degeneration and linguistic diversity issues, while temperature, top-k, and nucleus sampling yield diverse but often lower-quality outputs. In this work, we build upon Minimum Bayes Risk Decoding (MBRD), a family of decoding methods based on Bayesian risk minimization, to address this diversity-quality trade-off. Inspired by the principle of the wisdom of the crowd, MBRD seeks to select a candidate from a pool of candidates that has the least expected risk under a generative model according to a given utility function. The crowd of candidates serves as an approximation for the distribution over human-generated references. We show that MBRD generalizes numerous decoding methods, including majority voting, and can be used as a drop-in replacement for existing sampling methods. Across a wide range of tasks—such as summarization, data-to-text, translation, and textual style transfer—MBRD yields 3-7 ROUGE and BLEU point improvements, including state-of-the-art results on WebNLG and WMT’16."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1512,
      "original_url": "https://aclanthology.org/2023.findings-acl.263/",
      "title": "RobustQA: Benchmarking the Robustness of Domain Adaptation for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.263.pdf",
      "authors": [
        "Rujun Han",
        "Peng Qi",
        "Yuhao Zhang",
        "Lan Liu",
        "Juliette Burger",
        "William Yang Wang",
        "Zhiheng Huang",
        "Bing Xiang",
        "Dan Roth"
      ],
      "abstract": "Open-domain question answering (ODQA) is a crucial task in natural language processing. A typical ODQA system relies on a retriever module to select relevant contexts from a large corpus for a downstream reading comprehension model. Existing ODQA datasets consist mainly of Wikipedia corpus, and are insufficient to study models’ generalizability across diverse domains as models are trained and evaluated on the same genre of data. We propose **RobustQA**, a novel benchmark consisting of datasets from 8 different domains, which facilitates the evaluation of ODQA’s domain robustness. To build **RobustQA**, we annotate QA pairs in retrieval datasets with rigorous quality control. We further examine improving QA performances by incorporating unsupervised learning methods with target-domain corpus and adopting large generative language models. These methods can effectively improve model performances on **RobustQA**. However, experimental results demonstrate a significant gap from in-domain training, suggesting that **RobustQA** is a challenging benchmark to evaluate ODQA domain robustness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1513,
      "original_url": "https://aclanthology.org/2023.findings-acl.264/",
      "title": "SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.264.pdf",
      "authors": [
        "Victoria Lin",
        "Louis-Philippe Morency"
      ],
      "abstract": "Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model’s decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1514,
      "original_url": "https://aclanthology.org/2023.findings-acl.265/",
      "title": "Reinforcement Learning for Topic Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.265.pdf",
      "authors": [
        "Jeremy Costello",
        "Marek Reformat"
      ],
      "abstract": "We apply reinforcement learning techniques to topic modeling by replacing the variational autoencoder in ProdLDA with a continuous action space reinforcement learning policy. We train the system with a policy gradient algorithm REINFORCE. Additionally, we introduced several modifications: modernize the neural network architecture, weight the ELBO loss, use contextual embeddings, and monitor the learning process via computing topic diversity and coherence for each training step. Experiments areperformed on 11 data sets. Our unsupervised model outperforms all other unsupervised models and performs on par with or better than most models using supervised labeling. Our model is outperformed on certain data sets by a model using supervised labeling and contrastive learning. We have also conducted an ablation study to provide empirical evidence of performance improvements from changes we made to ProdLDA and found that the reinforcement learning formulation boosts performance. We open-source our code implementation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1515,
      "original_url": "https://aclanthology.org/2023.findings-acl.266/",
      "title": "Contextualized Soft Prompts for Extraction of Event Arguments",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.266.pdf",
      "authors": [
        "Chien Nguyen",
        "Hieu Man",
        "Thien Nguyen"
      ],
      "abstract": "Event argument extraction (EAE) is a sub-task of event extraction where the goal is to identify roles of entity mentions for events in text. The current state-of-the-art approaches for this problem explore prompt-based methods to prompt pre-trained language models for arguments over input context. However, existing prompt-based methods mainly rely on discrete and manually-designed prompts that cannot exploit specific context for each example to improve customization for optimal performance. In addition, the discrete nature of current prompts prevents the incorporation of relevant context from multiple external documents to enrich prompts for EAE. To this end, we propose a novel prompt-based method for EAE that introduces soft prompts to facilitate the encoding of individual example context and multiple relevant documents to boost EAE. We extensively evaluate the proposed method on benchmark datasets for EAE to demonstrate its benefits with state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1516,
      "original_url": "https://aclanthology.org/2023.findings-acl.267/",
      "title": "TextVerifier: Robustness Verification for Textual Classifiers with Certifiable Guarantees",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.267.pdf",
      "authors": [
        "Siqi Sun",
        "Wenjie Ruan"
      ],
      "abstract": "When textual classifiers are deployed in safety-critical workflows, they must withstand the onslaught of AI-enabled model confusion caused by adversarial examples with minor alterations. In this paper, the main objective is to provide a formal verification framework, called TextVerifier, with certifiable guarantees on deep neural networks in natural language processing against word-level alteration attacks. We aim to provide an approximation of the maximal safe radius by deriving provable bounds both mathematically and automatically, where a minimum word-level L_0 distance is quantified as a guarantee for the classification invariance of victim models. Here, we illustrate three strengths of our strategy: i) certifiable guarantee: effective verification with convergence to ensure approximation of maximal safe radius with tight bounds ultimately; ii) high-efficiency: it yields an efficient speed edge by a novel parallelization strategy that can process a set of candidate texts simultaneously on GPUs; and iii) reliable anytime estimation: the verification can return intermediate bounds, and robustness estimates that are gradually, but strictly, improved as the computation proceeds. Furthermore, experiments are conducted on text classification on four datasets over three victim models to demonstrate the validity of tightening bounds. Our tool TextVerifier is available at https://github.com/TrustAI/TextVerifer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1517,
      "original_url": "https://aclanthology.org/2023.findings-acl.268/",
      "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.268.pdf",
      "authors": [
        "Xianjun Yang",
        "Kaiqiang Song",
        "Sangwoo Cho",
        "Xiaoyang Wang",
        "Xiaoman Pan",
        "Linda Petzold",
        "Dong Yu"
      ],
      "abstract": "Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users’ interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, on a relatively small scale, or contains only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1518,
      "original_url": "https://aclanthology.org/2023.findings-acl.269/",
      "title": "On the Limitations of Simulating Active Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.269.pdf",
      "authors": [
        "Katerina Margatina",
        "Nikolaos Aletras"
      ],
      "abstract": "Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve data efficiency over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question “Why do Active Learning algorithms sometimes fail to outperform random sampling?”. We argue that evaluating AL algorithms on available labeled datasets might provide a lower bound as to their effectiveness in real data. We believe it is essential to collectively shape the best practices for AL research, especially now that the stellar engineering advances (e.g. ChatGPT) shift the research focus to data-driven approaches. To this end, we present guidelines for future work, hoping that by bringing these limitations to the community’s attention, we can explore ways to address them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1519,
      "original_url": "https://aclanthology.org/2023.findings-acl.270/",
      "title": "Towards Alleviating the Object Bias in Prompt Tuning-based Factual Knowledge Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.270.pdf",
      "authors": [
        "Yuhang Wang",
        "Dongyuan Lu",
        "Chao Kong",
        "Jitao Sang"
      ],
      "abstract": "Many works employed prompt tuning methods to automatically optimize prompt queries and extract the factual knowledge stored in Pre-trained Language Models. In this paper, we observe that the optimized prompts, including discrete prompts and continuous prompts, exhibit undesirable object bias. To handle this problem, we propose a novel prompt tuning method called MeCoD consisting of three modules: Prompt Encoder, Object Equalization and Biased Object Obstruction. Experimental results show that MeCoD can significantly reduce the object bias and at the same time improve accuracy of factual knowledge extraction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1520,
      "original_url": "https://aclanthology.org/2023.findings-acl.271/",
      "title": "vONTSS: vMF based semi-supervised neural topic modeling with optimal transport",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.271.pdf",
      "authors": [
        "Weijie Xu",
        "Xiaoyu Jiang",
        "Srinivasan Sengamedu Hanumantha Rao",
        "Francis Iannacci",
        "Jinjin Zhao"
      ],
      "abstract": "Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1521,
      "original_url": "https://aclanthology.org/2023.findings-acl.272/",
      "title": "Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.272.pdf",
      "authors": [
        "Seraphina Goldfarb-Tarrant",
        "Adam Lopez",
        "Roi Blanco",
        "Diego Marcheggiani"
      ],
      "abstract": "Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. We demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: What biases do systems import from pre-trained models when compared to a baseline with no pre-training? Our evaluation corpus, by virtue of being counterfactual, not only reveals which models have less bias, but also pinpoints changes in model bias behaviour, which enables more targeted mitigation strategies. We release our code and evaluation corpora to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1522,
      "original_url": "https://aclanthology.org/2023.findings-acl.273/",
      "title": "Complementary Explanations for Effective In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.273.pdf",
      "authors": [
        "Xi Ye",
        "Srinivasan Iyer",
        "Asli Celikyilmaz",
        "Veselin Stoyanov",
        "Greg Durrett",
        "Ramakanth Pasunuru"
      ],
      "abstract": "Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two dif- ferent factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By per- turbing explanations on three controlled tasks, we show that both factors contribute to the ef- fectiveness of explanations. We further study how to form maximally effective sets of expla- nations for solving a given test query. We find that LLMs can benefit from the complemen- tarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as comple- mentary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1523,
      "original_url": "https://aclanthology.org/2023.findings-acl.274/",
      "title": "MISMATCH: Fine-grained Evaluation of Machine-generated Text with Mismatch Error Types",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.274.pdf",
      "authors": [
        "Keerthiram Murugesan",
        "Sarathkrishna Swaminathan",
        "Soham Dan",
        "Subhajit Chaudhury",
        "Chulaka Gunasekara",
        "Maxwell Crouse",
        "Diwakar Mahajan",
        "Ibrahim Abdelaziz",
        "Achille Fokoue",
        "Pavan Kapanipathi",
        "Salim Roukos",
        "Alexander Gray"
      ],
      "abstract": "With the growing interest in large language models, the need for evaluating the quality of machine text compared to reference (typically human-generated) text has become focal attention. Most recent works focus either on task-specific evaluation metrics or study the properties of machine-generated text captured by the existing metrics. In this work, we propose a new evaluation scheme to model human judgments in 7 NLP tasks, based on the fine-grained mismatches between a pair of texts. Inspired by the recent efforts in several NLP tasks for fine-grained evaluation, we introduce a set of 13 mismatch error types such as spatial/geographic errors, entity errors, etc, to guide the model for better prediction of human judgments. We propose a neural framework for evaluating machine texts that uses these mismatch error types as auxiliary tasks and re-purposes the existing single-number evaluation metrics as additional scalar features, in addition to textual features extracted from the machine and reference texts. Our experiments reveal key insights about the existing metrics via the mismatch errors. We show that the mismatch errors between the sentence pairs on the held-out datasets from 7 NLP tasks align well with the human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1524,
      "original_url": "https://aclanthology.org/2023.findings-acl.275/",
      "title": "RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.275.pdf",
      "authors": [
        "Ziwei Ji",
        "Zihan Liu",
        "Nayeon Lee",
        "Tiezheng Yu",
        "Bryan Wilie",
        "Min Zeng",
        "Pascale Fung"
      ],
      "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, which further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO (ρ) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG (Moon et al., 2019) show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA (Durmus et al., 2020))."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1525,
      "original_url": "https://aclanthology.org/2023.findings-acl.276/",
      "title": "Transformer Language Models Handle Word Frequency in Prediction Head",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.276.pdf",
      "authors": [
        "Goro Kobayashi",
        "Tatsuki Kuribayashi",
        "Sho Yokoi",
        "Kentaro Inui"
      ],
      "abstract": "Prediction head is a crucial component of Transformer language models. Despite its direct impact on prediction, this component has often been overlooked in analyzing Transformers.In this study, we investigate the inner workings of the prediction head, specifically focusing on bias parameters. Our experiments with BERT and GPT-2 models reveal that the biases in their word prediction heads play a significant role in the models’ ability to reflect word frequency in a corpus, aligning with the logit adjustment method commonly used in long-tailed learning. We also quantify the effect of controlling the biases in practical auto-regressive text generation scenarios;under a particular setting, more diverse text can be generated without compromising text quality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1526,
      "original_url": "https://aclanthology.org/2023.findings-acl.277/",
      "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.277.pdf",
      "authors": [
        "Gibbeum Lee",
        "Volker Hartmann",
        "Jongho Park",
        "Dimitris Papailiopoulos",
        "Kangwook Lee"
      ],
      "abstract": "In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1527,
      "original_url": "https://aclanthology.org/2023.findings-acl.278/",
      "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.278.pdf",
      "authors": [
        "Fengzhu Zeng",
        "Wei Gao"
      ],
      "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to Prompt pre-trained language models (PLMs) To be Consistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1528,
      "original_url": "https://aclanthology.org/2023.findings-acl.279/",
      "title": "Model Analysis & Evaluation for Ambiguous Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.279.pdf",
      "authors": [
        "Konstantinos Papakostas",
        "Irene Papadopoulou"
      ],
      "abstract": "Ambiguous questions are a challenge for Question Answering models, as they require answers that cover multiple interpretations of the original query. To this end, these models are required to generate long-form answers that often combine conflicting pieces of information. Although recent advances in the field have shown strong capabilities in generating fluent responses, certain research questions remain unanswered. Does model/data scaling improve the answers’ quality? Do automated metrics align with human judgment? To what extent do these models ground their answers in evidence? In this study, we aim to thoroughly investigate these aspects, and provide valuable insights into the limitations of the current approaches. To aid in reproducibility and further extension of our work, we open-source our code."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1529,
      "original_url": "https://aclanthology.org/2023.findings-acl.280/",
      "title": "Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.280.pdf",
      "authors": [
        "Robert Morabito",
        "Jad Kabbara",
        "Ali Emami"
      ],
      "abstract": "Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1530,
      "original_url": "https://aclanthology.org/2023.findings-acl.281/",
      "title": "Critic-Guided Decoding for Controlled Text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.281.pdf",
      "authors": [
        "Minbeom Kim",
        "Hwanhee Lee",
        "Kang Min Yoo",
        "Joonsuk Park",
        "Hwaran Lee",
        "Kyomin Jung"
      ],
      "abstract": "Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1531,
      "original_url": "https://aclanthology.org/2023.findings-acl.282/",
      "title": "MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.282.pdf",
      "authors": [
        "Yan Wang",
        "Heidi Donovan",
        "Sabit Hassan",
        "Malihe Alikhani"
      ],
      "abstract": "Patients who effectively manage their symptoms often demonstrate higher levels of engagement in conversations and interventions with healthcare practitioners. This engagement is multifaceted, encompassing cognitive and social dimensions. Consequently, it is crucial for AI systems to understand the engagement in natural conversations between patients and practitioners to better contribute toward patient care. In this paper, we present a novel dataset (MedNgage), which consists of patient-nurse conversations about cancer symptom management. We manually annotate the dataset with a novel framework of categories of patient engagement from two different angles, namely: i) socio-affective engagement (3.1K spans), and ii) cognitive engagement (1.8K spans). Through statistical analysis of the data that is annotated using our framework, we show a positive correlation between patient symptom management outcomes and their engagement in conversations. Additionally, we demonstrate that pre-trained transformer models fine-tuned on our dataset can reliably predict engagement categories in patient-nurse conversations. Lastly, we use LIME (Ribeiro et al., 2016) to analyze the underlying challenges of the tasks that state-of-the-art transformer models encounter. The de-identified data is available for research purposes upon request."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1532,
      "original_url": "https://aclanthology.org/2023.findings-acl.283/",
      "title": "SEAG: Structure-Aware Event Causality Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.283.pdf",
      "authors": [
        "Zhengwei Tao",
        "Zhi Jin",
        "Xiaoying Bai",
        "Haiyan Zhao",
        "Chengfeng Dou",
        "Yongqiang Zhao",
        "Fang Wang",
        "Chongyang Tao"
      ],
      "abstract": "Extracting event causality underlies a broad spectrum of natural language processing applications. Cutting-edge methods break this task into Event Detection and Event Causality Identification. Although the pipelined solutions succeed in achieving acceptable results, the inherent nature of separating the task incurs limitations. On the one hand, it suffers from the lack of cross-task dependencies and may cause error propagation. On the other hand, it predicts events and relations separately, undermining the integrity of the event causality graph (ECG). To address such issues, in this paper, we propose an approach for Structure-Aware Event Causality Generation (SEAG). With a graph linearization module, we generate the ECG structure in a way of text2text generation based on a pre-trained language model. To foster the structural representation of the ECG, we introduce the novel Causality Structural Discrimination training paradigm in which we perform structural discriminative training alongside auto-regressive generation enabling the model to distinguish from constructed incorrect ECGs. We conduct experiments on three datasets. The experimental results demonstrate the effectiveness of structural event causality generation and the causality structural discrimination training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1533,
      "original_url": "https://aclanthology.org/2023.findings-acl.284/",
      "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.284.pdf",
      "authors": [
        "Ruixiang Tang",
        "Dehan Kong",
        "Longtao Huang",
        "Hui Xue"
      ],
      "abstract": "Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are “lazy learners” that tend to exploit such shortcuts. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1534,
      "original_url": "https://aclanthology.org/2023.findings-acl.285/",
      "title": "A Two-Stage Decoder for Efficient ICD Coding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.285.pdf",
      "authors": [
        "Thanh-Tung Nguyen",
        "Viktor Schlegel",
        "Abhinav Ramesh Kashyap",
        "Stefan Winkler"
      ],
      "abstract": "Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient’s condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set have shown that our model performs well in single-model settings without external data or knowledge."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1535,
      "original_url": "https://aclanthology.org/2023.findings-acl.286/",
      "title": "Asymmetric feature interaction for interpreting model predictions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.286.pdf",
      "authors": [
        "Xiaolei Lu",
        "Jianghong Ma",
        "Haode Zhang"
      ],
      "abstract": "In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential features for model predictions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1536,
      "original_url": "https://aclanthology.org/2023.findings-acl.287/",
      "title": "Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.287.pdf",
      "authors": [
        "Tharindu Cyril Weerasooriya",
        "Alexander Ororbia",
        "Raj Bhensadadia",
        "Ashiqur KhudaBukhsh",
        "Christopher Homan"
      ],
      "abstract": "Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard “ground truth” as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution. The model takes annotator-item pairs, rather than items alone, as input, and performs inference by aggregating over all annotators. Despite its simplicity, our experiments show that, on six benchmark datasets, our model is competitive with, and frequently outperforms, other, more complex models that either do not model specific annotators or were not designed for label distribution learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1537,
      "original_url": "https://aclanthology.org/2023.findings-acl.288/",
      "title": "Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.288.pdf",
      "authors": [
        "Pranav Nair",
        "Sukomal Pal",
        "Pradeepika Verma"
      ],
      "abstract": "Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding source documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines demonstrating the effectiveness of its prefix averaging scheme."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1538,
      "original_url": "https://aclanthology.org/2023.findings-acl.289/",
      "title": "ClaimDiff: Comparing and Contrasting Claims on Contentious Issues",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.289.pdf",
      "authors": [
        "Miyoung Ko",
        "Ingyu Seong",
        "Hwaran Lee",
        "Joonsuk Park",
        "Minsuk Chang",
        "Minjoon Seo"
      ],
      "abstract": "With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one’s argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1539,
      "original_url": "https://aclanthology.org/2023.findings-acl.290/",
      "title": "Unsupervised Paraphrasing of Multiword Expressions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.290.pdf",
      "authors": [
        "Takashi Wada",
        "Yuji Matsumoto",
        "Timothy Baldwin",
        "Jey Han Lau"
      ],
      "abstract": "We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1540,
      "original_url": "https://aclanthology.org/2023.findings-acl.291/",
      "title": "G-Tuning: Improving Generalization of Pre-trained Language Models with Generative Adversarial Network",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.291.pdf",
      "authors": [
        "Rongxiang Weng",
        "Wen Sen Cheng",
        "Min Zhang"
      ],
      "abstract": "The generalization ability of pre-trained language models (Plms) in downstream tasks is heavily influenced by fine-tuning. The objective of fine-tuning is to transform the latent representation of Plms from a universal space to a target space, allowing the model to be applied to downstream tasks with the capability of generalizing to unseen samples. However, the effect of Plms will be diminished when the training data coverage is insufficient, in which fine-tuning is inadequate to learn the complete mapping. In this study, we propose a new fine-tuning framework, referred to as G-Tuning, that aims to preserve the generalization ability of Plms in downstream tasks. Specifically, we integrate a generative adversarial network into the fine-tuning process to aid in the transformation of the latent representation in the entire space. Empirical evaluations on the GLUE benchmark, as well as two additional demanding scenarios involving domain and language generalization, demonstrate that G-Tuning can accurately map the universal representation to the target space, thus effectively enhancing the generalization performance of Plms across various downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1541,
      "original_url": "https://aclanthology.org/2023.findings-acl.292/",
      "title": "Unified Language Representation for Question Answering over Text, Tables, and Images",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.292.pdf",
      "authors": [
        "Bowen Yu",
        "Cheng Fu",
        "Haiyang Yu",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "When trying to answer complex questions, people often rely on multiple sources of information, such as visual, textual, and tabular data. Previous approaches to this problem have focused on designing input features or model structure in the multi-modal space, which is inflexible for cross-modal reasoning or data-efficient training. In this paper, we call for an alternative paradigm, which transforms the images and tables into unified language representations, so that we can simplify the task into a simpler textual QA problem that can be solved using three steps: retrieval, ranking, and generation, all within a language space. This idea takes advantage of the power of pre-trained language models and is implemented in a framework called Solar. Our experimental results show that Solar outperforms all existing methods by 10.6-32.3 pts on two datasets, MultimodalQA and MMCoQA, across ten different metrics. Additionally, Solar achieves the best performance on the WebQA leaderboard."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1542,
      "original_url": "https://aclanthology.org/2023.findings-acl.293/",
      "title": "A Set Prediction Network For Extractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.293.pdf",
      "authors": [
        "Xiaoxia Cheng",
        "Yongliang Shen",
        "Weiming Lu"
      ],
      "abstract": "Extractive summarization focuses on extracting salient sentences from the source document and incorporating them in the summary without changing their wording or structure. The naive approach for extractive summarization is sentence classification, which makes independent binary decisions for each sentence, resulting in the model cannot detect the dependencies between sentences in the summary. Recent approaches introduce an autoregressive decoder to detect redundancy relationship between sentences by step-by-step sentence selection, but bring train-inference gap. To address these issues, we formulate extractive summarization as a salient sentence set recognition task. To solve the sentence set recognition task, we propose a set prediction network (SetSum), which sets up a fixed set of learnable queries to extract the entire sentence set of the summary, while capturing the dependencies between them.Different from previous methods with an auto-regressive decoder, we employ a non-autoregressive decoder to predict the sentences within the summary in parallel during both the training and inference process, which eliminates the train-inference gap. Experimental results on both single-document and multi-document extracted summary datasets show that our approach outperforms previous state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1543,
      "original_url": "https://aclanthology.org/2023.findings-acl.294/",
      "title": "Geo-Seq2seq: Twitter User Geolocation on Noisy Data through Sequence to Sequence Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.294.pdf",
      "authors": [
        "Jingyu Zhang",
        "Alexandra DeLucia",
        "Chenyu Zhang",
        "Mark Dredze"
      ],
      "abstract": "Location information can support social media analyses by providing geographic context. Some of the most accurate and popular Twitter geolocation systems rely on rule-based methods that examine the user-provided profile location, which fail to handle informal or noisy location names. We propose Geo-Seq2seq, a sequence-to-sequence (seq2seq) model for Twitter user geolocation that rewrites noisy, multilingual user-provided location strings into structured English location names. We train our system on tens of millions of multilingual location string and geotagged-tweet pairs. Compared to leading methods, our model vastly increases coverage (i.e., the number of users we can geolocate) while achieving comparable or superior accuracy. Our error analysis reveals that constrained decoding helps the model produce valid locations according to a location database. Finally, we measure biases across language, country of origin, and time to evaluate fairness, and find that while our model can generalize well to unseen temporal data, performance does vary by language and country."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1544,
      "original_url": "https://aclanthology.org/2023.findings-acl.295/",
      "title": "Predicting Numerals in Text Using Nearest Neighbor Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.295.pdf",
      "authors": [
        "Taku Sakamoto",
        "Akiko Aizawa"
      ],
      "abstract": "Commonsense about quantitative properties is essential for a deep understanding of texts containing numerals. However, naive language models (LMs) treat numerals as string tokens; therefore, they lack an understanding of the magnitudes of numerals, resulting in a difficulty in acquiring the commonsense. In this study, we apply the k-nearest neighbor LM (kNN-LM) to the masked numeral prediction (MNP) task, which measures the quantitative commonsense of LMs.kNN-LM extends pre-trained neural LMs with the k-nearest neighbor (kNN) search.Since it can utilize patterns that appear in the datastore for prediction, we expect an improvement in numeral prediction accuracy, which is associated with a high rate of occurrence of out-of-vocabulary (OOV) words.Through experiments, we verified that the retrieval-based method is effective for fine-grained predictions of numerals from context, especially for the OOV numerals.We also compared two different context spans for context representations to improve the accuracy of kNN search by using only the words that are closely related to the masked numeral: the mask and its surrounding words, and the mask and its subsequent words.Our results reveal that using only the embeddings of mask tokens for numerals in kNN search is the most effective approach for realizing MNP tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1545,
      "original_url": "https://aclanthology.org/2023.findings-acl.296/",
      "title": "HonestBait: Forward References for Attractive but Faithful Headline Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.296.pdf",
      "authors": [
        "Chih Yao Chen",
        "Dennis Wu",
        "Lun-Wei Ku"
      ],
      "abstract": "Current methods for generating attractive headlines often learn directly from data, which bases attractiveness on the number of user clicks and views. Although clicks or views do reflect user interest, they can fail to reveal how much interest is raised by the writing style and how much is due to the event or topic itself. Also, such approaches can lead to harmful inventions by over-exaggerating the content, aggravating the spread of false information. In this work, we propose HonestBait, a novel framework for solving these issues from another aspect: generating headlines using forward references (FRs), a writing technique often used for clickbait. A self-verification process is included during training to avoid spurious inventions. We begin with a preliminary user study to understand how FRs affect user interest, after which we present PANCO, an innovative dataset containing pairs of fake news with verified news for attractive but faithful news headline generation. Auto matic metrics and human evaluations show that our framework yields more attractive results (+11.25% compared to human-written verified news headlines) while maintaining high veracity, which helps promote real information to fight against fake news."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1546,
      "original_url": "https://aclanthology.org/2023.findings-acl.297/",
      "title": "Few Shot Rationale Generation using Self-Training with Dual Teachers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.297.pdf",
      "authors": [
        "Aditya Srikanth Veerubhotla",
        "Lahari Poddar",
        "Jun Yin",
        "György Szarvas",
        "Sharanya Eswaran"
      ],
      "abstract": "Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly process, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization(MLR) which promotes explanations to be strongly conditioned on predicted labels. Evaluation on three public datasets demonstrate that the proposed methods are effective in modeling task labels and generating faithful rationales."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1547,
      "original_url": "https://aclanthology.org/2023.findings-acl.298/",
      "title": "Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.298.pdf",
      "authors": [
        "Yujin Baek",
        "Koanho Lee",
        "Dayeon Ki",
        "Cheonbok Park",
        "Hyoung-Gyu Lee",
        "Jaegul Choo"
      ],
      "abstract": "Lexically-constrained NMT (LNMT) aims to incorporate user-provided terminology into translations. Despite its practical advantages, existing work has not evaluated LNMT models under challenging real-world conditions. In this paper, we focus on two important but understudied issues that lie in the current evaluation process of LNMT studies. The model needs to cope with challenging lexical constraints that are “homographs” or “unseen” during training. To this end, we first design a homograph disambiguation module to differentiate the meanings of homographs. Moreover, we propose PLUMCOT which integrates contextually rich information about unseen lexical constraints from pre-trained language models and strengthens a copy mechanism of the pointer network via direct supervision of a copying score. We also release HOLLY, an evaluation benchmark for assessing the ability of model to cope with “homographic” and “unseen” lexical constraints. Experiments on HOLLY and the previous test setup show the effectiveness of our method. The effects of PLUMCOT are shown to be remarkable in “unseen” constraints. Our dataset is available at https://github.com/papago-lab/HOLLY-benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1548,
      "original_url": "https://aclanthology.org/2023.findings-acl.299/",
      "title": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.299.pdf",
      "authors": [
        "Tingting Wu",
        "Xiao Ding",
        "Minji Tang",
        "Hao Zhang",
        "Bing Qin",
        "Ting Liu"
      ],
      "abstract": "Large-scale datasets in the real world inevitably involve label noise. Deep models can gradually overfit noisy labels and thus degrade model generalization. To mitigate the effects of label noise, learning with noisy labels (LNL) methods are designed to achieve better generalization performance. Due to the lack of suitable datasets, previous studies have frequently employed synthetic label noise to mimic real-world label noise. However, synthetic noise is not instance-dependent, making this approximation not always effective in practice. Recent research has proposed benchmarks for learning with real-world noisy labels. However, the noise sources within may be single or fuzzy, making benchmarks different from data with heterogeneous label noises in the real world. To tackle these issues, we contribute NoisywikiHow, the largest NLP benchmark built with minimal supervision. Specifically, inspired by human cognition, we explicitly construct multiple sources of label noise to imitate human errors throughout the annotation, replicating real-world noise, whose corruption is affected by both ground-truth labels and instances. Moreover, we provide a variety of noise levels to support controlled experiments on noisy data, enabling us to evaluate LNL methods systematically and comprehensively. After that, we conduct extensive multi-dimensional experiments on a broad range of LNL methods, obtaining new and intriguing findings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1549,
      "original_url": "https://aclanthology.org/2023.findings-acl.300/",
      "title": "Sampling Better Negatives for Distantly Supervised Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.300.pdf",
      "authors": [
        "Lu Xu",
        "Lidong Bing",
        "Wei Lu"
      ],
      "abstract": "Distantly supervised named entity recognition (DS-NER) has been proposed to exploit the automatically labeled training data instead of human annotations. The distantly annotated datasets are often noisy and contain a considerable number of false negatives. The recent approach uses a weighted sampling approach to select a subset of negative samples for training. However, it requires a good classifier to assign weights to the negative samples. In this paper, we propose a simple and straightforward approach for selecting the top negative samples that have high similarities with all the positive samples for training. Our method achieves consistent performance improvements on four distantly supervised NER datasets. Our analysis also shows that it is critical to differentiate the true negatives from the false negatives."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1550,
      "original_url": "https://aclanthology.org/2023.findings-acl.301/",
      "title": "Prototype-Based Interpretability for Legal Citation Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.301.pdf",
      "authors": [
        "Chu Fei Luo",
        "Rohan Bhambhoria",
        "Samuel Dahan",
        "Xiaodan Zhu"
      ],
      "abstract": "Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work, we attempt to address these requirements applied to the important problem of legal citation prediction (LCP). We design the task with parallels to the thought-process of lawyers, i.e., with reference to both precedents and legislative provisions. After initial experimental results, we refine the target citation predictions with the feedback of legal experts. Additionally, we introduce a prototype architecture to add interpretability, achieving strong performance while adhering to decision parameters used by lawyers. Our study builds on and leverages the state-of-the-art language processing models for law, while addressing vital considerations for high-stakes tasks with practical societal impact."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1551,
      "original_url": "https://aclanthology.org/2023.findings-acl.302/",
      "title": "LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.302.pdf",
      "authors": [
        "Philipp Wicke"
      ],
      "abstract": "Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language models conceptualise embodied concepts to a degree that facilitates figurative language understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1552,
      "original_url": "https://aclanthology.org/2023.findings-acl.303/",
      "title": "Making Better Use of Training Corpus: Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.303.pdf",
      "authors": [
        "Guoxin Yu",
        "Lemao Liu",
        "Haiyun Jiang",
        "Shuming Shi",
        "Xiang Ao"
      ],
      "abstract": "In this paper, we aim to adapt the idea of retrieval-based neural approaches to the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicting the sentiment polarity and it is usually aspect-dependent. Semantic similar neighbors with different polarities will be infeasible even counterproductive. To tackle this issue, we propose a retrieval-based neural ASTE approach, named RLI (Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation), which exploits the label information of neighbors. Given an aspect-opinion term pair, we retrieve semantic similar triplets from the training corpus and interpolate their label information into the augmented representation of the target pair. The retriever is jointly trained with the whole ASTE framework, and neighbors with both similar semantics and sentiments can be recalled with the aid of this distant supervision. In addition, we design a simple yet effective pre-train method for the retriever that implicitly encodes the label similarities. Extensive experiments and analysis on two widely-used benchmarks show that the proposed model establishes a new state-of-the-art on ASTE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1553,
      "original_url": "https://aclanthology.org/2023.findings-acl.304/",
      "title": "Multi-Domain Dialogue State Tracking with Disentangled Domain-Slot Attention",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.304.pdf",
      "authors": [
        "Longfei Yang",
        "Jiyi Li",
        "Sheng Li",
        "Takahiro Shinozaki"
      ],
      "abstract": "As the core of task-oriented dialogue systems, dialogue state tracking (DST) is designed to track the dialogue state through the conversation between users and systems. Multi-domain DST has been an important challenge in which the dialogue states across multiple domains need to consider. In recent mainstream approaches, each domain and slot are aggregated and regarded as a single query feeding into attention with the dialogue history to obtain domain-slot specific representations. In this work, we propose disentangled domain-slot attention for multi-domain dialogue state tracking. The proposed approach disentangles the domain-slot specific information extraction in a flexible and context-dependent manner by separating the query about domains and slots in the attention component. Through a series of experiments on MultiWOZ 2.0 and MultiWOZ 2.4 datasets, we demonstrate that our proposed approach outperforms the standard multi-head attention with aggregated domain-slot query."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1554,
      "original_url": "https://aclanthology.org/2023.findings-acl.305/",
      "title": "Improved Visual Story Generation with Adaptive Context Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.305.pdf",
      "authors": [
        "Zhangyin Feng",
        "Yuchen Ren",
        "Xinmiao Yu",
        "Xiaocheng Feng",
        "Duyu Tang",
        "Shuming Shi",
        "Bing Qin"
      ],
      "abstract": "Diffusion models developed on top of powerful text-to-image generation models like Stable Diffusion achieve remarkable success in visual story generation. However, the best-performing approach considers historically generated results as flattened memory cells, ignoring the fact that not all preceding images contribute equally to the generation of the characters and scenes at the current stage. To address this, we present a simple method that improves the leading system with adaptive context modeling, which is not only incorporated in the encoder but also adopted as additional guidance in the sampling stage to boost the global consistency of the generated story. We evaluate our model on PororoSV and FlintstonesSV datasets and show that our approach achieves state-of-the-art FID scores on both story visualization and continuation scenarios. We conduct detailed model analysis and show that our model excels at generating semantically consistent images for stories."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1555,
      "original_url": "https://aclanthology.org/2023.findings-acl.306/",
      "title": "Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.306.pdf",
      "authors": [
        "Jiangnan Li",
        "Mo Yu",
        "Fandong Meng",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang",
        "Jie Zhou"
      ],
      "abstract": "We focus on dialogue reading comprehension (DRC) that extracts answers from dialogues. Compared to standard RC tasks, DRC has raised challenges because of the complex speaker information and noisy dialogue context. Essentially, the challenges come from the speaker-centric nature of dialogue utterances — an utterance is usually insufficient in its surface form, but requires to incorporate the role of its speaker and the dialogue context to fill the latent pragmatic and intention information. We propose to deal with these problems in two folds. First, we propose a new key-utterances-extracting method, which can realize more answer-contained utterances. Second, based on the extracted utterances, we then propose a Question-Interlocutor Scope Realized Graph (QuISG). QuISG involves the question and question-mentioning speaker as nodes. To realize interlocutor scopes, utterances are connected with corresponding speakers in the dialogue. Experiments on the benchmarks show that our method achieves state-of-the-art performance against previous works."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1556,
      "original_url": "https://aclanthology.org/2023.findings-acl.307/",
      "title": "Speech-to-Speech Translation for a Real-world Unwritten Language",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.307.pdf",
      "authors": [
        "Peng-Jen Chen",
        "Kevin Tran",
        "Yilin Yang",
        "Jingfei Du",
        "Justine Kao",
        "Yu-An Chung",
        "Paden Tomasello",
        "Paul-Ambroise Duquenne",
        "Holger Schwenk",
        "Hongyu Gong",
        "Hirofumi Inaguma",
        "Sravya Popuri",
        "Changhan Wang",
        "Juan Pino",
        "Wei-Ning Hsu",
        "Ann Lee"
      ],
      "abstract": "We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additional text supervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1557,
      "original_url": "https://aclanthology.org/2023.findings-acl.308/",
      "title": "Code Execution with Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.308.pdf",
      "authors": [
        "Chenxiao Liu",
        "Shuai Lu",
        "Weizhu Chen",
        "Daxin Jiang",
        "Alexey Svyatkovskiy",
        "Shengyu Fu",
        "Neel Sundaresan",
        "Nan Duan"
      ],
      "abstract": "Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1558,
      "original_url": "https://aclanthology.org/2023.findings-acl.309/",
      "title": "BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.309.pdf",
      "authors": [
        "Shibo Hao",
        "Bowen Tan",
        "Kaiwen Tang",
        "Bin Ni",
        "Xiyan Shao",
        "Hengzhe Zhang",
        "Eric Xing",
        "Zhiting Hu"
      ],
      "abstract": "It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations, from LMs of varying capacities such as RoBERTaNet. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., “A is capable of but not good at B”). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs’ knowledge capacities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1559,
      "original_url": "https://aclanthology.org/2023.findings-acl.310/",
      "title": "Sequential Path Signature Networks for Personalised Longitudinal Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.310.pdf",
      "authors": [
        "Talia Tseriotou",
        "Adam Tsakalidis",
        "Peter Foster",
        "Terence Lyons",
        "Maria Liakata"
      ],
      "abstract": "Longitudinal user modeling can provide a strong signal for various downstream tasks. Despite the rapid progress in representation learning, dynamic aspects of modelling individuals’ language have only been sparsely addressed. We present a novel extension of neural sequential models using the notion of path signatures from rough path theory, which constitute graduated summaries of continuous paths and have the ability to capture non-linearities in trajectories. By combining path signatures of users’ history with contextual neural representations and recursive neural networks we can produce compact time-sensitive user representations. Given the magnitude of mental health conditions with symptoms manifesting in language, we show the applicability of our approach on the task of identifying changes in individuals’ mood by analysing their online textual content. By directly integrating signature transforms of users’ history in the model architecture we jointly address the two most important aspects of the task, namely sequentiality and temporality. Our approach achieves state-of-the-art performance on macro-average F1 score on the two available datasets for the task, outperforming or performing on-par with state-of-the-art models utilising only historical posts and even outperforming prior models which also have access to future posts of users."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1560,
      "original_url": "https://aclanthology.org/2023.findings-acl.311/",
      "title": "A Multi-modal Debiasing Model with Dynamical Constraint for Robust Visual Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.311.pdf",
      "authors": [
        "Yu Li",
        "Bojie Hu",
        "Fengshuo Zhang",
        "Yahan Yu",
        "Jian Liu",
        "Yufeng Chen",
        "Jinan Xu"
      ],
      "abstract": "Recent studies have pointed out that many well-developed Visual Question Answering (VQA) systems suffer from bias problem. Despite the remarkable performance gained on In-Distribution (ID) datasets, the VQA model might merely capture the superficial correlation from question to answer rather than showing real reasoning abilities. Therefore, when switching to Out-of-Distribution (OOD) dataset, whose test distribution is unknown or even reversed with the training set, significant drops might be demonstrated. Although efforts have been devoted to easing the negative bias effect brought by language prior and analysing its inherent cause, they are still limited by the following two aspects. First, most current debiasing methods achieve promising OOD generalization ability with a major sacrifice of the ID performance. Second, existing researches are restricted by exploiting comprehensive biases, since weakening the language bias is mainly focused, while only a few works consider vision bias. In this paper, we investigate a straightforward way to mitigate bias problem for VQA task. Specifically, we reduce bias effect by subtracting bias score from standard VQA base score. Based on such a direct strategy, we design two bias learning branches to detect more bias information, which are combined with a dynamical constraint loss to alleviate the problem of over-correction and insufficient debiasing effect. We evaluate our method on the challenging VQA v2.0 and VQA-CP V2,0 datasets and the proposed method achievessignificant improvement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1561,
      "original_url": "https://aclanthology.org/2023.findings-acl.312/",
      "title": "Trigger-Argument based Explanation for Event Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.312.pdf",
      "authors": [
        "Yong Guan",
        "Jiaoyan Chen",
        "Freddy Lecue",
        "Jeff Pan",
        "Juanzi Li",
        "Ru Li"
      ],
      "abstract": "Event Detection (ED) is a critical task that aims to identify events of certain types in plain text. Neural models have achieved great success on ED, thus coming with a desire for higher interpretability. Existing works mainly exploit words or phrases of the input text to explain models’ inner mechanisms. However, for ED, the event structure, comprising of an event trigger and a set of arguments, are more enlightening clues to explain model behaviors. To this end, we propose a Trigger-Argument based Explanation method (TAE), which can utilize event structure knowledge to uncover a faithful interpretation for the existing ED models at neuron level. Specifically, we design group, sparsity, support mechanisms to construct the event structure from structuralization, compactness, and faithfulness perspectives. We evaluate our model on the large-scale MAVEN and the widely-used ACE 2005 datasets, and observe that TAE is able to reveal the process by which the model predicts. Experimental results also demonstrate that TAE can not only improve the interpretability on standard evaluation metrics, but also effectively facilitate the human understanding."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1562,
      "original_url": "https://aclanthology.org/2023.findings-acl.313/",
      "title": "Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.313.pdf",
      "authors": [
        "Maria Leonor Pacheco",
        "Tunazzina Islam",
        "Lyle Ungar",
        "Ming Yin",
        "Dan Goldwasser"
      ],
      "abstract": "Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a theme to account for more than just a word distribution, and include generalized concepts deemed relevant by domain experts. Then, we propose an interactive framework that receives and encodes expert feedback at different levels of abstraction. Our framework strikes a balance between automation and manual coding, allowing experts to maintain control of their study while reducing the manual effort required."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1563,
      "original_url": "https://aclanthology.org/2023.findings-acl.314/",
      "title": "NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.314.pdf",
      "authors": [
        "Farhad Moghimifar",
        "Shilin Qu",
        "Tongtong Wu",
        "Yuan-Fang Li",
        "Gholamreza Haffari"
      ],
      "abstract": "Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose NormMark, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables conditioned on the conversation history, and improve the model’s ability in norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited norm annotations, we show that our approach achieves higher F1 score, outperforming current state-of-the-art methods, including GPT3."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1564,
      "original_url": "https://aclanthology.org/2023.findings-acl.315/",
      "title": "VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.315.pdf",
      "authors": [
        "Hoang-Quoc Nguyen-Son",
        "Seira Hidano",
        "Kazuhide Fukushima",
        "Shinsaku Kiyomoto",
        "Isao Echizen"
      ],
      "abstract": "Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without training by voting on hard labels from predictions of transformations, namely, VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the hard labels of input text and its transformation. The evaluation demonstrates that VoteTRANS effectively detects adversarial text across various state-of-the-art attacks, models, and datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1565,
      "original_url": "https://aclanthology.org/2023.findings-acl.316/",
      "title": "Fusion or Defusion? Flexible Vision-and-Language Pre-Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.316.pdf",
      "authors": [
        "Rongyi Sun",
        "Ziran Li",
        "Yifeng Ding",
        "Qifan Wang",
        "Jingang Wang",
        "Haitao Zheng",
        "Wei Wu",
        "Yunsen Xian"
      ],
      "abstract": "Existing approaches in the vision-and-language pre-training (VLP) paradigm mainly deploy either fusion-based encoders or dual-encoders, failing to achieve both effectiveness and efficiency in downstream multimodal tasks. In this paper, we build a flexible VLP model by incorporating cross-modal fusions into a dual-encoder architecture, where the introduced fusion modules can be easily decoupled from the dual encoder so as to switch the model to a fusion-free one. To better absorb cross-modal features from the fusion modules, we design a cross-modal knowledge transfer strategy along with other comprehensive pre-training tasks to guide the training process, which can further strengthen both the fusion-based and fusion-free representation learning. Extensive experiments conducted on various downstream vision-language tasks show that our proposed model is well-equipped with effectiveness as well as efficiency, demonstrating a superior performance compared with other strong VLP models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1566,
      "original_url": "https://aclanthology.org/2023.findings-acl.317/",
      "title": "COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.317.pdf",
      "authors": [
        "Fanny Jourdan",
        "Agustin Picard",
        "Thomas Fel",
        "Laurent Risser",
        "Jean-Michel Loubes",
        "Nicholas Asher"
      ],
      "abstract": "Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. We conduct experiments in single and multi-aspect sentiment analysis tasks and we show COCKATIEL’s superior ability to discover concepts that align with humans’ on Transformer models without any supervision, we objectively verify the faithfulness of its explanations through fidelity metrics, and we showcase its ability to provide meaningful explanations in two different datasets. Our code is freely available: https://github.com/fanny-jourdan/cockatiel"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1567,
      "original_url": "https://aclanthology.org/2023.findings-acl.318/",
      "title": "Code-Switched Text Synthesis in Unseen Language Pairs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.318.pdf",
      "authors": [
        "I-Hung Hsu",
        "Avik Ray",
        "Shubham Garg",
        "Nanyun Peng",
        "Jing Huang"
      ],
      "abstract": "Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target language pairs further to enhance the reliability of GLOSS. Automatic evaluations on four language pairs show that GLOSS achieves at least 55% relative BLEU and METEOR scores improvements compared to strong baselines. Human evaluations on two language pairs further validate the success of GLOSS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1568,
      "original_url": "https://aclanthology.org/2023.findings-acl.319/",
      "title": "Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.319.pdf",
      "authors": [
        "Justus-Jonas Erker",
        "Stefan Schaffer",
        "Gerasimos Spanakis"
      ],
      "abstract": "Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56% over 3 turns) in conversations from the DailyDialog dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1569,
      "original_url": "https://aclanthology.org/2023.findings-acl.320/",
      "title": "Data-Efficient French Language Modeling with CamemBERTa",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.320.pdf",
      "authors": [
        "Wissam Antoun",
        "Benoît Sagot",
        "Djamé Seddah"
      ],
      "abstract": "Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model’s performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to CamemBERT, despite being trained on only 30% of its total number of input tokens. In addition to our experimental results, we also publicly release the weights and code implementation of CamemBERTa, making it the first publicly available DeBERTaV3 model outside of the original paper and the first openly available implementation of a DeBERTaV3 training objective."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1570,
      "original_url": "https://aclanthology.org/2023.findings-acl.321/",
      "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.321.pdf",
      "authors": [
        "Zhun Yang",
        "Adam Ishay",
        "Joohyung Lee"
      ],
      "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM’s adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1571,
      "original_url": "https://aclanthology.org/2023.findings-acl.322/",
      "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.322.pdf",
      "authors": [
        "Derek Tam",
        "Anisha Mascarenhas",
        "Shiyue Zhang",
        "Sarah Kwan",
        "Mohit Bansal",
        "Colin Raffel"
      ],
      "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model’s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of {pasted macro ‘BENCHMARK’}, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1572,
      "original_url": "https://aclanthology.org/2023.findings-acl.323/",
      "title": "Text Generation Model Enhanced with Semantic Information in Aspect Category Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.323.pdf",
      "authors": [
        "Tu Tran",
        "Kiyoaki Shirai",
        "Natthawut Kertkeidkachorn"
      ],
      "abstract": "Aspect Category Sentiment Analysis (ACSA) is one of the main subtasks of sentiment analysis, which aims at predicting polarity over a given aspect category. Recently, generative methods emerge as an efficient way to utilize a pre-trained language model for solving ACSA. However, those methods fail to model relations of target words and opinion words in a sentence including multiple aspects. To tackle this problem, this paper proposes a method to incorporate Abstract Meaning Representation (AMR), which describes semantic representation of a sentence as a directed graph, into a text generation model. Furthermore, two regularizers are designed to guide cross attention weights allocation over AMR graphs. One is the identical regularizer that constrains attention weights of aligned nodes, the other is the entropy regularizer that helps the decoder generate tokens by heavily considering only a few related nodes in the AMR graph. Experimental results on three datasets show that the proposed method outperforms state-of-the-art methods, proving the effectiveness of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1573,
      "original_url": "https://aclanthology.org/2023.findings-acl.324/",
      "title": "Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.324.pdf",
      "authors": [
        "Ruixi Lin",
        "Hwee Tou Ng"
      ],
      "abstract": "We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people’s lives if users trust their results too much. In this work, we reveal two bias modes leveraging cognitive biases when we prompt BERT, accompanied by two bias metrics. On a drug-drug interaction extraction task, our bias measurements reveal an error pattern similar to the availability bias when the labels for training prompts are imbalanced, and show that a toning-down transformation of the drug-drug description in a prompt can elicit a bias similar to the framing effect, warning users to distrust when prompting language models for answers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1574,
      "original_url": "https://aclanthology.org/2023.findings-acl.325/",
      "title": "CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.325.pdf",
      "authors": [
        "YunSeok Choi",
        "Jee-Hyong Lee"
      ],
      "abstract": "In order to solve the inefficient parameter update and storage issues of fine-tuning in Natural Language Generation (NLG) tasks, prompt-tuning methods have emerged as lightweight alternatives. Furthermore, efforts to reduce the gap between pre-training and fine-tuning have shown successful results in low-resource settings. As large Pre-trained Language Models (PLMs) for Program and Language Generation (PLG) tasks are constantly being developed, prompt tuning methods are necessary for the tasks. However, due to the gap between pre-training and fine-tuning different from PLMs for natural language, a prompt tuning method that reflects the traits of PLM for program language is needed. In this paper, we propose a Task-Agnostic prompt tuning method for the PLG tasks, CodePrompt, that combines Input-Dependent Prompt Template (to bridge the gap between pre-training and fine-tuning of PLMs for program and language) and Corpus-Specific Prefix Tuning (to update the parameters of PLMs for program and language efficiently).Also, we propose a method to provide richer prefix word information for limited prefix lengths. We prove that our method is effective in three PLG tasks, not only in the full-data setting but also in the low-resource setting and cross-domain setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1575,
      "original_url": "https://aclanthology.org/2023.findings-acl.326/",
      "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale.",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.326.pdf",
      "authors": [
        "Vijeta Deshpande",
        "Dan Pechi",
        "Shree Thatte",
        "Vladislav Lialin",
        "Anna Rumshisky"
      ],
      "abstract": "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below 2.2 × 1015 FLOPs. We also find that adding layers does not always benefit downstream performance.Our filtered pre-training data, reduced English vocabulary, and code are available at https://github.com/text-machine-lab/mini_bertgithub.com/text-machine-lab/mini_bert"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1576,
      "original_url": "https://aclanthology.org/2023.findings-acl.327/",
      "title": "Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.327.pdf",
      "authors": [
        "Yi Liu",
        "Xiaohan Bi",
        "Lei Li",
        "Sishuo Chen",
        "Wenkai Yang",
        "Xu Sun"
      ],
      "abstract": "Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck. In this paper, we propose a communication-efficient Fed-MNMT framework that addresses this issue by keeping PLMs frozen and only transferring lightweight adapter modules between clients. Since different language pairs exhibit substantial discrepancies in data distributions, adapter parameters of clients may conflict with each other. To tackle this, we explore various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters. Experimental results demonstrate that our framework reduces communication cost by over 98% while achieving similar or even better performance compared to competitive baselines. Further analysis reveals that clustering strategies effectively solve the problem of linguistic discrepancy and pruning adapter modules further improves communication efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1577,
      "original_url": "https://aclanthology.org/2023.findings-acl.328/",
      "title": "Cross-task Knowledge Transfer for Extremely Weakly Supervised Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.328.pdf",
      "authors": [
        "Seongmin Park",
        "Kyungho Kim",
        "Jihwa Lee"
      ],
      "abstract": "Text classification with extremely weak supervision (EWS) imposes stricter supervision constraints compared to regular weakly supervise classification. Absolutely no labeled training samples or hand-crafted rules specific to the evaluation data are allowed. Such restrictions limit state-of-the-art EWS classification methods to indirect weak labeling techniques that assign unnatural label uncertainty estimates. We present PLAT, a framework that creates weak labels by leveraging recent developments in zero-shot text classification. PLAT employs models trained for sub-tasks other than classification to label documents. Most importantly, PLAT refrains from assigning overly confident weak labels and improves soft-label training performance for downstream classifiers. Classifiers trained with PLAT significantly outperform those trained on weak labels generated by the previous state-of-the-art in extremely weakly supervised text classification."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1578,
      "original_url": "https://aclanthology.org/2023.findings-acl.329/",
      "title": "GVdoc - Graph-based Visual DOcument Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.329.pdf",
      "authors": [
        "Fnu Mohbat",
        "Mohammed J Zaki",
        "Catherine Finegan-Dollak",
        "Ashish Verma"
      ],
      "abstract": "The robustness of a model for real-world deployment is decided by how well it performs on unseen data and distinguishes between in-domain and out-of-domain samples. Visual document classifiers have shown impressive performance on in-distribution test sets. However, they tend to have a hard time correctly classifying and differentiating out-of-distribution examples. Image-based classifiers lack the text component, whereas multi-modality transformer-based models face the token serialization problem in visual documents due to their diverse layouts. They also require a lot of computing power during inference, making them impractical for many real-world applications. We propose, GVdoc, a graph-based document classification model that addresses both of these challenges. Our approach generates a document graph based on its layout, and then trains a graph neural network to learn node and graph embeddings. Through experiments, we show that our model, even with fewer parameters, outperforms state-of-the-art models on out-of-distribution data while retaining comparable performance on the in-distribution test set."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1579,
      "original_url": "https://aclanthology.org/2023.findings-acl.330/",
      "title": "A Sequence-to-Sequence&Set Model for Text-to-Table Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.330.pdf",
      "authors": [
        "Tong Li",
        "Zhihao Wang",
        "Liangying Shao",
        "Xuling Zheng",
        "Xiaoli Wang",
        "Jinsong Su"
      ],
      "abstract": "Recently, the text-to-table generation task has attracted increasing attention due to its wide applications. In this aspect, the dominant model formalizes this task as a sequence-to-sequence generation task and serializes each table into a token sequence during training by concatenating all rows in a top-down order. However, it suffers from two serious defects: 1) the predefined order introduces a wrong bias during training, which highly penalizes shifts in the order between rows; 2) the error propagation problem becomes serious when the model outputs a long token sequence. In this paper, we first conduct a preliminary study to demonstrate the generation of most rows is order-insensitive. Furthermore, we propose a novel sequence-to-sequence&set text-to-table generation model. Specifically, in addition to a text encoder encoding the input text, our model is equipped with a table header generator to first output a table header, i.e., the first row of the table, in the manner of sequence generation. Then we use a table body generator with learnable row embeddings and column embeddings to generate a set of table body rows in parallel. Particularly, to deal with the issue that there is no correspondence between each generated table body row and target during training, we propose a target assignment strategy based on the bipartite matching between the first cells of generated table body rows and targets. Experiment results show that our model significantly surpasses the baselines, achieving state-of-the-art performance on commonly-used datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1580,
      "original_url": "https://aclanthology.org/2023.findings-acl.331/",
      "title": "Automatic Readability Assessment for Closely Related Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.331.pdf",
      "authors": [
        "Joseph Marvin Imperial",
        "Ekaterina Kochmar"
      ],
      "abstract": "In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models’ accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines—Tagalog, Bikol, and Cebuano—to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, significantly improves the performance of ARA models compared to the use of off-the-shelf large multilingual language models alone. Consequently, when both linguistic representations are combined, we achieve state-of-the-art results for Tagalog and Cebuano, and baseline scores for ARA in Bikol."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1581,
      "original_url": "https://aclanthology.org/2023.findings-acl.332/",
      "title": "Towards Robust Ranker for Text Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.332.pdf",
      "authors": [
        "Yucheng Zhou",
        "Tao Shen",
        "Xiubo Geng",
        "Chongyang Tao",
        "Can Xu",
        "Guodong Long",
        "Binxing Jiao",
        "Daxin Jiang"
      ],
      "abstract": "A neural ranker plays an indispensable role in the de facto ‘retrieval & rerank’ pipeline, but its training still lags behind due to the weak negative mining during contrastive learning. Compared to retrievers boosted by self-adversarial (i.e., in-distribution) negative mining, the ranker’s heavy structure suffers from query-document combinatorial explosions, so it can only resort to the negative sampled by the fast yet out-of-distribution retriever. Thereby, the moderate negatives compose ineffective contrastive learning samples, becoming the main barrier to learning a robust ranker. To alleviate this, we propose a multi-adversarial training strategy that leverages multiple retrievers as generators to challenge a ranker, where i) diverse hard negatives from a joint distribution are prone to fool the ranker for more effective adversarial learning and ii) involving extensive out-of-distribution label noises renders the ranker against each noise distribution, leading to more challenging and robust contrastive learning. To evaluate our robust ranker (dubbed R2anker), we conduct experiments in various settings on the passage retrieval benchmarks, including BM25-reranking, full-ranking, retriever distillation, etc. The empirical results verify the new state-of-the-art effectiveness of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1582,
      "original_url": "https://aclanthology.org/2023.findings-acl.333/",
      "title": "Semi-Supervised Domain Adaptation for Emotion-Related Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.333.pdf",
      "authors": [
        "Mahshid Hosseini",
        "Cornelia Caragea"
      ],
      "abstract": "Semi-supervised domain adaptation (SSDA) adopts a model trained from a label-rich source domain to a new but related domain with a few labels of target data. It is shown that, in an SSDA setting, a simple combination of domain adaptation (DA) with semi-supervised learning (SSL) techniques often fails to effectively utilize the target supervision and cannot address distribution shifts across different domains due to the training data bias toward the source-labeled samples. In this paper, inspired by the co-learning of multiple classifiers for the computer vision tasks, we propose to decompose the SSDA framework for emotion-related tasks into two subcomponents of unsupervised domain adaptation (UDA) from the source to the target domain and semi-supervised learning (SSL) in the target domain where the two models iteratively teach each other by interchanging their high confident predictions. We further propose a novel data cartography-based regularization technique for pseudo-label denoising that employs training dynamics to further hone our models’ performance. We publicly release our code."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1583,
      "original_url": "https://aclanthology.org/2023.findings-acl.334/",
      "title": "Boosting Distress Support Dialogue Responses with Motivational Interviewing Strategy",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.334.pdf",
      "authors": [
        "Anuradha Welivita",
        "Pearl Pu"
      ],
      "abstract": "AI-driven chatbots have become an emerging solution to address psychological distress. Due to the lack of psychotherapeutic data, researchers use dialogues scraped from online peer support forums to train them. But since the responses in such platforms are not given by professionals, they contain both conforming and non-conforming responses. In this work, we attempt to recognize these conforming and non-conforming response types present in online distress-support dialogues using labels adapted from a well-established behavioral coding scheme named Motivational Interviewing Treatment Integrity (MITI) code and show how some response types could be rephrased into a more MI adherent form that can, in turn, enable chatbot responses to be more compliant with the MI strategy. As a proof of concept, we build several rephrasers by fine-tuning Blender and GPT3 to rephrase MI non-adherent Advise without permission responses into Advise with permission. We show how this can be achieved with the construction of pseudo-parallel corpora avoiding costs for human labor. Through automatic and human evaluation we show that in the presence of less training data, techniques such as prompting and data augmentation can be used to produce substantially good rephrasings that reflect the intended style and preserve the content of the original text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1584,
      "original_url": "https://aclanthology.org/2023.findings-acl.335/",
      "title": "ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.335.pdf",
      "authors": [
        "Zhen Han",
        "Ruotong Liao",
        "Jindong Gu",
        "Yao Zhang",
        "Zifeng Ding",
        "Yujia Gu",
        "Heinz Koeppl",
        "Hinrich Schütze",
        "Volker Tresp"
      ],
      "abstract": "Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning temporally relevant texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three new datasets for training and evaluating ECOLA. Extensive experiments show that ECOLA significantly enhances temporal KG embedding models with up to 287% relative improvements regarding Hits@1 on the link prediction task. The code and models are publicly available on https://github.com/mayhugotong/ECOLA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1585,
      "original_url": "https://aclanthology.org/2023.findings-acl.336/",
      "title": "Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.336.pdf",
      "authors": [
        "Somayeh Ghanbarzadeh",
        "Yan Huang",
        "Hamid Palangi",
        "Radames Cruz Moreno",
        "Hamed Khanpour"
      ],
      "abstract": "Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore, these methods hurt the PLMs’ performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks’ datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning’s training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs’ performance on downstream tasks solely using the downstream tasks’ dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1586,
      "original_url": "https://aclanthology.org/2023.findings-acl.337/",
      "title": "TextObfuscator: Making Pre-trained Language Model a Privacy Protector via Obfuscating Word Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.337.pdf",
      "authors": [
        "Xin Zhou",
        "Yi Lu",
        "Ruotian Ma",
        "Tao Gui",
        "Yuran Wang",
        "Yong Ding",
        "Yibo Zhang",
        "Qi Zhang",
        "Xuanjing Huang"
      ],
      "abstract": "In real-world applications, pre-trained language models are typically deployed on the cloud, allowing clients to upload data and perform compute-intensive inference remotely. To avoid sharing sensitive data directly with service providers, clients can upload numerical representations rather than plain text to the cloud. However, recent text reconstruction techniques have demonstrated that it is possible to transform representations into original words, suggesting that privacy risk remains. In this paper, we propose TextObfuscator, a novel framework for protecting inference privacy by applying random perturbations to clustered representations. The random perturbations make the representations indistinguishable from surrounding clustered representations, thus obscuring word information while retaining the original word functionality. To achieve this, we utilize prototypes to learn clustered representation, where tokens of similar functionality are encouraged to be closer to the same prototype during training. Additionally, we design different methods to find prototypes for token-level and sentence-level tasks, which can improve performance by incorporating semantic and task information. Experimental results on token and sentence classification tasks show that TextObfuscator achieves improvement over compared methods without increasing inference cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1587,
      "original_url": "https://aclanthology.org/2023.findings-acl.338/",
      "title": "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.338.pdf",
      "authors": [
        "Kelly Marchisio",
        "Patrick Lewis",
        "Yihong Chen",
        "Mikel Artetxe"
      ],
      "abstract": "Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model’s parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MINIJOINT, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MINIPOST, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a small number of parameters on top. Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches the performance of the standard approach using up to 2.3x less compute on average."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1588,
      "original_url": "https://aclanthology.org/2023.findings-acl.339/",
      "title": "DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.339.pdf",
      "authors": [
        "Bo Lv",
        "Xin Liu",
        "Shaojie Dai",
        "Nayu Liu",
        "Fan Yang",
        "Ping Luo",
        "Yue Yu"
      ],
      "abstract": "Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios. Typically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.However, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed. In this work, we propose a novel Discriminate Soft Prompts (DSP) approach to take advantage of the prompt-based methods to strengthen the transmission of general knowledge. Specifically, we develop a discriminative prompt method, which reformulates zero-shot tasks into token discrimination tasks without having to construct verbalizers.Furthermore, to improve the inference speed of the prompt-based methods, we design a soft prompt co-reference strategy, which leverages soft prompts to approximately refer to the vector representation of text tokens. The experimental results show that, our model outperforms baselines on two zero-shot entity recognition datasets with higher inference speed, and obtains a 7.5% average relation F1-score improvement over previous state-of-the-art models on Wiki-ZSL and FewRel."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1589,
      "original_url": "https://aclanthology.org/2023.findings-acl.340/",
      "title": "Exploring Robust Overfitting for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.340.pdf",
      "authors": [
        "Bin Zhu",
        "Yanghui Rao"
      ],
      "abstract": "We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model’s memorization of the adversarial training data. We attempt to mitigate robust overfitting by combining regularization methods with adversarial training. Following the philosophy that prevents the model from memorizing the adversarial data, we find that flooding, a regularization method with loss scaling, can mitigate robust overfitting for pre-trained language models. Eventually, we investigate the effect of flooding levels and evaluate the models’ adversarial robustness under textual attacks. Extensive experiments demonstrate that our methods can mitigate robust overfitting upon three top adversarial training methods and further promote adversarial robustness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1590,
      "original_url": "https://aclanthology.org/2023.findings-acl.341/",
      "title": "Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.341.pdf",
      "authors": [
        "Jifan Chen",
        "Yuhao Zhang",
        "Lan Liu",
        "Rui Dong",
        "Xinchi Chen",
        "Patrick Ng",
        "William Yang Wang",
        "Zhiheng Huang"
      ],
      "abstract": "There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).However, existing methods typically encode task information with a simple dataset name as a prefix to the encoder. This not only limits the effectiveness of multi-task learning, but also hinders the model’s ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications. In this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models. We design the task configurations to explicitly specify the task type, as well as its input and output types. We show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner. We demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1591,
      "original_url": "https://aclanthology.org/2023.findings-acl.342/",
      "title": "D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.342.pdf",
      "authors": [
        "Sabit Hassan",
        "Malihe Alikhani"
      ],
      "abstract": "Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. While active learning (AL) has shown promise in training models with a small amount of annotated data, AL’s reliance on the model’s behavior for selective sampling can lead to an accumulation of unwanted bias rather than bias mitigation. However, infusing clustering with AL can overcome the bias issue of both AL and traditional annotation methods while exploiting AL’s annotation efficiency. In this paper, we propose a novel adaptive clustering-based active learning algorithm, D-CALM, that dynamically adjusts clustering and annotation efforts in response to an estimated classifier error-rate. Experiments on eight datasets for a diverse set of text classification tasks, including emotion, hatespeech, dialog act, and book type detection, demonstrate that our proposed algorithm significantly outperforms baseline AL approaches with both pretrained transformers and traditional Support Vector Machines. D-CALM showcases robustness against different measures of information gain and, as evident from our analysis of label and error distribution, can significantly reduce unwanted model bias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1592,
      "original_url": "https://aclanthology.org/2023.findings-acl.343/",
      "title": "Language Anisotropic Cross-Lingual Model Editing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.343.pdf",
      "authors": [
        "Yang Xu",
        "Yutai Hou",
        "Wanxiang Che",
        "Min Zhang"
      ],
      "abstract": "Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model’s raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual model editing task, we empirically demonstrate the failure of monolingual baselines in propagating the edit to multiple languages and the effectiveness of the proposed language anisotropic model editing. Our code is publicly available at https://github.com/franklear/LiME."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1593,
      "original_url": "https://aclanthology.org/2023.findings-acl.344/",
      "title": "Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.344.pdf",
      "authors": [
        "Brendan King",
        "Jeffrey Flanigan"
      ],
      "abstract": "There has been significant interest in zero and few-shot learning for dialogue state tracking (DST) due to the high cost of collecting and annotating task-oriented dialogues. Recent work has demonstrated that in-context learning requires very little data and zero parameter updates, and even outperforms trained methods in the few-shot setting. We propose RefPyDST, which advances the state of the art with three advancements to in-context learning for DST.First, we formulate DST as a Python programming task, explicitly modeling language coreference as variable reference in Python. Second, since in-context learning depends highly on the context examples, we propose a method to retrieve a diverse set of relevant examples to improve performance. Finally, we introduce a novel re-weighting method during decoding that takes into account probabilities of competing surface forms, and produces a more accurate dialogue state prediction. We evaluate our approach using MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero and few-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1594,
      "original_url": "https://aclanthology.org/2023.findings-acl.345/",
      "title": "Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.345.pdf",
      "authors": [
        "Chunliu Wang",
        "Huiyuan Lai",
        "Malvina Nissim",
        "Johan Bos"
      ],
      "abstract": "Pre-trained language models (PLMs) have achieved great success in NLP and have recently been used for tasks in computational semantics. However, these tasks do not fully benefit from PLMs since meaning representations are not explicitly included. We introduce multilingual pre-trained language-meaning models based on Discourse Representation Structures (DRSs), including meaning representations besides natural language texts in the same model, and design a new strategy to reduce the gap between the pre-training and fine-tuning objectives. Since DRSs are language neutral, cross-lingual transfer learning is adopted to further improve the performance of non-English tasks. Automatic evaluation results show that our approach achieves the best performance on both the multilingual DRS parsing and DRS-to-text generation tasks. Correlation analysis between automatic metrics and human judgements on the generation task further validates the effectiveness of our model. Human inspection reveals that out-of-vocabulary tokens are the main cause of erroneous results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1595,
      "original_url": "https://aclanthology.org/2023.findings-acl.346/",
      "title": "Multi-modal Sarcasm Generation: Dataset and Solution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.346.pdf",
      "authors": [
        "Wenye Zhao",
        "Qingbao Huang",
        "Dongsheng Xu",
        "Peizhi Zhao"
      ],
      "abstract": "As an interesting and challenging task, sarcasm generation has attracted widespread attention. Although very recent studies have made promising progress, none of them considers generating a sarcastic description for a given image - as what people are doing on Twitter. In this paper, we present a Multi-modal Sarcasm Generation (MSG) task: Given an image with hashtags that provide the sarcastic target, MSG aims to generate sarcastic descriptions like humans. Different from textual sarcasm generation, MSG is more challenging as it is difficult to accurately capture the key information from images, hashtags, and OCR tokens and exploit multi-modal incongruity to generate sarcastic descriptions. To support the research on MSG, we develop MuSG, a new dataset with 5000 images and related Twitter text. We also propose a multi-modal Transformer-based method as a solution to this MSG task. The input features are embedded in the common space and passed through the multi-modal Transformer layers to generate the sarcastic descriptions by the auto-regressive paradigm. Both automatic and manual evaluations demonstrate the superiority of our method. The dataset and code will be available soon."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1596,
      "original_url": "https://aclanthology.org/2023.findings-acl.347/",
      "title": "Rethinking Semi-supervised Learning with Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.347.pdf",
      "authors": [
        "Zhengxiang Shi",
        "Francesco Tonolini",
        "Nikolaos Aletras",
        "Emine Yilmaz",
        "Gabriella Kazai",
        "Yunlong Jiao"
      ],
      "abstract": "Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of the unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist, and highlights TAPT as a potential solution."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1597,
      "original_url": "https://aclanthology.org/2023.findings-acl.348/",
      "title": "Retrieval-Based Transformer for Table Augmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.348.pdf",
      "authors": [
        "Michael Glass",
        "Xueqing Wu",
        "Ankita Rajaram Naik",
        "Gaetano Rossiello",
        "Alfio Gliozzo"
      ],
      "abstract": "Data preparation, also called data wrangling, is considered one of the most expensive and time-consuming steps when performing analytics or building machine learning models. Preparing data typically involves collecting and merging data from complex heterogeneous, and often large-scale data sources, such as data lakes. In this paper, we introduce a novel approach toward automatic data wrangling in an attempt to alleviate the effort of end-users, e.g. data analysts, in structuring dynamic views from data lakes in the form of tabular data. Given a corpus of tables, we propose a retrieval augmented transformer model that is self-trained for the table augmentation tasks of row/column population and data imputation. Our self-learning strategy consists in randomly ablating tables from the corpus and training the retrieval-based model with the objective of reconstructing the partial tables given as input with the original values or headers. We adopt this strategy to first train the dense neural retrieval model encoding portions of tables to vectors, and then the end-to-end model trained to perform table augmentation tasks. We test on EntiTables, the standard benchmark for table augmentation, as well as introduce a new benchmark to advance further research: WebTables. Our model consistently and substantially outperforms both supervised statistical methods and the current state-of-the-art transformer-based models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1598,
      "original_url": "https://aclanthology.org/2023.findings-acl.349/",
      "title": "ECG-QALM: Entity-Controlled Synthetic Text Generation using Contextual Q&A for NER",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.349.pdf",
      "authors": [
        "Karan Aggarwal",
        "Henry Jin",
        "Aitzaz Ahmad"
      ],
      "abstract": "Named Entity Recognition (NER) state-of-the-art methods requires high-quality labeled datasets. Issues such as scarcity of labeled data, under-representation of entities, and privacy concerns with using sensitive data for training, can be significant barriers. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose ECG-QALM, a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text. Generated text is then used to augment small labeled datasets for downstream NER tasks. We evaluate our method on two publicly available datasets. We find ECG-QALM is capable of producing full text samples with desired entities appearing in a controllable way, while retaining sentence coherence closest to the real world data. Evaluations on NER tasks show significant improvements (75% - 140%) in low-labeled data regimes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1599,
      "original_url": "https://aclanthology.org/2023.findings-acl.350/",
      "title": "Tokenization Impacts Multilingual Language Modeling: Assessing Vocabulary Allocation and Overlap Across Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.350.pdf",
      "authors": [
        "Tomasz Limisiewicz",
        "Jiří Balhar",
        "David Mareček"
      ],
      "abstract": "Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1600,
      "original_url": "https://aclanthology.org/2023.findings-acl.351/",
      "title": "The Whole Truth and Nothing But the Truth: Faithful and Controllable Dialogue Response Generation with Dataflow Transduction and Constrained Decoding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.351.pdf",
      "authors": [
        "Hao Fang",
        "Anusha Balakrishnan",
        "Harsh Jhamtani",
        "John Bufe",
        "Jean Crawford",
        "Jayant Krishnamurthy",
        "Adam Pauls",
        "Jason Eisner",
        "Jacob Andreas",
        "Dan Klein"
      ],
      "abstract": "In a real-world dialogue system, generated text must be truthful and informative while remaining fluent and adhering to a prescribed style. Satisfying these constraints simultaneously isdifficult for the two predominant paradigms in language generation: neural language modeling and rule-based generation. We describe a hybrid architecture for dialogue response generation that combines the strengths of both paradigms. The first component of this architecture is a rule-based content selection model defined using a new formal framework called dataflow transduction, which uses declarative rules to transduce a dialogue agent’s actions and their results (represented as dataflow graphs) into context-free grammars representing the space of contextually acceptable responses. The second component is a constrained decoding procedure that uses these grammars to constrain the output of a neural language model, which selects fluent utterances. Our experiments show that this system outperforms both rule-based and learned approaches in human evaluations of fluency, relevance, and truthfulness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1601,
      "original_url": "https://aclanthology.org/2023.findings-acl.352/",
      "title": "Know What I don’t Know: Handling Ambiguous and Unknown Questions for Text-to-SQL",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.352.pdf",
      "authors": [
        "Bing Wang",
        "Yan Gao",
        "Zhoujun Li",
        "Jian-Guang Lou"
      ],
      "abstract": "The task of text-to-SQL aims to convert a natural language question into its corresponding SQL query within the context of relational tables. Existing text-to-SQL parsers generate a plausible SQL query for an arbitrary user question, thereby failing to correctly handle problematic user questions. To formalize this problem, we conduct a preliminary study on the observed ambiguous and unanswerable cases in text-to-SQL and summarize them into 6 feature categories. Correspondingly, we identify the causes behind each category and propose requirements for handling ambiguous and unanswerable questions. Following this study, we propose a simple yet effective counterfactual example generation approach that automatically produces ambiguous and unanswerable text-to-SQL examples. Furthermore, we propose a weakly supervised DTE (Detecting-Then-Explaining) model for error detection, localization, and explanation. Experimental results show that our model achieves the best result on both real-world examples and generated examples compared with various baselines. We release our data and code at: https://github.com/wbbeyourself/DTE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1602,
      "original_url": "https://aclanthology.org/2023.findings-acl.353/",
      "title": "Rethinking Document-Level Relation Extraction: A Reality Check",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.353.pdf",
      "authors": [
        "Jing Li",
        "Yequan Wang",
        "Shuai Zhang",
        "Min Zhang"
      ],
      "abstract": "Recently, numerous efforts have continued to push up performance boundaries of document-level relation extraction (DocRE) and have claimed significant progress in DocRE. In this paper, we do not aim at proposing a novel model for DocRE. Instead, we take a closer look at the field to see if these performance gains are actually true. By taking a comprehensive literature review and a thorough examination of popular DocRE datasets, we find that these performance gains are achieved upon a strong or even untenable assumption in common: all named entities are perfectly localized, normalized, and typed in advance. Next, we construct four types of entity mention attacks to examine the robustness of typical DocRE models by behavioral probing. We also have a close check on model usability in a more realistic setting. Our findings reveal that most of current DocRE models are vulnerable to entity mention attacks and difficult to be deployed in real-world end-user NLP applications. Our study calls more attentions for future research to stop simplifying problem setups, and to model DocRE in the wild rather than in an unrealistic Utopian world."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1603,
      "original_url": "https://aclanthology.org/2023.findings-acl.354/",
      "title": "Optimizing Test-Time Query Representations for Dense Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.354.pdf",
      "authors": [
        "Mujeen Sung",
        "Jungsoo Park",
        "Jaewoo Kang",
        "Danqi Chen",
        "Jinhyuk Lee"
      ],
      "abstract": "Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient descent. Our theoretical analysis reveals that TOUR can be viewed as a generalization of the classical Rocchio algorithm for pseudo relevance feedback, and we present two variants that leverage pseudo-labels as hard binary or soft continuous labels. We first apply TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR greatly improves end-to-end open-domain question answering accuracy, as well as passage retrieval performance. TOUR also consistently improves direct re-ranking by up to 2.0% while running 1.3–2.4x faster with an efficient implementation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1604,
      "original_url": "https://aclanthology.org/2023.findings-acl.355/",
      "title": "A Customized Text Sanitization Mechanism with Differential Privacy",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.355.pdf",
      "authors": [
        "Sai Chen",
        "Fengran Mo",
        "Yanhao Wang",
        "Cen Chen",
        "Jian-Yun Nie",
        "Chengyu Wang",
        "Jamie Cui"
      ],
      "abstract": "As privacy issues are receiving increasing attention within the Natural Language Processing (NLP) community, numerous methods have been proposed to sanitize texts subject to differential privacy. However, the state-of-the-art text sanitization mechanisms based on a relaxed notion of metric local differential privacy (MLDP) do not apply to non-metric semantic similarity measures and cannot achieve good privacy-utility trade-offs. To address these limitations, we propose a novel Customized Text sanitization (CusText) mechanism based on the original 𝜖-differential privacy (DP) definition, which is compatible with any similarity measure.Moreover, CusText assigns each input token a customized output set to provide more advanced privacy protection at the token level.Extensive experiments on several benchmark datasets show that CusText achieves a better trade-off between privacy and utility than existing mechanisms.The code is available at https://github.com/sai4july/CusText."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1605,
      "original_url": "https://aclanthology.org/2023.findings-acl.356/",
      "title": "LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.356.pdf",
      "authors": [
        "Peng Lu",
        "Ahmad Rashid",
        "Ivan Kobyzev",
        "Mehdi Rezagholizadeh",
        "Phillippe Langlais"
      ],
      "abstract": "Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally, we conduct extensive experiments and demonstrate our LABO consistently yields improvement over conventional label regularization on various fields, including seven machine translation and three image classification tasks across various neural network architectures while maintaining training efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1606,
      "original_url": "https://aclanthology.org/2023.findings-acl.357/",
      "title": "Frustratingly Easy Label Projection for Cross-lingual Transfer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.357.pdf",
      "authors": [
        "Yang Chen",
        "Chao Jiang",
        "Alan Ritter",
        "Wei Xu"
      ],
      "abstract": "Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we call EasyProject, is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show EasyProject works well because it can accurately preserve label span boundaries after translation. We will publicly release all our code and data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1607,
      "original_url": "https://aclanthology.org/2023.findings-acl.358/",
      "title": "Enhancing Hierarchical Text Classification through Knowledge Graph Integration",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.358.pdf",
      "authors": [
        "Ye Liu",
        "Kai Zhang",
        "Zhenya Huang",
        "Kehang Wang",
        "Yanghai Zhang",
        "Qi Liu",
        "Enhong Chen"
      ],
      "abstract": "Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the taxonomic hierarchy, experts make inference based on their prior knowledge and experience. For machines to achieve this capability, we propose a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC. Specifically, K-HTC innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods. Additionally, a novel knowledge-aware contrastive learning strategy is proposed to further exploit the information inherent in the data. Extensive experiments on two publicly available HTC datasets show the efficacy of our proposed method, and indicate the necessity of incorporating knowledge graphs in HTC tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1608,
      "original_url": "https://aclanthology.org/2023.findings-acl.359/",
      "title": "How Many Answers Should I Give? An Empirical Study of Multi-Answer Reading Comprehension",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.359.pdf",
      "authors": [
        "Chen Zhang",
        "Jiuheng Lin",
        "Xiao Liu",
        "Yuxuan Lai",
        "Yansong Feng",
        "Dongyan Zhao"
      ],
      "abstract": "The multi-answer phenomenon, where a question may have multiple answers scattered in the document, can be well handled by humans but is challenging enough for machine reading comprehension (MRC) systems. Despite recent progress in multi-answer MRC, there lacks a systematic analysis of how this phenomenon arises and how to better address it. In this work, we design a taxonomy to categorize commonly-seen multi-answer MRC instances, with which we inspect three multi-answer datasets and analyze where the multi-answer challenge comes from. We further analyze how well different paradigms of current multi-answer MRC models deal with different types of multi-answer instances. We find that some paradigms capture well the key information in the questions while others better model the relation between questions and contexts. We thus explore strategies to make the best of the strengths of different paradigms. Experiments show that generation models can be a promising platform to incorporate different paradigms. Our annotations and code are released for further research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1609,
      "original_url": "https://aclanthology.org/2023.findings-acl.360/",
      "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.360.pdf",
      "authors": [
        "Yova Kementchedjhieva",
        "Ilias Chalkidis"
      ],
      "abstract": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets—two in the legal domain and two in the biomedical domain, each with two levels of label granularity— and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1610,
      "original_url": "https://aclanthology.org/2023.findings-acl.361/",
      "title": "Domain Incremental Lifelong Learning in an Open World",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.361.pdf",
      "authors": [
        "Yi Dai",
        "Hao Lang",
        "Yinhe Zheng",
        "Bowen Yu",
        "Fei Huang",
        "Yongbin Li"
      ],
      "abstract": "Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose Diana: a dynamic architecture-based lifelong learning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model’s generalization performance. Moreover, we dedicate separate prompts to explicitly model unseen tasks and introduce a set of prompt key vectors to facilitate knowledge sharing between tasks. Extensive experiments demonstrate that Diana outperforms state-of-the-art LL models, especially in handling unseen tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1611,
      "original_url": "https://aclanthology.org/2023.findings-acl.362/",
      "title": "Improving Knowledge Graph Completion with Generative Hard Negative Mining",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.362.pdf",
      "authors": [
        "Zile Qiao",
        "Wei Ye",
        "Dingyao Yu",
        "Tong Mo",
        "Weiping Li",
        "Shikun Zhang"
      ],
      "abstract": "Contrastive learning has recently shown great potential to improve text-based knowledge graph completion (KGC). In this paper, we propose to learn a more semantically structured entity representation space in text-based KGC via hard negatives mining. Specifically, we novelly leverage a sequence-to-sequence architecture to generate high-quality hard negatives. These negatives are sampled from the same decoding distributions as the anchor (or correct entity), inherently being semantically close to the anchor and thus enjoying good hardness. A self-information-enhanced contrasting strategy is further incorporated into the Seq2Seq generator to systematically diversify the produced negatives. Extensive experiments on three KGC benchmarks demonstrate the sound hardness and diversity of our generated negatives and the resulting performance superiority on KGC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1612,
      "original_url": "https://aclanthology.org/2023.findings-acl.363/",
      "title": "Visually-Enhanced Phrase Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.363.pdf",
      "authors": [
        "Tsu-Yuan Hsu",
        "Chen-An Li",
        "Chao-Wei Huang",
        "Yun-Nung Chen"
      ],
      "abstract": "Large-scale vision-language pre-training has exhibited strong performance in various visual and textual understanding tasks. Recently, the textual encoders of multi-modal pre-trained models have been shown to generate high-quality textual representations, which often outperform models that are purely text-based, such as BERT. In this study, our objective is to utilize both textual and visual encoders of multi-modal pre-trained models to enhance language understanding tasks. We achieve this by generating an image associated with a textual prompt, thus enriching the representation of a phrase for downstream tasks. Results from experiments conducted on four benchmark datasets demonstrate that our proposed method, which leverages visually-enhanced text representations, significantly improves performance in the entity clustering task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1613,
      "original_url": "https://aclanthology.org/2023.findings-acl.364/",
      "title": "Reasoning in Large Language Models Through Symbolic Math Word Problems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.364.pdf",
      "authors": [
        "Vedant Gaur",
        "Nikunj Saunshi"
      ],
      "abstract": "Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a “concise explanation” of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3’s davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model’s reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP-Sym dataset will be released for future research on symbolic math problems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1614,
      "original_url": "https://aclanthology.org/2023.findings-acl.365/",
      "title": "It’s not Sexually Suggestive; It’s Educative | Separating Sex Education from Suggestive Content on TikTok videos",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.365.pdf",
      "authors": [
        "Enfa George",
        "Mihai Surdeanu"
      ],
      "abstract": "We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator’s point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children’s exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable (Mitchell et al. 2014). The platform’s current system removes/punishes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1615,
      "original_url": "https://aclanthology.org/2023.findings-acl.366/",
      "title": "Dynamic Structured Neural Topic Model with Self-Attention Mechanism",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.366.pdf",
      "authors": [
        "Nozomu Miyamoto",
        "Masaru Isonuma",
        "Sho Takase",
        "Junichiro Mori",
        "Ichiro Sakata"
      ],
      "abstract": "This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies. Our model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism. Additionally, we introduce citation regularization, which induces attention weights to represent citation relations by modeling text and citations jointly. Our model outperforms a prior dynamic embedded topic model regarding perplexity and coherence, while maintaining sufficient diversity across topics. Furthermore, we confirm that our model can potentially predict emerging topics from academic literature."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1616,
      "original_url": "https://aclanthology.org/2023.findings-acl.367/",
      "title": "Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.367.pdf",
      "authors": [
        "Qiang Wang",
        "Xinhui Hu",
        "Ming Chen"
      ],
      "abstract": "This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT. Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction for every k tokens, k>1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Experiments on five translation tasks show that HRT achieves comparable translation quality with AT while having at least 1.5x faster inference regardless of batch size and device. Additionally, HRT successfully inherits the sound characteristics of AT in the deep-encoder-shallow-decoder architecture, allowing for further speedup without BLEU loss."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1617,
      "original_url": "https://aclanthology.org/2023.findings-acl.368/",
      "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.368.pdf",
      "authors": [
        "Wangchunshu Zhou",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "abstract": "Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text. In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a general-purpose pre-trained language model. It first exploits general texts to form queries for extracting commonsense knowledge from the neural commonsense knowledge model and then refines the language model with two self-supervised objectives: commonsense mask infilling and commonsense relation prediction, which align human language with the underlying commonsense knowledge. Empirical results show that our approach consistently improves the model’s performance on downstream tasks that require commonsense reasoning. Moreover, we find that the improvement is more significant in the few-shot setting. This suggests that our approach helps language models better transfer to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1618,
      "original_url": "https://aclanthology.org/2023.findings-acl.369/",
      "title": "Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.369.pdf",
      "authors": [
        "Shadi Iskander",
        "Kira Radinsky",
        "Yonatan Belinkov"
      ],
      "abstract": "Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model’s representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1619,
      "original_url": "https://aclanthology.org/2023.findings-acl.370/",
      "title": "Focal Training and Tagger Decouple for Grammatical Error Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.370.pdf",
      "authors": [
        "Minghuan Tan",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "abstract": "In this paper, we investigate how to improve tagging-based Grammatical Error Correction models. We address two issues of current tagging-based approaches, label imbalance issue, and tagging entanglement issue. Then we propose to down-weight the loss of well-classified labels using Focal Loss and decouple the error detection layer from the label tagging layer through an extra self-attention-based matching module. Experiments over three latest Chinese Grammatical Error Correction datasets show that our proposed methods are effective. We further analyze choices of hyper-parameters for Focal Loss and inference tweaking."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1620,
      "original_url": "https://aclanthology.org/2023.findings-acl.371/",
      "title": "LET: Leveraging Error Type Information for Grammatical Error Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.371.pdf",
      "authors": [
        "Lingyu Yang",
        "Hongjia Li",
        "Lei Li",
        "Chengyin Xu",
        "Shutao Xia",
        "Chun Yuan"
      ],
      "abstract": "Grammatical error correction (GEC) aims to correct errors in given sentences and is significant to many downstream natural language understanding tasks. Recent work introduces the idea of grammatical error detection (GED) to improve the GEC task performance. In contrast, these explicit multi-stage works propagate and amplify the problem of misclassification of the GED module. To introduce more convincing error type information, we propose an end-to-end framework in this paper, which Leverages Error Type (LET) information in the generation process. First, the input text is fed into a classification module to obtain the error type corresponding to each token. Then, we introduce the category information into the decoder’s input and cross-attention module in two ways, respectively. Experiments on various datasets show that our proposed method outperforms existing methods by a clear margin."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1621,
      "original_url": "https://aclanthology.org/2023.findings-acl.372/",
      "title": "On the Role of Parallel Data in Cross-lingual Transfer Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.372.pdf",
      "authors": [
        "Machel Reid",
        "Mikel Artetxe"
      ],
      "abstract": "While prior work has established that the use of parallel data is conducive for cross-lingual learning, it is unclear if the improvements come from the data itself, or if it is the modeling of parallel interactions that matters. Exploring this, we examine the usage of unsupervised machine translation to generate synthetic parallel data, and compare it to supervised machine translation and gold parallel data. We find that even model generated parallel data can be useful for downstream tasks, in both a general setting (continued pretraining) as well as the task-specific setting (translate-train), although our best results are still obtained using real parallel data. Our findings suggest that existing multilingual models do not exploit the full potential of monolingual data, and prompt the community to reconsider the traditional categorization of cross-lingual learning approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1622,
      "original_url": "https://aclanthology.org/2023.findings-acl.373/",
      "title": "CoMave: Contrastive Pre-training with Multi-scale Masking for Attribute Value Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.373.pdf",
      "authors": [
        "Xinnan Guo",
        "Wentao Deng",
        "Yongrui Chen",
        "Yang Li",
        "Mengdi Zhou",
        "Guilin Qi",
        "Tianxing Wu",
        "Dong Yang",
        "Liubin Wang",
        "Yong Pan"
      ],
      "abstract": "Attribute Value Extraction (AVE) aims to automatically obtain attribute value pairs from product descriptions to aid e-commerce. Despite the progressive performance of existing approaches in e-commerce platforms, they still suffer from two challenges: 1) difficulty in identifying values at different scales simultaneously; 2) easy confusion by some highly similar fine-grained attributes. This paper proposes a pre-training technique for AVE to address these issues. In particular, we first improve the conventional token-level masking strategy, guiding the language model to understand multi-scale values by recovering spans at the phrase and sentence level. Second, we apply clustering to build a challenging negative set for each example and design a pre-training objective based on contrastive learning to force the model to discriminate similar attributes. Comprehensive experiments show that our solution provides a significant improvement over traditional pre-trained models in the AVE task, and achieves state-of-the-art on four benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1623,
      "original_url": "https://aclanthology.org/2023.findings-acl.374/",
      "title": "Phrase Retrieval for Open Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.374.pdf",
      "authors": [
        "Soyeong Jeong",
        "Jinheon Baek",
        "Sung Ju Hwang",
        "Jong Park"
      ],
      "abstract": "Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when retrieving the phrase for the current context, by maximizing representational similarities of consecutive turns in a conversation while minimizing irrelevant conversational contexts. We validate our model on two ODConvQA datasets, whose experimental results show that it substantially outperforms the relevant baselines with the retriever-reader. Code is available at: https://github.com/starsuzi/PRO-ConvQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1624,
      "original_url": "https://aclanthology.org/2023.findings-acl.375/",
      "title": "Unlearning Bias in Language Models by Partitioning Gradients",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.375.pdf",
      "authors": [
        "Charles Yu",
        "Sullam Jeoung",
        "Anish Kasi",
        "Pengfei Yu",
        "Heng Ji"
      ],
      "abstract": "Recent research has shown that large-scale pretrained language models, specifically transformers, tend to exhibit issues relating to racism, sexism, religion bias, and toxicity in general. Unfortunately, these pretrained language models are used almost universally in downstream tasks, and natural language processing is often applied to make real-world predictions. Thus, debiasing these language models as early in development as possible is increasingly crucial for preventing unintentional harms caused by natural language systems. To this end, we propose a new technique called partitioned contrastive gradient unlearning (PCGU), a gray-box method for debiasing pretrained masked language models. PCGU aims to optimize only the weights that contribute most to a specific domain of bias, doing so by computing a first-order approximation based on the gradients of contrastive sentence pairs. Our experiments show that PCGU is both low-cost and seems particularly effective at pinpointing the sources of implicit social bias in large pretrained transformers. Although we train using PCGU in the gender-profession domain only, we find that doing so can also partially mitigate bias across other domains. All code for our implementation and experiments can be found at https://github.com/CharlesYu2000/PCGU-UnlearningBias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1625,
      "original_url": "https://aclanthology.org/2023.findings-acl.376/",
      "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.376.pdf",
      "authors": [
        "Aaron Mueller",
        "Kanika Narang",
        "Lambert Mathias",
        "Qifan Wang",
        "Hamed Firooz"
      ],
      "abstract": "Large language models show impressive results on few-shot NLP tasks. However, these models are memory and computation-intensive. Meta-training allows one to leverage smaller models for few-shot generalization in a domain-general and task-agnostic manner; however, these methods alone results in models that may not have sufficient parameterization or knowledge to adapt quickly to a large variety of tasks. To overcome this issue, we propose meta-training with demonstration retrieval, where we use a dense passage retriever to retrieve semantically similar labeled demonstrations to each example for more varied supervision. By separating external knowledge from model parameters, we can use meta-training to train parameter-efficient models that generalize well on a larger variety of tasks. We construct a meta-training set from UnifiedQA and CrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our knowledge, our work is the first to combine retrieval with meta-training, to use DPR models to retrieve demonstrations, and to leverage demonstrations from many tasks simultaneously, rather than randomly sampling demonstrations from the training set of the target task. Our approach outperforms a variety of targeted parameter-efficient and retrieval-augmented few-shot methods on QA, NLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our approach can be meta-trained and fine-tuned quickly on a single GPU."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1626,
      "original_url": "https://aclanthology.org/2023.findings-acl.377/",
      "title": "VCSUM: A Versatile Chinese Meeting Summarization Dataset",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.377.pdf",
      "authors": [
        "Han Wu",
        "Mingjie Zhan",
        "Haochen Tan",
        "Zhaohui Hou",
        "Ding Liang",
        "Linqi Song"
      ],
      "abstract": "Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1627,
      "original_url": "https://aclanthology.org/2023.findings-acl.378/",
      "title": "LEDA: a Large-Organization Email-Based Decision-Dialogue-Act Analysis Dataset",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.378.pdf",
      "authors": [
        "Mladen Karan",
        "Prashant Khare",
        "Ravi Shekhar",
        "Stephen McQuistin",
        "Ignacio Castro",
        "Gareth Tyson",
        "Colin Perkins",
        "Patrick Healey",
        "Matthew Purver"
      ],
      "abstract": "Collaboration increasingly happens online. This is especially true for large groups working on global tasks, with collaborators all around the globe. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of decision-making mechanisms in such groups, and provides a new annotated dataset based on real-world data from the public mail-archives of one such organisation – the Internet Engineering Task Force (IETF). We provide an initial data analysis showing that this dataset can be used to better understand decision-making in such organisations. Finally, we experiment with a preliminary transformer-based dialog act tagging model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1628,
      "original_url": "https://aclanthology.org/2023.findings-acl.379/",
      "title": "Negation Scope Refinement via Boundary Shift Loss",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.379.pdf",
      "authors": [
        "Yin Wu",
        "Aixin Sun"
      ],
      "abstract": "Negation in natural language may affect many NLP applications, e.g., information extraction and sentiment analysis. The key sub-task of negation detection is negation scope resolution which aims to extract the portion of a sentence that is being negated by a negation cue (e.g., keyword “not” and never”) in the sentence. Due to the long spans, existing methods tend to make wrong predictions around the scope boundaries. In this paper, we propose a simple yet effective model named R-BSL which engages the Boundary Shift Loss to refine the predicted boundary. On multiple benchmark datasets, we show that the extremely simple R-BSL achieves best results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1629,
      "original_url": "https://aclanthology.org/2023.findings-acl.380/",
      "title": "Towards Diverse and Effective Question-Answer Pair Generation from Children Storybooks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.380.pdf",
      "authors": [
        "Sugyeong Eo",
        "Hyeonseok Moon",
        "Jinsung Kim",
        "Yuna Hur",
        "Jeongwook Kim",
        "SongEun Lee",
        "Changwoo Chun",
        "Sungsoo Park",
        "Heuiseok Lim"
      ],
      "abstract": "Recent advances in QA pair generation (QAG) have raised interest in applying this technique to the educational field. However, the diversity of QA types remains a challenge despite its contributions to comprehensive learning and assessment of children. In this paper, we propose a QAG framework that enhances QA type diversity by producing different interrogative sentences and implicit/explicit answers. Our framework comprises a QFS-based answer generator, an iterative QA generator, and a relevancy-aware ranker. The two generators aim to expand the number of candidates while covering various types. The ranker trained on the in-context negative samples clarifies the top-N outputs based on the ranking score. Extensive evaluations and detailed analyses demonstrate that our approach outperforms previous state-of-the-art results by significant margins, achieving improved diversity and quality. Our task-oriented processes are consistent with real-world demand, which highlights our system’s high applicability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1630,
      "original_url": "https://aclanthology.org/2023.findings-acl.381/",
      "title": "Pulling Out All The Full Stops: Punctuation Sensitivity in Neural Machine Translation and Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.381.pdf",
      "authors": [
        "Prathyusha Jwalapuram"
      ],
      "abstract": "Much of the work testing machine translation systems for robustness and sensitivity has been adversarial or tended towards testing noisy input such as spelling errors, or non-standard input such as dialects. In this work, we take a step back to investigate a sensitivity problem that can seem trivial and is often overlooked: punctuation. We perform basic sentence-final insertion and deletion perturbation tests with full stops, exclamation and questions marks across source languages and demonstrate a concerning finding: commercial, production-level machine translation systems are vulnerable to mere single punctuation insertion or deletion, resulting in unreliable translations. Moreover, we demonstrate that both string-based and model-based evaluation metrics also suffer from this vulnerability, producing significantly different scores when translations only differ in a single punctuation, with model-based metrics penalizing each punctuation differently. Our work calls into question the reliability of machine translation systems and their evaluation metrics, particularly for real-world use cases, where inconsistent punctuation is often the most common and the least disruptive noise."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1631,
      "original_url": "https://aclanthology.org/2023.findings-acl.382/",
      "title": "Reimagining Retrieval Augmented Language Models for Answering Queries",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.382.pdf",
      "authors": [
        "Wang-Chiew Tan",
        "Yuliang Li",
        "Pedro Rodriguez",
        "Richard James",
        "Xi Victoria Lin",
        "Alon Halevy",
        "Wen-tau Yih"
      ],
      "abstract": "We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1632,
      "original_url": "https://aclanthology.org/2023.findings-acl.383/",
      "title": "Numeric Magnitude Comparison Effects in Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.383.pdf",
      "authors": [
        "Raj Shah",
        "Vijay Marupudi",
        "Reba Koenen",
        "Khushi Bhardwaj",
        "Sashank Varma"
      ],
      "abstract": "Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that 4<5) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number of representations of LLMs and their cognitive plausibility."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1633,
      "original_url": "https://aclanthology.org/2023.findings-acl.384/",
      "title": "Multi-Relational Probabilistic Event Representation Learning via Projected Gaussian Embedding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.384.pdf",
      "authors": [
        "Linhai Zhang",
        "Congzhi Zhang",
        "Deyu Zhou"
      ],
      "abstract": "Event representation learning has been shown beneficial in various downstream tasks. Current event representation learning methods, which mainly focus on capturing the semantics of events via deterministic vector embeddings, have made notable progress. However, they ignore two important properties: the multiple relations between events and the uncertainty within events. In this paper, we propose a novel approach to learning multi-relational probabilistic event embeddings based on contrastive learning. Specifically, the proposed method consists of three major modules, a multi-relational event generation module to automatically generate multi-relational training data, a probabilistic event encoding module to model uncertainty of events by Gaussian density embeddings, and a relation-aware projection module to adapt unseen relations by projecting Gaussian embeddings into relation-aware subspaces. Moreover, a novel contrastive learning loss is elaborately designed for learning the multi-relational probabilistic embeddings. Since the existing benchmarks for event representation learning ignore relations and uncertainty of events, a novel dataset named MRPES is constructed to investigate whether multiple relations between events and uncertainty within events are learned. Experimental results show that the proposed approach outperforms other state-of-the-art baselines on both existing and newly constructed datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1634,
      "original_url": "https://aclanthology.org/2023.findings-acl.385/",
      "title": "PragmatiCQA: A Dataset for Pragmatic Question Answering in Conversations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.385.pdf",
      "authors": [
        "Peng Qi",
        "Nina Du",
        "Christopher Manning",
        "Jing Huang"
      ],
      "abstract": "Pragmatic reasoning about another speaker’s unspoken intent and state of mind is crucial to efficient and effective human communication. It is virtually omnipresent in conversations between humans, e.g., when someone asks “do you have a minute?”, instead of interpreting it literally as a query about your schedule, you understand that the speaker might have requests that take time, and respond accordingly. In this paper, we present PragmatiCQA, the first large-scale open-domain question answering (QA) dataset featuring 6873 QA pairs that explores pragmatic reasoning in conversations over a diverse set of topics. We designed innovative crowdsourcing mechanisms for interest-based and task-driven data collection to address the common issue of incentive misalignment between crowdworkers and potential users. To compare computational models’ capability at pragmatic reasoning, we also propose several quantitative metrics to evaluate question answering systems on PragmatiCQA. We find that state-of-the-art systems still struggle to perform human-like pragmatic reasoning, and highlight their limitations for future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1635,
      "original_url": "https://aclanthology.org/2023.findings-acl.386/",
      "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.386.pdf",
      "authors": [
        "Lukas Hauzenberger",
        "Shahed Masoudian",
        "Deepak Kumar",
        "Markus Schedl",
        "Navid Rekabsaz"
      ],
      "abstract": "Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of diff pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1636,
      "original_url": "https://aclanthology.org/2023.findings-acl.387/",
      "title": "Scientific Fact-Checking: A Survey of Resources and Approaches",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.387.pdf",
      "authors": [
        "Juraj Vladika",
        "Florian Matthes"
      ],
      "abstract": "The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1637,
      "original_url": "https://aclanthology.org/2023.findings-acl.388/",
      "title": "Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.388.pdf",
      "authors": [
        "Chiyu Song",
        "Hongliang He",
        "Haofei Yu",
        "Pengfei Fang",
        "Leyang Cui",
        "Zhenzhong Lan"
      ],
      "abstract": "Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates to ensure they are treated equally and design a new attention mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking paradigms using different attention and response concatenation methods. Extensive experiments show that our proposed paradigm achieves new state-of-the-art results on four benchmark datasets with high computational efficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X faster inference speed on the Ubuntu V2 dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1638,
      "original_url": "https://aclanthology.org/2023.findings-acl.389/",
      "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.389.pdf",
      "authors": [
        "Amr Keleg",
        "Walid Magdy"
      ],
      "abstract": "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1639,
      "original_url": "https://aclanthology.org/2023.findings-acl.390/",
      "title": "Self-adaptive Context and Modal-interaction Modeling For Multimodal Emotion Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.390.pdf",
      "authors": [
        "Haozhe Yang",
        "Xianqiang Gao",
        "Jianlong Wu",
        "Tian Gan",
        "Ning Ding",
        "Feijun Jiang",
        "Liqiang Nie"
      ],
      "abstract": "The multimodal emotion recognition in conversation task aims to predict the emotion label for a given utterance with its context and multiple modalities. Existing approaches achieve good results but also suffer from the following two limitations: 1) lacking modeling of diverse dependency ranges, i.e., long, short, and independent context-specific representations and without consideration of the different recognition difficulty for each utterance; 2) consistent treatment of the contribution for various modalities. To address the above challenges, we propose the Self-adaptive Context and Modal-interaction Modeling (SCMM) framework. We first design the context representation module, which consists of three submodules to model multiple contextual representations. Thereafter, we propose the modal-interaction module, including three interaction submodules to make full use of each modality. Finally, we come up with a self-adaptive path selection module to select an appropriate path in each module and integrate the features to obtain the final representation. Extensive experiments under four settings on three multimodal datasets, including IEMOCAP, MELD, and MOSEI, demonstrate that our proposed method outperforms the state-of-the-art approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1640,
      "original_url": "https://aclanthology.org/2023.findings-acl.391/",
      "title": "Structure-Discourse Hierarchical Graph for Conditional Question Answering on Long Documents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.391.pdf",
      "authors": [
        "Haowei Du",
        "Yansong Feng",
        "Chen Li",
        "Yang Li",
        "Yunshi Lan",
        "Dongyan Zhao"
      ],
      "abstract": "Conditional question answering on long documents aims to find probable answers and identify conditions that need to be satisfied to make the answers correct over long documents. Existing approaches solve this task by segmenting long documents into multiple sections, and attending information at global and local tokens to predict the answers and corresponding conditions. However, the natural structure of the document and discourse relations between sentences in each document section are ignored, which are crucial for condition retrieving across sections, as well as logical interaction over the question and conditions. To address this issue, this paper constructs a Structure-Discourse Hierarchical Graph (SDHG) and conducts bottom-up information propagation. Firstly we build the sentence-level discourse graphs for each section and encode the discourse relations by graph attention. Secondly, we construct a section-level structure graph based on natural structures, and conduct interactions over the question and contexts. Finally different levels of representations are integrated into jointly answer and condition decoding. The experiments on the benchmark ConditionalQA shows our approach gains over the prior state-of-the-art, by 3.0 EM score and 2.4 F1 score on answer measuring, as well as 2.2 EM score and 1.9 F1 score on jointly answer and condition measuring."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1641,
      "original_url": "https://aclanthology.org/2023.findings-acl.392/",
      "title": "COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.392.pdf",
      "authors": [
        "Xuhui Zhou",
        "Hao Zhu",
        "Akhila Yerukola",
        "Thomas Davidson",
        "Jena D. Hwang",
        "Swabha Swayamdipta",
        "Maarten Sap"
      ],
      "abstract": "Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance “your English is very good” may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement’s offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1642,
      "original_url": "https://aclanthology.org/2023.findings-acl.393/",
      "title": "Distilling Calibrated Knowledge for Stance Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.393.pdf",
      "authors": [
        "Yingjie Li",
        "Cornelia Caragea"
      ],
      "abstract": "Stance detection aims to determine the position of an author toward a target and provides insights into people’s views on controversial topics such as marijuana legalization. Despite recent progress in this task, most existing approaches use hard labels (one-hot vectors) during training, which ignores meaningful signals among categories offered by soft labels. In this work, we explore knowledge distillation for stance detection and present a comprehensive analysis. Our contributions are: 1) we propose to use knowledge distillation over multiple generations in which a student is taken as a new teacher to transfer knowledge to a new fresh student; 2) we propose a novel dynamic temperature scaling for knowledge distillation to calibrate teacher predictions in each generation step. Extensive results on three stance detection datasets show that knowledge distillation benefits stance detection and a teacher is able to transfer knowledge to a student more smoothly via calibrated guiding signals. We publicly release our code to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1643,
      "original_url": "https://aclanthology.org/2023.findings-acl.394/",
      "title": "PTCSpell: Pre-trained Corrector Based on Character Shape and Pinyin for Chinese Spelling Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.394.pdf",
      "authors": [
        "Xiao Wei",
        "Jianbao Huang",
        "Hang Yu",
        "Qian Liu"
      ],
      "abstract": "Chinese spelling correction (CSC) is a challenging task with the goal of correcting each wrong character in Chinese texts. Incorrect characters in a Chinese text are mainly due to the similar shape and similar pronunciation of Chinese characters. Recently, the paradigm of pre-training and fine-tuning has achieved remarkable success in natural language processing. However, the pre-training objectives in existing methods are not tailored for the CSC task since they neglect the visual and phonetic properties of characters, resulting in suboptimal spelling correction. In this work, we propose to pre-train a new corrector named PTCSpell for the CSC task under the detector-corrector architecture. The corrector we propose has the following two improvements. First, we design two novel pre-training objectives to capture pronunciation and shape information in Chinese characters. Second, we propose a new strategy to tackle the issue that the detector’s prediction results mislead the corrector by balancing the loss of wrong characters and correct characters. Experiments on three benchmarks (i.e., SIGHAN 2013, 2014, and 2015) show that our model achieves an average of 5.8% F1 improvements at the correction level over state-of-the-art methods, verifying its effectiveness."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1644,
      "original_url": "https://aclanthology.org/2023.findings-acl.395/",
      "title": "Disentangling Text Representation With Counter-Template For Unsupervised Opinion Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.395.pdf",
      "authors": [
        "Yanyue Zhang",
        "Deyu Zhou"
      ],
      "abstract": "Approaches for unsupervised opinion summarization are generally based on the reconstruction model and generate a summary by decoding the aggregated representation of inputs. Recent work has shown that aggregating via simple average leads to vector degeneration, generating the generic summary. To tackle the challenge, some approaches select the inputs before aggregating. However, we argue that the selection is too coarse as not all information in each input is equally essential for the summary. For example, the content information such as “great coffee maker, easy to set up” is more valuable than the pattern such as “this is a great product”. Therefore, we propose a novel framework for unsupervised opinion summarization based on text representation disentanglement with counter-template. In specific, a disentangling module is added to the encoder-decoder architecture which decouples the input text representation into two parts: content and pattern. To capture the pattern information, a counter-template is utilized as supervision, which is automatically generated based on contrastive learning. Experimental results on two benchmark datasets show that the proposed approach outperforms the state-of-the-art baselines on both quality and stability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1645,
      "original_url": "https://aclanthology.org/2023.findings-acl.396/",
      "title": "Evaluation of Question Generation Needs More References",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.396.pdf",
      "authors": [
        "Shinhyeok Oh",
        "Hyojun Go",
        "Hyeongdon Moon",
        "Yunsung Lee",
        "Myeongho Jeong",
        "Hyun Seung Lee",
        "Seungtaek Choi"
      ],
      "abstract": "Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single reference."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1646,
      "original_url": "https://aclanthology.org/2023.findings-acl.397/",
      "title": "XtremeCLIP: Extremely Parameter-efficient Tuning for Low-resource Vision Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.397.pdf",
      "authors": [
        "Moming Tang",
        "Chengyu Wang",
        "Jianing Wang",
        "Chuanqi Tan",
        "Songfang Huang",
        "Cen Chen",
        "Weining Qian"
      ],
      "abstract": "Recently, Contrastive Visual-Language Pre-training (CLIP) has demonstrated remarkable capability in various Visual Language Understanding (VLU) tasks. Yet, most CLIP-based methods require tasks-specific designs and sufficient training data. In this paper, we introduce a simple yet efficient paradigm for low-resource VLU named XtremeCLIP, which involves very few trainable parameters to improve the generalization ability of the trained models. In our XtremeCLIP framework, we reformulate a series of VLU tasks as a unified open-book affinity-matching problem. Furthermore, to handle the insufficient supervised signals in small datasets, we adopt contrastive learning to utilize the implicit sorting information of ground-truth labels to provide more supervised cues. Extensive experiments over multiple datasets on visual entailment, visual question answering, and image classification show that XtremeCLIP consistently outperforms existing baselines in low-resource settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1647,
      "original_url": "https://aclanthology.org/2023.findings-acl.398/",
      "title": "FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.398.pdf",
      "authors": [
        "Zhuang Li",
        "Yuyang Chai",
        "Terry Yue Zhuo",
        "Lizhen Qu",
        "Gholamreza Haffari",
        "Fei Li",
        "Donghong Ji",
        "Quan Hung Tran"
      ],
      "abstract": "Textual scene graph parsing has become increasingly important in various vision-language applications, including image caption evaluation and image retrieval. However, existing scene graph parsers that convert image captions into scene graphs often suffer from two types of errors. First, the generated scene graphs fail to capture the true semantics of the captions or the corresponding images, resulting in a lack of faithfulness. Second, the generated scene graphs have high inconsistency, with the same semantics represented by different annotations. To address these challenges, we propose a novel dataset, which involves re-annotating the captions in Visual Genome (VG) using a new intermediate representation called FACTUAL-MR. FACTUAL-MR can be directly converted into faithful and consistent scene graph annotations. Our experimental results clearly demonstrate that the parser trained on our dataset outperforms existing approaches in terms of faithfulness and consistency. This improvement leads to a significant performance boost in both image caption evaluation and zero-shot image retrieval tasks. Furthermore, we introduce a novel metric for measuring scene graph similarity, which, when combined with the improved scene graph parser, achieves state-of-the-art (SOTA) results on multiple benchmark datasets for the aforementioned tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1648,
      "original_url": "https://aclanthology.org/2023.findings-acl.399/",
      "title": "Target-Oriented Relation Alignment for Cross-Lingual Stance Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.399.pdf",
      "authors": [
        "Ruike Zhang",
        "Nan Xu",
        "Hanxuan Yang",
        "Yuan Tian",
        "Wenji Mao"
      ],
      "abstract": "Stance detection is an important task in text mining and social media analytics, aiming to automatically identify the user’s attitude toward a specific target from text, and has wide applications in a variety of domains. Previous work on stance detection has mainly focused on monolingual setting. To address the problem of imbalanced language resources, cross-lingual stance detection is proposed to transfer the knowledge learned from a high-resource (source) language (typically English) to another low-resource (target) language. However, existing research on cross-lingual stance detection has ignored the inconsistency in the occurrences and distributions of targets between languages, which consequently degrades the performance of stance detection in low-resource languages. In this paper, we first identify the target inconsistency issue in cross-lingual stance detection, and propose a fine-grained Target-oriented Relation Alignment (TaRA) method for the task, which considers both target-level associations and language-level alignments. Specifically, we propose the Target Relation Graph to learn the in-language and cross-language target associations. We further devise the relation alignment strategy to enable knowledge transfer between semantically correlated targets across languages. Experimental results on the representative datasets demonstrate the effectiveness of our method compared to competitive methods under variant settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1649,
      "original_url": "https://aclanthology.org/2023.findings-acl.400/",
      "title": "NonFactS: NonFactual Summary Generation for Factuality Evaluation in Document Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.400.pdf",
      "authors": [
        "Amir Soleimani",
        "Christof Monz",
        "Marcel Worring"
      ],
      "abstract": "Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context document and contain nonfactual information. To evaluate factuality in document summarization, a document-level Natural Language Inference (NLI) classifier can be used. However, training such a classifier requires large-scale high-quality factual and nonfactual samples. To that end, we introduce NonFactS, a data generation model, to synthesize nonfactual summaries given a context document and a human-annotated (reference) factual summary. Compared to previous methods, our nonfactual samples are more abstractive and more similar to their corresponding factual samples, resulting in state-of-the-art performance on two factuality evaluation benchmarks, FALSESUM and SUMMAC. Our experiments demonstrate that even without human-annotated summaries, NonFactS can use random sentences to generate nonfactual summaries and a classifier trained on these samples generalizes to out-of-domain documents."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1650,
      "original_url": "https://aclanthology.org/2023.findings-acl.401/",
      "title": "When to Read Documents or QA History: On Unified and Selective Open-domain QA",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.401.pdf",
      "authors": [
        "Kyungjae Lee",
        "Sang-eun Han",
        "Seung-won Hwang",
        "Moontae Lee"
      ],
      "abstract": "This paper studies the problem of open-domain question answering, with the aim of answering a diverse range of questions leveraging knowledge resources. Two types of sources, QA-pair and document corpora, have been actively leveraged with the following complementary strength. The former is highly precise when the paraphrase of given question q was seen and answered during training, often posed as a retrieval problem, while the latter generalizes better for unseen questions. A natural follow-up is thus leveraging both models, while a naive pipelining or integration approaches have failed to bring additional gains over either model alone. Our distinction is interpreting the problem as calibration, which estimates the confidence of predicted answers as an indicator to decide when to use a document or QA-pair corpus. The effectiveness of our method was validated on widely adopted benchmarks such as Natural Questions and TriviaQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1651,
      "original_url": "https://aclanthology.org/2023.findings-acl.402/",
      "title": "Interpretable Automatic Fine-grained Inconsistency Detection in Text Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.402.pdf",
      "authors": [
        "Hou Pong Chan",
        "Qi Zeng",
        "Heng Ji"
      ],
      "abstract": "Existing factual consistency evaluation approaches for text summarization provide binary predictions and limited insights into the weakness of summarization systems. Therefore, we propose the task of fine-grained inconsistency detection, the goal of which is to predict the fine-grained types of factual errors in a summary. Motivated by how humans inspect factual inconsistency in summaries, we propose an interpretable fine-grained inconsistency detection model, FineGrainFact, which explicitly represents the facts in the documents and summaries with semantic frames extracted by semantic role labeling, and highlights the related semantic frames to predict inconsistency. The highlighted semantic frames help verify predicted error types and correct inconsistent summaries. Experiment results demonstrate that our model outperforms strong baselines and provides evidence to support or refute the summary."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1652,
      "original_url": "https://aclanthology.org/2023.findings-acl.403/",
      "title": "A Multi-dimensional study on Bias in Vision-Language models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.403.pdf",
      "authors": [
        "Gabriele Ruggeri",
        "Debora Nozza"
      ],
      "abstract": "In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities. This paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions. When subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5% of the time, with higher percentages for female and young subjects. Bias presence in downstream models has been tested on Visual Question Answering. We developed a novel bias metric called the Vision-Language Association Test based on questions designed to elicit biased associations between stereotypical concepts and targets. Our findings demonstrate that pre-trained VL models contain biases that are perpetuated in downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1653,
      "original_url": "https://aclanthology.org/2023.findings-acl.404/",
      "title": "Correction of Errors in Preference Ratings from Automated Metrics for Text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.404.pdf",
      "authors": [
        "Jan Deriu",
        "Pius von Däniken",
        "Don Tuggener",
        "Mark Cieliebak"
      ],
      "abstract": "A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreements with human judgments. In this paper, we propose to apply automated metrics for Text Generation in a preference-based evaluation protocol. The protocol features a statistical model that incorporates various levels of uncertainty to account for the error-proneness of the metrics. We show that existing metrics are generally over-confident in assigning significant differences between systems. As a remedy, the model allows to combine human ratings with automated ratings. We show that it can reduce the required amounts of human ratings to arrive at robust and statistically significant results by more than 50%, while yielding the same evaluation outcome as the pure human evaluation in 95% of cases. We showcase the benefits of the evaluation protocol for three text generation tasks: dialogue systems, machine translation, and text summarization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1654,
      "original_url": "https://aclanthology.org/2023.findings-acl.405/",
      "title": "PEER: Pre-training ELECTRA Extended by Ranking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.405.pdf",
      "authors": [
        "Ru He",
        "Wei Wang",
        "Songfang Huang",
        "Fei Huang"
      ],
      "abstract": "The BERT model and its variants have made great achievements in many downstream natural language processing tasks. The achievements of these models, however, demand highly expensive pre-training computation cost. To address this pre-training efficiency issue, the ELECTRA model is proposed to use a discriminator to perform replaced token detection (RTD) task, that is, to classify whether each input token is original or replaced by a generator. The RTD task performed by the ELECTRA accelerates pre-training so substantially, such that it is very challenging to further improve the pre-training efficiency established by the ELECTRA by using or adding other pre-training tasks, as the recent comprehensive study of Bajaj et al. (2022) summarizes. To further advance this pre-training efficiency frontier, in this paper we propose to extend the RTD task into a task of ranking input tokens according to K different quality levels. Essentially, we generalize the binary classifier in the ELECTRA into a K-level ranker to undertake a more precise task with negligible additional computation cost. Our extensive experiments show that our proposed method is able to outperform the state-of-the-art pre-training efficient models including ELECTRA in downstream GLUE tasks given the same computation cost."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1655,
      "original_url": "https://aclanthology.org/2023.findings-acl.406/",
      "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.406.pdf",
      "authors": [
        "Xuxin Cheng",
        "Bowen Cao",
        "Qichen Ye",
        "Zhihong Zhu",
        "Hongxiang Li",
        "Yuexian Zou"
      ],
      "abstract": "Spoken language understanding (SLU) is a fundamental task in the task-oriented dialogue systems. However, the inevitable errors from automatic speech recognition (ASR) usually impair the understanding performance and lead to error propagation. Although there are some attempts to address this problem through contrastive learning, they (1) treat clean manual transcripts and ASR transcripts equally without discrimination in fine-tuning; (2) neglect the fact that the semantically similar pairs are still pushed away when applying contrastive learning; (3) suffer from the problem of Kullback–Leibler (KL) vanishing. In this paper, we propose Mutual Learning and Large-Margin Contrastive Learning (ML-LMCL), a novel framework for improving ASR robustness in SLU. Specifically, in fine-tuning, we apply mutual learning and train two SLU models on the manual transcripts and the ASR transcripts, respectively, aiming to iteratively share knowledge between these two models. We also introduce a distance polarization regularizer to avoid pushing away the intra-cluster pairs as much as possible. Moreover, we use a cyclical annealing schedule to mitigate KL vanishing issue. Experiments on three datasets show that ML-LMCL outperforms existing models and achieves new state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1656,
      "original_url": "https://aclanthology.org/2023.findings-acl.407/",
      "title": "Guiding Dialogue Agents to Complex Semantic Targets by Dynamically Completing Knowledge Graph",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.407.pdf",
      "authors": [
        "Yue Tan",
        "Bo Wang",
        "Anqi Liu",
        "Dongming Zhao",
        "Kun Huang",
        "Ruifang He",
        "Yuexian Hou"
      ],
      "abstract": "In the target-oriented dialogue, the representation and achievement of targets are two interrelated essential issues. In current approaches, the target is typically supposed to be a single object represented as a word, which makes it relatively easy to achieve the target through dialogue with the help of a knowledge graph (KG). However, when the target has complex semantics, the existing knowledge graph is often incomplete in tracking complex semantic relations. This paper studies target-oriented dialog where the target is a topic sentence. We combine the methods of knowledge retrieval and relationship prediction to construct a context-related dynamic KG. On dynamic KG, we can track the implicit semantic paths in the speaker’s mind that may not exist in the existing KGs. In addition, we also designed a novel metric to evaluate the tracked path automatically. The experimental results show that our method can control the agent more logically and smoothly toward the complex target."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1657,
      "original_url": "https://aclanthology.org/2023.findings-acl.408/",
      "title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.408.pdf",
      "authors": [
        "Dingjun Wu",
        "Jing Zhang",
        "Xinmei Huang"
      ],
      "abstract": "The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1658,
      "original_url": "https://aclanthology.org/2023.findings-acl.409/",
      "title": "TACR: A Table Alignment-based Cell Selection Method for HybridQA",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.409.pdf",
      "authors": [
        "Jian Wu",
        "Yicheng Xu",
        "Yan Gao",
        "Jian-Guang Lou",
        "Börje Karlsson",
        "Manabu Okumura"
      ],
      "abstract": "Hybrid Question-Answering (HQA), which targets reasoning over tables and passages linked from table cells, has witnessed significant research in recent years. A common challenge in HQA and other passage-table QA datasets is that it is generally unrealistic to iterate over all table rows, columns, and linked passages to retrieve evidence. Such a challenge made it difficult for previous studies to show their reasoning ability in retrieving answers. To bridge this gap, we propose a novel Table-alignment-based Cell-selection and Reasoning model (TACR) for hybrid text and table QA, evaluated on the HybridQA and WikiTableQuestions datasets. In evidence retrieval, we design a table-question-alignment enhanced cell-selection method to retrieve fine-grained evidence. In answer reasoning, we incorporate a QA module that treats the row containing selected cells as context. Experimental results over the HybridQA and WikiTableQuestions (WTQ) datasets show that TACR achieves state-of-the-art results on cell selection and outperforms fine-grained evidence retrieval baselines on HybridQA, while achieving competitive performance on WTQ. We also conducted a detailed analysis to demonstrate that being able to align questions to tables in the cell-selection stage can result in important gains from experiments of over 90% table row and column selection accuracy, meanwhile also improving output explainability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1659,
      "original_url": "https://aclanthology.org/2023.findings-acl.410/",
      "title": "Modeling Cross-Cultural Pragmatic Inference with Codenames Duet",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.410.pdf",
      "authors": [
        "Omar Shaikh",
        "Caleb Ziems",
        "William Held",
        "Aryan Pariani",
        "Fred Morstatter",
        "Diyi Yang"
      ],
      "abstract": "Pragmatic reference enables efficient interpersonal communication. Prior work uses simple reference games to test models of pragmatic reasoning, often with unidentified speakers and listeners. In practice, however, speakers’ sociocultural background shapes their pragmatic assumptions. For example, readers of this paper assume NLP refers to Natural Language Processing, and not “Neuro-linguistic Programming.” This work introduces the Cultural Codes dataset, which operationalizes sociocultural pragmatic inference in a simple word reference game. Cultural Codes is based on the multi-turn collaborative two-player game, Codenames Duet. Our dataset consists of 794 games with 7,703 turns, distributed across 153 unique players. Alongside gameplay, we collect information about players’ personalities, values, and demographics. Utilizing theories of communication and pragmatics, we predict each player’s actions via joint modeling of their sociocultural priors and the game context. Our experiments show that accounting for background characteristics significantly improves model performance for tasks related to both clue-giving and guessing, indicating that sociocultural priors play a vital role in gameplay decisions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1660,
      "original_url": "https://aclanthology.org/2023.findings-acl.411/",
      "title": "Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.411.pdf",
      "authors": [
        "Bolin Lai",
        "Hongxin Zhang",
        "Miao Liu",
        "Aryan Pariani",
        "Fiona Ryan",
        "Wenqi Jia",
        "Shirley Anugrah Hayati",
        "James Rehg",
        "Diyi Yang"
      ],
      "abstract": "Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at https://github.com/SALT-NLP/PersuationGames."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1661,
      "original_url": "https://aclanthology.org/2023.findings-acl.412/",
      "title": "Long to reign over us: A Case Study of Machine Translation and a New Monarch",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.412.pdf",
      "authors": [
        "Rebecca Knowles",
        "Samuel Larkin"
      ],
      "abstract": "Novel terminology and changes in terminology are often a challenge for machine translation systems. The passing of Queen Elizabeth II and the accession of King Charles III provide a striking example of translation shift in the real world, particularly in translation contexts that have ambiguity. Examining translation between French and English, we present a focused case-study of translations about King Charles III as produced both by publicly-available MT systems and by a neural machine translation system trained specifically on Canadian parliamentary text. We find that even in cases where human translators would have adequate context to disambiguate terms from the source language, machine translation systems do not always produce the expected output. Where we are able to analyze the training data, we note that this may represent artifacts in the data, raising important questions about machine translation updates in light of real world events."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1662,
      "original_url": "https://aclanthology.org/2023.findings-acl.413/",
      "title": "A Unified Generative Approach to Product Attribute-Value Identification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.413.pdf",
      "authors": [
        "Keiji Shinzato",
        "Naoki Yoshinaga",
        "Yandi Xia",
        "Wei-Te Chen"
      ],
      "abstract": "Product attribute-value identification (PAVI) has been studied to link products on e-commerce sites with their attribute values (e.g., ⟨Material, Cotton⟩) using product text as clues. Technical demands from real-world e-commerce platforms require PAVI methods to handle unseen values, multi-attribute values, and canonicalized values, which are only partly addressed in existing extraction- and classification-based approaches. Motivated by this, we explore a generative approach to the PAVI task. We finetune a pre-trained generative model, T5, to decode a set of attribute-value pairs as a target sequence from the given product text. Since the attribute value pairs are unordered set elements, how to linearize them will matter; we, thus, explore methods of composing an attribute-value pair and ordering the pairs for the task. Experimental results confirm that our generation-based approach outperforms the existing extraction and classification-based methods on large-scale real-world datasets meant for those methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1663,
      "original_url": "https://aclanthology.org/2023.findings-acl.414/",
      "title": "K-UniMorph: Korean Universal Morphology and its Feature Schema",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.414.pdf",
      "authors": [
        "Eunkyul Jo",
        "Kim Kyuwon",
        "Xihan Wu",
        "KyungTae Lim",
        "Jungyeul Park",
        "Chulwoo Park"
      ],
      "abstract": "We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from CITATION and CITATION for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1664,
      "original_url": "https://aclanthology.org/2023.findings-acl.415/",
      "title": "How does the brain process syntactic structure while listening?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.415.pdf",
      "authors": [
        "Subba Reddy Oota",
        "Mounika Marreddy",
        "Manish Gupta",
        "Raju Bapi"
      ],
      "abstract": "Syntactic parsing is the task of assigning a syntactic structure to a sentence. There are two popular syntactic parsing methods: constituency and dependency parsing. Recent works have used syntactic embeddings based on constituency trees, incremental top-down parsing, and other word syntactic features for brain activity prediction given the text stimuli to study how the syntax structure is represented in the brain’s language network. However, the effectiveness of dependency parse trees or the relative predictive power of the various syntax parsers across brain areas, especially for the listening task, is yet unexplored. In this study, we investigate the predictive power of the brain encoding models in three settings: (i) individual performance of the constituency and dependency syntactic parsing based embedding methods, (ii) efficacy of these syntactic parsing based embedding methods when controlling for basic syntactic signals, (iii) relative effectiveness of each of the syntactic embedding methods when controlling for the other. Further, we explore the relative importance of syntactic information (from these syntactic embedding methods) versus semantic information using BERT embeddings. We find that constituency parsers help explain activations in the temporal lobe and middle-frontal gyrus, while dependency parsers better encode syntactic structure in the angular gyrus and posterior cingulate cortex. Although semantic signals from BERT are more effective compared to any of the syntactic features or embedding methods, syntactic embedding methods explain additional variance for a few brain regions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1665,
      "original_url": "https://aclanthology.org/2023.findings-acl.416/",
      "title": "Towards Imperceptible Document Manipulations against Neural Ranking Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.416.pdf",
      "authors": [
        "Xuanang Chen",
        "Ben He",
        "Zheng Ye",
        "Le Sun",
        "Yingfei Sun"
      ],
      "abstract": "Adversarial attacks have gained traction in order to identify vulnerabilities in neural ranking models (NRMs), but current attack methods often introduce noticeable errors. Moreover, current methods rely heavily on using a well-imitated surrogate NRM to guarantee the attack effect, making them difficult to use in practice. This paper proposes a framework called Imperceptible DocumEnt Manipulation (IDEM) to produce adversarial documents that are less noticeable to both algorithms and humans. IDEM instructs a well-established generative language model like BART to generate error-free connection sentences, and employs a separate position-wise merging strategy to balance between relevance and coherence of the perturbed text. Evaluation results on the MS MARCO benchmark demonstrate that IDEM outperforms strong baselines while preserving fluency and correctness of the target documents. Furthermore, the separation of adversarial text generation from the surrogate NRM makes IDEM more robust and less affected by the quality of the surrogate NRM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1666,
      "original_url": "https://aclanthology.org/2023.findings-acl.417/",
      "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.417.pdf",
      "authors": [
        "Qiang Zhang",
        "Jason Naradowsky",
        "Yusuke Miyao"
      ],
      "abstract": "Existing dialogue models may encounter scenarios which are not well-represented in the training data, and as a result generate responses that are unnatural, inappropriate, or unhelpful. We propose the “Ask an Expert” framework in which the model is trained with access to an “expert” which it can consult at each turn. Advice is solicited via a structured dialogue with the expert, and the model is optimized to selectively utilize (or ignore) it given the context and dialogue history. In this work the expert takes the form of an LLM.We evaluate this framework in a mental health support domain, where the structure of the expert conversation is outlined by pre-specified prompts which reflect a reasoning strategy taught to practitioners in the field. Blenderbot models utilizing “Ask an Expert” show quality improvements across all expert sizes, including those with fewer parameters than the dialogue model itself. Our best model provides a ~10% improvement over baselines, approaching human-level scores on “engingingness” and “helpfulness” metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1667,
      "original_url": "https://aclanthology.org/2023.findings-acl.418/",
      "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.418.pdf",
      "authors": [
        "Tetsu Kasanishi",
        "Masaru Isonuma",
        "Junichiro Mori",
        "Ichiro Sakata"
      ],
      "abstract": "Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting of over 10,000 literature reviews and 690,000 papers cited in the reviews. Based on the dataset, we evaluate recent transformer-based summarization models on the literature review generation task, including Fusion-in-Decoder extended for literature review generation. Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information. Our dataset and code are available at [https://github.com/tetsu9923/SciReviewGen](https://github.com/tetsu9923/SciReviewGen)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1668,
      "original_url": "https://aclanthology.org/2023.findings-acl.419/",
      "title": "Revisiting Sample Size Determination in Natural Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.419.pdf",
      "authors": [
        "Ernie Chang",
        "Muhammad Hassan Rashid",
        "Pin-Jie Lin",
        "Changsheng Zhao",
        "Vera Demberg",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "Knowing exactly how many data points need to be labeled to achieve a certain model performance is a hugely beneficial step towards reducing the overall budgets for annotation. It pertains to both active learning and traditional data annotation, and is particularly beneficial for low resource scenarios. Nevertheless, it remains a largely under-explored area of research in NLP. We therefore explored various techniques for estimating the training sample size necessary to achieve a targeted performance value. We derived a simple yet effective approach to predict the maximum achievable model performance based on small amount of training samples – which serves as an early indicator during data annotation for data quality and sample size determination. We performed ablation studies on four language understanding tasks, and showed that the proposed approach allows us to forecast model performance within a small margin of mean absolute error (~0.9%) with only 10% data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1669,
      "original_url": "https://aclanthology.org/2023.findings-acl.420/",
      "title": "TransESC: Smoothing Emotional Support Conversation via Turn-Level State Transition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.420.pdf",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Shilong Wang",
        "Bing Qin"
      ],
      "abstract": "Emotion Support Conversation (ESC) is an emerging and challenging task with the goal of reducing the emotional distress of people. Previous attempts fail to maintain smooth transitions between utterances in ESC because they ignoring to grasp the fine-grained transition information at each dialogue turn. To solve this problem, we propose to take into account turn-level state Transitions of ESC (TransESC) from three perspectives, including semantics transition, strategy transition and emotion transition, to drive the conversation in a smooth and natural way. Specifically, we construct the state transition graph with a two-step way, named transit-then-interact, to grasp such three types of turn-level transition information. Finally, they are injected into the transition aware decoder to generate more engaging responses. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of TransESC to generate more smooth and effective supportive responses. Our source code will be publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1670,
      "original_url": "https://aclanthology.org/2023.findings-acl.421/",
      "title": "Residual Prompt Tuning: improving prompt tuning with residual reparameterization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.421.pdf",
      "authors": [
        "Anastasiia Razdaibiedina",
        "Yuning Mao",
        "Madian Khabsa",
        "Mike Lewis",
        "Rui Hou",
        "Jimmy Ba",
        "Amjad Almahairi"
      ],
      "abstract": "Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning across T5-Large, T5-Base and BERT-Base models. Notably, our method reaches +7 points improvement over prompt tuning on SuperGLUE benchmark with T5-Base model and allows to reduce the prompt length by 10 times without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1671,
      "original_url": "https://aclanthology.org/2023.findings-acl.422/",
      "title": "Attend, Select and Eliminate: Accelerating Multi-turn Response Selection with Dual-attention-based Content Elimination",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.422.pdf",
      "authors": [
        "Jianxin Liang",
        "Chang Liu",
        "Chongyang Tao",
        "Jiazhan Feng",
        "Dongyan Zhao"
      ],
      "abstract": "Although the incorporation of pre-trained language models (PLMs) significantly pushes the research frontier of multi-turn response selection, it brings a new issue of heavy computation costs. To alleviate this problem and make the PLM-based response selection model both effective and efficient, we propose an inference framework together with a post-training strategy that builds upon any pre-trained transformer-based response selection models to accelerate inference by progressively selecting and eliminating unimportant content under the guidance of context-response dual-attention. Specifically, at each transformer layer, we first identify the importance of each word based on context-to-response and response-to-context attention, then select a number of unimportant words to be eliminated following a retention configuration derived from evolutionary search while passing the rest of the representations into deeper layers. To mitigate the training-inference gap posed by content elimination, we introduce a post-training strategy where we use knowledge distillation to force the model with progressively eliminated content to mimic the predictions of the original model with no content elimination. Experiments on three benchmarks indicate that our method can effectively speeds-up SOTA models without much performance degradation and shows a better trade-off between speed and performance than previous methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1672,
      "original_url": "https://aclanthology.org/2023.findings-acl.423/",
      "title": "Medical Dialogue Generation via Dual Flow Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.423.pdf",
      "authors": [
        "Kaishuai Xu",
        "Wenjun Hou",
        "Yi Cheng",
        "Jian Wang",
        "Wenjie Li"
      ],
      "abstract": "Medical dialogue systems (MDS) aim to provide patients with medical services, such as diagnosis and prescription. Since most patients cannot precisely describe their symptoms, dialogue understanding is challenging for MDS. Previous studies mainly addressed this by extracting the mentioned medical entities as critical dialogue history information. In this work, we argue that it is also essential to capture the transitions of the medical entities and the doctor’s dialogue acts in each turn, as they help the understanding of how the dialogue flows and enhance the prediction of the entities and dialogue acts to be adopted in the following turn. Correspondingly, we propose a Dual Flow enhanced Medical (DFMed) dialogue generation framework. It extracts the medical entities and dialogue acts used in the dialogue history and models their transitions with an entity-centric graph flow and a sequential act flow, respectively. We employ two sequential models to encode them and devise an interweaving component to enhance their interactions. Experiments on two datasets demonstrate that our method exceeds baselines in both automatic and manual evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1673,
      "original_url": "https://aclanthology.org/2023.findings-acl.424/",
      "title": "Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.424.pdf",
      "authors": [
        "Liming Wang",
        "Junrui Ni",
        "Heting Gao",
        "Jialu Li",
        "Kai Chieh Chang",
        "Xulin Fan",
        "Junkai Wu",
        "Mark Hasegawa-Johnson",
        "Chang Yoo"
      ],
      "abstract": "Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We propose speech2sign-U, a neural network-based approach capable of both character-level and word-level SSR-U. Our approach significantly outperforms baselines directly adapted from unsupervised speech recognition (ASR-U) models by as much as 50% recall@10 on several challenging American sign language corpora with various levels of sample sizes, vocabulary sizes, and audio and visual variability. The code is available at https://github.com/cactuswiththoughts/UnsupSpeech2Sign.gitcactuswiththoughts/UnsupSpeech2Sign.git."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1674,
      "original_url": "https://aclanthology.org/2023.findings-acl.425/",
      "title": "Distinguishing Address vs. Reference Mentions of Personal Names in Text",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.425.pdf",
      "authors": [
        "Vinodkumar Prabhakaran",
        "Aida Mostafazadeh Davani",
        "Melissa Ferguson",
        "Stav Atir"
      ],
      "abstract": "Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off. While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context. In this paper, we present a new annotated dataset that captures the address vs. reference distinction in English, an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1675,
      "original_url": "https://aclanthology.org/2023.findings-acl.426/",
      "title": "“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.426.pdf",
      "authors": [
        "Zhiying Jiang",
        "Matthew Yang",
        "Mikhail Tsirlin",
        "Raphael Tang",
        "Yiqin Dai",
        "Jimmy Lin"
      ],
      "abstract": "Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that’s easy, lightweight, and universal in text classification: a combination of a simple compressor like gzip with a k-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1676,
      "original_url": "https://aclanthology.org/2023.findings-acl.427/",
      "title": "LR-Sum: Summarization for Less-Resourced Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.427.pdf",
      "authors": [
        "Chester Palen-Michel",
        "Constantine Lignos"
      ],
      "abstract": "We introduce LR-Sum, a new permissively-licensed dataset created with the goal of enabling further research in automatic summarization for less-resourced languages.LR-Sum contains human-written summaries for 40 languages, many of which are less-resourced. We describe our process for extracting and filtering the dataset from the Multilingual Open Text corpus (Palen-Michel et al., 2022).The source data is public domain newswire collected from from Voice of America websites, and LR-Sum is released under a Creative Commons license (CC BY 4.0), making it one of the most openly-licensed multilingual summarization datasets. We describe abstractive and extractive summarization experiments to establish baselines and discuss the limitations of this dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1677,
      "original_url": "https://aclanthology.org/2023.findings-acl.428/",
      "title": "RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.428.pdf",
      "authors": [
        "Alireza Mohammadshahi",
        "Thomas Scialom",
        "Majid Yazdani",
        "Pouya Yanki",
        "Angela Fan",
        "James Henderson",
        "Marzieh Saeidi"
      ],
      "abstract": "Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several adversarial corruptions. Furthermore, we illustrate that we can significantly improve the performance of QA models on out-of-domain datasets by fine-tuning on synthetic data generated by a question generation model and reranked by RQUGE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1678,
      "original_url": "https://aclanthology.org/2023.findings-acl.429/",
      "title": "Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.429.pdf",
      "authors": [
        "Taichi Aida",
        "Danushka Bollegala"
      ],
      "abstract": "Languages are dynamic entities, where the meanings associated with words constantly change with time. Detecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions. Existing work on semantic variation prediction have predominantly focused on comparing some form of an averaged contextualised representation of a target word computed from a given corpus. However, some of the previously associated meanings of a target word can become obsolete over time (e.g. meaning of gay as happy), while novel usages of existing words are observed (e.g. meaning of cell as a mobile phone).We argue that mean representations alone cannot accurately capture such semantic variations and propose a method that uses the entire cohort of the contextualised embeddings of the target word, which we refer to as the sibling distribution. Experimental results on SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show that our method outperforms prior work that consider only the mean embeddings, and is comparable to the current state-of-the-art. Moreover, a qualitative analysis shows that our method detects important semantic changes in words that are not captured by the existing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1679,
      "original_url": "https://aclanthology.org/2023.findings-acl.430/",
      "title": "TranSFormer: Slow-Fast Transformer for Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.430.pdf",
      "authors": [
        "Bei Li",
        "Yi Jing",
        "Xu Tan",
        "Zhen Xing",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "abstract": "Learning multiscale Transformer models has been evidenced as a viable approach to augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into multiscale Transformer has not yet been explored. In this work, we present a Slow-Fast two-stream learning model, referred to as TranSFormer, which utilizes a “slow” branch to deal with subword sequences and a “fast” branch to deal with longer character sequences. This model is efficient since the fast branch is very lightweight by reducing the model width, and yet provides useful fine-grained features for the slow branch. Our TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point) on several machine translation benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1680,
      "original_url": "https://aclanthology.org/2023.findings-acl.431/",
      "title": "Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.431.pdf",
      "authors": [
        "Jian Guan",
        "Minlie Huang"
      ],
      "abstract": "Despite the huge progress in myriad generation tasks, pretrained language models (LMs) such as GPT2 still tend to generate repetitive texts with maximization-based decoding algorithms for open-ended generation. We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss. We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition, which is shown to mitigate repetition effectively while maintaining fluency on two datasets. Furthermore, we find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones, which may be the cause of sentence-level repetition loops."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1681,
      "original_url": "https://aclanthology.org/2023.findings-acl.432/",
      "title": "Digging out Discrimination Information from Generated Samples for Robust Visual Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.432.pdf",
      "authors": [
        "Zhiquan Wen",
        "Yaowei Wang",
        "Mingkui Tan",
        "Qingyao Wu",
        "Qi Wu"
      ],
      "abstract": "Visual Question Answering (VQA) aims to answer a textual question based on a given image. Nevertheless, recent studies have shown that VQA models tend to capture the biases to answer the question, instead of using the reasoning ability, resulting in poor generalisation ability. To alleviate the issue, some existing methods consider the natural distribution of the data, and construct samples to balance the dataset, achieving remarkable performance. However, these methods may encounter some limitations: 1) rely on additional annotations, 2) the generated samples may be inaccurate, e.g., assigned wrong answers, and 3) ignore the power of positive samples. In this paper, we propose a method to Dig out Discrimination information from Generated samples (DDG) to address the above limitations. Specifically, we first construct positive and negative samples in vision and language modalities, without using additional annotations. Then, we introduce a knowledge distillation mechanism to promote the learning of the original samples by the positive samples. Moreover, we impel the VQA models to focus on vision and language modalities using the negative samples. Experimental results on the VQA-CP v2 and VQA v2 datasets show the effectiveness of our DDG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1682,
      "original_url": "https://aclanthology.org/2023.findings-acl.433/",
      "title": "Words as Gatekeepers: Measuring Discipline-specific Terms and Meanings in Scholarly Publications",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.433.pdf",
      "authors": [
        "Li Lucy",
        "Jesse Dodge",
        "David Bamman",
        "Katherine Keith"
      ],
      "abstract": "Scholarly text is often laden with jargon, or specialized language that can facilitate efficient in-group communication within fields but hinder understanding for out-groups. In this work, we develop and validate an interpretable approach for measuring scholarly jargon from text. Expanding the scope of prior work which focuses on word types, we use word sense induction to also identify words that are widespread but overloaded with different meanings across fields. We then estimate the prevalence of these discipline-specific words and senses across hundreds of subfields, and show that word senses provide a complementary, yet unique view of jargon alongside word types. We demonstrate the utility of our metrics for science of science and computational sociolinguistics by highlighting two key social implications. First, though most fields reduce their use of jargon when writing for general-purpose venues, and some fields (e.g., biological sciences) do so less than others. Second, the direction of correlation between jargon and citation rates varies among fields, but jargon is nearly always negatively correlated with interdisciplinary impact. Broadly, our findings suggest that though multidisciplinary venues intend to cater to more general audiences, some fields’ writing norms may act as barriers rather than bridges, and thus impede the dispersion of scholarly ideas."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1683,
      "original_url": "https://aclanthology.org/2023.findings-acl.434/",
      "title": "Trade-Offs Between Fairness and Privacy in Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.434.pdf",
      "authors": [
        "Cleo Matzken",
        "Steffen Eger",
        "Ivan Habernal"
      ],
      "abstract": "Protecting privacy in contemporary NLP models is gaining in importance. So does the need to mitigate social biases of such models. But can we have both at the same time? Existing research suggests that privacy preservation comes at the price of worsening biases in classification tasks. In this paper, we explore the extent to which this tradeoff really holds when we incorporate both privacy preservation and de-biasing techniques into training text generation models. How does improving the model along one dimension affect the other dimension as well as the utility of the model? We conduct an extensive set of experiments that include bias detection, privacy attacks, language modeling, and performance on downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1684,
      "original_url": "https://aclanthology.org/2023.findings-acl.435/",
      "title": "CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.435.pdf",
      "authors": [
        "Hanchong Zhang",
        "Jieyu Li",
        "Lu Chen",
        "Ruisheng Cao",
        "Yunyan Zhang",
        "Yu Huang",
        "Yefeng Zheng",
        "Kai Yu"
      ],
      "abstract": "The cross-domain text-to-SQL task aims to build a system that can parse user questions into SQL on complete unseen databases, and the single-domain text-to-SQL task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications. To this end, we introduce the cross-schema text-to-SQL task, where the databases of evaluation data are different from that in the training data but come from the same domain. Furthermore, we present CSS, a large-scale CrosS-Schema Chinese text-to-SQL dataset, to carry on corresponding studies. CSS originally consisted of 4,340 question/SQL pairs across 2 databases. In order to generalize models to different medical systems, we extend CSS and create 19 new databases along with 29,280 corresponding dataset examples. Moreover, CSS is also a large corpus for single-domain Chinese text-to-SQL studies. We present the data collection approach and a series of analyses of the data statistics. To show the potential and usefulness of CSS, benchmarking baselines have been conducted and reported. Our dataset is publicly available at https://huggingface.co/datasets/zhanghanchong/css."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1685,
      "original_url": "https://aclanthology.org/2023.findings-acl.436/",
      "title": "Silver Syntax Pre-training for Cross-Domain Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.436.pdf",
      "authors": [
        "Elisa Bassignana",
        "Filip Ginter",
        "Sampo Pyysalo",
        "Rob van der Goot",
        "Barbara Plank"
      ],
      "abstract": "Relation Extraction (RE) remains a challenging task, especially when considering realistic out-of-domain evaluations. One of the main reasons for this is the limited training size of current RE datasets: obtaining high-quality (manually annotated) data is extremely expensive and cannot realistically be repeated for each new domain. An intermediate training step on data from related tasks has shown to be beneficial across many NLP tasks. However, this setup still requires supplementary annotated data, which is often not available. In this paper, we investigate intermediate pre-training specifically for RE. We exploit the affinity between syntactic structure and semantic RE, and identify the syntactic relations which are closely related to RE by being on the shortest dependency path between two entities. We then take advantage of the high accuracy of current syntactic parsers in order to automatically obtain large amounts of low-cost pre-training data. By pre-training our RE model on the relevant syntactic relations, we are able to outperform the baseline in five out of six cross-domain setups, without any additional annotated data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1686,
      "original_url": "https://aclanthology.org/2023.findings-acl.437/",
      "title": "FastDiff 2: Revisiting and Incorporating GANs and Diffusion Models in High-Fidelity Speech Synthesis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.437.pdf",
      "authors": [
        "Rongjie Huang",
        "Yi Ren",
        "Ziyue Jiang",
        "Chenye Cui",
        "Jinglin Liu",
        "Zhou Zhao"
      ],
      "abstract": "Generative adversarial networks (GANs) and denoising diffusion probabilistic models (DDPMs) have recently achieved impressive performances in image and audio synthesis. After revisiting their success in conditional speech synthesis, we find that 1) GANs sacrifice sample diversity for quality and speed, 2) diffusion models exhibit outperformed sample quality and diversity at a high computational cost, where achieving high-quality, fast, and diverse speech synthesis challenges all neural synthesizers. In this work, we propose to converge advantages from GANs and diffusion models by incorporating both classes, introducing dual-empowered modeling perspectives: 1) FastDiff 2 (DiffGAN), a diffusion model whose denoising process is parametrized by conditional GANs, and the non-Gaussian denoising distribution makes it much more stable to implement the reverse process with large steps sizes; and 2) FastDiff 2 (GANDiff), a generative adversarial network whose forward process is constructed by multiple denoising diffusion iterations, which exhibits better sample diversity than traditional GANs. Experimental results show that both variants enjoy an efficient 4-step sampling process and demonstrate superior sample quality and diversity. Audio samples are available at https://RevisitSpeech.github.io/"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1687,
      "original_url": "https://aclanthology.org/2023.findings-acl.438/",
      "title": "Uncovering Hidden Consequences of Pre-training Objectives in Sequence-to-Sequence Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.438.pdf",
      "authors": [
        "Tannon Kew",
        "Rico Sennrich"
      ],
      "abstract": "Some variants of self-supervised denoising objectives for pre-training encoder-decoder language models have been reported to have a negligible impact on downstream performance. Yet the design of these pre-training objectives leads to behavioural differences that can be uncovered with specific manipulations. We reproduce a recently proposed zero-shot control method and find that it is only successful on a subset of models. To understand what causes the difference in its effectiveness, we perform a set of controlled experiments, varying only the pre-training objective, and find unexpected interactions between the pre-training method and downstream controllability of models after fine-tuning. Our results show that different pre-training objectives have consequences that may not be visible in standard downstream evaluation, but which should be taken into account when developing models with controllability in mind."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1688,
      "original_url": "https://aclanthology.org/2023.findings-acl.439/",
      "title": "Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.439.pdf",
      "authors": [
        "Katharina Haemmerl",
        "Alina Fastowski",
        "Jindřich Libovický",
        "Alexander Fraser"
      ],
      "abstract": "Previous work has shown that the representations output by contextual language models are more anisotropic than static type embeddings, and typically display outlier dimensions. This seems to be true for both monolingual and multilingual models, although much less work has been done on the multilingual context. Why these outliers occur and how they affect the representations is still an active area of research. We investigate outlier dimensions and their relationship to anisotropy in multiple pre-trained multilingual language models. We focus on cross-lingual semantic similarity tasks, as these are natural tasks for evaluating multilingual representations. Specifically, we examine sentence representations. Sentence transformers which are fine-tuned on parallel resources (that are not always available) perform better on this task, and we show that their representations are more isotropic. However, we aim to improve multilingual representations in general. We investigate how much of the performance difference can be made up by only transforming the embedding space without fine-tuning, and visualise the resulting spaces. We test different operations: Removing individual outlier dimensions, cluster-based isotropy enhancement, and ZCA whitening. We publish our code for reproducibility."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1689,
      "original_url": "https://aclanthology.org/2023.findings-acl.440/",
      "title": "Revisiting Sentence Union Generation as a Testbed for Text Consolidation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.440.pdf",
      "authors": [
        "Eran Hirsch",
        "Valentina Pyatkin",
        "Ruben Wolhandler",
        "Avi Caciularu",
        "Asi Shefer",
        "Ido Dagan"
      ],
      "abstract": "Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information. However, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models’ consolidation capabilities. In this paper, we suggest revisiting the sentence union generation task as an effective well-defined testbed for assessing text consolidation capabilities, decoupling the consolidation challenge from subjective content selection. To support research on this task, we present refined annotation methodology and tools for crowdsourcing sentence union, create the largest union dataset to date and provide an analysis of its rich coverage of various consolidation aspects. We then propose a comprehensive evaluation protocol for union generation, including both human and automatic evaluation. Finally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1690,
      "original_url": "https://aclanthology.org/2023.findings-acl.441/",
      "title": "Distilling Reasoning Capabilities into Smaller Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.441.pdf",
      "authors": [
        "Kumar Shridhar",
        "Alessandro Stolfo",
        "Mrinmaya Sachan"
      ],
      "abstract": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available: https://github.com/kumar-shridhar/Distiiling-LM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1691,
      "original_url": "https://aclanthology.org/2023.findings-acl.442/",
      "title": "AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.442.pdf",
      "authors": [
        "Ruiqi Li",
        "Rongjie Huang",
        "Lichao Zhang",
        "Jinglin Liu",
        "Zhou Zhao"
      ],
      "abstract": "The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performance in terms of both objective and subjective metrics. Audio samples are available at https://alignsts.github.io."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1692,
      "original_url": "https://aclanthology.org/2023.findings-acl.443/",
      "title": "A New Task and Dataset on Detecting Attacks on Human Rights Defenders",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.443.pdf",
      "authors": [
        "Shihao Ran",
        "Di Lu",
        "Aoife Cahill",
        "Joel Tetreault",
        "Alejandro Jaimes"
      ],
      "abstract": "The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500 online news articles. The annotations include fine-grained information about the type and location of the attacks, as well as information about the victim(s). We demonstrate the usefulness of the dataset by using it to train and evaluate baseline models on several sub-tasks to predict the annotated characteristics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1693,
      "original_url": "https://aclanthology.org/2023.findings-acl.444/",
      "title": "Improving Language Model Integration for Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.444.pdf",
      "authors": [
        "Christian Herold",
        "Yingbo Gao",
        "Mohammad Zeineldeen",
        "Hermann Ney"
      ],
      "abstract": "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1694,
      "original_url": "https://aclanthology.org/2023.findings-acl.445/",
      "title": "Type Enhanced BERT for Correcting NER Errors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.445.pdf",
      "authors": [
        "Kuai Li",
        "Chen Chen",
        "Tao Yang",
        "Tianming Du",
        "Peijie Yu",
        "Dong Du",
        "Feng Zhang"
      ],
      "abstract": "We introduce the task of correcting named entity recognition (NER) errors without re-training model. After an NER model is trained and deployed in production,it makes prediction errors, which usually need to be fixed quickly. To address this problem, we firstly construct a gazetteer containing named entities and corresponding possible entity types. And then, we propose type enhanced BERT (TyBERT),a method that integrates the named entity’s type information into BERT by an adapter layer. When errors are identified, we can repair the model by updating the gazetteer. In other words, the gazetteer becomes a trigger to control NER model’s output. The experiment results in multiple corpus show the effectiveness of our method, which outperforms strong baselines.x"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1695,
      "original_url": "https://aclanthology.org/2023.findings-acl.446/",
      "title": "Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.446.pdf",
      "authors": [
        "Lifan Yuan",
        "YiChi Zhang",
        "Yangyi Chen",
        "Wei Wei"
      ],
      "abstract": "Despite recent success on various tasks, deep learning techniques still perform poorly on adversarial examples with small perturbations. While optimization-based methods for adversarial attacks are well-explored in the field of computer vision, it is impractical to directly apply them in natural language processing due to the discrete nature of the text. To address the problem, we propose a unified framework to extend the existing optimization-based adversarial attack methods in the vision domain to craft textual adversarial samples. In this framework, continuously optimized perturbations are added to the embedding layer and amplified in the forward propagation process. Then the final perturbed latent representations are decoded with a masked language model head to obtain potential adversarial samples. In this paper, we instantiate our framework with an attack algorithm named Textual Projected Gradient Descent (T-PGD). We find our algorithm effective even using proxy gradient information. Therefore, we perform the more challenging transfer black-box attack and conduct comprehensive experiments to evaluate our attack algorithm with several models on three benchmark datasets. Experimental results demonstrate that our method achieves overall better performance and produces more fluent and grammatical adversarial samples compared to strong baseline methods. The code and data are available at https://github.com/Phantivia/T-PGD."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1696,
      "original_url": "https://aclanthology.org/2023.findings-acl.447/",
      "title": "DUB: Discrete Unit Back-translation for Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.447.pdf",
      "authors": [
        "Dong Zhang",
        "Rong Ye",
        "Tom Ko",
        "Mingxuan Wang",
        "Yaqian Zhou"
      ],
      "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST.Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation(DUB) to answer two questions (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://anonymous.4open.science/r/DUB/."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1697,
      "original_url": "https://aclanthology.org/2023.findings-acl.448/",
      "title": "Knowledge Graph Embeddings using Neural Ito Process: From Multiple Walks to Stochastic Trajectories",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.448.pdf",
      "authors": [
        "Mojtaba Nayyeri",
        "Bo Xiong",
        "Majid Mohammadi",
        "Mst. Mahfuja Akter",
        "Mirza Mohtashim Alam",
        "Jens Lehmann",
        "Steffen Staab"
      ],
      "abstract": "Knowledge graphs mostly exhibit a mixture of branching relations, e.g., hasFriend, and complex structures, e.g., hierarchy and loop. Most knowledge graph embeddings have problems expressing them, because they model a specific relation r from a head h to tails by starting at the node embedding of h and transitioning deterministically to exactly one other point in the embedding space. We overcome this issue in our novel framework ItCAREToE by modeling relations between nodes by relation-specific, stochastic transitions. Our framework is based on stochastic ItCARETo processes, which operate on low-dimensional manifolds. ItCAREToE is highly expressive and generic subsuming various state-of-the-art models operating on different, also non-Euclidean, manifolds. Experimental results show the superiority of ItCAREToE over other deterministic embedding models with regard to the KG completion task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1698,
      "original_url": "https://aclanthology.org/2023.findings-acl.449/",
      "title": "Leveraging Denoised Abstract Meaning Representation for Grammatical Error Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.449.pdf",
      "authors": [
        "Hejing Cao",
        "Dongyan Zhao"
      ],
      "abstract": "Grammatical Error Correction (GEC) is the task of correcting errorful sentences into grammatically correct, semantically consistent, and coherent sentences. Popular GEC models either use large-scale synthetic corpora or use a large number of human-designed rules. The former is costly to train, while the latter requires quite a lot of human expertise. In recent years, AMR, a semantic representation framework, has been widely used by many natural language tasks due to its completeness and flexibility. A non-negligible concern is that AMRs of grammatically incorrect sentences may not be exactly reliable. In this paper, we propose the AMR-GEC, a seq-to-seq model that incorporates denoised AMR as additional knowledge. Specifically, We design a semantic aggregated GEC model and explore denoising methods to get AMRs more reliable. Experiments on the BEA-2019 shared task and the CoNLL-2014 shared task have shown that AMR-GEC performs comparably to a set of strong baselines with a large number of synthetic data. Compared with the T5 model with synthetic data, AMR-GEC can reduce the training time by 32% while inference time is comparable. To the best of our knowledge, we are the first to incorporate AMR for grammatical error correction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1699,
      "original_url": "https://aclanthology.org/2023.findings-acl.450/",
      "title": "Prediction and Calibration: Complex Reasoning over Knowledge Graph with Bi-directional Directed Acyclic Graph Neural Network",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.450.pdf",
      "authors": [
        "Yao Xu",
        "Shizhu He",
        "Li Cai",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Answering complex logical queries is a challenging task for knowledge graph (KG) reasoning. Recently, query embedding (QE) has been proposed to encode queries and entities into the same vector space, and obtain answers based on numerical computation. However, such models obtain the node representations of a query only based on its predecessor nodes, which ignore the information contained in successor nodes. In this paper, we proposed a Bi-directional Directed Acyclic Graph neural network (BiDAG) that splits the reasoning process into prediction and calibration. The joint probability of all nodes is considered by applying a graph neural network (GNN) to the query graph in the calibration process. By the prediction in the first layer and the calibration in deep layers of GNN, BiDAG can outperform previous QE based methods on FB15k, FB15k-237, and NELL995."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1700,
      "original_url": "https://aclanthology.org/2023.findings-acl.451/",
      "title": "Prompt-Based Metric Learning for Few-Shot NER",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.451.pdf",
      "authors": [
        "Yanru Chen",
        "Yanan Zheng",
        "Zhilin Yang"
      ],
      "abstract": "Few-shot named entity recognition (NER) targets generalizing to unseen labels and/or domains with few labeled examples. Existing metric learning methods compute token-level similarities between query and support sets, but are not able to fully incorporate label semantics into modeling. To address this issue, we propose a simple method to largely improve metric learning for NER: 1) multiple prompt schemas are designed to enhance label semantics; 2) we propose a novel architecture to effectively combine multiple prompt-based representations. Empirically, our method achieves new state-of-the-art (SOTA) results under 16 of the 18 considered settings, substantially outperforming the previous SOTA by an average of 9.12% and a maximum of 34.51% in relative gains of micro F1."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1701,
      "original_url": "https://aclanthology.org/2023.findings-acl.452/",
      "title": "OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.452.pdf",
      "authors": [
        "Xueqing Wu",
        "Sha Li",
        "Heng Ji"
      ],
      "abstract": "Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset OpenPI-C using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric’s preference for repetition. Model-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the memory slots during decoding. On the other hand, the state of the world is naturally a union of the states of involved entities. Since the entities are unknown in the open-vocabulary setting, we propose a two-stage model that refines the state change prediction conditioned on entities predicted from the first stage. Empirical results show the effectiveness of our proposed model, especially on the cleaned dataset and the cluster-based metric. The code and data are released at https://github.com/shirley-wu/openpi-c"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1702,
      "original_url": "https://aclanthology.org/2023.findings-acl.453/",
      "title": "I run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.453.pdf",
      "authors": [
        "Longxuan Ma",
        "Wei-Nan Zhang",
        "Shuhan Zhou",
        "Churui Sun",
        "Changxin Ke",
        "Ting Liu"
      ],
      "abstract": "A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as “like” or “as”. The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually focuses on similes in a triplet tuple (tenor, property, vehicle) or a single sentence where the tenor and vehicle are usually entities or noun phrases, which could not reflect complex simile phenomena in real scenarios. In this paper, we propose a novel and high-quality multilingual simile dialogue (MSD) dataset to facilitate the study of complex simile phenomena. The MSD is the largest manually annotated simile data (~21K) and it contains both English and Chinese data. Meanwhile, the MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes. We design 3 simile tasks (recognition, interpretation, and generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each task, we provide experimental results from strong pre-trained or state-of-the-art models. The experiments demonstrate the challenge of MSD and we will release the data/code on GitHub."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1703,
      "original_url": "https://aclanthology.org/2023.findings-acl.454/",
      "title": "Controllable Conversation Generation with Conversation Structures via Diffusion Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.454.pdf",
      "authors": [
        "Jiaao Chen",
        "Diyi Yang"
      ],
      "abstract": "Generating coherent conversation is an important and challenging long text generation task, as it has various applications such as daily entertainment, children education or building conversational AI to facilitate human-computer interaction. However, current generation models often fail to effectively utilize rich linguistic and world knowledge to generate conversations just like human. In this work, we introduce a novel conversation generation framework to effectively incorporate human knowledge and conversation structures with both controllability and interpretability for better conversation generation. Specifically, we first generate the prototype conversations from short descriptions. We then gradually and strategically incorporate different levels of conversation structures including the action triples, dialogue acts and discourse relations via diffusion models to directly edit the prototype conversations. We demonstrate the effectiveness of our framework through experiments on two datasets by comparing our method with the state-of-the-art baseline models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1704,
      "original_url": "https://aclanthology.org/2023.findings-acl.455/",
      "title": "Few-shot Low-resource Knowledge Graph Completion with Reinforced Task Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.455.pdf",
      "authors": [
        "Shichao Pei",
        "Qiannan Zhang",
        "Xiangliang Zhang"
      ],
      "abstract": "Despite becoming a prevailing paradigm for organizing knowledge, most knowledge graphs (KGs) suffer from the low-resource issue due to the deficiency of data sources. The enrichment of KGs by automatic knowledge graph completion is impeded by the intrinsic long-tail property of KGs. In spite of their prosperity, existing few-shot learning-based models have difficulty alleviating the impact of the long-tail issue on low-resource KGs because of the lack of training tasks. To tackle the challenging long-tail issue on low-resource KG completion, in this paper, we propose a novel few-shot low-resource knowledge graph completion framework, which is composed of three components, i.e., few-shot learner, task generator, and task selector. The key idea is to generate and then select the beneficial few-shot tasks that complement the current tasks and enable the optimization of the few-shot learner using the selected few-shot tasks. Extensive experiments conducted on several real-world knowledge graphs validate the effectiveness of our proposed method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1705,
      "original_url": "https://aclanthology.org/2023.findings-acl.456/",
      "title": "Incomplete Utterance Rewriting as Sequential Greedy Tagging",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.456.pdf",
      "authors": [
        "Yunshan Chen"
      ],
      "abstract": "The task of incomplete utterance rewriting has recently gotten much attention. Previous models struggled to extract information from the dialogue context, as evidenced by the low restoration scores. To address this issue, we propose a novel sequence tagging-based model, which is more adept at extracting information from context. Meanwhile, we introduce speaker-aware embedding to model speaker variation. Experiments on multiple public datasets show that our model achieves optimal results on all nine restoration scores while having other metric scores comparable to previous state-of-the-art models. Furthermore, benefitting from the model’s simplicity, our approach outperforms most previous models on inference speed."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1706,
      "original_url": "https://aclanthology.org/2023.findings-acl.457/",
      "title": "Exploiting Commonsense Knowledge about Objects for Visual Activity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.457.pdf",
      "authors": [
        "Tianyu Jiang",
        "Ellen Riloff"
      ],
      "abstract": "Situation recognition is the task of recognizing the activity depictedin an image, including the people and objects involved. Previousmodels for this task typically train a classifier to identify theactivity using a backbone image feature extractor. We propose thatcommonsense knowledge about the objects depicted in an image can alsobe a valuable source of information for activity identification. Previous NLP research has argued that knowledge about the prototypicalfunctions of physical objects is important for language understanding,and NLP techniques have been developed to acquire this knowledge. Our work investigates whether this prototypical function knowledgecan also be beneficial for visual situation recognition. Webuild a framework that incorporates this type of commonsense knowledgein a transformer-based model that is trained to predict the actionverb for situation recognition. Our experimental results show thatadding prototypical function knowledge about physical objects doesimprove performance for the visual activity recognition task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1707,
      "original_url": "https://aclanthology.org/2023.findings-acl.458/",
      "title": "Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.458.pdf",
      "authors": [
        "Likang Xiao",
        "Richong Zhang",
        "Zijie Chen",
        "Junfan Chen"
      ],
      "abstract": "Temporal Knowledge Graph Completion aims to complete missing entities or relations under temporal constraints. Previous tensor decomposition-based models for TKGC only independently consider the combination of one single relation with one single timestamp, ignoring the global nature of the embedding. We propose a Frequency Attention (FA) model to capture the global temporal dependencies between one relation and the entire timestamp. Specifically, we use Discrete Cosine Transform (DCT) to capture the frequency of the timestamp embedding and further compute the frequency attention weight to scale embedding. Meanwhile, the previous temporal tucker decomposition method uses a simple norm regularization to constrain the core tensor, which limits the optimization performance. Thus, we propose Orthogonal Regularization (OR) variants for the core tensor, which can limit the non-superdiagonal elements of the 3-rd core tensor. Experiments on three standard TKGC datasets demonstrate that our method outperforms the state-of-the-art results on several metrics. The results suggest that the direct-current component is not the best feature for TKG representation learning. Additional analysis shows the effectiveness of our FA and OR models, even with smaller embedding dimensions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1708,
      "original_url": "https://aclanthology.org/2023.findings-acl.459/",
      "title": "Another Dead End for Morphological Tags? Perturbed Inputs and Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.459.pdf",
      "authors": [
        "Alberto Muñoz-Ortiz",
        "David Vilares"
      ],
      "abstract": "The usefulness of part-of-speech tags for parsing has been heavily questioned due to the success of word-contextualized parsers. Yet, most studies are limited to coarse-grained tags and high quality written content; while we know little about their influence when it comes to models in production that face lexical errors. We expand these setups and design an adversarial attack to verify if the use of morphological information by parsers: (i) contributes to error propagation or (ii) if on the other hand it can play a role to correct mistakes that word-only neural parsers make. The results on 14 diverse UD treebanks show that under such attacks, for transition- and graph-based models their use contributes to degrade the performance even faster, while for the (lower-performing) sequence labeling parsers they are helpful. We also show that if morphological tags were utopically robust against lexical perturbations, they would be able to correct parsing mistakes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1709,
      "original_url": "https://aclanthology.org/2023.findings-acl.460/",
      "title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.460.pdf",
      "authors": [
        "Tzuf Paz-Argaman",
        "Tal Bauman",
        "Itai Mondshine",
        "Itzhak Omer",
        "Sagi Dalyot",
        "Reut Tsarfaty"
      ],
      "abstract": "The task of textual geolocation — retrieving the coordinates of a place based on a free-form language description — calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1710,
      "original_url": "https://aclanthology.org/2023.findings-acl.461/",
      "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.461.pdf",
      "authors": [
        "Xuanjie Fang",
        "Sijie Cheng",
        "Yang Liu",
        "Wei Wang"
      ],
      "abstract": "Pre-trained language models (PLMs) have been widely used to underpin various downstream tasks. However, the adversarial attack task has found that PLMs are vulnerable to small perturbations. Mainstream methods adopt a detached two-stage framework to attack without considering the subsequent influence of substitution at each step. In this paper, we formally model the adversarial attack task on PLMs as a sequential decision-making problem, where the whole attack process is sequential with two decision-making problems, i.e., word finder and word substitution. Considering the attack process can only receive the final state without any direct intermediate signals, we propose to use reinforcement learning to find an appropriate sequential attack path to generate adversaries, named SDM-ATTACK. Our experimental results show that SDM-ATTACK achieves the highest attack success rate with a comparable modification rate and semantic similarity to attack fine-tuned BERT. Furthermore, our analyses demonstrate the generalization and transferability of SDM-ATTACK.Resources of this work will be released after this paper’s publication."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1711,
      "original_url": "https://aclanthology.org/2023.findings-acl.462/",
      "title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.462.pdf",
      "authors": [
        "Liang Chen",
        "Hongru Wang",
        "Yang Deng",
        "Wai Chung Kwan",
        "Zezhong Wang",
        "Kam-Fai Wong"
      ],
      "abstract": "Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART). To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation. Experiments on the Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1712,
      "original_url": "https://aclanthology.org/2023.findings-acl.463/",
      "title": "Cost-effective Distillation of Large Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.463.pdf",
      "authors": [
        "Sayantan Dasgupta",
        "Trevor Cohn",
        "Timothy Baldwin"
      ],
      "abstract": "Knowledge distillation (KD) involves training a small “student” model to replicate the strong performance of a high-capacity “teacher” model, enabling efficient deployment in resource-constrained settings. Top-performing methods tend to be task- or architecture-specific and lack generalizability. Several existing approaches require pretraining of the teacher on task-specific datasets, which can be costly for large and unstable for small datasets. Here we propose an approach for improving KD through a novel distillation loss agnostic to the task and model architecture. We successfully apply our method to the distillation of the BERT-base and achieve highly competitive results from the distilled student across a range of GLUE tasks, especially for tasks with smaller datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1713,
      "original_url": "https://aclanthology.org/2023.findings-acl.464/",
      "title": "Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.464.pdf",
      "authors": [
        "Namo Bang",
        "Jeehyun Lee",
        "Myoung-Wan Koo"
      ],
      "abstract": "Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1714,
      "original_url": "https://aclanthology.org/2023.findings-acl.465/",
      "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.465.pdf",
      "authors": [
        "Tuhin Chakrabarty",
        "Arkadiy Saakyan",
        "Olivia Winn",
        "Artemis Panagopoulou",
        "Yue Yang",
        "Marianna Apidianaki",
        "Smaranda Muresan"
      ],
      "abstract": "Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL⋅E 2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3 (davinci-002) with Chain-of-Thought prompting generates text that represents a visual elaboration of the linguistic metaphor containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic metaphors and their associated visual elaborations. Evaluation by professional illustrators shows the promise of LLM-Diffusion Model collaboration for this task.To evaluate the utility of our Human-AI collaboration framework and the quality of our dataset, we perform both an intrinsic human-based evaluation and an extrinsic evaluation using visual entailment as a downstream task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1715,
      "original_url": "https://aclanthology.org/2023.findings-acl.466/",
      "title": "Text Augmentation Using Dataset Reconstruction for Low-Resource Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.466.pdf",
      "authors": [
        "Adir Rahamim",
        "Guy Uziel",
        "Esther Goldbraich",
        "Ateret Anaby Tavor"
      ],
      "abstract": "In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods. One of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1716,
      "original_url": "https://aclanthology.org/2023.findings-acl.467/",
      "title": "LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.467.pdf",
      "authors": [
        "Sayan Ghosh",
        "Rakesh R. Menon",
        "Shashank Srivastava"
      ],
      "abstract": "A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as ‘always’ or ‘rarely’) and mimicking humans in compositionally learning complex tasks. Here, we present LaSQuE, a method that can learn zero-shot classifiers from language explanations by using three new strategies - (1) modeling the semantics of linguistic quantifiers in explanations (including exploiting ordinal strength relationships, such as ‘always’ > ‘likely’), (2) aggregating information from multiple explanations using an attention-based mechanism, and (3) model training via curriculum learning. With these strategies, LaSQuE outperforms prior work, showing an absolute gain of up to 7% in generalizing to unseen real-world classification tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1717,
      "original_url": "https://aclanthology.org/2023.findings-acl.468/",
      "title": "Learned Adapters Are Better Than Manually Designed Adapters",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.468.pdf",
      "authors": [
        "Yuming Zhang",
        "Peng Wang",
        "Ming Tan",
        "Wei Zhu"
      ],
      "abstract": "Recently, a series of works have looked into further improving the adapter-based tuning by manually designing better adapter architectures. Understandably, these manually designed solutions are sub-optimal. In this work, we propose the Learned Adapter framework to automatically learn the optimal adapter architectures for better task adaptation of pre-trained models (PTMs). First, we construct a unified search space for adapter architecture designs. In terms of the optimization method on the search space, we propose a simple-yet-effective method, GDNAS for better architecture optimization. Extensive experiments show that our Learned Adapter framework can outperform the previous parameter-efficient tuning (PETuning) baselines while tuning comparable or fewer parameters. Moreover: (a) the learned adapter architectures are explainable and transferable across tasks. (b) We demonstrate that our architecture search space design is valid."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1718,
      "original_url": "https://aclanthology.org/2023.findings-acl.469/",
      "title": "Automatic Identification of Code-Switching Functions in Speech Transcripts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.469.pdf",
      "authors": [
        "Ritu Belani",
        "Jeffrey Flanigan"
      ],
      "abstract": "Code-switching, or switching between languages, occurs for many reasons and has important linguistic, sociological, and cultural implications. Multilingual speakers code-switch for a variety of communicative functions, such as expressing emotions, borrowing terms, making jokes, introducing a new topic, etc. The function of code-switching may be quite useful for the analysis of linguists, cognitive scientists, speech therapists, and others, but is not readily apparent. To remedy this situation, we annotate and release a new dataset of functions of code-switching in Spanish-English. We build the first system (to our knowledge) to automatically identify a wide range of functions for which speakers code-switch in everyday speech, achieving an accuracy of 75% across all functions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1719,
      "original_url": "https://aclanthology.org/2023.findings-acl.470/",
      "title": "Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.470.pdf",
      "authors": [
        "Rui Wang",
        "Tong Yu",
        "Junda Wu",
        "Handong Zhao",
        "Sungchul Kim",
        "Ruiyi Zhang",
        "Subrata Mitra",
        "Ricardo Henao"
      ],
      "abstract": "Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of multiple domains. In addition, we consider a practical and challenging scenario, where NER datasets of different platforms of federated learning are annotated with heterogeneous tag sets, i.e., different sets of entity types. The goal is to train a global model with federated learning, such that it can predict with a complete tag set, i.e., with all the occurring entity types for data across all platforms. To cope with the heterogeneous tag sets in a multi-domain setting, we propose a distillation approach along with a mechanism of instance weighting to facilitate knowledge transfer across platforms. Besides, we release two re-annotated clinic NER datasets, for testing the proposed method in the clinic domain. Our method shows superior empirical performance for NER with federated learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1720,
      "original_url": "https://aclanthology.org/2023.findings-acl.471/",
      "title": "Interpreting Sentiment Composition with Latent Semantic Tree",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.471.pdf",
      "authors": [
        "Zhongtao Jiang",
        "Yuanzhe Zhang",
        "Cao Liu",
        "Jiansong Chen",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and sentiment ones, which are intrinsically suboptimal in our view. To address this, we propose semantic tree, a new tree form capable of interpreting the sentiment composition in a principled way. Semantic tree is a derivation of a context-free grammar (CFG) describing the specific composition rules on difference semantic roles, which is designed carefully following previous linguistic conclusions. However, semantic tree is a latent variable since there is no its annotation in regular datasets. Thus, in our method, it is marginalized out via inside algorithm and learned to optimize the classification performance. Quantitative and qualitative results demonstrate that our method not only achieves better or competitive results compared to baselines in the setting of regular and domain adaptation classification, and also generates plausible tree explanations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1721,
      "original_url": "https://aclanthology.org/2023.findings-acl.472/",
      "title": "Beyond Positive Scaling: How Negation Impacts Scaling Trends of Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.472.pdf",
      "authors": [
        "Yuhui Zhang",
        "Michihiro Yasunaga",
        "Zhengping Zhou",
        "Jeff Z. HaoChen",
        "James Zou",
        "Percy Liang",
        "Serena Yeung"
      ],
      "abstract": "Language models have been shown to exhibit positive scaling, where performance improves as models are scaled up in terms of size, compute, or data. In this work, we introduce NeQA, a dataset consisting of questions with negation in which language models do not exhibit straightforward positive scaling. We show that this task can exhibit inverse scaling, U-shaped scaling, or positive scaling, and the three scaling trends shift in this order as we use more powerful prompting methods or model families. We hypothesize that solving NeQA depends on two subtasks: question answering (task 1) and negation understanding (task 2). We find that task 1 has linear scaling, while task 2 has sigmoid-shaped scaling with an emergent transition point, and composing these two scaling trends yields the final scaling trend of NeQA. Our work reveals and provides a way to analyze the complex scaling trends of language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1722,
      "original_url": "https://aclanthology.org/2023.findings-acl.473/",
      "title": "Contrastive Training Improves Zero-Shot Classification of Semi-structured Documents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.473.pdf",
      "authors": [
        "Muhammad Khalifa",
        "Yogarshi Vyas",
        "Shuai Wang",
        "Graham Horwood",
        "Sunil Mallya",
        "Miguel Ballesteros"
      ],
      "abstract": "We investigate semi-structured document classification in a zero-shot setting. Classification of semi-structured documents is more challenging than that of standard unstructured documents, as positional, layout, and style information play a vital role in interpreting such documents. The standard classification setting where categories are fixed during both training and testing falls short in dynamic environments where new classification categories could potentially emerge. We focus exclusively on the zero-shot learning setting where inference is done on new unseen classes. To address this task, we propose a matching-based approach that relies on a pairwise contrastive objective for both pretraining and fine-tuning. Our results show a significant boost in Macro F1 from the proposed pretraining step and comparable performance of the contrastive fine-tuning to a standard prediction objective in both supervised and unsupervised zero-shot settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1723,
      "original_url": "https://aclanthology.org/2023.findings-acl.474/",
      "title": "Extracting Shopping Interest-Related Product Types from the Web",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.474.pdf",
      "authors": [
        "Yinghao Li",
        "Colin Lockard",
        "Prashant Shiralkar",
        "Chao Zhang"
      ],
      "abstract": "Recommending a diversity of product types (PTs) is important for a good shopping experience when customers are looking for products around their high-level shopping interests (SIs) such as hiking. However, the SI-PT connection is typically absent in e-commerce product catalogs and expensive to construct manually due to the volume of potential SIs, which prevents us from establishing a recommender with easily accessible knowledge systems. To establish such connections, we propose to extract PTs from the Web pages containing hand-crafted PT recommendations for SIs. The extraction task is formulated as binary HTML node classification given the general observation that an HTML node in our target Web pages can present one and only one PT phrase. Accordingly, we introduce TrENC, which stands for Tree-Transformer Encoders for Node Classification. It improves the inter-node dependency modeling with modified attention mechanisms that preserve the long-term sibling and ancestor-descendant relations. TrENC also injects SI into node features for better semantic representation. Trained on pages regarding limited SIs, TrEnc is ready to be applied to other unobserved interests. Experiments on our manually constructed dataset, WebPT, show that TrENC outperforms the best baseline model by 2.37 F1 points in the zero-shot setup. The performance indicates the feasibility of constructing SI-PT relations and using them to power downstream applications such as search and recommendation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1724,
      "original_url": "https://aclanthology.org/2023.findings-acl.475/",
      "title": "Multilingual Pre-training with Self-supervision from Global Co-occurrence Information",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.475.pdf",
      "authors": [
        "Xi Ai",
        "Bin Fang"
      ],
      "abstract": "Global co-occurrence information is the primary source of structural information on multilingual corpora, and we find that analogical/parallel compound words across languages have similar co-occurrence counts/frequencies (normalized) giving weak but stable self-supervision for cross-lingual transfer. Following the observation, we aim at associating contextualized representations with relevant (contextualized) representations across languages with the help of co-occurrence counts. The result is MLM-GC (MLM with Global Co-occurrence) pre-training that the model learns local bidirectional information from MLM and global co-occurrence information from a log-bilinear regression. Experiments show that MLM-GC pre-training substantially outperforms MLM pre-training for 4 downstream cross-lingual tasks and 1 additional monolingual task, showing the advantages of forming isomorphic spaces across languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1725,
      "original_url": "https://aclanthology.org/2023.findings-acl.476/",
      "title": "Low-Rank Updates of pre-trained Weights for Multi-Task Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.476.pdf",
      "authors": [
        "Alexandre Audibert",
        "Massih R Amini",
        "Konstantin Usevich",
        "Marianne Clausel"
      ],
      "abstract": "Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor. We show that low-rank updates in the canonical polyadic tensor decomposition of this tensor of weights lead to a simple, yet efficient algorithm, which without loss of performance allows to reduce considerably the model parameters. We investigate the interactions between tasks inside the model as well as the inclusion of sparsity to find the best tensor rank and to increase the compression rate. Our strategy is consistent with recent efforts that attempt to use constraints to fine-tune some model components. More precisely, we achieve equivalent performance as the state-of-the-art on the General Language Understanding Evaluation benchmark by training only 0.3 of the parameters per task while not modifying the baseline weights."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1726,
      "original_url": "https://aclanthology.org/2023.findings-acl.477/",
      "title": "Sequential Integrated Gradients: a simple but effective method for explaining language models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.477.pdf",
      "authors": [
        "Joseph Enguehard"
      ],
      "abstract": "Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of language models, we also propose to replace the baseline token “pad” with the trained token “mask”. While being a simple improvement over the original IG method, we show on various models and datasets that SIG proves to be a very effective method for explaining language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1727,
      "original_url": "https://aclanthology.org/2023.findings-acl.478/",
      "title": "DiffuDetox: A Mixed Diffusion Model for Text Detoxification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.478.pdf",
      "authors": [
        "Griffin Floto",
        "Mohammad Mahdi Abdollah Pour",
        "Parsa Farinneya",
        "Zhenwei Tang",
        "Ali Pesaranghader",
        "Manasa Bharadwaj",
        "Scott Sanner"
      ],
      "abstract": "Text detoxification is a conditional text generation task aiming to remove offensive content from toxic text. It is highly useful for online forums and social media, where offensive content is frequently encountered. Intuitively, there are diverse ways to detoxify sentences while preserving their meanings, and we can select from detoxified sentences before displaying text to users. Conditional diffusion models are particularly suitable for this task given their demonstrated higher generative diversity than existing conditional text generation models based on language models. Nonetheless, text fluency declines when they are trained with insufficient data, which is the case for this task. In this work, we propose DiffuDetox, a mixed conditional and unconditional diffusion model for text detoxification. The conditional model takes toxic text as the condition and reduces its toxicity, yielding a diverse set of detoxified sentences. The unconditional model is trained to recover the input text, which allows the introduction of additional fluent text for training and thus ensures text fluency. Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed DiffuDetox."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1728,
      "original_url": "https://aclanthology.org/2023.findings-acl.479/",
      "title": "Separating Context and Pattern: Learning Disentangled Sentence Representations for Low-Resource Extractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.479.pdf",
      "authors": [
        "Ruifeng Yuan",
        "Shichao Sun",
        "Zili Wang",
        "Ziqiang Cao",
        "Wenjie Li"
      ],
      "abstract": "Extractive summarization aims to select a set of salient sentences from the source document to form a summary. Context information has been considered one of the key factors for this task. Meanwhile, there also exist other pattern factors that can identify sentence importance, such as sentence position or certain n-gram tokens. However, such pattern information is only effective in specific datasets or domains and can not be generalized like the context information when there only exists limited data. In this case, current extractive summarization models may suffer from a performance drop when transferring to a new dataset. In this paper, we attempt to apply disentangled representation learning on extractive summarization, and separate the two key factors for the task, context and pattern, for a better generalization ability in the low-resource setting. To achieve this, we propose two groups of losses for encoding and disentangling sentence representations into context representations and pattern representations. In this case, we can either use only the context information in the zero-shot setting or fine-tune the pattern information in the few-shot setting. Experimental results on three summarization datasets from different domains show the effectiveness of our proposed approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1729,
      "original_url": "https://aclanthology.org/2023.findings-acl.480/",
      "title": "Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.480.pdf",
      "authors": [
        "Wanjun Zhong",
        "Tingting Ma",
        "Jiahai Wang",
        "Jian Yin",
        "Tiejun Zhao",
        "Chin-Yew Lin",
        "Nan Duan"
      ],
      "abstract": "This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled thinking) are decoupled to capture different levels of cognition. Upon the top of the representation module, the pre-trained reasoning modules are modular and professional in specific and fundamental reasoning skills (e.g., logic, simple QA, etc). To mimic the controlled compositional thinking process, different reasoning modules are dynamically activated and composed in both parallel and cascaded manners to control what reasoning skills are activated and how deep the reasoning process will be reached to solve the current problems. The unified reasoning framework solves multiple tasks with a single model, and is trained and inferred in an end-to-end manner. Evaluated on 11 datasets requiring different reasoning skills and complexity, ReasonFormer demonstrates substantial performance boosts, revealing the compositional reasoning ability. Few-shot experiments exhibit better generalization ability by learning to compose pre-trained skills for new tasks with limited data, and decoupling the representation module and the reasoning modules. Further analysis shows the modularity of reasoning modules as different tasks activate distinct reasoning skills at different reasoning depths."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1730,
      "original_url": "https://aclanthology.org/2023.findings-acl.481/",
      "title": "Towards Argument-Aware Abstractive Summarization of Long Legal Opinions with Summary Reranking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.481.pdf",
      "authors": [
        "Mohamed Elaraby",
        "Yang Zhong",
        "Diane Litman"
      ],
      "abstract": "We propose a simple approach for the abstractive summarization of long legal opinions that takes into account the argument structure of the document. Legal opinions often contain complex and nuanced argumentation, making it challenging to generate a concise summary that accurately captures the main points of the legal opinion. Our approach involves using argument role information to generate multiple candidate summaries, then reranking these candidates based on alignment with the document’s argument structure. We demonstrate the effectiveness of our approach on a dataset of long legal opinions and show that it outperforms several strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1731,
      "original_url": "https://aclanthology.org/2023.findings-acl.482/",
      "title": "Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.482.pdf",
      "authors": [
        "Haoyi Wu",
        "Kewei Tu"
      ],
      "abstract": "Syntactic structures used to play a vital role in natural language processing (NLP), but since the deep learning revolution, NLP has been gradually dominated by neural models that do not consider syntactic structures in their design. One vastly successful class of neural models is transformers. When used as an encoder, a transformer produces contextual representation of words in the input sentence. In this work, we propose a new model of contextual word representation, not from a neural perspective, but from a purely syntactic and probabilistic perspective. Specifically, we design a conditional random field that models discrete latent representations of all words in a sentence as well as dependency arcs between them; and we use mean field variational inference for approximate inference. Strikingly, we find that the computation graph of our model resembles transformers, with correspondences between dependencies and self-attention and between distributions over latent representations and contextual embeddings of words. Experiments show that our model performs competitively to transformers on small to medium sized datasets. We hope that our work could help bridge the gap between traditional syntactic and probabilistic approaches and cutting-edge neural approaches to NLP, and inspire more linguistically-principled neural approaches in the future."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1732,
      "original_url": "https://aclanthology.org/2023.findings-acl.483/",
      "title": "Joint Speech Transcription and Translation: Pseudo-Labeling with Out-of-Distribution Data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.483.pdf",
      "authors": [
        "Mozhdeh Gheini",
        "Tatiana Likhomanenko",
        "Matthias Sperber",
        "Hendra Setiawan"
      ],
      "abstract": "Self-training has been shown to be helpful in addressing data scarcity for many domains, including vision, speech, and language. Specifically, self-training, or pseudo-labeling, labels unsupervised data and adds that to the training pool. In this work, we investigate and use pseudo-labeling for a recently proposed novel setup: joint transcription and translation of speech, which suffers from an absence of sufficient parallel data resources. We show that under such data-deficient circumstances, the unlabeled data can significantly vary in domain from the supervised data, which results in pseudo-label quality degradation. We investigate two categories of remedies that require no additional supervision and target the domain mismatch: pseudo-label filtering and data augmentation. We show that pseudo-label analysis and processing in this way results in additional gains on top of the vanilla pseudo-labeling setup providing a total improvement of up to 0.4% absolute WER and 2.1 BLEU points for En–De and 0.6% absolute WER and 2.2 BLEU points for En–Zh."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1733,
      "original_url": "https://aclanthology.org/2023.findings-acl.484/",
      "title": "Word-level Prefix/Suffix Sense Detection: A Case Study on Negation Sense with Few-shot Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.484.pdf",
      "authors": [
        "Yameng Li",
        "Zicheng Li",
        "Ying Chen",
        "Shoushan Li"
      ],
      "abstract": "Morphological analysis is an important research issue in the field of natural language processing. In this study, we propose a context-free morphological analysis task, namely word-level prefix/suffix sense detection, which deals with the ambiguity of sense expressed by prefix/suffix. To research this novel task, we first annotate a corpus with prefixes/suffixes expressing negation (e.g., il-, un-, -less) and then propose a novel few-shot learning approach that applies an input-augmentation prompt to a token-replaced detection pre-training model. Empirical studies demonstrate the effectiveness of the proposed approach to word-level prefix/suffix negation sense detection."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1734,
      "original_url": "https://aclanthology.org/2023.findings-acl.485/",
      "title": "End-to-End Simultaneous Speech Translation with Differentiable Segmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.485.pdf",
      "authors": [
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "abstract": "End-to-end simultaneous speech translation (SimulST) outputs translation while receiving the streaming speech inputs (a.k.a. streaming speech translation), and hence needs to segment the speech inputs and then translate based on the current received speech. However, segmenting the speech inputs at unfavorable moments can disrupt the acoustic integrity and adversely affect the performance of the translation model. Therefore, learning to segment the speech inputs at those moments that are beneficial for the translation model to produce high-quality translation is the key to SimulST. Existing SimulST methods, either using the fixed-length segmentation or external segmentation model, always separate segmentation from the underlying translation model, where the gap results in segmentation outcomes that are not necessarily beneficial for the translation process. In this paper, we propose Differentiable Segmentation (DiSeg) for SimulST to directly learn segmentation from the underlying translation model. DiSeg turns hard segmentation into differentiable through the proposed expectation training, enabling it to be jointly trained with the translation model and thereby learn translation-beneficial segmentation. Experimental results demonstrate that DiSeg achieves state-of-the-art performance and exhibits superior segmentation capability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1735,
      "original_url": "https://aclanthology.org/2023.findings-acl.486/",
      "title": "Joint Generator-Ranker Learning for Natural Language Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.486.pdf",
      "authors": [
        "Weizhou Shen",
        "Yeyun Gong",
        "Yelong Shen",
        "Song Wang",
        "Xiaojun Quan",
        "Nan Duan",
        "Weizhu Chen"
      ],
      "abstract": "Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are publicly available at https://github.com/microsoft/ProphetNet/tree/master/JGR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1736,
      "original_url": "https://aclanthology.org/2023.findings-acl.487/",
      "title": "Multilingual Sequence-to-Sequence Models for Hebrew NLP",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.487.pdf",
      "authors": [
        "Matan Eyal",
        "Hila Noga",
        "Roee Aharoni",
        "Idan Szpektor",
        "Reut Tsarfaty"
      ],
      "abstract": "Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature of Hebrew. In this paper we argue that sequence-to-sequence generative architectures are more suitable for large LMs in morphologically rich languages (MRLs) such as Hebrew. We demonstrate this by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, for which we can leverage powerful multilingual, pretrained sequence-to-sequence models as mT5, eliminating the need for a separate, specialized, morpheme-based, decoder. Using this approach, our experiments show substantial improvements over previously published results on all existing Hebrew NLP benchmarks. These results suggest that multilingual sequence-to-sequence models present a promising building block for NLP for MRLs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1737,
      "original_url": "https://aclanthology.org/2023.findings-acl.488/",
      "title": "Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.488.pdf",
      "authors": [
        "Ran Song",
        "Shizhu He",
        "Shengxiang Gao",
        "Li Cai",
        "Kang Liu",
        "Zhengtao Yu",
        "Jun Zhao"
      ],
      "abstract": "Multilingual Knowledge Graph Completion (mKGC) aim at solving queries in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias. This makes it difficult for mKGC to achieve good results, particularly in the context of low-resource languages. To overcome previous problems, this paper introduces global and local knowledge constraints for mKGC. The former is used to constrain the reasoning of answer entities , while the latter is used to enhance the representation of query contexts. The proposed method makes the pretrained model better adapt to the mKGC task. Experimental results on public datasets demonstrate that our method outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32% and 16.03%, which indicates that our proposed method has significant enhancement on mKGC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1738,
      "original_url": "https://aclanthology.org/2023.findings-acl.489/",
      "title": "Towards Better Hierarchical Text Classification with Data Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.489.pdf",
      "authors": [
        "Yue Wang",
        "Dan Qiao",
        "Juntao Li",
        "Jinxiong Chang",
        "Qishen Zhang",
        "Zhongyi Liu",
        "Guannan Zhang",
        "Min Zhang"
      ],
      "abstract": "Hierarchical text classification (HTC) focuses on classifying one text into multiple labels, which are organized as a hierarchical taxonomy. Due to its wide involution in realistic scenarios, HTC attracts long-term attention from both industry and academia. However, the high cost of hierarchical multi-label annotation makes HTC suffer from the data scarcity problem. In view of the difficulty in balancing the controllability of multiple structural labels and text diversity, automatically generating high-quality data for HTC is challenging and under-explored. To fill this blank, we propose a novel data generation framework tailored for HTC, which can achieve both label controllability and text diversity by extracting high-quality semantic-level and phrase-level hierarchical label information. Experimental results on three benchmarks demonstrate that, compared with existing data augmentation methods, the data generated from our method can bring the most significant performance improvements of several strong HTC models. Extensive analysis confirms that the improvements yielded by our proposed method do correlate to the enhancement of label controllability and text diversity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1739,
      "original_url": "https://aclanthology.org/2023.findings-acl.490/",
      "title": "History repeats: Overcoming catastrophic forgetting for event-centric temporal knowledge graph completion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.490.pdf",
      "authors": [
        "Mehrnoosh Mirtaheri",
        "Mohammad Rostami",
        "Aram Galstyan"
      ],
      "abstract": "Temporal knowledge graph (TKG) completion models typically rely on having access to the entire graph during training. However, in real-world scenarios, TKG data is often received incrementally as events unfold, leading to a dynamic non-stationary data distribution over time. While one could incorporate fine-tuning to existing methods to allow them to adapt to evolving TKG data, this can lead to forgetting previously learned patterns. Alternatively, retraining the model with the entire updated TKG can mitigate forgetting but is computationally burdensome. To address these challenges, we propose a general continual training framework that is applicable to any TKG completion method, and leverages two key ideas: (i) a temporal regularization that encourages repurposing of less important model parameters for learning new knowledge, and (ii) a clustering-based experience replay that reinforces the past knowledge by selectively preserving only a small portion of the past data. Our experimental results on widely used event-centric TKG datasets demonstrate the effectiveness of our proposed continual training framework in adapting to new events while reducing catastrophic forgetting. Further, we perform ablation studies to show the effectiveness of each component of our proposed framework. Finally, we investigate the relation between the memory dedicated to experience replay and the benefit gained from our clustering-based sampling strategy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1740,
      "original_url": "https://aclanthology.org/2023.findings-acl.491/",
      "title": "Multi-Agent Language Learning: Symbolic Mapping",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.491.pdf",
      "authors": [
        "Yicheng Feng",
        "Zongqing Lu"
      ],
      "abstract": "The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a ‘natural’ way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a curriculum learning method called task transfer, and propose a novel architecture called symbolic mapping. We find that task transfer distinctly helps language learning in difficult tasks, and symbolic mapping promotes the effect. Further, we explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, we find that a process from simplicity to complexity can serve as a natural way to help multi-agent language learning, and the proposed symbolic mapping is effective for this process."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1741,
      "original_url": "https://aclanthology.org/2023.findings-acl.492/",
      "title": "Scaling Laws for BERT in Low-Resource Settings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.492.pdf",
      "authors": [
        "Gorka Urbizu",
        "Iñaki San Vicente",
        "Xabier Saralegi",
        "Rodrigo Agerri",
        "Aitor Soroa"
      ],
      "abstract": "Large language models are very resource intensive, both financially and environmentally, and require an amount of training data which is simply unobtainable for the majority of NLP practitioners. Previous work has researched the scaling laws of such models, but optimal ratios of model parameters, dataset size, and computation costs focused on the large scale. In contrast, we analyze the effect those variables have on the performance of language models in constrained settings, by building three lightweight BERT models (16M/51M/124M parameters) trained over a set of small corpora (5M/25M/125M words).We experiment on four languages of different linguistic characteristics (Basque, Spanish, Swahili and Finnish), and evaluate the models on MLM and several NLU tasks. We conclude that the power laws for parameters, data and compute for low-resource settings differ from the optimal scaling laws previously inferred, and data requirements should be higher. Our insights are consistent across all the languages we study, as well as across the MLM and downstream tasks. Furthermore, we experimentally establish when the cost of using a Transformer-based approach is worth taking, instead of favouring other computationally lighter solutions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1742,
      "original_url": "https://aclanthology.org/2023.findings-acl.493/",
      "title": "Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.493.pdf",
      "authors": [
        "Wenjie Xu",
        "Ben Liu",
        "Miao Peng",
        "Xu Jia",
        "Min Peng"
      ],
      "abstract": "Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that our model has great competitiveness compared to other models with four metrics. Our model can effectively incorporate information from temporal knowledge graphs into the language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1743,
      "original_url": "https://aclanthology.org/2023.findings-acl.494/",
      "title": "Is Continuous Prompt a Combination of Discrete Prompts? Towards a Novel View for Interpreting Continuous Prompts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.494.pdf",
      "authors": [
        "Tianjie Ju",
        "Yubin Zheng",
        "Hanyi Wang",
        "Haodong Zhao",
        "Gongshen Liu"
      ],
      "abstract": "The broad adoption of continuous prompts has brought state-of-the-art results on a diverse array of downstream natural language processing (NLP) tasks. Nonetheless, little attention has been paid to the interpretability and transferability of continuous prompts. Faced with the challenges, we investigate the feasibility of interpreting continuous prompts as the weighting of discrete prompts by jointly optimizing prompt fidelity and downstream fidelity. Our experiments show that: (1) one can always find a combination of discrete prompts as the replacement of continuous prompts that performs well on downstream tasks; (2) our interpretable framework faithfully reflects the reasoning process of source prompts; (3) our interpretations provide effective readability and plausibility, which is helpful to understand the decision-making of continuous prompts and discover potential shortcuts. Moreover, through the bridge constructed between continuous prompts and discrete prompts using our interpretations, it is promising to implement the cross-model transfer of continuous prompts without extra training signals. We hope this work will lead to a novel perspective on the interpretations of continuous prompts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1744,
      "original_url": "https://aclanthology.org/2023.findings-acl.495/",
      "title": "Putting Natural in Natural Language Processing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.495.pdf",
      "authors": [
        "Grzegorz Chrupała"
      ],
      "abstract": "Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation oflanguage, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written ratherthan spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processingcommunity which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to afortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of thesetwo fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processingcould lead to better integration with the rest of language science and could lead to systems which are more data-efficient and morehuman-like, and which can communicate beyond the textual modality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1745,
      "original_url": "https://aclanthology.org/2023.findings-acl.496/",
      "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.496.pdf",
      "authors": [
        "Enes Altinisik",
        "Hassan Sajjad",
        "Husrev Sencar",
        "Safa Messaoud",
        "Sanjay Chawla"
      ],
      "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveal that the improved generalization is due to ‘more specialized’ neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1746,
      "original_url": "https://aclanthology.org/2023.findings-acl.497/",
      "title": "Benchmarking Diverse-Modal Entity Linking with Generative Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.497.pdf",
      "authors": [
        "Sijia Wang",
        "Alexander Hanbo Li",
        "Henghui Zhu",
        "Sheng Zhang",
        "Pramuditha Perera",
        "Chung-Wei Hang",
        "Jie Ma",
        "William Yang Wang",
        "Zhiguo Wang",
        "Vittorio Castelli",
        "Bing Xiang",
        "Patrick Ng"
      ],
      "abstract": "Entities can be expressed in diverse formats, such as texts, images, or column names and cell values in tables. While existing entity linking (EL) models work well on per modality configuration, such as text-only EL, visual grounding or schema linking, it is more challenging to design a unified model for diverse modality configurations. To bring various modality configurations together, we constructed a benchmark for diverse-modal EL (DMEL) from existing EL datasets, covering all three modalities including text, image and table. To approach the DMEL task, we proposed a generative diverse-modal model (GDMM) following a multimodal-encoder-decoder paradigm. Pre-training GDMM with rich corpora builds a solid foundation for DMEL without storing the entire KB for inference. Fine-tuning GDMM builds a stronger DMEL baseline, outperforming state-of-the-art task-specific EL models by 8.51 F1 score on average. Additionally, extensive error analyses are conducted to highlight the challenge of DMEL, facilitating future researches on this task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1747,
      "original_url": "https://aclanthology.org/2023.findings-acl.498/",
      "title": "Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.498.pdf",
      "authors": [
        "Hua Cai",
        "Xuli Shen",
        "Qing Xu",
        "Weilin Shen",
        "Xiaomei Wang",
        "Weifeng Ge",
        "Xiaoqing Zheng",
        "Xiangyang Xue"
      ],
      "abstract": "In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker’s emotion. Besides, external commonsense knowledge has been applied to enhance the system’s understandings of the speaker’s situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion, generated response and speaker’s contextual information. To this end, we propose a novel approach for empathetic response generation, which incorporates an adaptive module for commonsense knowledge selection to ensure consistency between the generated empathetic responses and the speaker’s situation. This selected knowledge is used to refine the commonsense cognition and empathy expression for generated responses. Experimental results show that our approach significantly outperforms baseline models in both automatic and human evaluations, exhibiting the generation of more coherent and empathetic responses. Moreover, case studies highlight the interpretability of knowledge selection in the responses and the effectiveness of adaptive module in our model. Code: https://github.com/Hanscal/DCKS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1748,
      "original_url": "https://aclanthology.org/2023.findings-acl.499/",
      "title": "Additive manifesto decomposition: A policy domain aware method for understanding party positioning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.499.pdf",
      "authors": [
        "Tanise Ceron",
        "Dmitry Nikolaev",
        "Sebastian Padó"
      ],
      "abstract": "Automatic extraction of party (dis)similarities from texts such as party election manifestos or parliamentary speeches plays an increasing role in computational political science. However, existing approaches are fundamentally limited to targeting only global party (dis)-similarity: they condense the relationship between a pair of parties into a single figure, their similarity. In aggregating over all policy domains (e.g., health or foreign policy), they do not provide any qualitative insights into which domains parties agree or disagree on. This paper proposes a workflow for estimating policy domain aware party similarity that overcomes this limitation. The workflow covers (a) definition of suitable policy domains; (b) automatic labeling of domains, if no manual labels are available; (c) computation of domain-level similarities and aggregation at a global level; (d) extraction of interpretable party positions on major policy axes via multidimensional scaling. We evaluate our workflow on manifestos from the German federal elections. We find that our method (a) yields high correlation when predicting party similarity at a global level and (b) provides accurate party-specific positions, even with automatically labelled policy domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1749,
      "original_url": "https://aclanthology.org/2023.findings-acl.500/",
      "title": "Similarizing the Influence of Words with Contrastive Learning to Defend Word-level Adversarial Text Attack",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.500.pdf",
      "authors": [
        "Pengwei Zhan",
        "Jing Yang",
        "He Wang",
        "Chao Zheng",
        "Xiao Huang",
        "Liming Wang"
      ],
      "abstract": "Neural language models are vulnerable to word-level adversarial text attacks, which generate adversarial examples by directly substituting discrete input words. Previous search methods for word-level attacks assume that the information in the important words is more influential on prediction than unimportant words. In this paper, motivated by this assumption, we propose a self-supervised regularization method for Similarizing the Influence of Words with Contrastive Learning (SIWCon) that encourages the model to learn sentence representations in which words of varying importance have a more uniform influence on prediction. Experiments show that SIWCon is compatible with various training methods and effectively improves model robustness against various unforeseen adversarial attacks. The effectiveness of SIWCon is also intuitively shown through qualitative analysis and visualization of the loss landscape, sentence representation, and changes in model confidence."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1750,
      "original_url": "https://aclanthology.org/2023.findings-acl.501/",
      "title": "Responsibility Perspective Transfer for Italian Femicide News",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.501.pdf",
      "authors": [
        "Gosse Minnema",
        "Huiyuan Lai",
        "Benedetta Muscato",
        "Malvina Nissim"
      ],
      "abstract": "Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader’s perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the novel task of automatically rewriting GBV descriptions as a means to alter the perceived level of blame on the perpetrator. We present a quasi-parallel dataset of sentences with low and high perceived responsibility levels for the perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire study and a suite of automatic metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1751,
      "original_url": "https://aclanthology.org/2023.findings-acl.502/",
      "title": "Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.502.pdf",
      "authors": [
        "Eddie Ungless",
        "Bjorn Ross",
        "Anne Lauscher"
      ],
      "abstract": "Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this work, we are the first to investigate how multimodal models handle diverse gender identities. Concretely, we conduct a thorough analysis in which we compare the output of three image generation models for prompts containing cisgender vs. non-cisgender identity terms. Our findings demonstrate that certain non-cisgender identities are consistently (mis)represented as less human, more stereotyped and more sexualised. We complement our experimental analysis with (a) a survey among non-cisgender individuals and (b) a series of interviews, to establish which harms affected individuals anticipate, and how they would like to be represented. We find respondents are particularly concerned about misrepresentation, and the potential to drive harmful behaviours and beliefs. Simple heuristics to limit offensive content are widely rejected, and instead respondents call for community involvement, curated training data and the ability to customise. These improvements could pave the way for a future where change is led by the affected community, and technology is used to positively ”[portray] queerness in ways that we haven’t even thought of”’ rather than reproducing stale, offensive stereotypes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1752,
      "original_url": "https://aclanthology.org/2023.findings-acl.503/",
      "title": "Fine-grained Artificial Neurons in Audio-transformers for Disentangling Neural Auditory Encoding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.503.pdf",
      "authors": [
        "Mengyue Zhou",
        "Xu Liu",
        "David Liu",
        "Zihao Wu",
        "Zhengliang Liu",
        "Lin Zhao",
        "Dajiang Zhu",
        "Lei Guo",
        "Junwei Han",
        "Tianming Liu",
        "Xintao Hu"
      ],
      "abstract": "The Wav2Vec and its variants have achieved unprecedented success in computational auditory and speech processing. Meanwhile, neural encoding studies that integrate the superb representation capability of Wav2Vec and link those representations to brain activities have provided novel insights into a fundamental question of how auditory and speech processing unfold in the human brain. Without an explicit definition, most existing studies treat each transformer encoding layer in Wav2Vec as a single artificial neuron (AN). That is, the layer-level embeddings are used to predict neural responses. However, the comprehensive layer-level embedding aggregates multiple types of contextual attention captured by multi-head self-attention (MSA) modules. Thus, the layer-level ANs lack fine-granularity for neural encoding. To address this limitation, we define the elementary units, i.e., each hidden dimension, as neuron-level ANs in Wav2Vec2.0, quantify their temporal responses, and couple those ANs with their biological-neuron (BN) counterparts in the human brain. Our experimental results demonstrated that: 1) The proposed neuron-level ANs carry meaningful neurolinguistic information; 2) Those ANs anchor to their BN signatures; 3) The AN-BN anchoring patterns are interpretable from a neurolinguistic perspective. More importantly, our results suggest an intermediate stage in both the computational representation in Wav2Vec2.0 and the cortical representation in the brain. Our study validates the fine-grained ANs in Wav2Vec2.0, which may serve as a novel and general strategy to link transformer-based deep learning models to neural responses for probing the sensory processing in the brain."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1753,
      "original_url": "https://aclanthology.org/2023.findings-acl.504/",
      "title": "Deeply Coupled Cross-Modal Prompt Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.504.pdf",
      "authors": [
        "Xuejing Liu",
        "Wei Tang",
        "Jinghui Lu",
        "Rui Zhao",
        "Zhaojun Guo",
        "Fei Tan"
      ],
      "abstract": "Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly accommodates the interplay between vision and language with a Cross-Modal Prompt Attention (CMPA) mechanism, which enables the mutual exchange of respective representation through a well-connected multi-head attention progressively and strongly. We then conduct comprehensive few-shot learning experiments on 11 image classification datasets and analyze the robustness to domain shift as well. Thorough experimental analysis evidently demonstrates the superb few-shot generalization and compelling domain adaption capacity of a well-executed DCP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1754,
      "original_url": "https://aclanthology.org/2023.findings-acl.505/",
      "title": "Opinion Tree Parsing for Aspect-based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.505.pdf",
      "authors": [
        "Xiaoyi Bao",
        "Xiaotong Jiang",
        "Zhongqing Wang",
        "Yue Zhang",
        "Guodong Zhou"
      ],
      "abstract": "Extracting sentiment elements using pre-trained generative models has recently led to large improvements in aspect-based sentiment analysis benchmarks. These models avoid explicit modeling of structure between sentiment elements, which are succinct yet lack desirable properties such as structure well-formedness guarantees or built-in elements alignments. In this study, we propose an opinion tree parsing model, aiming to parse all the sentiment elements from an opinion tree, which can explicitly reveal a more comprehensive and complete aspect-level sentiment structure. In particular, we first introduce a novel context-free opinion grammar to normalize the sentiment structure. We then employ a neural chart-based opinion tree parser to fully explore the correlations among sentiment elements and parse them in the opinion tree form. Extensive experiments show the superiority of our proposed model and the capacity of the opinion tree parser with the proposed context-free opinion grammar. More importantly, our model is much faster than previous models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1755,
      "original_url": "https://aclanthology.org/2023.findings-acl.506/",
      "title": "CoMix: Guide Transformers to Code-Mix using POS structure and Phonetics",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.506.pdf",
      "authors": [
        "Gaurav Arora",
        "Srujana Merugu",
        "Vivek Sembium"
      ],
      "abstract": "Code-mixing is ubiquitous in multilingual societies, which makes it vital to build models for code-mixed data to power human language interfaces. Existing multilingual transformer models trained on pure corpora lack the ability to intermix words of one language into the structure of another. These models are also not robust to orthographic variations. We propose CoMixCoMix is not a trademark and only used to refer to our models for code-mixed data for presentational brevity., a pretraining approach to improve representation of code-mixed data in transformer models by incorporating phonetic signals, a modified attention mechanism, and weak supervision guided generation by parts-of-speech constraints. We show that CoMix improves performance across four code-mixed tasks: machine translation, sequence classification, named entity recognition (NER), and abstractive summarization. It also achieves the new SOTA performance for English-Hinglish translation and NER on LINCE Leaderboard and provides better generalization on out-of-domain translation. Motivated by variations in human annotations, we also propose a new family of metrics based on phonetics and demonstrate that the phonetic variant of BLEU correlates better with human judgement than BLEU on code-mixed text."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1756,
      "original_url": "https://aclanthology.org/2023.findings-acl.507/",
      "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.507.pdf",
      "authors": [
        "Cheng-Yu Hsieh",
        "Chun-Liang Li",
        "Chih-kuan Yeh",
        "Hootan Nakhost",
        "Yasuhisa Fujii",
        "Alex Ratner",
        "Ranjay Krishna",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for training small models within a multi-task framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to few-shot prompted LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our finetuned 770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80% of available data on a benchmark, whereas standard finetuning the same T5 model struggles to match even by using 100% of the dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1757,
      "original_url": "https://aclanthology.org/2023.findings-acl.508/",
      "title": "Prosody-TTS: Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.508.pdf",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yi Ren",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "abstract": "Expressive text-to-speech aims to generate high-quality samples with rich and diverse prosody, which is hampered by dual challenges: 1) prosodic attributes in highly dynamic voices are difficult to capture and model without intonation; and 2) highly multimodal prosodic representations cannot be well learned by simple regression (e.g., MSE) objectives, which causes blurry and over-smoothing predictions. This paper proposes Prosody-TTS, a two-stage pipeline that enhances prosody modeling and sampling by introducing several components: 1) a self-supervised masked autoencoder to model the prosodic representation without relying on text transcriptions or local prosody attributes, which ensures to cover diverse speaking voices with superior generalization; and 2) a diffusion model to sample diverse prosodic patterns within the latent space, which prevents TTS models from generating samples with dull prosodic performance. Experimental results show that Prosody-TTS achieves new state-of-the-art in text-to-speech with natural and expressive synthesis. Both subjective and objective evaluation demonstrate that it exhibits superior audio quality and prosody naturalness with rich and diverse prosodic attributes. Audio samples are available at https://improved_prosody.github.io"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1758,
      "original_url": "https://aclanthology.org/2023.findings-acl.509/",
      "title": "Duplex Diffusion Models Improve Speech-to-Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.509.pdf",
      "authors": [
        "Xianchao Wu"
      ],
      "abstract": "Speech-to-speech translation is a typical sequence-to-sequence learning task that naturally has two directions. How to effectively leverage bidirectional supervision signals to produce high-fidelity audio for both directions? Existing approaches either train two separate models or a multitask-learned model with low efficiency and inferior performance. In this paper, we propose a duplex diffusion model that applies diffusion probabilistic models to both sides of a reversible duplex Conformer, so that either end can simultaneously input and output a distinct language’s speech. Our model enables reversible speech translation by simply flipping the input and output ends. Experiments show that our model achieves the first success of reversible speech translation with significant improvements of ASR-BLEU scores compared with a list of state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1759,
      "original_url": "https://aclanthology.org/2023.findings-acl.510/",
      "title": "Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.510.pdf",
      "authors": [
        "Yuxin Jiang",
        "Linhan Zhang",
        "Wei Wang"
      ],
      "abstract": "Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GlObal and Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of hierarchies with the aid of multi-task learning and contrastive learning. Experimental results on PDTB 2.0 and PDTB 3.0 datasets demonstrate that our method remarkably outperforms current state-of-the-art models at all hierarchical levels."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1760,
      "original_url": "https://aclanthology.org/2023.findings-acl.511/",
      "title": "PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.511.pdf",
      "authors": [
        "Zhuocheng Gong",
        "Jiahao Liu",
        "Qifan Wang",
        "Yang Yang",
        "Jingang Wang",
        "Wei Wu",
        "Yunsen Xian",
        "Dongyan Zhao",
        "Rui Yan"
      ],
      "abstract": "While transformer-based pre-trained language models (PLMs) have dominated a number of NLP applications, these models are heavy to deploy and expensive to use. Therefore, effectively compressing large-scale PLMs becomes an increasingly important problem. Quantization, which represents high-precision tensors with low-bit fix-point format, is a viable solution. However, most existing quantization methods are task-specific, requiring customized training and quantization with a large number of trainable parameters on each individual task. Inspired by the observation that the over-parameterization nature of PLMs makes it possible to freeze most of the parameters during the fine-tuning stage, in this work, we propose a novel “quantize before fine-tuning” framework, PreQuant, that differs from both quantization-aware training and post-training quantization. {pasted macro ‘OUR’} is compatible with various quantization strategies, with outlier-aware parameter-efficient fine-tuning incorporated to correct the induced quantization error. We demonstrate the effectiveness of PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an empirical investigation into the workflow of PreQuant, which sheds light on its efficacy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1761,
      "original_url": "https://aclanthology.org/2023.findings-acl.512/",
      "title": "Synthetic Pre-Training Tasks for Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.512.pdf",
      "authors": [
        "Zexue He",
        "Graeme Blackwood",
        "Rameswar Panda",
        "Julian McAuley",
        "Rogerio Feris"
      ],
      "abstract": "Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely synthetic parallel data. We hope the findings from our comprehensive empirical analysis will shed light on understanding what matters for NMT pre-training, as well as pave the way for the development of more efficient and less toxic models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1762,
      "original_url": "https://aclanthology.org/2023.findings-acl.513/",
      "title": "IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.513.pdf",
      "authors": [
        "Zihang Xu",
        "Ziqing Yang",
        "Yiming Cui",
        "Shijin Wang"
      ],
      "abstract": "In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LoGic Pre-training (LGP). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability through testing on tasks in GLUE. Besides, at the beginning of the era of large language models, we take several of them like ChatGPT into comparison and find that IDOL still shows its advantage."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1763,
      "original_url": "https://aclanthology.org/2023.findings-acl.514/",
      "title": "Adversarial Training for Low-Resource Disfluency Correction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.514.pdf",
      "authors": [
        "Vineet Bhat",
        "Preethi Jyothi",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Disfluencies commonly occur in conversational speech. Speech with disfluencies can result in noisy Automatic Speech Recognition (ASR) transcripts, which affects downstream tasks like machine translation. In this paper, we propose an adversarially-trained sequence-tagging model for Disfluency Correction (DC) that utilizes a small amount of labeled real disfluent data in conjunction with a large amount of unlabeled data. We show the benefit of our proposed technique, which crucially depends on synthetically generated disfluent data, by evaluating it for DC in three Indian languages- Bengali, Hindi, and Marathi (all from the Indo-Aryan family). Our technique also performs well in removing stuttering disfluencies in ASR transcripts introduced by speech impairments. We achieve an average 6.15 points improvement in F1-score over competitive baselines across all three languages mentioned. To the best of our knowledge, we are the first to utilize adversarial training for DC and use it to correct stuttering disfluencies in English, establishing a new benchmark for this task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1764,
      "original_url": "https://aclanthology.org/2023.findings-acl.515/",
      "title": "Computer says “No”: The Case Against Empathetic Conversational AI",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.515.pdf",
      "authors": [
        "Alba Cercas Curry",
        "Amanda Cercas Curry"
      ],
      "abstract": "Emotions are an integral part of human cognition and they guide not only our understanding of the world but also our actions within it. As such, whether we soothe or flame an emotion is not inconsequential. Recent work in conversational AI has focused on responding empathetically to users, validating and soothing their emotions without a real basis. This AI-aided emotional regulation can have negative consequences for users and society, tending towards a one-noted happiness defined as only the absence of “negative” emotions. We argue that we must carefully consider whether and how to respond to users’ emotions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1765,
      "original_url": "https://aclanthology.org/2023.findings-acl.516/",
      "title": "Stubborn Lexical Bias in Data and Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.516.pdf",
      "authors": [
        "Sofia Serrano",
        "Jesse Dodge",
        "Noah A. Smith"
      ],
      "abstract": "In NLP, recent work has seen increased focus on spurious correlations between various features and labels in training data, and how these influence model behavior. However, the presence and effect of such correlations are typically examined feature by feature. We investigate the cumulative impact on a model of many such intersecting features. Using a new statistical method, we examine whether such spurious patterns in data appear in models trained on the data. We select two tasks— natural language inference and duplicate-question detection— for which any unigram feature on its own should ideally be uninformative, which gives us a large pool of automatically extracted features with which to experiment. The large size of this pool allows us to investigate the intersection of features spuriously associated with (potentially different) labels. We then apply an optimization approach to *reweight* the training data, reducing thousands of spurious correlations, and examine how doing so affects models trained on the reweighted data. Surprisingly, though this method can successfully reduce lexical biases in the training data, we still find strong evidence of corresponding bias in the trained models, including worsened bias for slightly more complex features (bigrams). We close with discussion about the implications of our results on what it means to “debias” training data, and how issues of data quality can affect model bias."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1766,
      "original_url": "https://aclanthology.org/2023.findings-acl.517/",
      "title": "Distilling Efficient Language-Specific Models for Cross-Lingual Transfer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.517.pdf",
      "authors": [
        "Alan Ansell",
        "Edoardo Maria Ponti",
        "Anna Korhonen",
        "Ivan Vulić"
      ],
      "abstract": "Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs’ language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which retain the capacity of the original MMTs for cross-lingual transfer. This is achieved by distilling the MMT *bilingually*, i.e., using data from only the source and target language of interest. Specifically, we use a two-phase distillation approach, termed BiStil: (i) the first phase distils a general bilingual model from the MMT, while (ii) the second, task-specific phase sparsely fine-tunes the bilingual “student” model using a task-tuned variant of the original MMT as its “teacher”. We evaluate this distillation technique in zero-shot cross-lingual transfer across a number of standard cross-lingual benchmarks. The key results indicate that the distilled models exhibit minimal degradation in target language performance relative to the base MMT despite being significantly smaller and faster. Furthermore, we find that they outperform multilingually distilled models such as DistilmBERT and MiniLMv2 while having a very modest training budget in comparison, even on a per-language basis. We also show that bilingual models distilled from MMTs greatly outperform bilingual models trained from scratch."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1767,
      "original_url": "https://aclanthology.org/2023.findings-acl.518/",
      "title": "An Extensive Exploration of Back-Translation in 60 Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.518.pdf",
      "authors": [
        "Paul McNamee",
        "Kevin Duh"
      ],
      "abstract": "Back-translation is a data augmentation technique that has been shown to improve model quality through the creation of synthetic training bitext. Early studies showed the promise of the technique and follow on studies have produced additional refinements. We have undertaken a broad investigation using back-translation to train models from 60 languages into English; the majority of these languages are considered moderate- or low-resource languages. We observed consistent gains, though compared to prior work we saw conspicuous gains in quite a number of lower-resourced languages. We analyzed differences in translations between baseline and back-translation models, and observed many indications of improved translation quality. Translation of both rare and common terms is improved, and these improvements occur despite the less natural synthetic source-language text used in training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1768,
      "original_url": "https://aclanthology.org/2023.findings-acl.519/",
      "title": "AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.519.pdf",
      "authors": [
        "Ru Zhou",
        "Wenya Guo",
        "Xumeng Liu",
        "Shenglong Yu",
        "Ying Zhang",
        "Xiaojie Yuan"
      ],
      "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolutional network to model the vision-text and text-text interaction. Extensive experiments demonstrate the superiority of AoM to existing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1769,
      "original_url": "https://aclanthology.org/2023.findings-acl.520/",
      "title": "Forecasting Earnings Surprises from Conference Call Transcripts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.520.pdf",
      "authors": [
        "Ross Koval",
        "Nicholas Andrews",
        "Xifeng Yan"
      ],
      "abstract": "There is a multitude of textual data relevant to the financial markets, spanning genres such as financial news, earnings conference calls, and social media posts. Earnings conference calls are one of the most important to information flow as they reflect a direct communication between company executives, financial analysts, and large shareholders. Since these calls contain content that is forward-looking in nature, they can be used to forecast the future performance of the company relative to market expectations. However, they typically contain over 5,000 words of text and large amounts of industry jargon. This length and domain-specific language present problems for many generic pretrained language models. In this work, we introduce a novel task of predicting earnings surprises from earnings call transcripts and contribute a new long document dataset that tests financial understanding with complex signals. We explore a variety of approaches for this long document classification task and establish some strong baselines. Furthermore, we demonstrate that it is possible to predict companies’ future earnings surprises from solely the text of their conference calls with reasonable accuracy. Finally, we probe the models through different interpretability methods and reveal some intuitive explanations of the linguistic features captured that go beyond traditional sentiment analysis."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1770,
      "original_url": "https://aclanthology.org/2023.findings-acl.521/",
      "title": "MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.521.pdf",
      "authors": [
        "Sebastian Vincent",
        "Robert Flynn",
        "Carolina Scarton"
      ],
      "abstract": "Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker’s gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCue significantly outperforms a “tagging” baseline at translating English text. Analysis reveals that the context encoder of MTCue learns a representation space that organises context based on specific attributes, such as formality, enabling effective zero-shot control. Pre-training on context embeddings also improves MTCue’s few-shot performance compared to the “tagging” baseline. Finally, an ablation study conducted on model components and contextual variables further supports the robustness of MTCue for context-based NMT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1771,
      "original_url": "https://aclanthology.org/2023.findings-acl.522/",
      "title": "Evaluation for Change",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.522.pdf",
      "authors": [
        "Rishi Bommasani"
      ],
      "abstract": "Evaluation is the central means for assessing, understanding, and communicating about NLP models. In this position paper, we argue evaluation should be more than that: it is a force for driving change, carrying a sociological and political character beyond its technical dimensions. As a force, evaluation’s power arises from its adoption: under our view, evaluation succeeds when it achieves the desired change in the field. Further, by framing evaluation as a force, we consider how it competes with other forces. Under our analysis, we conjecture that the current trajectory of NLP suggests evaluation’s power is waning, in spite of its potential for realizing more pluralistic ambitions in the field. We conclude by discussing the legitimacy of this power, who acquires this power and how it distributes. Ultimately, we hope the research community will more aggressively harness evaluation to drive change."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1772,
      "original_url": "https://aclanthology.org/2023.findings-acl.523/",
      "title": "Reconstruction Probing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.523.pdf",
      "authors": [
        "Najoung Kim",
        "Jatin Khilnani",
        "Alex Warstadt",
        "Abdelrahim Qaddoumi"
      ],
      "abstract": "We propose reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decontextualized lexical prior of the model. This comparison can be understood as quantifying the contribution of contextualization towards reconstruction—the difference in the reconstruction probabilities can only be attributed to the representational change of the single token induced by contextualization. We apply this analysis to three MLMs and find that contextualization boosts reconstructability of tokens that are close to the token being reconstructed in terms of linear and syntactic distance. Furthermore, we extend our analysis to finer-grained decomposition of contextualized representations, and we find that these boosts are largely attributable to static and positional embeddings at the input layer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1773,
      "original_url": "https://aclanthology.org/2023.findings-acl.524/",
      "title": "Towards Distribution-shift Robust Text Classification of Emotional Content",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.524.pdf",
      "authors": [
        "Luana Bulla",
        "Aldo Gangemi",
        "Misael Mongiovi’"
      ],
      "abstract": "Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are usually created ad-hoc and whose data distribution often differs from the one in real applications, showing significant performance degradation in real-world scenarios. We perform an extensive assessment of the out-of-distribution performances of supervised models for classification in the emotion and hate-speech detection tasks and show that NLI-based zero-shot models often outperform them, making task-specific annotation useless when the characteristics of final-user data are not known in advance. To benefit from both supervised and zero-shot approaches, we propose to fine-tune an NLI-based model on the task-specific dataset. The resulting model often outperforms all available supervised models both in distribution and out of distribution, with only a few thousand training samples."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1774,
      "original_url": "https://aclanthology.org/2023.findings-acl.525/",
      "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.525.pdf",
      "authors": [
        "Anubha Kabra",
        "Emmy Liu",
        "Simran Khanuja",
        "Alham Fikri Aji",
        "Genta Winata",
        "Samuel Cahyawijaya",
        "Anuoluwapo Aremu",
        "Perez Ogayo",
        "Graham Neubig"
      ],
      "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, {pasted macro ‘DATASETNAME’}, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs’ abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training. Data and code is released at https://anonymous.4open.science/r/Multilingual-Fig-QA-7B03/"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1775,
      "original_url": "https://aclanthology.org/2023.findings-acl.526/",
      "title": "Open-WikiTable : Dataset for Open Domain Question Answering with Complex Reasoning over Table",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.526.pdf",
      "authors": [
        "Sunjun Kweon",
        "Yeonsu Kwon",
        "Seonhee Cho",
        "Yohan Jo",
        "Edward Choi"
      ],
      "abstract": "Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. These datasets assume answers reside as a single cell value and do not necessitate exploring over multiple cells such as aggregation, comparison, and sorting. Thus, we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon WikiSQL and WikiTableQuestions to be applicable in the open-domain setting. As each question is coupled with both textual answers and SQL queries, Open-WikiTable opens up a wide range of possibilities for future research, as both reader and parser methods can be applied. The dataset is publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1776,
      "original_url": "https://aclanthology.org/2023.findings-acl.527/",
      "title": "What In-Context Learning “Learns” In-Context: Disentangling Task Recognition and Task Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.527.pdf",
      "authors": [
        "Jane Pan",
        "Tianyu Gao",
        "Howard Chen",
        "Danqi Chen"
      ],
      "abstract": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations – even without ground-truth labels – and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL’s performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1777,
      "original_url": "https://aclanthology.org/2023.findings-acl.528/",
      "title": "Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.528.pdf",
      "authors": [
        "Ercong Nie",
        "Sheng Liang",
        "Helmut Schmid",
        "Hinrich Schütze"
      ],
      "abstract": "Multilingual Pretrained Language Models (MPLMs) perform strongly in cross-lingual transfer. We propose Prompts Augmented by Retrieval Crosslingually (PARC) to improve zero-shot performance on low-resource languages (LRLs) by augmenting the context with prompts consisting of semantically similar sentences retrieved from a high-resource language (HRL). PARC improves zero-shot performance on three downstream tasks (sentiment classification, topic categorization, natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in unlabeled (+5.1%) and labeled settings (+16.3%). PARC also outperforms finetuning by 3.7%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1778,
      "original_url": "https://aclanthology.org/2023.findings-acl.529/",
      "title": "Unsupervised Summarization Re-ranking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.529.pdf",
      "authors": [
        "Mathieu Ravaut",
        "Shafiq Joty",
        "Nancy Chen"
      ],
      "abstract": "With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1779,
      "original_url": "https://aclanthology.org/2023.findings-acl.530/",
      "title": "GRACE: Gradient-guided Controllable Retrieval for Augmenting Attribute-based Text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.530.pdf",
      "authors": [
        "Zhihua Wen",
        "Zhiliang Tian",
        "Zhen Huang",
        "Yuxin Yang",
        "Zexin Jian",
        "Changjian Wang",
        "Dongsheng Li"
      ],
      "abstract": "Attribute-based generation methods are of growing significance in controlling the generation of large pre-trained language models (PLMs). Existing studies control the generation by (1) finetuning the model with attributes or (2) guiding the inference processing toward control signals while freezing the PLM. However, finetuning approaches infuse domain bias into generation, making it hard to generate out-of-domain texts. Besides, many methods guide the inference in its word-by-word generation, pushing the word probability to the target attributes, resulting in less fluent sentences. We argue that distilling controlling information from natural texts can produce fluent sentences while maintaining high controllability. In this paper, we propose GRAdient-guided Controllable rEtrieval (GRACE), a retrieval-augmented generation framework to facilitate the generation of fluent sentences with high attribute relevance. GRACE memorizes the semantic and attribute information from unlabeled corpora and applies a controllable retrieval to obtain desired information. For the generation, we design techniques to eliminate the domain bias from the retrieval results and integrate it into the generation model. Additionally, we propose a gradient-guided generation scheme that iteratively steers generation toward higher attribute relevance. Experimental results and quantities of examples verify the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1780,
      "original_url": "https://aclanthology.org/2023.findings-acl.531/",
      "title": "So many design choices: Improving and interpreting neural agent communication in signaling games",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.531.pdf",
      "authors": [
        "Timothée Bernard",
        "Timothee Mickus"
      ],
      "abstract": "Emergent language games are experimental protocols designed to model how communication may arise among a group of agents. In this paper, we focus on how to improve performances of neural agents playing a signaling game: a sender is exposed to an image and generates a sequence of symbols that is transmitted to a receiver, which uses it to distinguish between two images, one that is semantically related to the original image, and one that is not. We consider multiple design choices, such as pretraining the visual components of the agents, introducing regularization terms, how to sample training items from the dataset, and we study how these different choices impact the behavior and performances of the agents. To that end, we introduce a number of automated metrics to measure the properties of the emergent language. We find that some implementation choices are always beneficial, and that the information that is conveyed by the agents’ messages is shaped not only by the game, but also by the overall design of the agents as well as seemingly unrelated implementation choices."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1781,
      "original_url": "https://aclanthology.org/2023.findings-acl.532/",
      "title": "Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.532.pdf",
      "authors": [
        "Fanyu Wang",
        "Zhenping Xie"
      ],
      "abstract": "As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current uninterpretable semantic representations of pre-trained models. Our experiments for performance evaluation and interpretable analysis are executed on several types of datasets, including SIGHAN, Weibo, and ChnSenti. Wherein a novel evaluation strategy for the interpretability of machine learning models is first proposed. According to the experimental results, our language model can achieve better performance and highly credible interpretable ability compared to related state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1782,
      "original_url": "https://aclanthology.org/2023.findings-acl.533/",
      "title": "Fixed Input Parameterization for Efficient Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.533.pdf",
      "authors": [
        "Eunbi Choi",
        "Yongrae Jo",
        "Joel Jang",
        "Joonwon Jang",
        "Minjoon Seo"
      ],
      "abstract": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We formally define Fixed Input Parameterization (FIP) problem that focuses on injecting the fixed prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for FIP and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that FIP can be a promising direction for conditioning language models, in scenarios with long and fixed prompts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1783,
      "original_url": "https://aclanthology.org/2023.findings-acl.534/",
      "title": "Data Augmentation for Low-Resource Keyphrase Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.534.pdf",
      "authors": [
        "Krishna Garg",
        "Jishnu Ray Chowdhury",
        "Cornelia Caragea"
      ],
      "abstract": "Keyphrase generation is the task of summarizing the contents of any given article into a few salient phrases (or keyphrases). Existing works for the task mostly rely on large-scale annotated datasets, which are not easy to acquire. Very few works address the problem of keyphrase generation in low-resource settings, but they still rely on a lot of additional unlabeled data for pretraining and on automatic methods for pseudo-annotations. In this paper, we present data augmentation strategies specifically to address keyphrase generation in purely resource-constrained domains. We design techniques that use the full text of the articles to improve both present and absent keyphrase generation. We test our approach comprehensively on three datasets and show that the data augmentation strategies consistently improve the state-of-the-art performance. We release our source code at https://github.com/kgarg8/kpgen-lowres-data-aug."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1784,
      "original_url": "https://aclanthology.org/2023.findings-acl.535/",
      "title": "BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.535.pdf",
      "authors": [
        "Liyan Kang",
        "Luyang Huang",
        "Ningxin Peng",
        "Peihao Zhu",
        "Zewei Sun",
        "Shanbo Cheng",
        "Mingxuan Wang",
        "Degen Huang",
        "Jinsong Su"
      ],
      "abstract": "We present a large-scale video subtitle translation dataset, *BigVideo*, to facilitate the study of multi-modality machine translation. Compared with the widely used *How2* and *VaTeX* datasets, *BigVideo* is more than 10 times larger, consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also introduce two deliberately designed test sets to verify the necessity of visual information: *Ambiguous* with the presence of ambiguous words, and *Unambiguous* in which the text context is self-contained for translation. To better model the common semantics shared across texts and videos, we introduce a contrastive learning method in the cross-modal encoder. Extensive experiments on the *BigVideo* shows that: a) Visual information consistently improves the NMT model in terms of BLEU, BLEURT and COMET on both Ambiguous and Unambiguous test sets. b) Visual information helps disambiguation, compared to the strong text baseline on terminology-targeted scores and human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1785,
      "original_url": "https://aclanthology.org/2023.findings-acl.536/",
      "title": "Constructing Procedural Graphs with Multiple Dependency Relations: A New Dataset and Baseline",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.536.pdf",
      "authors": [
        "Haopeng Ren",
        "Yushi Zeng",
        "Yi Cai",
        "Bihan Zhou",
        "Zetao Lian"
      ],
      "abstract": "Current structured and semi-structured knowledge bases mainly focus on representing descriptive knowledge but ignore another commonsense knowledge (Procedural Knowledge). To structure the procedural knowledge, existing methods are proposed to automatically generate flow graphs from procedural documents. They focus on extracting sequential dependency between sentences but neglect another two important dependencies (i.e., inclusion dependency and constraint dependency) in procedural documents. In our paper, we explore a problem of automatically generating procedural graph with multiple dependency relations to extend the flow graph constructed by existing methods and propose a procedural graph construction method with syntactic information and discourse structures. A new dataset (WHPG) is built and extensive experiments are conducted to evaluate the effectiveness of our proposed model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1786,
      "original_url": "https://aclanthology.org/2023.findings-acl.537/",
      "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.537.pdf",
      "authors": [
        "Sameer Jain",
        "Vaishakh Keshava",
        "Swarnashree Mysore Sathyendra",
        "Patrick Fernandes",
        "Pengfei Liu",
        "Graham Neubig",
        "Chunting Zhou"
      ],
      "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning-based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1787,
      "original_url": "https://aclanthology.org/2023.findings-acl.538/",
      "title": "Learning to Rank Utterances for Query-Focused Meeting Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.538.pdf",
      "authors": [
        "Xingxian Liu",
        "Yajing Xu"
      ],
      "abstract": "Query-focused meeting summarization(QFMS) aims to generate a specific summary for the given query according to the meeting transcripts. Due to the conflict between long meetings and limited input size, previous works mainly adopt extract-then-summarize methods, which use extractors to simulate binary labels or ROUGE scores to extract utterances related to the query and then generate a summary. However, the previous approach fails to fully use the comparison between utterances. To the extractor, comparison orders are more important than specific scores. In this paper, we propose a Ranker-Generator framework. It learns to rank the utterances by comparing them in pairs and learning from the global orders, then uses top utterances as the generator’s input. We show that learning to rank utterances helps to select utterances related to the query effectively, and the summarizer can benefit from it. Experimental results on QMSum show that the proposed model outperforms all existing multi-stage models with fewer parameters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1788,
      "original_url": "https://aclanthology.org/2023.findings-acl.539/",
      "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.539.pdf",
      "authors": [
        "Neal Lawton",
        "Anoop Kumar",
        "Govind Thattai",
        "Aram Galstyan",
        "Greg Ver Steeg"
      ],
      "abstract": "Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1789,
      "original_url": "https://aclanthology.org/2023.findings-acl.540/",
      "title": "Aligning Offline Metrics and Human Judgments of Value for Code Generation Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.540.pdf",
      "authors": [
        "Victor Dibia",
        "Adam Fourney",
        "Gagan Bansal",
        "Forough Poursabzi-Sangdeh",
        "Han Liu",
        "Saleema Amershi"
      ],
      "abstract": "Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code are most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N=49 experienced programmers, we show that while correctness captures high-value generations, programmers still rate code that fails unit tests as valuable if it reduces the overall effort needed to complete a coding task. Finally, we propose a hybrid metric that combines functional correctness and syntactic similarity and show that it achieves a 14% stronger correlation with value and can therefore better represent real-world gains when evaluating and comparing models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1790,
      "original_url": "https://aclanthology.org/2023.findings-acl.541/",
      "title": "Do transformer models do phonology like a linguist?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.541.pdf",
      "authors": [
        "Saliha Muradoglu",
        "Mans Hulden"
      ],
      "abstract": "Neural sequence-to-sequence models have been very successful at tasks in phonology and morphology that seemingly require a capacity for intricate linguistic generalisations. In this paper, we perform a detailed breakdown of the power of such models to capture various phonological generalisations and to benefit from exposure to one phonological rule to infer the behaviour of another similar rule. We present two types of experiments, one of which establishes the efficacy of the transformer model on 29 different processes. The second experiment type follows a priming and held-out case split where our model is exposed to two (or more) phenomena; one which is used as a primer to make the model aware of a linguistic category (e.g. voiceless stops) and a second one which contains a rule with a withheld case that the model is expected to infer (e.g. word-final devoicing with a missing training example such as b→p) results show that the transformer model can successfully model all 29 phonological phenomena considered, regardless of perceived process difficulty. We also show that the model can generalise linguistic categories and structures, such as vowels and syllables, through priming processes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1791,
      "original_url": "https://aclanthology.org/2023.findings-acl.542/",
      "title": "DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.542.pdf",
      "authors": [
        "Sajad Norouzi",
        "Rasa Hosseinzadeh",
        "Felipe Perez",
        "Maksims Volkovs"
      ],
      "abstract": "The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher’s knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining 7.8 and 12.9 BLEU points improvements in single-step translation accuracy on distilled and raw versions of WMT’14 De-En.Full code for this work is available here: https://github.com/layer6ai-labs/DiMS"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1792,
      "original_url": "https://aclanthology.org/2023.findings-acl.543/",
      "title": "Retrieval-augmented Video Encoding for Instructional Captioning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.543.pdf",
      "authors": [
        "Yeonjoon Jung",
        "Minsoo Kim",
        "Seungtaek Choi",
        "Jihyuk Kim",
        "Minji Seo",
        "Seung-won Hwang"
      ],
      "abstract": "Instructional videos make learning knowledge more efficient, by providing a detailed multimodal context of each procedure in instruction.A unique challenge posed by instructional videos is key-object degeneracy, where any single modality fails to sufficiently capture the key objects referred to in the procedure. For machine systems, such degeneracy can disturb the performance of a downstream task such as dense video captioning, leading to the generation of incorrect captions omitting key objects. To repair degeneracy, we propose a retrieval-based framework to augment the model representations in the presence of such key-object degeneracy. We validate the effectiveness and generalizability of our proposed framework over baselines using modalities with key-object degeneracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1793,
      "original_url": "https://aclanthology.org/2023.findings-acl.544/",
      "title": "Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.544.pdf",
      "authors": [
        "Sai Ashish Somayajula",
        "Lifeng Jin",
        "Linfeng Song",
        "Haitao Mi",
        "Dong Yu"
      ],
      "abstract": "Training a large language model in low-resource settings is challenging since they are susceptible to overfitting with limited generalization abilities. Previous work addresses this issue by approaches such as tunable parameters reduction or data augmentation. However, they either limit the trained models’ expressiveness or rely on task-independent knowledge. In this paper, we propose the Bi-level Finetuning with Task-dependent Similarity Structure framework where all parameters, including the embeddings for unseen tokens, are finetuned with task-dependent information from the training data only. In this framework, a task-dependent similarity structure is learned in a data-driven fashion, which in turn is used to compose soft embeddings from conventional embeddings to be used in training to update all parameters. In order to learn the similarity structure and model parameters, we propose a bi-level optimization algorithm with two stages—search and finetune—to ensure successful learning. Results of experiments on several classification datasets in low-resource scenarios demonstrate that models trained with our method outperform strong baselines. Ablation experiments further support the effectiveness of different components in our framework. Code is available at https://github.com/Sai-Ashish/BFTSS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1794,
      "original_url": "https://aclanthology.org/2023.findings-acl.545/",
      "title": "Kanbun-LM: Reading and Translating Classical Chinese in Japanese Methods by Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.545.pdf",
      "authors": [
        "Hao Wang",
        "Hirofumi Shimizu",
        "Daisuke Kawahara"
      ],
      "abstract": "Recent studies in natural language processing (NLP) have focused on modern languages and achieved state-of-the-art results in many tasks. Meanwhile, little attention has been paid to ancient texts and related tasks. Classical Chinese first came to Japan approximately 2,000 years ago. It was gradually adapted to a Japanese form called Kanbun-Kundoku (Kanbun) in Japanese reading and translating methods, which has significantly impacted Japanese literature. However, compared to the rich resources of ancient texts in mainland China, Kanbun resources remain scarce in Japan.To solve this problem, we construct the first Classical-Chinese-to-Kanbun dataset in the world. Furthermore, we introduce two tasks, character reordering and machine translation, both of which play a significant role in Kanbun comprehension. We also test the current language models on these tasks and discuss the best evaluation method by comparing the results with human scores. We release our code and dataset on GitHub."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1795,
      "original_url": "https://aclanthology.org/2023.findings-acl.546/",
      "title": "Adaptive Attention for Sparse-based Long-sequence Transformer",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.546.pdf",
      "authors": [
        "Xuanyu Zhang",
        "Zhepeng Lv",
        "Qing Yang"
      ],
      "abstract": "Recently, Transformers have been widely used in various fields and have achieved remarkable results. But it is still difficult for Transformer-based models to process longer sequences because self-attention in them scales quadratically with the sequence length. Although some models attempt to use sparse attention to reduce computational complexity, hand-crafted attention patterns are unable to select useful tokens adaptively according to the context. Thus, in this paper, we propose a novel efficient Transformer model with adaptive attention, A2-Former, for long sequence modeling. It can select useful tokens automatically in sparse attention by learnable position vectors, which consist of meta position and offset position vectors. Because the learnable offset position is not an integer vector, we utilize the interpolation technique to gather corresponding vectors from the input embedding matrix by discrete indexes. Experiments on Long Range Arena (LRA), a systematic and unified benchmark with different tasks, show that our model has achieved further improvement in performance compared with other sparse-based Transformers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1796,
      "original_url": "https://aclanthology.org/2023.findings-acl.547/",
      "title": "Sentiment Analysis using the Relationship between Users and Products",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.547.pdf",
      "authors": [
        "Natthawut Kertkeidkachorn",
        "Kiyoaki Shirai"
      ],
      "abstract": "In product reviews, user and product aspects are useful in sentiment analysis. Nevertheless, previous studies mainly focus on modeling user and product aspects without considering the relationship between users and products. The relationship between users and products is typically helpful in estimating the bias of a user toward a product. In this paper, we, therefore, introduce the Graph Neural Network-based model with the pre-trained Language Model (GNNLM), where the relationship between users and products is incorporated. We conducted experiments on three well-known benchmarks for sentiment classification with the user and product information. The experimental results show that the relationship between users and products improves the performance of sentiment analysis. Furthermore, GNNLM achieves state-of-the-art results on yelp-2013 and yelp-2014 datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1797,
      "original_url": "https://aclanthology.org/2023.findings-acl.548/",
      "title": "Entropy-guided Vocabulary Augmentation of Multilingual Language Models for Low-resource Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.548.pdf",
      "authors": [
        "Arijit Nag",
        "Bidisha Samanta",
        "Animesh Mukherjee",
        "Niloy Ganguly",
        "Soumen Chakrabarti"
      ],
      "abstract": "Multilingual language models (MLLMs) like mBERTpromise to extend the benefits of NLP research to low-resource languages (LRLs). However, LRL words are under-represented in the wordpiece/subword vocabularies of MLLMs. This leads to many LRL words getting replaced by UNK, or concatenated from morphologically unrelated wordpieces, leading to low task accuracy. (Pre)-training MLLMs after including LRL documents is resource-intensive in terms of both human inputs and computational resources. In response, we propose EVALM (entropy-based vocabulary augmented language model), which uses a new task-cognizant measurement to detect the most vulnerable LRL words, whose wordpiece segmentations are undesirable. EVALM then provides reasonable initializations of their embeddings, followed by limited fine-tuning using the small LRL task corpus. Our experiments show significant performance improvements and also some surprising limits to such vocabulary augmentation strategies in various classification tasks for multiple diverse LRLs, as well as code-mixed texts. We will release the code and data to enable further research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1798,
      "original_url": "https://aclanthology.org/2023.findings-acl.549/",
      "title": "Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.549.pdf",
      "authors": [
        "Qingyu Tan",
        "Lu Xu",
        "Lidong Bing",
        "Hwee Tou Ng"
      ],
      "abstract": "Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as ‘no_relation’. Models trained with such data inevitably make similar mistakes during the inference stage. Self-training has been proven effective in alleviating the false negative problem. However, traditional self-training is vulnerable to confirmation bias and exhibits poor performance in minority classes. To overcome this limitation, we proposed a novel class-adaptive re-sampling self-training framework. Specifically, we re-sampled the pseudo-labels for each class by precision and recall scores. Our re-sampling strategy favored the pseudo-labels of classes with high precision and low recall, which improved the overall recall without significantly compromising precision. We conducted experiments on document-level and biomedical relation extraction datasets, and the results showed that our proposed self-training framework consistently outperforms existing competitive methods on the Re-DocRED and ChemDisgene datasets when the training data are incompletely annotated."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1799,
      "original_url": "https://aclanthology.org/2023.findings-acl.550/",
      "title": "Solving Cosine Similarity Underestimation between High Frequency Words by ℓ2 Norm Discounting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.550.pdf",
      "authors": [
        "Saeth Wannasuphoprasit",
        "Yi Zhou",
        "Danushka Bollegala"
      ],
      "abstract": "Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words CITATION.This similarity underestimation problem is particularly severe for high frequent words. Although this problem has been noted in prior work, no solution has been proposed thus far. We observe that the ℓ2 norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus.Consequently, the larger ℓ2 norms associated with the high frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores.To solve this issue, we propose a method to discount the ℓ2 norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words.We show that the so called stop words behave differently from the rest of the words, which require special consideration during their discounting process.Experimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem.An anonymized version of the source code of our proposed method is submitted to the reviewing system."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1800,
      "original_url": "https://aclanthology.org/2023.findings-acl.551/",
      "title": "Do Large Language Models Know What They Don’t Know?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.551.pdf",
      "authors": [
        "Zhangyue Yin",
        "Qiushi Sun",
        "Qipeng Guo",
        "Jiawen Wu",
        "Xipeng Qiu",
        "Xuanjing Huang"
      ],
      "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs’ self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1801,
      "original_url": "https://aclanthology.org/2023.findings-acl.552/",
      "title": "AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.552.pdf",
      "authors": [
        "Zhongzhi Chen",
        "Guang Liu",
        "Bo-Wen Zhang",
        "Qinghong Yang",
        "Ledell Wu"
      ],
      "abstract": "CLIP (Contrastive Language–Image Pretraining) is an English multimodal representation model learned from a massive amount of English text-image pairs and has achieved great success in various downstream tasks, including image classification, text-to-image retrieval, and image generation. When extending CLIP to other languages, the major problem is the lack of good-quality text-image pairs. In this work, we present AltCLIP, a simple and low-resource method to build a strong multilingual multimodal representation model. Instead of training a model from scratch on multilingual text-image pairs, we take the original CLIP model trained on English text-image pairs and alter its text encoder with a pre-trained multilingual text encoder (XLM-R). We then align text and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. Our method utilizes the existence of rich parallel text data and pre-trained multilingual language models. We present extensive experimental evaluations to demonstrate the effectiveness of our proposed method. Our model sets new state-of-the-art zero-shot performances on a wide range of tasks in multilingual multimodal benchmarks, including ImageNet-CN/IT/JA/KO serials, Flicker30k-CN, COCO-CN, Multi30k, and XTD. Further, our model outperforms the original CLIP model on zero-shot cross-modal retrieval, Image Classification in the Wild (ICinW) tasks, and CLIP Benchmark. We plan to open-source our code, pre-trained model weights, and evaluation toolkits of multilingual multimodal tasks, to facilitate research on multilingual multimodal representation learning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1802,
      "original_url": "https://aclanthology.org/2023.findings-acl.553/",
      "title": "RHGN: Relation-gated Heterogeneous Graph Network for Entity Alignment in Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.553.pdf",
      "authors": [
        "Xukai Liu",
        "Kai Zhang",
        "Ye Liu",
        "Enhong Chen",
        "Zhenya Huang",
        "Linan Yue",
        "Jiaxian Yan"
      ],
      "abstract": "Entity Alignment, which aims to identify equivalent entities from various Knowledge Graphs (KGs), is a fundamental and crucial task in knowledge graph fusion. Existing methods typically use triple or neighbor information to represent entities, and then align those entities using similarity matching. Most of them, however, fail to account for the heterogeneity among KGs and the distinction between KG entities and relations. To better solve these problems, we propose a Relation-gated Heterogeneous Graph Network (RHGN) for entity alignment. Specifically, RHGN contains a relation-gated convolutional layer to distinguish relations and entities in the KG. In addition, RHGN adopts a cross-graph embedding exchange module and a soft relation alignment module to address the neighbor heterogeneity and relation heterogeneity between different KGs, respectively. Extensive experiments on four benchmark datasets demonstrate that RHGN is superior to existing state-of-the-art entity alignment methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1803,
      "original_url": "https://aclanthology.org/2023.findings-acl.554/",
      "title": "Feature Interactions Reveal Linguistic Structure in Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.554.pdf",
      "authors": [
        "Jaap Jumelet",
        "Willem Zuidema"
      ],
      "abstract": "We study feature interactions in the context of feature attribution methods for post-hoc interpretability. In interpretability research, getting to grips with feature interactions is increasingly recognised as an important challenge, because interacting features are key to the success of neural networks. Feature interactions allow a model to build up hierarchical representations for its input, and might provide an ideal starting point for the investigation into linguistic structure in language models. However, uncovering the exact role that these interactions play is also difficult, and a diverse range of interaction attribution methods has been proposed. In this paper, we focus on the question which of these methods most faithfully reflects the inner workings of the target models. We work out a grey box methodology, in which we train models to perfection on a formal language classification task, using PCFGs. We show that under specific configurations, some methods are indeed able to uncover the grammatical rules acquired by a model. Based on these findings we extend our evaluation to a case study on language models, providing novel insights into the linguistic structure that these models have acquired."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1804,
      "original_url": "https://aclanthology.org/2023.findings-acl.555/",
      "title": "Clustering-Aware Negative Sampling for Unsupervised Sentence Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.555.pdf",
      "authors": [
        "Jinghao Deng",
        "Fanqi Wan",
        "Tao Yang",
        "Xiaojun Quan",
        "Rui Wang"
      ],
      "abstract": "Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been made publicly available at github.com/djz233/ClusterNS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1805,
      "original_url": "https://aclanthology.org/2023.findings-acl.556/",
      "title": "An Effective Deployment of Contrastive Learning in Multi-label Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.556.pdf",
      "authors": [
        "Nankai Lin",
        "Guanqiu Qin",
        "Gang Wang",
        "Dong Zhou",
        "Aimin Yang"
      ],
      "abstract": "The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific tasks. We further perform an interpretable analysis of our approach to show how different components of contrastive learning losses play their roles. The experimental results show that our proposed contrastive losses can bring improvement to multi-label text classification tasks. Our work also explores how contrastive learning should be adapted for multi-label text classification tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1806,
      "original_url": "https://aclanthology.org/2023.findings-acl.557/",
      "title": "Segment-Level and Category-Oriented Network for Knowledge-Based Referring Expression Comprehension",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.557.pdf",
      "authors": [
        "Yuqi Bu",
        "Xin Wu",
        "Liuwu Li",
        "Yi Cai",
        "Qiong Liu",
        "Qingbao Huang"
      ],
      "abstract": "Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant information in unstructured knowledge sentences. To address these limitations, we propose a segment-level and category-oriented network (SLCO). Our approach includes a segment-level and prompt-based knowledge retrieval method to mitigate the similarity bias problem and a category-based grounding method to alleviate interference from irrelevant information in knowledge sentences. Experimental results show that our SLCO can eliminate interference and improve the overall performance of the KB-REC task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1807,
      "original_url": "https://aclanthology.org/2023.findings-acl.558/",
      "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.558.pdf",
      "authors": [
        "Tianyi Tang",
        "Junyi Li",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. “supervised pre-training”) showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model’s capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3% and Flan-T5 by 5.8%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1808,
      "original_url": "https://aclanthology.org/2023.findings-acl.559/",
      "title": "From Alignment to Entailment: A Unified Textual Entailment Framework for Entity Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.559.pdf",
      "authors": [
        "Yu Zhao",
        "Yike Wu",
        "Xiangrui Cai",
        "Ying Zhang",
        "Haiwei Zhang",
        "Xiaojie Yuan"
      ],
      "abstract": "Entity Alignment (EA) aims to find the equivalent entities between two Knowledge Graphs (KGs). Existing methods usually encode the triples of entities as embeddings and learn to align the embeddings, which prevents the direct interaction between the original information of the cross-KG entities. Moreover, they encode the relational triples and attribute triples of an entity in heterogeneous embedding spaces, which prevents them from helping each other. In this paper, we transform both triples into unified textual sequences, and model the EA task as a bi-directional textual entailment task between the sequences of cross-KG entities. Specifically, we feed the sequences of two entities simultaneously into a pre-trained language model (PLM) and propose two kinds of PLM-based entity aligners that model the entailment probability between sequences as the similarity between entities. Our approach captures the unified correlation pattern of two kinds of information between entities, and explicitly models the fine-grained interaction between original entity information. The experiments on five cross-lingual EA datasets show that our approach outperforms the state-of-the-art EA methods and enables the mutual enhancement of the heterogeneous information. Codes are available at https://github.com/OreOZhao/TEA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1809,
      "original_url": "https://aclanthology.org/2023.findings-acl.560/",
      "title": "It is a Bird Therefore it is a Robin: On BERT’s Internal Consistency Between Hypernym Knowledge and Logical Words",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.560.pdf",
      "authors": [
        "Nicolas Guerin",
        "Emmanuel Chemla"
      ],
      "abstract": "The lexical knowledge of NLP systems shouldbe tested (i) for their internal consistency(avoiding groundedness issues) and (ii) bothfor content words and logical words. In thispaper we propose a new method to test the understandingof the hypernymy relationship bymeasuring its antisymmetry according to themodels. Previous studies often rely only on thedirect question (e.g., A robin is a ...), where weargue a correct answer could only rely on collocationalcues, rather than hierarchical cues. We show how to control for this, and how it isimportant. We develop a method to ask similarquestions about logical words that encode anentailment-like relation (e.g., because or therefore).Our results show important weaknessesof BERT-like models on these semantic tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1810,
      "original_url": "https://aclanthology.org/2023.findings-acl.561/",
      "title": "Defending against Insertion-based Textual Backdoor Attacks via Attribution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.561.pdf",
      "authors": [
        "Jiazhao Li",
        "Zhuofeng Wu",
        "Wei Ping",
        "Chaowei Xiao",
        "V.G.Vinod Vydiswaran"
      ],
      "abstract": "Textual backdoor attack, as a novel attack model, has been shown to be effective in adding a backdoor to the model during training. Defending against such backdoor attacks has become urgent and important. In this paper, we propose AttDef, an efficient attribution-based pipeline to defend against two insertion-based poisoning attacks, BadNL and InSent. Specifically, we regard the tokens with larger attribution scores as potential triggers since larger attribution words contribute more to the false prediction results and therefore are more likely to be poison triggers. Additionally, we further utilize an external pre-trained language model to distinguish whether input is poisoned or not. We show that our proposed method can generalize sufficiently well in two common attack scenarios (poisoning training data and testing data), which consistently improves previous methods. For instance, AttDef can successfully mitigate both attacks with an average accuracy of 79.97% (56.59% up) and 48.34% (3.99% up) under pre-training and post-training attack defense respectively, achieving the new state-of-the-art performance on prediction recovery over four benchmark datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1811,
      "original_url": "https://aclanthology.org/2023.findings-acl.562/",
      "title": "ActiveAED: A Human in the Loop Improves Annotation Error Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.562.pdf",
      "authors": [
        "Leon Weber",
        "Barbara Plank"
      ],
      "abstract": "Manually annotated datasets are crucial for training and evaluating Natural Language Processing models. However, recent work has discovered that even widely-used benchmark datasets contain a substantial number of erroneous annotations. This problem has been addressed with Annotation Error Detection (AED) models, which can flag such errors for human re-annotation. However, even though many of these AED methods assume a final curation step in which a human annotator decides whether the annotation is erroneous, they have been developed as static models without any human-in-the-loop component. In this work, we propose ActiveAED, an AED method that can detect errors more accurately by repeatedly querying a human for error corrections in its prediction loop. We evaluate ActiveAED on eight datasets spanning five different tasks and find that it leads to improvements over the state of the art on seven of them, with gains of up to six percentage points in average precision."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1812,
      "original_url": "https://aclanthology.org/2023.findings-acl.563/",
      "title": "Assessing Word Importance Using Models Trained for Semantic Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.563.pdf",
      "authors": [
        "Dávid Javorský",
        "Ondřej Bojar",
        "François Yvon"
      ],
      "abstract": "Many NLP tasks require to automatically identify the most significant words in a text. In this work, we derive word significance from models trained to solve semantic task: Natural Language Inference and Paraphrase Identification. Using an attribution method aimed to explain the predictions of these models, we derive importance scores for each input token. We evaluate their relevance using a so-called cross-task evaluation: Analyzing the performance of one model on an input masked according to the other model’s weight, we show that our method is robust with respect to the choice of the initial task. Additionally, we investigate the scores from the syntax point of view and observe interesting patterns, e.g. words closer to the root of a syntactic tree receive higher importance scores. Altogether, these observations suggest that our method can be used to identify important words in sentences without any explicit word importance labeling in training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1813,
      "original_url": "https://aclanthology.org/2023.findings-acl.564/",
      "title": "In-context Examples Selection for Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.564.pdf",
      "authors": [
        "Sweta Agrawal",
        "Chunting Zhou",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Marjan Ghazvininejad"
      ],
      "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated examples can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1814,
      "original_url": "https://aclanthology.org/2023.findings-acl.565/",
      "title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.565.pdf",
      "authors": [
        "Sihao Chen",
        "Senaka Buthpitiya",
        "Alex Fabrikant",
        "Dan Roth",
        "Tal Schuster"
      ],
      "abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1815,
      "original_url": "https://aclanthology.org/2023.findings-acl.566/",
      "title": "CIF-PT: Bridging Speech and Text Representations for Spoken Language Understanding via Continuous Integrate-and-Fire Pre-Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.566.pdf",
      "authors": [
        "Linhao Dong",
        "Zhecheng An",
        "Peihao Wu",
        "Jun Zhang",
        "Lu Lu",
        "Ma Zejun"
      ],
      "abstract": "Speech or text representation generated by pre-trained models contains modal-specific information that could be combined for benefiting spoken language understanding (SLU) tasks. In this work, we propose a novel pre-training paradigm termed Continuous Integrate-and-Fire Pre-Training (CIF-PT). It relies on a simple but effective frame-to-token alignment: continuous integrate-and-fire (CIF) to bridge the representations between speech and text. It jointly performs speech-to-text training and language model distillation through CIF as the pre-training (PT). Evaluated on SLU benchmark SLURP dataset, CIF-PT outperforms the state-of-the-art model by 1.94% of accuracy and 2.71% of SLU-F1 on the tasks of intent classification and slot filling, respectively. We also observe the cross-modal representation extracted by CIF-PT obtains better performance than other neural interfaces for the tasks of SLU, including the dominant speech representation learned from self-supervised pre-training."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1816,
      "original_url": "https://aclanthology.org/2023.findings-acl.567/",
      "title": "Improving Diachronic Word Sense Induction with a Nonparametric Bayesian method",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.567.pdf",
      "authors": [
        "Ashjan Alsulaimani",
        "Erwan Moreau"
      ],
      "abstract": "Diachronic Word Sense Induction (DWSI) is the task of inducing the temporal representations of a word meaning from the context, as a set of senses and their prevalence over time. We introduce two new models for DWSI, based on topic modelling techniques: one is based on Hierarchical Dirichlet Processes (HDP), a nonparametric model; the other is based on the Dynamic Embedded Topic Model (DETM), a recent dynamic neural model. We evaluate these models against two state of the art DWSI models, using a time-stamped labelled dataset from the biomedical domain. We demonstrate that the two proposed models perform better than the state of the art. In particular, the HDP-based model drastically outperforms all the other models, including the dynamic neural model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1817,
      "original_url": "https://aclanthology.org/2023.findings-acl.568/",
      "title": "What to Fuse and How to Fuse: Exploring Emotion and Personality Fusion Strategies for Explainable Mental Disorder Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.568.pdf",
      "authors": [
        "Sourabh Zanwar",
        "Xiaofei Li",
        "Daniel Wiechmann",
        "Yu Qiao",
        "Elma Kerz"
      ],
      "abstract": "Mental health disorders (MHD) are increasingly prevalent worldwide and constitute one of the greatest challenges facing our healthcare systems and modern societies in general. In response to this societal challenge, there has been a surge in digital mental health research geared towards the development of new techniques for unobtrusive and efficient automatic detection of MHD. Within this area of research, natural language processing techniques are playing an increasingly important role, showing promising detection results from a variety of textual data. Recently, there has been a growing interest in improving mental illness detection from textual data by way of leveraging emotions: ‘Emotion fusion’ refers to the process of integrating emotion information with general textual information to obtain enhanced information for decision-making. However, while the available research has shown that MHD prediction can be improved through a variety of different fusion strategies, previous works have been confined to a particular fusion strategy applied to a specific dataset, and so is limited by the lack of meaningful comparability. In this work, we integrate and extend this research by conducting extensive experiments with three types of deep learning-based fusion strategies: (i) feature-level fusion, where a pre-trained masked language model for mental health detection (MentalRoBERTa) was infused with a comprehensive set of engineered features, (ii) model fusion, where the MentalRoBERTa model was infused with hidden representations of other language models and (iii) task fusion, where a multi-task framework was leveraged to learn the features for auxiliary tasks. In addition to exploring the role of different fusion strategies, we expand on previous work by broadening the information infusion to include a second domain related to mental health, namely personality. We evaluate algorithm performance on data from two benchmark datasets, encompassing five mental health conditions: attention deficit hyperactivity disorder, anxiety, bipolar disorder, depression and psychological stress."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1818,
      "original_url": "https://aclanthology.org/2023.findings-acl.569/",
      "title": "Adaptive Contrastive Knowledge Distillation for BERT Compression",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.569.pdf",
      "authors": [
        "Jinyang Guo",
        "Jiaheng Liu",
        "Zining Wang",
        "Yuqing Ma",
        "Ruihao Gong",
        "Ke Xu",
        "Xianglong Liu"
      ],
      "abstract": "In this paper, we propose a new knowledge distillation approach called adaptive contrastive knowledge distillation (ACKD) for BERT compression. Different from existing knowledge distillation methods for BERT that implicitly learn discriminative student features by mimicking the teacher features, we first introduce a novel contrastive distillation loss (CDL) based on hidden state features in BERT as the explicit supervision to learn discriminative student features. We further observe sentences with similar features may have completely different meanings, which makes them hard to distinguish. Existing methods do not pay sufficient attention to these hard samples with less discriminative features. Therefore, we propose a new strategy called sample adaptive reweighting (SAR) to adaptively pay more attention to these hard samples and strengthen their discrimination abilities. We incorporate our SAR strategy into our CDL and form the adaptive contrastive distillation loss, based on which we construct our ACKD framework. Comprehensive experiments on multiple natural language processing tasks demonstrate the effectiveness of our ACKD framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1819,
      "original_url": "https://aclanthology.org/2023.findings-acl.570/",
      "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.570.pdf",
      "authors": [
        "Ziwei He",
        "Meng Yang",
        "Minwei Feng",
        "Jingcheng Yin",
        "Xinbing Wang",
        "Jingwen Leng",
        "Zhouhan Lin"
      ],
      "abstract": "The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer’s inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code will be publicly available at https://github.com/LUMIA-Group/FourierTransformer"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1820,
      "original_url": "https://aclanthology.org/2023.findings-acl.571/",
      "title": "Zero-Shot Classification by Logical Reasoning on Natural Language Explanations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.571.pdf",
      "authors": [
        "Chi Han",
        "Hengzhi Pei",
        "Xinya Du",
        "Heng Ji"
      ],
      "abstract": "Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as “it has a slim straight relatively short bill, yellow eyes and a long tail”, so that others can use their knowledge of attributes “slim straight relatively short bill”, “yellow eyes” and “long tail” to recognize a sage thrasher. Inspired by this observation, in this work we tackle zero-shot classification task by logically parsing and reasoning on natural language explanations. To this end, we propose the framework CLORE (Classification by LOgical Reasoning on Explanations). While previous methods usually regard textual information as implicit features, CLORE parses explanations into logical structures and then explicitly reasons along this structure on the input to produce a classification score. Experimental results on explanation-based zero-shot classification benchmarks demonstrate that CLORE is superior to baselines, which we show is mainly due to higher scores on tasks requiring more logical reasoning. We also demonstrate that our framework can be extended to zero-shot classification on visual modality. Alongside classification decisions, CLORE can provide the logical parsing and reasoning process as a clear form of rationale. Through empirical analysis we demonstrate that CLORE is also less affected by linguistic biases than baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1821,
      "original_url": "https://aclanthology.org/2023.findings-acl.572/",
      "title": "Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.572.pdf",
      "authors": [
        "Qian Li",
        "Shu Guo",
        "Cheng Ji",
        "Xutan Peng",
        "Shiyao Cui",
        "Jianxin Li",
        "Lihong Wang"
      ],
      "abstract": "Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful visual information and thus limiting relation extraction performance. In this paper, we propose a novel MMRE framework to better capture the deeper correlations of text, entity pair, and image/objects, so as to mine more helpful information for the task, termed as DGF-PT. We first propose a prompt-based autoregressive encoder, which builds the associations of intra-modal and inter-modal features related to the task, respectively by entity-oriented and object-oriented prefixes. To better integrate helpful visual information, we design a dual-gated fusion module to distinguish the importance of image/objects and further enrich text representations. In addition, a generative decoder is introduced with entity type restriction on relations, better filtering out candidates. Extensive experiments conducted on the benchmark dataset show that our approach achieves excellent performance compared to strong competitors, even in the few-shot situation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1822,
      "original_url": "https://aclanthology.org/2023.findings-acl.573/",
      "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.573.pdf",
      "authors": [
        "Siyu Ren",
        "Kenny Zhu"
      ],
      "abstract": "Iterative pruning is one of the most effective compression methods for pre-trained language models. We discovered that finding the optimal pruning decision is an equality-constrained 0-1 Integer Linear Programming problem. The solution to this optimization problem leads to a principled importance criterion which we use to rank parameters during iterative model pruning. To mitigate the poor generalization at high sparsity levels, we propose a self-regularization scheme where model prediction is regularized by the latest checkpoint with increasing sparsity throughout pruning. Our experiments on natural language understanding, question answering, named entity recognition, and data-to-text generation with various Transformer-based PLMs show the effectiveness of the approach at various sparsity levels."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1823,
      "original_url": "https://aclanthology.org/2023.findings-acl.574/",
      "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.574.pdf",
      "authors": [
        "Xiao Liu",
        "Da Yin",
        "Chen Zhang",
        "Yansong Feng",
        "Dongyan Zhao"
      ],
      "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like “if“, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure. Code and data are available at https://github.com/xxxiaol/magic-if."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1824,
      "original_url": "https://aclanthology.org/2023.findings-acl.575/",
      "title": "Learning to Leverage High-Order Medical Knowledge Graph for Joint Entity and Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.575.pdf",
      "authors": [
        "Zhe Yang",
        "Yi Huang",
        "Junlan Feng"
      ],
      "abstract": "Automatic medical entity and relation extraction is essential for daily electronic medical record (EMR) analysis, and has attracted a lot of academic attention. Tremendous progress has been made in recent years. However, medical terms are difficult to understand, and their relations are more complicated than general ones. Based on this situation, domain knowledge gives better background and contexts for medical terms. Despite the benefits of medical domain knowledge, the utilization way of it for joint entity and relation extraction is inadequate. To foster this line of research, in this work, we propose to leverage the medical knowledge graph for extracting entities and relations for Chinese Medical Texts in a collective way. Specifically, we propose to construct a high-order heterogeneous graph based on medical knowledge graph, which is linked to the entity mentions in the text. In this way, neighbors from the high-order heterogeneous graph can pass the message to each other for better global context representations. Our experiments on real Chinese Medical Texts show that our method is more effective than state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1825,
      "original_url": "https://aclanthology.org/2023.findings-acl.576/",
      "title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.576.pdf",
      "authors": [
        "Hamish Ivison",
        "Noah A. Smith",
        "Hannaneh Hajishirzi",
        "Pradeep Dasigi"
      ],
      "abstract": "Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors from SuperNaturalInstructions, representing about 5% of the pool, obtain comparable performance to state-of-the-art models on 12 held-out tasks from that pool. Moreover, the models produced by our approach also provide a better initialization than single multitask finetuned models for few-shot finetuning on target-task data, as shown by a 2-23% relative improvement over few-shot finetuned T0-3B models on 8 datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1826,
      "original_url": "https://aclanthology.org/2023.findings-acl.577/",
      "title": "CoAug: Combining Augmentation of Labels and Labelling Rules",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.577.pdf",
      "authors": [
        "Rakesh R. Menon",
        "Bingqing Wang",
        "Jun Araki",
        "Zhengyu Zhou",
        "Zhe Feng",
        "Liu Ren"
      ],
      "abstract": "Collecting labeled data for Named Entity Recognition (NER) tasks is challenging due to the high cost of manual annotations. Instead, researchers have proposed few-shot self-training and rule-augmentation techniques to minimize the reliance on large datasets. However, inductive biases and restricted logical language lexicon, respectively, can limit the ability of these models to perform well. In this work, we propose CoAug, a co-augmentation framework that allows us to improve few-shot models and rule-augmentation models by bootstrapping predictions from each model. By leveraging rules and neural model predictions to train our models, we complement the benefits of each and achieve the best of both worlds. In our experiments, we show that our best CoAug model can outperform strong weak-supervision-based NER models at least by 6.5 F1 points."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1827,
      "original_url": "https://aclanthology.org/2023.findings-acl.578/",
      "title": "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.578.pdf",
      "authors": [
        "Xuming Hu",
        "Yong Jiang",
        "Aiwei Liu",
        "Zhongqiang Huang",
        "Pengjun Xie",
        "Fei Huang",
        "Lijie Wen",
        "Philip S. Yu"
      ],
      "abstract": "Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data and low resource settings) show that EnTDA could bring more performance improvements compared to the baseline augmentation techniques."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1828,
      "original_url": "https://aclanthology.org/2023.findings-acl.579/",
      "title": "World Models for Math Story Problems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.579.pdf",
      "authors": [
        "Andreas Opedal",
        "Niklas Stoehr",
        "Abulhair Saparov",
        "Mrinmaya Sachan"
      ],
      "abstract": "Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1829,
      "original_url": "https://aclanthology.org/2023.findings-acl.580/",
      "title": "AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.580.pdf",
      "authors": [
        "Ganesh Jawahar",
        "Subhabrata Mukherjee",
        "Xiaodong Liu",
        "Young Jin Kim",
        "Muhammad Abdul-Mageed",
        "Laks Lakshmanan, V.S.",
        "Ahmed Hassan Awadallah",
        "Sebastien Bubeck",
        "Jianfeng Gao"
      ],
      "abstract": "Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE – a framework for designing heterogeneous MoE’s under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT.Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute – where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1830,
      "original_url": "https://aclanthology.org/2023.findings-acl.581/",
      "title": "Language Agnostic Multilingual Information Retrieval with Contrastive Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.581.pdf",
      "authors": [
        "Xiyang Hu",
        "Xinchi Chen",
        "Peng Qi",
        "Deguang Kong",
        "Kunlun Liu",
        "William Yang Wang",
        "Zhiheng Huang"
      ],
      "abstract": "Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the value of our model for a practical setting when a parallel corpus is only available for a few languages, but a lack of parallel corpora resources persists for many other low-resource languages. Our model can work well even with a small number of parallel sentences, and be used as an add-on module to any backbones and other tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1831,
      "original_url": "https://aclanthology.org/2023.findings-acl.582/",
      "title": "Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.582.pdf",
      "authors": [
        "Josip Jukić",
        "Martin Tutek",
        "Jan Snajder"
      ],
      "abstract": "A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement – if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-r is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low for easy-to-learn instances. Finally, we connect the improvement in agreement across instance categories to local representation space statistics of instances, paving the way for work on analyzing which intrinsic model properties improve their predisposition to interpretability methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1832,
      "original_url": "https://aclanthology.org/2023.findings-acl.583/",
      "title": "Enhancing Cross-lingual Transfer via Phonemic Transcription Integration",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.583.pdf",
      "authors": [
        "Hoang Nguyen",
        "Chenwei Zhang",
        "Tao Zhang",
        "Eugene Rohrbaugh",
        "Philip Yu"
      ],
      "abstract": "Previous cross-lingual transfer methods are restricted to orthographic representation learning via textual scripts. This limitation hampers cross-lingual transfer and is biased towards languages sharing similar well-known scripts. To alleviate the gap between languages from different writing scripts, we propose PhoneXL, a framework incorporating phonemic transcriptions as an additional linguistic modality beyond the traditional orthographic transcriptions for cross-lingual transfer. Particularly, we propose unsupervised alignment objectives to capture (1) local one-to-one alignment between the two different modalities, (2) alignment via multi-modality contexts to leverage information from additional modalities, and (3) alignment via multilingual contexts where additional bilingual dictionaries are incorporated. We also release the first phonemic-orthographic alignment dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech Tagging) among the understudied but interconnected Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals phonemic transcription provides essential information beyond the orthography to enhance cross-lingual transfer and bridge the gap among CJKV languages, leading to consistent improvements on cross-lingual token-level tasks over orthographic-based multilingual PLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1833,
      "original_url": "https://aclanthology.org/2023.findings-acl.584/",
      "title": "Human-in-the-loop Abstractive Dialogue Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.584.pdf",
      "authors": [
        "Jiaao Chen",
        "Mohan Dodda",
        "Diyi Yang"
      ],
      "abstract": "Abstractive dialogue summarization has received increasing attention recently. Despite the fact that most of the current dialogue summarization systems are trained to maximize the likelihood of human-written summaries and have achieved significant results, there is still a huge gap in generating high-quality summaries as determined by humans, such as coherence and faithfulness, partly due to the misalignment in maximizing a single human-written summary. To this end, we propose to incorporate different levels of human feedback into the training process. This will enable us to guide the models to capture the behaviors humans care about for summaries. Specifically, we ask humans to highlight the salient information to be included in summaries to provide the local feedback, and to make overall comparisons among summaries in terms of coherence, accuracy, coverage, concise and overall quality, as the global feedback. We then combine both local and global feedback to fine-tune the dialog summarization policy with Reinforcement Learning. Experiments conducted on multiple datasets demonstrate the effectiveness and generalization of our methods over the state-of-the-art supervised baselines, especially in terms of human judgments."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1834,
      "original_url": "https://aclanthology.org/2023.findings-acl.585/",
      "title": "A Multi-task Learning Framework for Quality Estimation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.585.pdf",
      "authors": [
        "Sourabh Deoghare",
        "Paramveer Choudhary",
        "Diptesh Kanojia",
        "Tharindu Ranasinghe",
        "Pushpak Bhattacharyya",
        "Constantin Orăsan"
      ],
      "abstract": "Quality Estimation (QE) is the task of evaluating machine translation output in the absence of reference translation. Conventional approaches to QE involve training separate models at different levels of granularity viz., word-level, sentence-level, and document-level, which sometimes lead to inconsistent predictions for the same input. To overcome this limitation, we focus on jointly training a single model for sentence-level and word-level QE tasks in a multi-task learning framework. Using two multi-task learning-based QE approaches, we show that multi-task learning improves the performance of both tasks. We evaluate these approaches by performing experiments in different settings, viz., single-pair, multi-pair, and zero-shot. We compare the multi-task learning-based approach with baseline QE models trained on single tasks and observe an improvement of up to 4.28% in Pearson’s correlation (r) at sentence-level and 8.46% in F1-score at word-level, in the single-pair setting. In the multi-pair setting, we observe improvements of up to 3.04% at sentence-level and 13.74% at word-level; while in the zero-shot setting, we also observe improvements of up to 5.26% and 3.05%, respectively. We make the models proposed in this paper publically available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1835,
      "original_url": "https://aclanthology.org/2023.findings-acl.586/",
      "title": "The Devil is in the Details: On the Pitfalls of Event Extraction Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.586.pdf",
      "authors": [
        "Hao Peng",
        "Xiaozhi Wang",
        "Feng Yao",
        "Kaisheng Zeng",
        "Lei Hou",
        "Juanzi Li",
        "Zhiyuan Liu",
        "Weixing Shen"
      ],
      "abstract": "Event extraction (EE) is a crucial task aiming at extracting events from texts, which includes two subtasks: event detection (ED) and event argument extraction (EAE). In this paper, we check the reliability of EE evaluations and identify three major pitfalls: (1) The data preprocessing discrepancy makes the evaluation results on the same dataset not directly comparable, but the data preprocessing details are not widely noted and specified in papers. (2) The output space discrepancy of different model paradigms makes different-paradigm EE models lack grounds for comparison and also leads to unclear mapping issues between predictions and annotations. (3) The absence of pipeline evaluation of many EAE-only works makes them hard to be directly compared with EE works and may not well reflect the model performance in real-world pipeline scenarios. We demonstrate the significant influence of these pitfalls through comprehensive meta-analyses of recent papers and empirical experiments. To avoid these pitfalls, we suggest a series of remedies, including specifying data preprocessing, standardizing outputs, and providing pipeline evaluation results. To help implement these remedies, we develop a consistent evaluation framework OmniEvent, which can be obtained from https://github.com/THU-KEG/OmniEvent."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1836,
      "original_url": "https://aclanthology.org/2023.findings-acl.587/",
      "title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.587.pdf",
      "authors": [
        "Philipp Sadler",
        "Sherzod Hakimov",
        "David Schlangen"
      ],
      "abstract": "The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a referential language game as a controllable example of a task-oriented collaborative joint activity. A teacher utters a referring expression generated by a well-known symbolic algorithm (the “Incremental Algorithm”) as an initial instruction and then monitors the follower’s actions to possibly intervene with intra-episodic feedback (which does not explicitly have to be requested). We frame this task as a reinforcement learning problem with sparse rewards and learn a follower policy for a heuristic teacher. Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs better than providing only the initial statement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1837,
      "original_url": "https://aclanthology.org/2023.findings-acl.588/",
      "title": "Investigating Transformer-Guided Chaining for Interpretable Natural Logic Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.588.pdf",
      "authors": [
        "Kanagasabai Rajaraman",
        "Saravanan Rajamanickam",
        "Wei Shi"
      ],
      "abstract": "Natural logic reasoning has received increasing attention lately, with several datasets and neural models proposed, though with limited success. More recently, a new class of works have emerged adopting a Neuro-Symbolic approach, called transformer guided chaining, whereby the idea is to iteratively perform 1-step neural inferences and chain together the results to generate a multi-step reasoning trace. Several works have adapted variants of this central idea and reported significantly high accuracies compared to vanilla LLM’s. In this paper, we perform a critical empirical investigation of the chaining approach on a multi-hop First-Order Logic (FOL) reasoning benchmark. In particular, we develop a reference implementation, called Chainformer, and conduct several experiments to analyze the accuracy, generalization, interpretability, and performance over FOLs. Our findings highlight key strengths and possible current limitations and suggest potential areas for future research in logic reasoning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1838,
      "original_url": "https://aclanthology.org/2023.findings-acl.589/",
      "title": "Multilingual Multi-Figurative Language Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.589.pdf",
      "authors": [
        "Huiyuan Lai",
        "Antonio Toral",
        "Malvina Nissim"
      ],
      "abstract": "Figures of speech help people express abstract concepts and evoke stronger emotions than literal expressions, thereby making texts more creative and engaging. Due to its pervasive and fundamental character, figurative language understanding has been addressed in Natural Language Processing, but it’s highly understudied in a multilingual setting and when considering more than one figure of speech at the same time. To bridge this gap, we introduce multilingual multi-figurative language modelling, and provide a benchmark for sentence-level figurative language detection, covering three common figures of speech and seven languages. Specifically, we develop a framework for figurative language detection based on template-based prompt learning. In so doing, we unify multiple detection tasks that are interrelated across multiple figures of speech and languages, without requiring task- or language-specific modules. Experimental results show that our framework outperforms several strong baselines and may serve as a blueprint for the joint modelling of other interrelated tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1839,
      "original_url": "https://aclanthology.org/2023.findings-acl.590/",
      "title": "Zero-shot Visual Question Answering with Language Model Feedback",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.590.pdf",
      "authors": [
        "Yifan Du",
        "Junyi Li",
        "Tianyi Tang",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "abstract": "In this paper, we propose a novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA). Our approach employs the generated captions by a captioning model as the context of an answer prediction model, which is a Pre-Trained Language model (PLM). As the major contribution, we leverage the guidance and feedback of the prediction model to improve the capability of the captioning model. In this way, the captioning model can become aware of the task goal and information need from the PLM. To develop our approach, we design two specific training stages, where the first stage adapts the captioning model to the prediction model (selecting more suitable caption propositions for training) and the second stage tunes the captioning model according to the task goal (learning from feedback of the PLM). Extensive experiments demonstrate the effectiveness of the proposed approach on the knowledge-based VQA task. Specifically, on the challenging A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model. Our code is publicly available at https://github.com/RUCAIBox/LAMOC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1840,
      "original_url": "https://aclanthology.org/2023.findings-acl.591/",
      "title": "Prompted Opinion Summarization with GPT-3.5",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.591.pdf",
      "authors": [
        "Adithya Bhaskar",
        "Alex Fabbri",
        "Greg Durrett"
      ],
      "abstract": "Large language models have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3.5 to summarize a large collection of user reviews in aprompted fashion. To handle arbitrarily large numbers of user reviews, we explore recursive summarization as well as methods for selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews (SPACE) and a generic summarization dataset of Amazon and Yelp reviews (FewSum), we show that GPT-3.5 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reflect this, and introduce three new metrics targeting faithfulness, factuality, and genericity to contrast these different methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1841,
      "original_url": "https://aclanthology.org/2023.findings-acl.592/",
      "title": "Sentence Ordering with a Coherence Verifier",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.592.pdf",
      "authors": [
        "Sainan Jia",
        "Wei Song",
        "Jiefu Gong",
        "Shijin Wang",
        "Ting Liu"
      ],
      "abstract": "This paper presents a novel sentence ordering method by plugging a coherence verifier (CoVer) into pair-wise ranking-based and sequence generation-based methods. It does not change the model parameters of the baseline, and only verifies the coherence of candidate (partial) orders produced by the baseline and reranks them in beam search. We also propose a coherence model as CoVer with a novel graph formulation and a novel data construction strategy for contrastive pre-training independently of the sentence ordering task. Experimental results on four benchmarks demonstrate the effectiveness of our method with topological sorting-based and pointer network-based methods as the baselines. Detailed analyses illustrate how CoVer improves the baselines and confirm the importance of its graph formulation and training strategy. Our code is available at https://github.com/SN-Jia/SO_with_CoVer."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1842,
      "original_url": "https://aclanthology.org/2023.findings-acl.593/",
      "title": "GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.593.pdf",
      "authors": [
        "Yang Janet Liu",
        "Amir Zeldes"
      ],
      "abstract": "Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to ‘hallucinations’, low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023’s ‘Reality Check’ theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1843,
      "original_url": "https://aclanthology.org/2023.findings-acl.594/",
      "title": "Improving Grammatical Error Correction with Multimodal Feature Integration",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.594.pdf",
      "authors": [
        "Tao Fang",
        "Jinpeng Hu",
        "Derek F. Wong",
        "Xiang Wan",
        "Lidia S. Chao",
        "Tsung-Hui Chang"
      ],
      "abstract": "Grammatical error correction (GEC) is a promising task aimed at correcting errors in a text. Many methods have been proposed to facilitate this task with remarkable results. However, most of them only focus on enhancing textual feature extraction without exploring the usage of other modalities’ information (e.g., speech), which can also provide valuable knowledge to help the model detect grammatical errors. To shore up this deficiency, we propose a novel framework that integrates both speech and text features to enhance GEC. In detail, we create new multimodal GEC datasets for English and German by generating audio from text using the advanced text-to-speech models. Subsequently, we extract acoustic and textual representations by a multimodal encoder that consists of a speech and a text encoder. A mixture-of-experts (MoE) layer is employed to selectively align representations from the two modalities, and then a dot attention mechanism is used to fuse them as final multimodal representations. Experimental results on CoNLL14, BEA19 English, and Falko-MERLIN German show that our multimodal GEC models achieve significant improvements over strong baselines and achieve a new state-of-the-art result on the Falko-MERLIN test set."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1844,
      "original_url": "https://aclanthology.org/2023.findings-acl.595/",
      "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.595.pdf",
      "authors": [
        "Renliang Sun",
        "Wei Xu",
        "Xiaojun Wan"
      ],
      "abstract": "Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1845,
      "original_url": "https://aclanthology.org/2023.findings-acl.596/",
      "title": "Acquiring Frame Element Knowledge with Deep Metric Learning for Semantic Frame Induction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.596.pdf",
      "authors": [
        "Kosuke Yamada",
        "Ryohei Sasano",
        "Koichi Takeda"
      ],
      "abstract": "The semantic frame induction tasks are defined as a clustering of words into the frames that they evoke, and a clustering of their arguments according to the frame element roles that they should fill. In this paper, we address the latter task of argument clustering, which aims to acquire frame element knowledge, and propose a method that applies deep metric learning. In this method, a pre-trained language model is fine-tuned to be suitable for distinguishing frame element roles through the use of frame-annotated data, and argument clustering is performed with embeddings obtained from the fine-tuned model. Experimental results on FrameNet demonstrate that our method achieves substantially better performance than existing methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1846,
      "original_url": "https://aclanthology.org/2023.findings-acl.597/",
      "title": "Leveraging Synthetic Targets for Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.597.pdf",
      "authors": [
        "Sarthak Mittal",
        "Oleksii Hrinchuk",
        "Oleksii Kuchaiev"
      ],
      "abstract": "In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1847,
      "original_url": "https://aclanthology.org/2023.findings-acl.598/",
      "title": "Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.598.pdf",
      "authors": [
        "Saleh Soltan",
        "Andy Rosenbaum",
        "Tobias Falke",
        "Qin Lu",
        "Anna Rumshisky",
        "Wael Hamza"
      ],
      "abstract": "Pre-trained encoder-only and sequence-to-sequence (seq2seq) models each have advantages, however training both model types from scratch is computationally expensive. We explore recipes to improve pre-training efficiency by initializing one model from the other. (1) Extracting the encoder from a seq2seq model, we show it under-performs a Masked Language Modeling (MLM) encoder, particularly on sequence labeling tasks. Variations of masking during seq2seq training, reducing the decoder size, and continuing with a small amount of MLM training do not close the gap. (2) Conversely, using an encoder to warm-start seq2seq training, we show that by unfreezing the encoder partway through training, we can match task performance of a from-scratch seq2seq model. Overall, this two-stage approach is an efficient recipe to obtain both a multilingual encoder and a seq2seq model, matching the performance of training each model from scratch while reducing the total compute cost by 27%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1848,
      "original_url": "https://aclanthology.org/2023.findings-acl.599/",
      "title": "Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.599.pdf",
      "authors": [
        "Hao Fei",
        "Meishan Zhang",
        "Min Zhang",
        "Tat-Seng Chua"
      ],
      "abstract": "Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD- based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to the parallel target-side language, for both of which we parse the UD tree respectively. Then, we merge the source-/target-side UD structures as a unified code-mixed UD forest. With such forest features, the gaps of UD-based XRE between the training and predicting phases can be effectively closed. We conduct experiments on the ACE XRE benchmark datasets, where the results demonstrate that the proposed code-mixed UD forests help unbiased UD-based XRE transfer, with which we achieve significant XRE performance gains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1849,
      "original_url": "https://aclanthology.org/2023.findings-acl.600/",
      "title": "Spontaneous gestures encoded by hand positions improve language models: An Information-Theoretic motivated study",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.600.pdf",
      "authors": [
        "Yang Xu",
        "Yang Cheng"
      ],
      "abstract": "The multi-modality nature of human communication has been utilized to enhance the performance of language modeling-related tasks. Driven by the development of large-scale end-to-end learning techniques and the availability of multi-modal data, it becomes possible to represent non-verbal communication behaviors through joint-learning, and directly study their interaction with verbal communication. However, there is still gaps in existing studies to better address the underlying mechanism of how non-verbal expression contributes to the overall communication purpose. Therefore, we explore two questions using mixed-modal language models trained against monologue video data: first, whether incorporating gesture representations can improve the language model’s performance (perplexity); second, whether spontaneous gestures demonstrate entropy rate constancy (ERC), which is an empirical pattern found in most verbal language data that supports the rational communication assumption from Information Theory. We have positive and interesting findings for both questions: speakers indeed use spontaneous gestures to convey “meaningful” information that enhances verbal communication, which can be captured with a simple spatial encoding scheme. More importantly, gestures are produced and organized rationally in a similar way as words, which optimizes the communication efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1850,
      "original_url": "https://aclanthology.org/2023.findings-acl.601/",
      "title": "Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.601.pdf",
      "authors": [
        "Chaojun Wang",
        "Yang Liu",
        "Wai Lam"
      ],
      "abstract": "Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the “source-like” structure to the “target-like” structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signals can effectively improve the domain robustness of NMT and reduces the amount of hallucinations on out-of-domain translation. Further analysis shows that our methods are especially promising in low-resource scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1851,
      "original_url": "https://aclanthology.org/2023.findings-acl.602/",
      "title": "Controlled Text Generation with Hidden Representation Transformations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.602.pdf",
      "authors": [
        "Vaibhav Kumar",
        "Hana Koorehdavoudi",
        "Masud Moshtaghi",
        "Amita Misra",
        "Ankit Chadha",
        "Emilio Ferrara"
      ],
      "abstract": "We propose CHRT (Control HiddenRepresentation Transformation) – a con-trolled language generation framework thatsteers large language models to generatetext pertaining to certain attributes (such astoxicity). CHRT gains attribute control bymodifying the hidden representation of thebase model through learned transformations. We employ a contrastive-learning frameworkto learn these transformations that can becombined to gain multi-attribute control. Theeffectiveness of CHRT is experimentallyshown by comparing it with seven baselinesover three attributes. CHRT outperforms all thebaselines in the task of detoxification, positivesentiment steering, and text simplificationwhile minimizing the loss in linguistic qualities. Further, our approach has the lowest inferencelatency of only 0.01 seconds more than thebase model, making it the most suitable forhigh-performance production environments. We open-source our code and release two noveldatasets to further propel controlled languagegeneration research"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1852,
      "original_url": "https://aclanthology.org/2023.findings-acl.603/",
      "title": "Visual Coherence Loss for Coherent and Visually Grounded Story Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.603.pdf",
      "authors": [
        "Xudong Hong",
        "Vera Demberg",
        "Asad Sayeed",
        "Qiankun Zheng",
        "Bernt Schiele"
      ],
      "abstract": "Local coherence is essential for long-form text generation models. We identify two important aspects of local coherence within the visual storytelling task: (1) the model needs to represent re-occurrences of characters within the image sequence in order to mention them correctly in the story; (2) character representations should enable us to find instances of the same characters and distinguish different characters. In this paper, we propose a loss function inspired by a linguistic theory of coherence for self-supervised learning for image sequence representations. We further propose combining features from an object and a face detector to construct stronger character features. To evaluate input-output relevance that current reference-based metrics don’t measure, we propose a character matching metric to check whether the models generate referring expressions correctly for characters in input image sequences. Experiments on a visual story generation dataset show that our proposed features and loss function are effective for generating more coherent and visually grounded stories."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1853,
      "original_url": "https://aclanthology.org/2023.findings-acl.604/",
      "title": "AnaMeta: A Table Understanding Dataset of Field Metadata Knowledge Shared by Multi-dimensional Data Analysis Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.604.pdf",
      "authors": [
        "Xinyi He",
        "Mengyu Zhou",
        "Mingjie Zhou",
        "Jialiang Xu",
        "Xiao Lv",
        "Tianle Li",
        "Yijia Shao",
        "Shi Han",
        "Zejian Yuan",
        "Dongmei Zhang"
      ],
      "abstract": "Tabular data analysis is performed everyday across various domains. It requires an accurate understanding of field semantics to correctly operate on table fields and find common patterns in daily analysis. In this paper, we introduce the AnaMeta dataset, a collection of 467k tables with derived supervision labels for four types of commonly used field metadata: measure/dimension dichotomy, common field roles, semantic field type, and default aggregation function. We evaluate a wide range of models for inferring metadata as the benchmark. We also propose a multi-encoder framework, called KDF, which improves the metadata understanding capability of tabular models by incorporating distribution and knowledge information. Furthermore, we propose four interfaces for incorporating field metadata into downstream analysis tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1854,
      "original_url": "https://aclanthology.org/2023.findings-acl.605/",
      "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.605.pdf",
      "authors": [
        "Suet-Ying Lam",
        "Qingcheng Zeng",
        "Kexun Zhang",
        "Chenyu You",
        "Rob Voigt"
      ],
      "abstract": "While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by asking whether LLMs display human-like referential biases using stimuli and procedures from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with recent exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson & Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. By contrast, FLAN-UL2 fails to generate meaningful patterns. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns. Our data and code are available at https://github.com/zkx06111/llm_priming."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1855,
      "original_url": "https://aclanthology.org/2023.findings-acl.606/",
      "title": "Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.606.pdf",
      "authors": [
        "George Filandrianos",
        "Edmund Dervakos",
        "Orfeas Menis Mastromichalakis",
        "Chrysoula Zerva",
        "Giorgos Stamou"
      ],
      "abstract": "In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary significantly and the lack of a universal ground truth for the counterfactual edits imposes an insuperable barrier on their evaluation. We propose a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers. We show that by iteratively feeding the counterfactual to the explainer we can obtain valuable insights into the behaviour of both the predictor and the explainer models, and infer patterns that would be otherwise obscured. Using this methodology, we conduct a thorough analysis and propose a novel metric to evaluate the consistency of counterfactual generation approaches with different characteristics across available performance indicators."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1856,
      "original_url": "https://aclanthology.org/2023.findings-acl.607/",
      "title": "A Pilot Study on Dialogue-Level Dependency Parsing for Chinese",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.607.pdf",
      "authors": [
        "Gongyao Jiang",
        "Shuang Liu",
        "Meishan Zhang",
        "Min Zhang"
      ],
      "abstract": "Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen syntactic dependencies into unseen ones between elementary discourse units (EDUs), where the signals are detected by masked language modeling. Besides, we apply single-view and multi-view data selection to access reliable pseudo-labeled instances. Experimental results show the effectiveness of these baselines. Moreover, we discuss several crucial points about our dataset and approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1857,
      "original_url": "https://aclanthology.org/2023.findings-acl.608/",
      "title": "On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.608.pdf",
      "authors": [
        "Liang Chen",
        "Shuming Ma",
        "Dongdong Zhang",
        "Furu Wei",
        "Baobao Chang"
      ],
      "abstract": "While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages’ vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation tasks is reduced from 29% to 8%, while the overall BLEU score is improved by an average of 1.9 points without extra training cost or sacrificing the supervised directions’ performance. We release the code at https://github.com/PKUnlp-icler/Off-Target-MNMT for reproduction."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1858,
      "original_url": "https://aclanthology.org/2023.findings-acl.609/",
      "title": "ORCA: A Challenging Benchmark for Arabic Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.609.pdf",
      "authors": [
        "AbdelRahim Elmadany",
        "ElMoatez Billah Nagoudi",
        "Muhammad Abdul-Mageed"
      ],
      "abstract": "Due to the crucial role pretrained language models play in modern NLP, several benchmarks have been proposed to evaluate their performance. In spite of these efforts, no public benchmark of diverse nature currently exists for evaluating Arabic NLU. This makes it challenging to measure progress for both Arabic and multilingual language models. This challenge is compounded by the fact that any benchmark targeting Arabic needs to take into account the fact that Arabic is not a single language but rather a collection of languages and language varieties. In this work, we introduce a publicly available benchmark for Arabic language understanding evaluation dubbed ORCA. It is carefully constructed to cover diverse Arabic varieties and a wide range of challenging Arabic understanding tasks exploiting 60 different datasets (across seven NLU task clusters). To measure current progress in Arabic NLU, we use ORCA to offer a comprehensive comparison between 18 multilingual and Arabic language models. We also provide a public leaderboard with a unified single-number evaluation metric (ORCA score) to facilitate future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1859,
      "original_url": "https://aclanthology.org/2023.findings-acl.610/",
      "title": "Delving into the Openness of CLIP",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.610.pdf",
      "authors": [
        "Shuhuai Ren",
        "Lei Li",
        "Xuancheng Ren",
        "Guangxiang Zhao",
        "Xu Sun"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP) formulates image classification as an image-to-text matching task, i.e., matching images to the corresponding natural language descriptions instead of discrete category IDs. This allows for open-vocabulary visual recognition, where the model can recognize images from an open class set (also known as an open vocabulary) in a zero-shot manner. However, evaluating the openness of CLIP-like models is challenging, as the models are open to arbitrary vocabulary in theory, but their accuracy varies in practice. To address this, we resort to an incremental perspective to assess the openness through vocabulary expansions, and define extensibility to measure a model’s ability to handle novel classes. Our evaluation shows that CLIP-like models are not truly open, and their performance deteriorates as the vocabulary expands. We further dissect the feature space of CLIP from the perspectives of representation alignment and uniformity. Our investigation reveals that the overestimation of openness is due to confusion among competing text features, rather than a failure to capture the similarity between image features and text features of novel classes. We hope that our investigation and analysis will facilitate future research on the CLIP openness issue."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1860,
      "original_url": "https://aclanthology.org/2023.findings-acl.611/",
      "title": "From Adversarial Arms Race to Model-centric Evaluation: Motivating a Unified Automatic Robustness Evaluation Framework",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.611.pdf",
      "authors": [
        "Yangyi Chen",
        "Hongcheng Gao",
        "Ganqu Cui",
        "Lifan Yuan",
        "Dehan Kong",
        "Hanlu Wu",
        "Ning Shi",
        "Bo Yuan",
        "Longtao Huang",
        "Hui Xue",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Heng Ji"
      ],
      "abstract": "Textual adversarial attacks can discover models’ weaknesses by adding semantic-preserved but misleading perturbations to the inputs. The long-lasting adversarial attack-and-defense arms race in Natural Language Processing (NLP) is algorithm-centric, providing valuable techniques for automatic robustness evaluation. However, the existing practice of robustness evaluation may exhibit issues of incomprehensive evaluation, impractical evaluation protocol, and invalid adversarial samples. In this paper, we aim to set up a unified automatic robustness evaluation framework, shifting towards model-centric evaluation to further exploit the advantages of adversarial attacks. To address the above challenges, we first determine robustness evaluation dimensions based on model capabilities and specify the reasonable algorithm to generate adversarial samples for each dimension. Then we establish the evaluation protocol, including evaluation settings and metrics, under realistic demands. Finally, we use the perturbation degree of adversarial samples to control the sample validity. We implement a toolkit RobTest that realizes our automatic robustness evaluation framework. In our experiments, we conduct a robustness evaluation of RoBERTa models to demonstrate the effectiveness of our evaluation framework, and further show the rationality of each component in the framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1861,
      "original_url": "https://aclanthology.org/2023.findings-acl.612/",
      "title": "An Empirical Study of Sentiment-Enhanced Pre-Training for Aspect-Based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.612.pdf",
      "authors": [
        "Yice Zhang",
        "Yifan Yang",
        "Bin Liang",
        "Shiwei Chen",
        "Bing Qin",
        "Ruifeng Xu"
      ],
      "abstract": "Aspect-Based Sentiment Analysis (ABSA) aims to recognize fine-grained opinions and sentiments of users, which is an important problem in sentiment analysis. Recent work has shown that Sentiment-enhanced Pre-Training (SPT) can substantially improve the performance of various ABSA tasks. However, there is currently a lack of comprehensive evaluation and fair comparison of existing SPT approaches. Therefore, this paper performs an empirical study to investigate the effectiveness of different SPT approaches. First, we develop an effective knowledge-mining method and leverage it to build a large-scale knowledge-annotated SPT corpus. Second, we systematically analyze the impact of integrating sentiment knowledge and other linguistic knowledge in pre-training. For each type of sentiment knowledge, we also examine and compare multiple integration methods. Finally, we conduct extensive experiments on a wide range of ABSA tasks to see how much SPT can facilitate the understanding of aspect-level sentiments."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1862,
      "original_url": "https://aclanthology.org/2023.findings-acl.613/",
      "title": "NatCS: Eliciting Natural Customer Support Dialogues",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.613.pdf",
      "authors": [
        "James Gung",
        "Emily Moeng",
        "Wesley Rose",
        "Arshit Gupta",
        "Yi Zhang",
        "Saab Mansour"
      ],
      "abstract": "Despite growing interest in applications based on natural customer support conversations,there exist remarkably few publicly available datasets that reflect the expected characteristics of conversations in these settings. Existing task-oriented dialogue datasets, which were collected to benchmark dialogue systems mainly in written human-to-bot settings, are not representative of real customer support conversations and do not provide realistic benchmarks for systems that are applied to natural data. To address this gap, we introduce NatCS, a multi-domain collection of spoken customer service conversations. We describe our process for collecting synthetic conversations between customers and agents based on natural language phenomena observed in real conversations. Compared to previous dialogue datasets, the conversations collected with our approach are more representative of real human-to-human conversations along multiple metrics. Finally, we demonstrate potential uses of NatCS, including dialogue act classification and intent induction from conversations as potential applications, showing that dialogue act annotations in NatCS provide more effective training data for modeling real conversations compared to existing synthetic written datasets. We publicly release NatCS to facilitate research in natural dialog systems"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1863,
      "original_url": "https://aclanthology.org/2023.findings-acl.614/",
      "title": "Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.614.pdf",
      "authors": [
        "Shicheng Tan",
        "Weng Lam Tam",
        "Yuanchun Wang",
        "Wenwen Gong",
        "Shu Zhao",
        "Peng Zhang",
        "Jie Tang"
      ],
      "abstract": "The large scale of pre-trained language models poses a challenge for their deployment on various devices, with a growing emphasis on methods to compress these models, particularly knowledge distillation. However, current knowledge distillation methods rely on the model’s intermediate layer features and the golden labels (also called hard labels), which usually require aligned model architecture and enough labeled data respectively. Moreover, the parameters of vocabulary are usually neglected in existing methods. To address these problems, we propose a general language model distillation (GLMD) method that performs two-stage word prediction distillation and vocabulary compression, which is simple and surprisingly shows extremely strong performance. Specifically, GLMD supports more general application scenarios by eliminating the constraints of dimension and structure between models and the need for labeled datasets through the absence of intermediate layers and golden labels. Meanwhile, based on the long-tailed distribution of word frequencies in the data, GLMD designs a strategy of vocabulary compression through decreasing vocabulary size instead of dimensionality. Experimental results show that our method outperforms 25 state-of-the-art methods on the SuperGLUE benchmark, achieving an average score that surpasses the best method by 3%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1864,
      "original_url": "https://aclanthology.org/2023.findings-acl.615/",
      "title": "Diable: Efficient Dialogue State Tracking as Operations on Tables",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.615.pdf",
      "authors": [
        "Pietro Lesci",
        "Yoshinari Fujinuma",
        "Momchil Hardalov",
        "Chao Shang",
        "Yassine Benajiba",
        "Lluis Marquez"
      ],
      "abstract": "Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1865,
      "original_url": "https://aclanthology.org/2023.findings-acl.616/",
      "title": "Neural Topic Modeling based on Cycle Adversarial Training and Contrastive Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.616.pdf",
      "authors": [
        "Boyu Wang",
        "Linhai Zhang",
        "Deyu Zhou",
        "Yi Cao",
        "Jiandong Ding"
      ],
      "abstract": "Neural topic models have been widely used to extract common topics across documents. Recently, contrastive learning has been applied to variational autoencoder-based neural topic models, achieving promising results. However, due to the limitation of the unidirectional structure of the variational autoencoder, the encoder is enhanced with the contrastive loss instead of the decoder, leading to a gap between model training and evaluation. To address the limitation, we propose a novel neural topic modeling framework based on cycle adversarial training and contrastive learning to apply contrastive learning on the generator directly. Specifically, a self-supervised contrastive loss is proposed to make the generator capture similar topic information, which leads to better topic-word distributions. Meanwhile, a discriminative contrastive loss is proposed to cooperate with the self-supervised contrastive loss to balance the generation and discrimination. Moreover, based on the reconstruction ability of the cycle generative adversarial network, a novel data augmentation strategy is designed and applied to the topic distribution directly. Experiments have been conducted on four benchmark datasets and results show that the proposed approach outperforms competitive baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1866,
      "original_url": "https://aclanthology.org/2023.findings-acl.617/",
      "title": "Alleviating Exposure Bias via Multi-level Contrastive Learning and Deviation Simulation in Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.617.pdf",
      "authors": [
        "Jiawen Xie",
        "Qi Su",
        "Shaoting Zhang",
        "Xiaofan Zhang"
      ],
      "abstract": "Most Transformer based abstractive summarization systems have a severe mismatch between training and inference, i.e., exposure bias. From diverse perspectives, we introduce a simple multi-level contrastive learning framework for abstractive summarization (SimMCS) and a tailored sparse decoder self-attention pattern (SDSA) to bridge the gap between training and inference to improve model performance. Compared with previous contrastive objectives focusing only on the relative order of probability mass assigned to non-gold summaries, SimMCS additionally takes their absolute positions into account, which guarantees that the relatively high-quality (positive) summaries among them could be properly assigned high probability mass, and further enhances the capability of discriminating summary quality beyond exploiting potential artifacts of specific metrics. SDSA simulates the possible inference scenarios of deviation in the training phase to get closer to the ideal paradigm. Our approaches outperform the previous state-of-the-art results on two summarization datasets while just adding fairly low overhead. Further empirical analysis shows our model preserves the advantages of prior contrastive methods and possesses strong few-shot learning ability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1867,
      "original_url": "https://aclanthology.org/2023.findings-acl.618/",
      "title": "Mapping Brains with Language Models: A Survey",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.618.pdf",
      "authors": [
        "Antonia Karamolegkou",
        "Mostafa Abdou",
        "Anders Søgaard"
      ],
      "abstract": "Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before we can draw conclusions? Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative. We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1868,
      "original_url": "https://aclanthology.org/2023.findings-acl.619/",
      "title": "Parameter-Efficient Finetuning for Robust Continual Multilingual Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.619.pdf",
      "authors": [
        "Kartikeya Badola",
        "Shachi Dave",
        "Partha Talukdar"
      ],
      "abstract": "We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance only on the languages included in the latest update (and a few closely related languages) while its performance on all the remaining languages degrade significantly. We address this challenge by proposing LAFT-URIEL, a parameter-efficient finetuning strategy which aims to increase the number of languages on which the model improves after an update, while reducing the magnitude of loss in performance for the remaining languages. LAFT-URIEL uses linguistic knowledge to balance overfitting and knowledge sharing across languages, allowing for an additional 25% of task languages to see an improvement in performance after an update, while also reducing the average magnitude of losses on the remaining languages by 78% relative."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1869,
      "original_url": "https://aclanthology.org/2023.findings-acl.620/",
      "title": "Interpretable Multimodal Misinformation Detection with Logic Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.620.pdf",
      "authors": [
        "Hui Liu",
        "Wenya Wang",
        "Haoliang Li"
      ],
      "abstract": "Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems’ reliability and practical deployment. Inspired by Neural-Symbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize the symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-predicates that can be instantiated with different correlations. Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the feasibility and versatility of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1870,
      "original_url": "https://aclanthology.org/2023.findings-acl.621/",
      "title": "Semantic-conditioned Dual Adaptation for Cross-domain Query-based Visual Segmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.621.pdf",
      "authors": [
        "Ye Wang",
        "Tao Jin",
        "Wang Lin",
        "Xize Cheng",
        "Linjun Li",
        "Zhou Zhao"
      ],
      "abstract": "Visual segmentation from language queries has attracted significant research interest. Despite the effectiveness, existing works require expensive labeling and suffer severe degradation when deployed to an unseen domain. In this paper, we investigate a novel task Cross-domain Query-based Visual Segmentation (CQVS), aiming to adapt the segmentation model from a labeled domain to a new unlabeled domain. The challenges of CQVS stem from three domain discrepancies: (1) multi-modal content shift, (2) uni-modal feature gap and (3) cross-modal relation bias. Existing domain adaptation methods fail to address them comprehensively and precisely (e.g. at pixel level), thus being suboptimal for CQVS. To overcome this limitation, we propose Semantic-conditioned Dual Adaptation (SDA), a novel framework to achieve precise feature- and relation-invariant across domains via a universal semantic structure. The SDA consists of two key components: Content-aware Semantic Modeling (CSM) and Dual Adaptive Branches (DAB). First, CSM introduces a common semantic space across domains to provide uniform guidance. Then, DAB seamlessly leverages this semantic information to develop a contrastive feature branch for category-wise pixel alignment, and design a reciprocal relation branch for relation enhancement via two complementary masks. Extensive experiments on three video benchmarks and three image benchmarks evidence the superiority of our approach over the state-of-the-arts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1871,
      "original_url": "https://aclanthology.org/2023.findings-acl.622/",
      "title": "Figurative Language Processing: A Linguistically Informed Feature Analysis of the Behavior of Language Models and Humans",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.622.pdf",
      "authors": [
        "Hyewon Jang",
        "Qi Yu",
        "Diego Frassinelli"
      ],
      "abstract": "Recent years have witnessed a growing interest in investigating what Transformer-based language models (TLMs) actually learn from the training data. This is especially relevant for complex tasks such as the understanding of non-literal meaning. In this work, we probe the performance of three black-box TLMs and two intrinsically transparent white-box models on figurative language classification of sarcasm, similes, idioms, and metaphors. We conduct two studies on the classification results to provide insights into the inner workings of such models. With our first analysis on feature importance, we identify crucial differences in model behavior. With our second analysis using an online experiment with human participants, we inspect different linguistic characteristics of the four figurative language types."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1872,
      "original_url": "https://aclanthology.org/2023.findings-acl.623/",
      "title": "Taxonomy of Problems in Lexical Semantics",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.623.pdf",
      "authors": [
        "Bradley Hauer",
        "Grzegorz Kondrak"
      ],
      "abstract": "Semantic tasks are rarely formally defined, and the exact relationship between them is an open question. We introduce a taxonomy that elucidates the connection between several problems in lexical semantics, including monolingual and cross-lingual variants. Our theoretical framework is based on the hypothesis of the equivalence of concept and meaning distinctions. Using algorithmic problem reductions, we demonstrate that all problems in the taxonomy can be reduced to word sense disambiguation (WSD), and that WSD itself can be reduced to some problems, making them theoretically equivalent. In addition, we carry out experiments that strongly support the soundness of the concept-meaning hypothesis, and the correctness of our reductions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1873,
      "original_url": "https://aclanthology.org/2023.findings-acl.624/",
      "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.624.pdf",
      "authors": [
        "Yangyi Chen",
        "Xingyao Wang",
        "Heng Ji"
      ],
      "abstract": "Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it’s equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1874,
      "original_url": "https://aclanthology.org/2023.findings-acl.625/",
      "title": "EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.625.pdf",
      "authors": [
        "Dae Yon Hwang",
        "Bilal Taha",
        "Yaroslav Nechaev"
      ],
      "abstract": "The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1875,
      "original_url": "https://aclanthology.org/2023.findings-acl.626/",
      "title": "Denoising Enhanced Distantly Supervised Ultrafine Entity Typing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.626.pdf",
      "authors": [
        "Yue Zhang",
        "Hongliang Fei",
        "Ping Li"
      ],
      "abstract": "Recently, the task of distantly supervised (DS) ultra-fine entity typing has received significant attention. However, DS data is noisy and often suffers from missing or wrong labeling issues resulting in low precision and low recall. This paper proposes a novel ultra-fine entity typing model with denoising capability. Specifically, we build a noise model to estimate the unknown labeling noise distribution over input contexts and noisy type labels. With the noise model, more trustworthy labels can be recovered by subtracting the estimated noise from the input. Furthermore, we propose an entity typing model, which adopts a bi-encoder architecture, is trained on the denoised data. Finally, the noise model and entity typing model are trained iteratively to enhance each other. We conduct extensive experiments on the Ultra-Fine entity typing dataset as well as OntoNotes dataset and demonstrate that our approach significantly outperforms other baseline methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1876,
      "original_url": "https://aclanthology.org/2023.findings-acl.627/",
      "title": "INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.627.pdf",
      "authors": [
        "Eunseop Yoon",
        "Hee Suk Yoon",
        "John Harvill",
        "Mark Hasegawa-Johnson",
        "Chang Yoo"
      ],
      "abstract": "Automatic Speech Recognition (ASR) systems have attained unprecedented performance with large speech models pre-trained based on self-supervised speech representation learning. However, these pre-trained speech models suffer from representational bias as they tend to better represent those prominent accents (i.e., native (L1) English accent) in the pre-training speech corpus than less represented accents, resulting in a deteriorated performance for non-native (L2) English accents. Although there have been some approaches to mitigate this issue, all of these methods require updating the pre-trained model weights. In this paper, we propose Information Theoretic Adversarial Prompt Tuning (INTapt), which introduces prompts concatenated to the original input that can re-modulate the attention of the pre-trained model such that the corresponding input resembles a native (L1) English speech without updating the backbone weights. INTapt is trained simultaneously in the following two manners: (1) adversarial training to reduce accent feature dependence between the original input and the prompt-concatenated input and (2) training to minimize CTC loss for improving ASR performance to a prompt-concatenated input. Experimental results show that INTapt improves the performance of L2 English and increases feature similarity between L2 and L1 accents."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1877,
      "original_url": "https://aclanthology.org/2023.findings-acl.628/",
      "title": "Local Temperature Beam Search: Avoid Neural Text DeGeneration via Enhanced Calibration",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.628.pdf",
      "authors": [
        "Dongkyu Lee",
        "Gyeonghun Kim",
        "Janghoon Han",
        "Taesuk Hong",
        "Yi-Reun Kim",
        "Stanley Jungkyu Choi",
        "Nevin L. Zhang"
      ],
      "abstract": "Previous studies have constantly observed that a language model repeats itself, creating repetitions in an output sequence. To cope with the issue, stochastic decoding schemes have been the de facto approaches; the strategies add randomness in inference, hence avoiding the “self-loop”. However, the remedy comes at the cost of sacrificing output quality due to the randomness involved. In this work, we introduce a deterministic decoding scheme, local temperature beam search. This inference algorithm is an embarrassingly simple variant of beam search, yet it reduces repetition, whose level is superior to that of a sampling-based decoding algorithm, while maintaining the level of coherence as in beam search. Our idea is rooted in the concept of model calibration; we view a repetition as a casualty from overconfidence in a model. Therefore, our work mitigates the miscalibration present in the course of inference with a post-calibration approach applied in beam-specific manner. Our inference scheme is validated on text completion tasks, in which the repetition problem is seen most clearly, and is exhaustively compared with existing inference schemes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1878,
      "original_url": "https://aclanthology.org/2023.findings-acl.629/",
      "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.629.pdf",
      "authors": [
        "Han Cui",
        "Shangzhan Li",
        "Yu Zhang",
        "Qi Shi"
      ],
      "abstract": "The generation of explanation graphs is a significant task that aims to produce explanation graphs in response to user input, revealing the internal reasoning process. This task is challenging due to the significant discrepancy be- tween unstructured user queries and structured explanation graphs. Current research commonly fine-tunes a text-based pre-trained language model on a small downstream dataset that is annotated with labeled graphs. However, due to the limited scale of available datasets, this approach may prove to be insufficient in bridging the gap between natural language text and structured graphs. In this paper, to alleviate the above limitations, we propose a novel pre-trained framework EG3P(for Explanation Graph Generation via Generative Pre-training over synthetic graphs) for the explanation graph generation task. Specifically, we first propose a text-to-graph generative task to pre-train the model with the goal of bridging the text-graph gap. Additionally, we propose an automatic corpus synthesis strategy for synthesizing a large scale of high-quality corpus, reducing the reliance on costly manual annotation methods. Experimental results on ExplaGraphs show the effectiveness of EG3P that our model surpasses all baseline systems with remarkable margins. Besides, further analysis demonstrates that EG3P is able to generate better explanation graphs on actual reasoning tasks such as CommonsenseQA and OpenbookQA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1879,
      "original_url": "https://aclanthology.org/2023.findings-acl.630/",
      "title": "NaSGEC: a Multi-Domain Chinese Grammatical Error Correction Dataset from Native Speaker Texts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.630.pdf",
      "authors": [
        "Yue Zhang",
        "Bo Zhang",
        "Haochen Jiang",
        "Zhenghua Li",
        "Chen Li",
        "Fei Huang",
        "Min Zhang"
      ],
      "abstract": "We introduce NaSGEC, a new dataset to facilitate research on Chinese grammatical error correction (CGEC) for native speaker texts from multiple domains. Previous CGEC research primarily focuses on correcting texts from a single domain, especially learner essays. To broaden the target domain, we annotate multiple references for 12,500 sentences from three native domains, i.e., social media, scientific writing, and examination. We provide solid benchmark results for NaSGEC by employing cutting-edge CGEC models and different training data. We further perform detailed analyses of the connections and gaps between our domains from both empirical and statistical views. We hope this work can inspire future studies on an important but under-explored direction–cross-domain GEC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1880,
      "original_url": "https://aclanthology.org/2023.findings-acl.631/",
      "title": "FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.631.pdf",
      "authors": [
        "Shramay Palta",
        "Rachel Rudinger"
      ],
      "abstract": "It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high performance on questions about the US culture, the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1881,
      "original_url": "https://aclanthology.org/2023.findings-acl.632/",
      "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.632.pdf",
      "authors": [
        "Zhuo Zhang",
        "Yuanhang Yang",
        "Yong Dai",
        "Qifan Wang",
        "Yue Yu",
        "Lizhen Qu",
        "Zenglin Xu"
      ],
      "abstract": "With increasing concerns about data privacy, there is an increasing necessity of fine-tuning pre-trained language models (PLMs) for adapting to downstream tasks located in end-user devices or local clients without transmitting data to the central server. This urgent necessity therefore calls the research of investigating federated learning (FL) for PLMs. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we investigate the parameter-efficient tuning (PETuning) of PLMs and develop a corresponding federated benchmark for four representative PETuning methods, dubbed FedPETuning. Specifically, FedPETuning provides the first holistic empirical study of representative PLMs tuning methods in FL, covering privacy attacks, performance comparisons, and resource-constrained analysis. Intensive experimental results have indicated that FedPETuning can efficiently defend against privacy attacks and maintains acceptable performance with reducing heavy resource consumption. The open-source code and data are available at https://github.com/SMILELab-FL/FedPETuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1882,
      "original_url": "https://aclanthology.org/2023.findings-acl.633/",
      "title": "MixPAVE: Mix-Prompt Tuning for Few-shot Product Attribute Value Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.633.pdf",
      "authors": [
        "Li Yang",
        "Qifan Wang",
        "Jingang Wang",
        "Xiaojun Quan",
        "Fuli Feng",
        "Yu Chen",
        "Madian Khabsa",
        "Sinong Wang",
        "Zenglin Xu",
        "Dongfang Liu"
      ],
      "abstract": "The task of product attribute value extraction is to identify values of an attribute from product information. Product attributes are important features, which help improve online shopping experience of customers, such as product search, recommendation and comparison. Most existing works only focus on extracting values for a set of known attributes with sufficient training data. However, with the emerging nature of e-commerce, new products with their unique set of new attributes are constantly generated from different retailers and merchants. Collecting a large number of annotations for every new attribute is costly and time consuming. Therefore, it is an important research problem for product attribute value extraction with limited data. In this work, we propose a novel prompt tuning approach with Mixed Prompts for few-shot Attribute Value Extraction, namely MixPAVE. Specifically, MixPAVE introduces only a small amount (< 1%) of trainable parameters, i.e., a mixture of two learnable prompts, while keeping the existing extraction model frozen. In this way, MixPAVE not only benefits from parameter-efficient training, but also avoids model overfitting on limited training examples. Experimental results on two product benchmarks demonstrate the superior performance of the proposed approach over several state-of-the-art baselines. A comprehensive set of ablation studies validate the effectiveness of the prompt design, as well as the efficiency of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1883,
      "original_url": "https://aclanthology.org/2023.findings-acl.634/",
      "title": "SlowBERT: Slow-down Attacks on Input-adaptive Multi-exit BERT",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.634.pdf",
      "authors": [
        "Shengyao Zhang",
        "Xudong Pan",
        "Mi Zhang",
        "Min Yang"
      ],
      "abstract": "For pretrained language models such as Google’s BERT, recent research designs several input-adaptive inference mechanisms to improve the efficiency on cloud and edge devices. In this paper, we reveal a new attack surface on input-adaptive multi-exit BERT, where the adversary imperceptibly modifies the input texts to drastically increase the average inference cost. Our proposed slow-down attack called SlowBERT integrates a new rank-and-substitute adversarial text generation algorithm to efficiently search for the perturbation which maximally delays the exiting time. With no direct access to the model internals, we further devise a time-based approximation algorithm to infer the exit position as the loss oracle. Our extensive evaluation on two popular instances of multi-exit BERT for GLUE classification tasks validates the effectiveness of SlowBERT. In the worst case, SlowBERT increases the inference cost by 4.57×, which would strongly hurt the service quality of multi-exit BERT in practice, e.g., increasing the real-time cloud services’ response times for online users."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1884,
      "original_url": "https://aclanthology.org/2023.findings-acl.635/",
      "title": "Compositional Mathematical Encoding for Math Word Problems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.635.pdf",
      "authors": [
        "Zhenwen Liang",
        "Jipeng Zhang",
        "Kehan Guo",
        "Xiaodong Wu",
        "Jie Shao",
        "Xiangliang Zhang"
      ],
      "abstract": "Solving math word problem (MWP) remains a challenging task, as it requires to understand both the semantic meanings of the text and the mathematical logic among quantities, i.e., for both semantics modal and quantity modal learning. Current MWP encoders work in a uni-modal setting and map the given problem description to a latent representation, then for decoding. The generalizability of these MWP encoders is thus limited because some problems are semantics-demanding and others are quantity-demanding. To address this problem, we propose a Compositional Math Word Problem Solver (C-MWP) which works in a bi-modal setting encoding in an interactive way. Extensive experiments validate the effectiveness of C-MWP and show its superiority over state-of-the-art models on public benchmarks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1885,
      "original_url": "https://aclanthology.org/2023.findings-acl.636/",
      "title": "PREADD: Prefix-Adaptive Decoding for Controlled Text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.636.pdf",
      "authors": [
        "Jonathan Pei",
        "Kevin Yang",
        "Dan Klein"
      ],
      "abstract": "We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three tasks—toxic output mitigation, gender bias reduction, and sentiment control—and find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12% or more in relative gain on our main metrics for each task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1886,
      "original_url": "https://aclanthology.org/2023.findings-acl.637/",
      "title": "EventOA: An Event Ontology Alignment Benchmark Based on FrameNet and Wikidata",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.637.pdf",
      "authors": [
        "Shaoru Guo",
        "Chenhao Wang",
        "Yubo Chen",
        "Kang Liu",
        "Ru Li",
        "Jun Zhao"
      ],
      "abstract": "Event ontology provides a shared and formal specification about what happens in the real world and can benefit many natural language understanding tasks. However, the independent development of event ontologies often results in heterogeneous representations that raise the need for establishing alignments between semantically related events. There exists a series of works about ontology alignment (OA), but they only focus on the entity-based OA, and neglect the event-based OA. To fill the gap, we construct an Event Ontology Alignment (EventOA) dataset based on FrameNet and Wikidata, which consists of 900+ event type alignments and 8,000+ event argument alignments. Furthermore, we propose a multi-view event ontology alignment (MEOA) method, which utilizes description information (i.e., name, alias and definition) and neighbor information (i.e., subclass and superclass) to obtain richer representation of the event ontologies. Extensive experiments show that our MEOA outperforms the existing entity-based OA methods and can serve as a strong baseline for EventOA research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1887,
      "original_url": "https://aclanthology.org/2023.findings-acl.638/",
      "title": "Enhancing Continual Relation Extraction via Classifier Decomposition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.638.pdf",
      "authors": [
        "Heming Xia",
        "Peiyi Wang",
        "Tianyu Liu",
        "Binghuai Lin",
        "Yunbo Cao",
        "Zhifang Sui"
      ],
      "abstract": "Continual relation extraction (CRE) models aim at handling emerging new relations while avoiding catastrophically forgetting old ones in the streaming data. Though improvements have been shown by previous CRE studies, most of them only adopt a vanilla strategy when models first learn representations of new relations. In this work, we point out that there exist two typical biases after training of this vanilla strategy: classifier bias and representation bias, which causes the previous knowledge that the model learned to be shaded. To alleviate those biases, we propose a simple yet effective classifier decomposition framework that splits the last FFN layer into separated previous and current classifiers, so as to maintain previous knowledge and encourage the model to learn more robust representations at this training stage. Experimental results on two standard benchmarks show that our proposed framework consistently outperforms the state-of-the-art CRE models, which indicates that the importance of the first training stage to CRE models may be underestimated. Our code will be released upon acceptance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1888,
      "original_url": "https://aclanthology.org/2023.findings-acl.639/",
      "title": "A Comparative Analysis of the Effectiveness of Rare Tokens on Creative Expression using ramBERT",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.639.pdf",
      "authors": [
        "Youbin Lee",
        "Deokgi Kim",
        "Byung-Won On",
        "Ingyu Lee"
      ],
      "abstract": "Until now, few studies have been explored on Automated Creative Essay Scoring (ACES), in which a pre-trained model automatically labels an essay as a creative or a non-creative. Since the creativity evaluation of essays is very subjective, each evaluator often has his or her own criteria for creativity. For this reason, quantifying creativity in essays is very challenging. In this work, as one of preliminary studies in developing a novel model for ACES, we deeply investigate the correlation between creative essays and expressiveness. Specifically, we explore how rare tokens affect the evaluation of creativity for essays. For such a journey, we present five distinct methods to extract rare tokens, and conduct a comparative study on the correlation between rare tokens and creative essay evaluation results using BERT. Our experimental results showed clear correlation between rare tokens and creative essays. In all test sets, accuracies of our rare token masking-based BERT (ramBERT) model were improved over the existing BERT model up to 14%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1889,
      "original_url": "https://aclanthology.org/2023.findings-acl.640/",
      "title": "MTR: A Dataset Fusing Inductive, Deductive, and Defeasible Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.640.pdf",
      "authors": [
        "Yitian Li",
        "Jidong Tian",
        "Caoyun Fan",
        "Wenqing Chen",
        "Hao He",
        "Yaohui Jin"
      ],
      "abstract": "A long-standing difficulty in AI is the introduction of human-like reasoning in machine reading comprehension. Since algorithmic models can already perform as well as humans on simple quality assurance tasks thanks to the development of deep learning techniques, more difficult reasoning datasets have been presented. However, these datasets mainly focus on a single type of reasoning. There are still significant gaps in the studies when compared to the complex reasoning used in daily life. In this work, we introduce a brand-new dataset, named MTR. There are two parts to it: the first combines deductive and inductive reasoning, and the second does the same with inductive and defeasible reasoning. It consists of more than 30k QA instances, inferring relations between characters in short stories. Results show that state-of-the-art neural models do noticeably worse than expected. Our empirical results highlight the gap in the models’ ability to handle sophisticated inference."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1890,
      "original_url": "https://aclanthology.org/2023.findings-acl.641/",
      "title": "NewsMet : A ‘do it all’ Dataset of Contemporary Metaphors in News Headlines",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.641.pdf",
      "authors": [
        "Rohan Joseph",
        "Timothy Liu",
        "Aik Beng Ng",
        "Simon See",
        "Sunny Rai"
      ],
      "abstract": "Metaphors are highly creative constructs of human language that grow old and eventually die. Popular datasets used for metaphor processing tasks were constructed from dated source texts. In this paper, we propose NewsMet, a large high-quality contemporary dataset of news headlines hand-annotated with metaphorical verbs. The dataset comprises headlines from various sources including political, satirical, reliable and fake. Our dataset serves the purpose of evaluation for the tasks of metaphor interpretation and generation. The experiments reveal several insights and limitations of using LLMs to automate metaphor processing tasks as frequently seen in the recent literature. The dataset is publicly available for research purposes https://github.com/AxleBlaze3/NewsMet_Metaphor_Dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1891,
      "original_url": "https://aclanthology.org/2023.findings-acl.642/",
      "title": "Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.642.pdf",
      "authors": [
        "Zijie Huang",
        "Daheng Wang",
        "Binxuan Huang",
        "Chenwei Zhang",
        "Jingbo Shang",
        "Yan Liang",
        "Zhengyang Wang",
        "Xian Li",
        "Christos Faloutsos",
        "Yizhou Sun",
        "Wei Wang"
      ],
      "abstract": "Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic semantics towards concepts’ granularity. We propose Concept2Box, a novel approach that jointly embeds the two views of a KG using dual geometric representations. We model concepts with box embeddings, which learn the hierarchy structure and complex relations such as overlap and disjoint among them. Box volumes can be interpreted as concepts’ granularity. Different from concepts, we model entities as vectors. To bridge the gap between concept box embeddings and entity vector embeddings, we propose a novel vector-to-box distance metric and learn both embeddings jointly. Experiments on both the public DBpedia KG and a newly-created industrial KG showed the effectiveness of Concept2Box."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1892,
      "original_url": "https://aclanthology.org/2023.findings-acl.643/",
      "title": "Noise-Robust Training with Dynamic Loss and Contrastive Learning for Distantly-Supervised Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.643.pdf",
      "authors": [
        "Zhiyuan Ma",
        "Jintao Du",
        "Shuheng Zhou"
      ],
      "abstract": "Distantly-supervised named entity recognition (NER) aims at training networks with distantly-labeled data, which is automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. Distant supervision may induce incomplete and noisy labels, so recent state-of-the-art methods employ sample selection mechanism to separate clean data from noisy data based on the model’s prediction scores. However, they ignore the noise distribution change caused by data selection, and they simply excludes noisy data during training, resulting in information loss. We propose to (1) use a dynamic loss function to better adapt to the changing noise during the training process, and (2) incorporate token level contrastive learning to fully utilize the noisy data as well as facilitate feature learning without relying on labels. Our method achieves superior performance on three benchmark datasets, outperforming existing distantly supervised NER models by significant margins."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1893,
      "original_url": "https://aclanthology.org/2023.findings-acl.644/",
      "title": "Take a Break in the Middle: Investigating Subgoals towards Hierarchical Script Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.644.pdf",
      "authors": [
        "Xinze Li",
        "Yixin Cao",
        "Muhao Chen",
        "Aixin Sun"
      ],
      "abstract": "Goal-oriented Script Generation is a new task of generating a list of steps that can fulfill the given goal. In this paper, we propose to extend the task from the perspective of cognitive theory. Instead of a simple flat structure, the steps are typically organized hierarchically — Human often decompose a complex task into subgoals, where each subgoal can be further decomposed into steps. To establish the benchmark, we contribute a new dataset, propose several baseline methods, and set up evaluation metrics. Both automatic and human evaluation verify the high-quality of dataset, as well as the effectiveness of incorporating subgoals into hierarchical script generation. Furthermore, We also design and evaluate the model to discover subgoal, and find that it is a bit more difficult to decompose the goals than summarizing from segmented steps."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1894,
      "original_url": "https://aclanthology.org/2023.findings-acl.645/",
      "title": "End-to-End Task-Oriented Dialogue Systems Based on Schema",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.645.pdf",
      "authors": [
        "Wiradee Imrattanatrai",
        "Ken Fukuda"
      ],
      "abstract": "This paper presents a schema-aware end-to-end neural network model for handling task-oriented dialogues based on a dynamic set of slots within a schema. Contrary to existing studies that proposed end-to-end approaches for task-oriented dialogue systems by relying on a unified schema across domains, we design our approach to support a domain covering multiple services where diverse schemas are available. To enable better generalizability among services and domains with different schemas, we supply the schema’s context information including slot descriptions and value constraints to the model. The experimental results on a well-known Schema-Guided Dialogue (SGD) dataset demonstrated the performance improvement by the proposed model compared to state-of-the-art baselines in terms of end-to-end modeling, dialogue state tracking task, and generalization on new services and domains using a limited number of dialogues."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1895,
      "original_url": "https://aclanthology.org/2023.findings-acl.646/",
      "title": "HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.646.pdf",
      "authors": [
        "Shantipriya Parida",
        "Idris Abdulmumin",
        "Shamsuddeen Hassan Muhammad",
        "Aneesh Bose",
        "Guneet Singh Kohli",
        "Ibrahim Said Ahmad",
        "Ketan Kotwal",
        "Sayan Deb Sarkar",
        "Ondřej Bojar",
        "Habeebah Kakudi"
      ],
      "abstract": "This paper presents “HaVQA”, the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We conducted several baseline experiments on the dataset, including visual question answering, visual question elicitation, text-only and multimodal machine translation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1896,
      "original_url": "https://aclanthology.org/2023.findings-acl.647/",
      "title": "Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.647.pdf",
      "authors": [
        "Martin Fajcik",
        "Petr Motlicek",
        "Pavel Smrz"
      ],
      "abstract": "We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidence jointly learns to identify: (i) the relevant evidences to the given claim (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way — the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to final verdict or to detect disagreeing evidence. Despite its interpretable nature, our system achieves results competetive with state-of-the-art on the FEVER dataset, as compared to typical two-stage system pipelines, while using significantly fewer parameters. Furthermore, our analysis shows that our model can learn fine-grained relevance cues while using coarse-grained supervision and we demonstrate it in 2 ways. (i) We show that our model can achieve competitive sentence recall while using only paragraph-level relevance supervision. (ii) Traversing towards the finest granularity of relevance, we show that our model is capable of identifying relevance at the token level. To do this, we present a new benchmark TLR-FEVER focusing on token-level interpretability — humans annotate tokens in relevant evidences they considered essential when making their judgment. Then we measure how similar are these annotations to the tokens our model is focusing on."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1897,
      "original_url": "https://aclanthology.org/2023.findings-acl.648/",
      "title": "StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.648.pdf",
      "authors": [
        "Truong Do",
        "Phuong Nguyen",
        "Minh Nguyen"
      ],
      "abstract": "We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism: hierarchical structure information strengthening and a final specific task. The model used is better than existing ones at learning the contextual representations of utterances embedded within its hierarchical semantic structure and thereby improves system performance. In addition, we created a mechanism using inductive grammar to dynamically prune the unpromising directions in the semantic structure parsing process. Finally, through experimentsOur code will be published when this paper is accepted. on the TOP and TOPv2 (low-resource setting) datasets, we achieved state-of-the-art (SOTA) performance, confirming the effectiveness of our proposed model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1898,
      "original_url": "https://aclanthology.org/2023.findings-acl.649/",
      "title": "GDA: Generative Data Augmentation Techniques for Relation Extraction Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.649.pdf",
      "authors": [
        "Xuming Hu",
        "Aiwei Liu",
        "Zeqi Tan",
        "Xin Zhang",
        "Chenwei Zhang",
        "Irwin King",
        "Philip S. Yu"
      ],
      "abstract": "Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to generate pseudo-annotated sentences beyond limited annotations. These techniques neither preserve the semantic consistency of the original sentences when rule-based augmentations are adopted, nor preserve the syntax structure of sentences when expressing relations using seq2seq models, resulting in less diverse augmentations. In this work, we propose a dedicated augmentation technique for relational texts, named GDA, which uses two complementary modules to preserve both semantic consistency and syntax structures. We adopt a generative formulation and design a multi-tasking solution to achieve synergies. Furthermore, GDA adopts entity hints as the prior knowledge of the generative model to augment diverse sentences. Experimental results in three datasets under a low-resource setting showed that GDA could bring 2.0% F1 improvements compared with no augmentation technique."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1899,
      "original_url": "https://aclanthology.org/2023.findings-acl.650/",
      "title": "WebDP: Understanding Discourse Structures in Semi-Structured Web Documents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.650.pdf",
      "authors": [
        "Peilin Liu",
        "Hongyu Lin",
        "Meng Liao",
        "Hao Xiang",
        "Xianpei Han",
        "Le Sun"
      ],
      "abstract": "Web documents have become rich data resources in current era, and understanding their discourse structure will potentially benefit various downstream document processing applications. Unfortunately, current discourse analysis and document intelligence research mostly focus on either discourse structure of plain text or superficial visual structures in document, which cannot accurately describe discourse structure of highly free-styled and semi-structured web documents. To promote discourse studies on web documents, in this paper we introduced a benchmark – WebDP, orienting a new task named Web Document Discourse Parsing. Specifically, a web document discourse structure representation schema is proposed by extending classical discourse theories and adding special features to well represent discourse characteristics of web documents. Then, a manually annotated web document dataset – WEBDOCS is developed to facilitate the study of this parsing task. We compared current neural models on WEBDOCS and experimental results show that WebDP is feasible but also challenging for current models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1900,
      "original_url": "https://aclanthology.org/2023.findings-acl.651/",
      "title": "Tab-CoT: Zero-shot Tabular Chain of Thought",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.651.pdf",
      "authors": [
        "Jin Ziqi",
        "Wei Lu"
      ],
      "abstract": "The chain-of-though (CoT) prompting methods were successful in various natural language processing (NLP) tasks thanks to their ability to unveil the underlying complex reasoning processes. Such reasoning processes typically exhibit highly structured steps. Recent efforts also started investigating methods to encourage more structured reasoning procedures to be captured (cite least to most).In this work, we propose Tab-CoT, a novel tabular-format CoT prompting method, which allows the complex reasoning process to be explicitly modeled in a highly structured manner. Despite its simplicity, we show that our approach is capable of performing reasoning across multiple dimensions (i.e., both rows and columns).We demonstrate our approach’s strong zero-shot and few-shot capabilities through extensive experiments on a range of reasoning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1901,
      "original_url": "https://aclanthology.org/2023.findings-acl.652/",
      "title": "KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.652.pdf",
      "authors": [
        "Wei Chen",
        "Shiqi Wei",
        "Zhongyu Wei",
        "Xuanjing Huang"
      ],
      "abstract": "Symptom diagnosis in medical conversations aims to correctly extract both symptom entities and their status from the doctor-patient dialogue. In this paper, we propose a novel framework called KNSE for symptom status recognition (SSR), where the SSR is formulated as a natural language inference (NLI) task. For each mentioned symptom in a dialogue window, we first generate knowledge about the symptom and hypothesis about status of the symptom, to form a (premise, knowledge, hypothesis) triplet. The BERT model is then used to encode the triplet, which is further processed by modules including utterance aggregation, self-attention, cross-attention, and GRU to predict the symptom status. Benefiting from the NLI formalization, the proposed framework can encode more informative prior knowledge to better localize and track symptom status, which can effectively improve the performance of symptom status recognition. Preliminary experiments on Chinese medical dialogue datasets show that KNSE outperforms previous competitive baselines and has advantages in cross-disease and cross-symptom scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1902,
      "original_url": "https://aclanthology.org/2023.findings-acl.653/",
      "title": "Augmenting Large Language Model Translators via Translation Memories",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.653.pdf",
      "authors": [
        "Yongyu Mu",
        "Abudurexiti Reheman",
        "Zhiquan Cao",
        "Yuchun Fan",
        "Bei Li",
        "Yinqiao Li",
        "Tong Xiao",
        "Chunliang Zhang",
        "Jingbo Zhu"
      ],
      "abstract": "Using translation memories (TMs) as prompts is a promising approach to in-context learning of machine translation models. In this work, we take a step towards prompting large language models (LLMs) with TMs and making them better translators. We find that the ability of LLMs to “understand” prompts is indeed helpful for making better use of TMs. Experiments show that the results of a pre-trained LLM translator can be greatly improved by using high-quality TM-based prompts. These results are even comparable to those of the state-of-the-art NMT systems which have access to large-scale in-domain bilingual data and are well tuned on the downstream tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1903,
      "original_url": "https://aclanthology.org/2023.findings-acl.654/",
      "title": "Character Coreference Resolution in Movie Screenplays",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.654.pdf",
      "authors": [
        "Sabyasachee Baruah",
        "Shrikanth Narayanan"
      ],
      "abstract": "Movie screenplays have a distinct narrative structure. It segments the story into scenes containing interleaving descriptions of actions, locations, and character dialogues.A typical screenplay spans several scenes and can include long-range dependencies between characters and events.A holistic document-level understanding of the screenplay requires several natural language processing capabilities, such as parsing, character identification, coreference resolution, action recognition, summarization, and attribute discovery. In this work, we develop scalable and robust methods to extract the structural information and character coreference clusters from full-length movie screenplays. We curate two datasets for screenplay parsing and character coreference — MovieParse and MovieCoref, respectively.We build a robust screenplay parser to handle inconsistencies in screenplay formatting and leverage the parsed output to link co-referring character mentions.Our coreference models can scale to long screenplay documents without drastically increasing their memory footprints."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1904,
      "original_url": "https://aclanthology.org/2023.findings-acl.655/",
      "title": "Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.655.pdf",
      "authors": [
        "Ruili Pu",
        "Yang Li",
        "Suge Wang",
        "Deyu Li",
        "Jianxing Zheng",
        "Jian Liao"
      ],
      "abstract": "Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and the event pair interaction information. In particular, 1) we design an event-causal-label-aware module to model the event causal label information, in which we design the event causal label prediction task as an auxiliary task of ECI, aiming to predict which events are involved in the causal relationship (we call them causality-related events) by mining the dependencies between events. 2) We further design an event pair interaction graph module to model the interaction information between event pairs, in which we construct the interaction graph with event pairs as nodes and leverage graph attention mechanism to model the degree of dependency between event pairs. The experimental results show that our approach outperforms previous state-of-the-art methods on two benchmark datasets EventStoryLine and Causal-TimeBank."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1905,
      "original_url": "https://aclanthology.org/2023.findings-acl.656/",
      "title": "LightFormer: Light-weight Transformer Using SVD-based Weight Transfer and Parameter Sharing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.656.pdf",
      "authors": [
        "Xiuqing Lv",
        "Peng Zhang",
        "Sunzhu Li",
        "Guobing Gan",
        "Yueheng Sun"
      ],
      "abstract": "Transformer has become an important technique for natural language processing tasks with great success. However, it usually requires huge storage space and computational cost, making it difficult to be deployed on resource-constrained edge devices. To compress and accelerate Transformer, we propose LightFormer, which adopts a low-rank factorization initialized by SVD-based weight transfer and parameter sharing. The SVD-based weight transfer can effectively utilize the well-trained Transformer parameter knowledge to speed up the model convergence, and effectively alleviate the low-rank bottleneck problem combined with parameter sharing. We validate our method on machine translation, text summarization and text classification tasks. Experiments show that on IWSLT’14 De-En and WMT’14 En-De, LightFormer achieves similar performance to the baseline Transformer with 3.8 times and 1.8 times fewer parameters, and achieves 2.3 times speedup and 1.5 times speedup respectively, generally outperforming recent light-weight Transformers."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1906,
      "original_url": "https://aclanthology.org/2023.findings-acl.657/",
      "title": "Multi-hop Evidence Retrieval for Cross-document Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.657.pdf",
      "authors": [
        "Keming Lu",
        "I-Hung Hsu",
        "Wenxuan Zhou",
        "Mingyu Derek Ma",
        "Muhao Chen"
      ],
      "abstract": "Relation Extraction (RE) has been extended to cross-document scenarios because many relations are not simply described in a single document. This inevitably brings the challenge of efficient open-space evidence retrieval to support the inference of cross-document relations,along with the challenge of multi-hop reasoning on top of entities and evidence scattered in an open set of documents. To combat these challenges, we propose Mr.Cod (Multi-hop evidence retrieval for Cross-document relation extraction), which is a multi-hop evidence retrieval method based on evidence path mining and ranking. We explore multiple variants of retrievers to show evidence retrieval is essential in cross-document RE.We also propose a contextual dense retriever for this setting. Experiments on CodRED show that evidence retrieval with Mr.Cod effectively acquires cross-document evidence and boosts end-to-end RE performance in both closed and open settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1907,
      "original_url": "https://aclanthology.org/2023.findings-acl.658/",
      "title": "Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.658.pdf",
      "authors": [
        "Connor Baumler",
        "Anna Sotnikova",
        "Hal Daumé III"
      ],
      "abstract": "Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However, capturing disagreement can increase annotation time and expense. Fortunately, for many tasks, not all examples are equally controversial; we develop an active learning approach, Disagreement Aware Active Learning (DAAL) that concentrates annotations on examples where model entropy and annotator entropy are the most different. Because we cannot know the true entropy of annotations on unlabeled examples, we estimate a model that predicts annotator entropy trained using very few multiply-labeled examples. We find that traditional uncertainty-based active learning underperforms simple passive learning on tasks with high levels of disagreement, but that our active learning approach is able to successfully improve on passive and active baselines, reducing the number of annotations required by at least 24% on average across several datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1908,
      "original_url": "https://aclanthology.org/2023.findings-acl.659/",
      "title": "PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.659.pdf",
      "authors": [
        "Yixin Wan",
        "Kuan-Hao Huang",
        "Kai-Wei Chang"
      ],
      "abstract": "Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods on this task is costly, as all parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model’s encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). Comparing to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Comparing to existing prefix-tuning methods, PIP excels at capturing syntax control information, achieving significantly higher performance at the same level of learnable parameter count."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1909,
      "original_url": "https://aclanthology.org/2023.findings-acl.660/",
      "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.660.pdf",
      "authors": [
        "Fangyu Liu",
        "Julian Eisenschlos",
        "Francesco Piccinno",
        "Syrine Krichene",
        "Chenxi Pang",
        "Kenton Lee",
        "Mandar Joshi",
        "Wenhu Chen",
        "Nigel Collier",
        "Yasemin Altun"
      ],
      "abstract": "Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than thousands of data points, DePlot+LLM with just one-shot prompting achieves a 29.4% improvement over finetuned SOTA on human-written queries from the task of chart QA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1910,
      "original_url": "https://aclanthology.org/2023.findings-acl.661/",
      "title": "Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.661.pdf",
      "authors": [
        "Weize Chen",
        "Xu Han",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
      ],
      "abstract": "Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states. Since it is non-trivial to directly model the intermediate states and design a running cost function, we propose to use latent stochastic bridges to regularize the intermediate states and use the regularization as the running cost of PETs. As the first work to propose regularized PETs that use stochastic bridges as the regularizers (running costs) for the intermediate states, we show the effectiveness and generality of this regularization across different tasks, PLMs and PETs. In view of the great potential and capacity, we believe more sophisticated regularizers can be designed for PETs and better performance can be achieved in the future."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1911,
      "original_url": "https://aclanthology.org/2023.findings-acl.662/",
      "title": "Learning from a Friend: Improving Event Extraction via Self-Training with Feedback from Abstract Meaning Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.662.pdf",
      "authors": [
        "Zhiyang Xu",
        "Jay Yoon Lee",
        "Lifu Huang"
      ],
      "abstract": "Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions as pseudo training samples, and (2) a novel scoring model that takes in each new predicted event trigger, an argument, its argument role, as well as their paths in the AMR graph to estimate a compatibility score indicating the correctness of the pseudo label. The compatibility scores further act as feedback to encourage or discourage the model learning on the pseudo labels during self-training. Experimental results on three benchmark datasets, including ACE05-E, ACE05-E+, and ERE, demonstrate the effectiveness of the STF framework on event extraction, especially event argument extraction, with significant performance gain over the base event extraction models and strong baselines. Our experimental analysis further shows that STF is a generic framework as it can be applied to improve most, if not all, event extraction models by leveraging large-scale unlabeled data, even when high-quality AMR graph annotations are not available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1912,
      "original_url": "https://aclanthology.org/2023.findings-acl.663/",
      "title": "How Well Do Large Language Models Perform on Faux Pas Tests?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.663.pdf",
      "authors": [
        "Natalie Shapira",
        "Guy Zwirn",
        "Yoav Goldberg"
      ],
      "abstract": "Motivated by the question of the extent to which large language models “understand” social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (“the ELIZA effect”). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1913,
      "original_url": "https://aclanthology.org/2023.findings-acl.664/",
      "title": "Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.664.pdf",
      "authors": [
        "Wangchunshu Zhou",
        "Ronan Le Bras",
        "Yejin Choi"
      ],
      "abstract": "Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible sequence-to-sequence model compression. Modular Transformers trains modularized layers that have the same function of two or more consecutive layers in the original model via module replacing and knowledge distillation. After training, the modularized layers can be flexibly assembled into sequence-to-sequence models that meet different performance-efficiency trade-offs. Experimental results show that after a single training phase, by simply varying the assemble strategy, Modular Transformers can achieve flexible compression ratios from 1.1x to 6x with little to moderate relative performance drop."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1914,
      "original_url": "https://aclanthology.org/2023.findings-acl.665/",
      "title": "ISLTranslate: Dataset for Translating Indian Sign Language",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.665.pdf",
      "authors": [
        "Abhinav Joshi",
        "Susmit Agrawal",
        "Ashutosh Modi"
      ],
      "abstract": "Sign languages are the primary means of communication for many hard-of-hearing people worldwide. Recently, to bridge the communication gap between the hard-of-hearing community and the rest of the population, several sign language translation datasets have been proposed to enable the development of statistical sign language translation systems. However, there is a dearth of sign language resources for the Indian sign language. This resource paper introduces ISLTranslate, a translation dataset for continuous Indian Sign Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best of our knowledge, it is the largest translation dataset for continuous Indian Sign Language. We provide a detailed analysis of the dataset. To validate the performance of existing end-to-end Sign language to spoken language translation systems, we benchmark the created dataset with a transformer-based model for ISL translation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1915,
      "original_url": "https://aclanthology.org/2023.findings-acl.666/",
      "title": "LMentry: A Language Model Benchmark of Elementary Language Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.666.pdf",
      "authors": [
        "Avia Efrat",
        "Or Honovich",
        "Omer Levy"
      ],
      "abstract": "As the performance of large language models rapidly improves, benchmarks are getting larger and more complex as well. We present LMentry, a benchmark that avoids this “arms race” by focusing on a compact set of tasks that are trivial to humans, e.g. writing a sentence containing a specific word, identifying which words in a list belong to a specific category, or choosing which of two words is longer.LMentry is specifically designed to provide quick and interpretable insights into the capabilities and robustness of large language models. Our experiments reveal a wide variety of failure cases that, while immediately obvious to humans, pose a considerable challenge for large language models, including OpenAI’s latest 175B-parameter instruction-tuned model, TextDavinci002.LMentry complements contemporary evaluation approaches of large language models, providing a quick, automatic, and easy-to-run “unit test”, without resorting to large benchmark suites of complex tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1916,
      "original_url": "https://aclanthology.org/2023.findings-acl.667/",
      "title": "Differentiable Instruction Optimization for Cross-Task Generalization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.667.pdf",
      "authors": [
        "Masaru Isonuma",
        "Junichiro Mori",
        "Ichiro Sakata"
      ],
      "abstract": "Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks. Although various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalization ability. This work presents instruction optimization, which optimizes training instructions with respect to generalization ability. Rather than manually tuning instructions, we introduce learnable instructions and optimize them with gradient descent by leveraging bilevel optimization. Experimental results show that the learned instruction enhances the diversity of instructions and improves the generalization ability compared to using only manually created instructions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1917,
      "original_url": "https://aclanthology.org/2023.findings-acl.668/",
      "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.668.pdf",
      "authors": [
        "Zhanming Jie",
        "Wei Lu"
      ],
      "abstract": "Chain-of-thought (CoT) prompting with large language models has proven effective in numerous natural language process tasks, but designing prompts that generalize well to diverse problem types can be challenging CITATION, especially in the context of math word problem solving. Additionally, it is common to have a large amount of training data that have a better diversity coverage but CoT annotations are not available, which limits the use of supervised learning techniques. To address these issues, we investigate two approaches to leverage the training data in few-shot prompting scenario: dynamic program prompting and program distillation.Our approach is largely inspired by CITATION where they proposed to replace the CoT with the programs as the intermediate reasoning step. Such a prompting strategy allows us to accurately verify the answer correctness through program execution in MWP solving.Our dynamic program prompting involves annotating the training data by sampling correct programs from a large language model, while program distillation involves adapting a smaller model to the program-annotated training data.Our experiments on three standard MWP datasets demonstrate the effectiveness of these approaches, yielding significant improvements over previous baselines for prompting and fine-tuning.Our results suggest that leveraging a large amount of training data can improve the generalization ability of prompts and boost the performance of fine-tuned smaller models in MWP solving."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1918,
      "original_url": "https://aclanthology.org/2023.findings-acl.669/",
      "title": "How does the task complexity of masked pretraining objectives affect downstream performance?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.669.pdf",
      "authors": [
        "Atsuki Yamaguchi",
        "Hiroaki Ozaki",
        "Terufumi Morishita",
        "Gaku Morio",
        "Yasuhiro Sogawa"
      ],
      "abstract": "Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective from the task complexity perspective."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1919,
      "original_url": "https://aclanthology.org/2023.findings-acl.670/",
      "title": "AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.670.pdf",
      "authors": [
        "Yu Lu",
        "Junwei Bao",
        "Zichen Ma",
        "Xiaoguang Han",
        "Youzheng Wu",
        "Shuguang Cui",
        "Xiaodong He"
      ],
      "abstract": "High-quality data is essential for conversational recommendation systems and serves as the cornerstone of the network architecture development and training strategy design. Existing works contribute heavy human efforts to manually labeling or designing and extending recommender dialogue templates. However, they suffer from: (i) the limited number of human annotators results in datasets can hardly capture rich and large-scale cases in the real world, (ii) the limited experience and knowledge of annotators accounts for the uninformative corpus and inappropriate recommendations. In this paper, we propose a novel automatic dataset synthesis approach that can generate large-scale and high-quality recommendation dialogues through a data2text generation process, where unstructured recommendation conversations are generated from structured graphs based on user-item information from the real world. In doing so, we comprehensively exploit: (i) rich personalized user profiles from traditional recommendation datasets, (ii) rich external knowledge from knowledge graphs, and (iii) the conversation ability contained in human-to-human conversational recommendation datasets. Extensive experiments validate the benefit brought by the automatically synthesized data under low-resource scenarios, and demonstrate the promising potential to facilitate developing a more effective conversational recommendation system."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1920,
      "original_url": "https://aclanthology.org/2023.findings-acl.671/",
      "title": "Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.671.pdf",
      "authors": [
        "Hongru Liang",
        "Jia Liu",
        "Weihong Du",
        "Dingnan Jin",
        "Wenqiang Lei",
        "Zujie Wen",
        "Jiancheng Lv"
      ],
      "abstract": "The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how & knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the sTeps and fActs in a gRAh (TARA), which supports a unified inference of various questions. Towards a systematical benchmarking study, we design a heuristic method to automatically parse user manuals into TARAs and build an annotated dataset to test the model’s ability in answering real-world questions. Empirical results demonstrate that representing user manuals as TARAs is a desired solution for the MRC of user manuals. An in-depth investigation of TARA further sheds light on the issues and broader impacts of future representations of user manuals. We hope our work can move the MRC of user manuals to a more complex and realistic stage."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1921,
      "original_url": "https://aclanthology.org/2023.findings-acl.672/",
      "title": "Deep Span Representations for Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.672.pdf",
      "authors": [
        "Enwei Zhu",
        "Yiyang Liu",
        "Jinpeng Li"
      ],
      "abstract": "Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics. With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on six NER benchmarks. Experimental results verify the importance of the depth for span representations, and show that DSpERT performs particularly well on long-span entities and nested structures. Further, the deep span representations are well structured and easily separable in the feature space."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1922,
      "original_url": "https://aclanthology.org/2023.findings-acl.673/",
      "title": "Disambiguated Lexically Constrained Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.673.pdf",
      "authors": [
        "Jinpeng Zhang",
        "Nini Xiao",
        "Ke Wang",
        "Chuanqi Dong",
        "Xiangyu Duan",
        "Yuqi Zhang",
        "Min Zhang"
      ],
      "abstract": "Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This assumption limits their application to real-world scenarios where a source lexicon may have multiple target constraints, and disambiguation is needed to select the most suitable one. In this paper, we propose disambiguated LCNMT (D-LCNMT) to solve the problem. D-LCNMT is a robust and effective two-stage framework that disambiguates the constraints based on contexts at first, then integrates the disambiguated constraints into LCNMT. Experimental results show that our approach outperforms strong baselines including existing data argumentation based approaches on benchmark datasets, and comprehensive experiments in scenarios where a source lexicon corresponds to multiple target constraints demonstrate the constraint disambiguation superiority of our approach."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1923,
      "original_url": "https://aclanthology.org/2023.findings-acl.674/",
      "title": "Curating Datasets for Better Performance with Example Training Dynamics",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.674.pdf",
      "authors": [
        "Aviad Sar-Shalom",
        "Roy Schwartz"
      ],
      "abstract": "The landscape of NLP research is dominated by large-scale models training on colossal datasets, relying on data quantity rather than quality. As an alternative to this landscape, we propose a method for weighing the relative importance of examples in a dataset based on their Example Training dynamics (swayamdipta et al., 2020) — a set of metrics computed during training. We propose a new way of computing the ETD of a dataset, and show that they can be used to improve performance in both in-distribution and out-of-distribution testing. We show that ETD can be transferable, i.e., they can be computed once and used for training different models, effectively reducing their computation cost. Finally, we suggest an active learning approach for computing ETD during training rather than as a preprocessing step — an approach that is not as effective, but dramatically reduces the extra computational costs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1924,
      "original_url": "https://aclanthology.org/2023.findings-acl.675/",
      "title": "Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.675.pdf",
      "authors": [
        "Inigo Urteaga",
        "Moulay Zaidane Draidia",
        "Tomer Lancewicki",
        "Shahram Khadivi"
      ],
      "abstract": "We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters.We propose a multi-armed bandit framework for the sequential selection of pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fewer epochs, across a variety of settings. In addition, GP-TS pre-trained TLMs attain competitive downstream performance, while avoiding expensive hyperparameter grid search. GP-TS provides an interactive framework for efficient and optimized TLM pre-training that, by circumventing costly hyperparameter selection, enables substantial computational savings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1925,
      "original_url": "https://aclanthology.org/2023.findings-acl.676/",
      "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.676.pdf",
      "authors": [
        "Yekun Chai",
        "Shuohuan Wang",
        "Chao Pang",
        "Yu Sun",
        "Hao Tian",
        "Hua Wu"
      ],
      "abstract": "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-to-text generation. We further show its advantage of zero-shot prompting on multilingual code summarization and text-to-text translation. We release our code and pre-trained checkpoints."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1926,
      "original_url": "https://aclanthology.org/2023.findings-acl.677/",
      "title": "PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.677.pdf",
      "authors": [
        "Xiangjue Dong",
        "Yun He",
        "Ziwei Zhu",
        "James Caverlee"
      ],
      "abstract": "A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user’s goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target any DST. Through experiments over state-of-the-art DSTs, the proposed framework leads to the greatest reduction in accuracy and the best attack success rate while maintaining good fluency and a low perturbation ratio. We also show how much the generated adversarial examples can bolster a DST through adversarial training. These results indicate the strength of prompt-based attacks on DSTs and leave open avenues for continued refinement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1927,
      "original_url": "https://aclanthology.org/2023.findings-acl.678/",
      "title": "Understanding Programs by Exploiting (Fuzzing) Test Cases",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.678.pdf",
      "authors": [
        "Jianyu Zhao",
        "Yuyang Rong",
        "Yiwen Guo",
        "Yifeng He",
        "Hao Chen"
      ],
      "abstract": "Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Hence, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of most part of the code, we resort to fuzz testing and propose fuzz tuning to boost the performance of program understanding and code representation learning, given a pre-trained LLM. The effectiveness of the proposed method is verified on two program understanding tasks including code clone detection and code classification, and it outperforms current state-of-the-arts by large margins. Code is available at https://github.com/rabbitjy/FuzzTuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1928,
      "original_url": "https://aclanthology.org/2023.findings-acl.679/",
      "title": "Hybrid Hierarchical Retrieval for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.679.pdf",
      "authors": [
        "Manoj Ghuhan Arivazhagan",
        "Lan Liu",
        "Peng Qi",
        "Xinchi Chen",
        "William Yang Wang",
        "Zhiheng Huang"
      ],
      "abstract": "Retrieval accuracy is crucial to the performance of open-domain question answering (ODQA) systems. Recent work has demonstrated that dense hierarchical retrieval (DHR), which retrieves document candidates first and then relevant passages from the refined document set, can significantly outperform the single stage dense passage retriever (DPR). While effective, this approach requires document structure information to learn document representation and is hard to adopt to other domains without this information. Additionally, the dense retrievers tend to generalize poorly on out-of-domain data comparing with sparse retrievers such as BM25. In this paper, we propose Hybrid Hierarchical Retrieval (HHR) to address the existing limitations. Instead of relying solely on dense retrievers, we can apply sparse retriever, dense retriever, and a combination of them in both stages of document and passage retrieval. We perform extensive experiments on ODQA benchmarks and observe that our framework not only brings in-domain gains, but also generalizes better to zero-shot TriviaQA and Web Questions datasets with an average of 4.69% improvement on recall@100 over DHR. We also offer practical insights to trade off between retrieval accuracy, latency, and storage cost. The code is available on github."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1929,
      "original_url": "https://aclanthology.org/2023.findings-acl.680/",
      "title": "Coherent or Not? Stressing a Neural Language Model for Discourse Coherence in Multiple Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.680.pdf",
      "authors": [
        "Dominique Brunato",
        "Felice Dell’Orletta",
        "Irene Dini",
        "Andrea Amelio Ravelli"
      ],
      "abstract": "In this study, we investigate the capability of a Neural Language Model (NLM) to distinguish between coherent and incoherent text, where the latter has been artificially created to gradually undermine local coherence within text. While previous research on coherence assessment using NLMs has primarily focused on English, we extend our investigation to multiple languages. We employ a consistent evaluation framework to compare the performance of monolingual and multilingual models in both in-domain and out-domain settings. Additionally, we explore the model’s performance in a cross-language scenario."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1930,
      "original_url": "https://aclanthology.org/2023.findings-acl.681/",
      "title": "Understanding Differential Search Index for Text Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.681.pdf",
      "authors": [
        "Xiaoyang Chen",
        "Yanjiang Liu",
        "Ben He",
        "Le Sun",
        "Yingfei Sun"
      ],
      "abstract": "The Differentiable Search Index (DSI) is a novel information retrieval (IR) framework that utilizes a differentiable function to generate a sorted list of document identifiers in response to a given query. However, due to the black-box nature of the end-to-end neural architecture, it remains to be understood to what extent DSI possesses the basic indexing and retrieval abilities. To mitigate this gap, in this study, we define and examine three important abilities that a functioning IR framework should possess, namely, exclusivity, completeness, and relevance ordering. Our analytical experimentation shows that while DSI demonstrates proficiency in memorizing the unidirectional mapping from pseudo queries to document identifiers, it falls short in distinguishing relevant documents from random ones, thereby negatively impacting its retrieval effectiveness. To address this issue, we propose a multi-task distillation approach to enhance the retrieval quality without altering the structure of the model and successfully endow it with improved indexing abilities. Through experiments conducted on various datasets, we demonstrate that our proposed method outperforms previous DSI baselinesThe code and data for this work can be found at https://github.com/VerdureChen/Understang_DSI."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1931,
      "original_url": "https://aclanthology.org/2023.findings-acl.682/",
      "title": "Masked Audio Text Encoders are Effective Multi-Modal Rescorers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.682.pdf",
      "authors": [
        "Jinglun Cai",
        "Monica Sunkara",
        "Xilai Li",
        "Anshu Bhatia",
        "Xiao Pan",
        "Sravan Bodapati"
      ],
      "abstract": "Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours) MATE achieves a WER reduction of 8%-23% over the first-pass baseline."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1932,
      "original_url": "https://aclanthology.org/2023.findings-acl.683/",
      "title": "Replace and Report: NLP Assisted Radiology Report Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.683.pdf",
      "authors": [
        "Kaveri Kale",
        "Pushpak Bhattacharyya",
        "Kshitij Jadhav"
      ],
      "abstract": "Clinical practice frequently uses medical imaging for diagnosis and treatment. A significant challenge for automatic radiology report generation is that the radiology reports are long narratives consisting of multiple sentences for both abnormal and normal findings. Therefore, applying conventional image captioning approaches to generate the whole report proves to be insufficient, as these are designed to briefly describe images with short sentences. We propose a template-based approach to generate radiology reports from radiographs. Our approach involves the following: i) using a multilabel image classifier, produce the tags for the input radiograph; ii) using a transformer-based model, generate pathological descriptions (a description of abnormal findings seen on radiographs) from the tags generated in step (i); iii) using a BERT-based multi-label text classifier, find the spans in the normal report template to replace with the generated pathological descriptions; and iv) using a rule-based system, replace the identified span with the generated pathological description. We performed experiments with the two most popular radiology report datasets, IU Chest X-ray and MIMIC-CXR and demonstrated that the BLEU-1, ROUGE-L, METEOR, and CIDEr scores are better than the State-of-the-Art models by 25%, 36%, 44% and 48% respectively, on the IU X-RAY dataset. To the best of our knowledge, this is the first attempt to generate chest X-ray radiology reports by first creating small sentences for abnormal findings and then replacing them in the normal report template."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1933,
      "original_url": "https://aclanthology.org/2023.findings-acl.684/",
      "title": "Pre-trained Personalized Review Summarization with Effective Salience Estimation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.684.pdf",
      "authors": [
        "Hongyan Xu",
        "Hongtao Liu",
        "Zhepeng Lv",
        "Qing Yang",
        "Wenjun Wang"
      ],
      "abstract": "Personalized review summarization in recommender systems is a challenging task of generating condensed summaries for product reviews while preserving the salient content of reviews. Recently, Pretrained Language Models (PLMs) have become a new paradigm in text generation for the strong ability of natural language comprehension. However, it is nontrivial to apply PLMs in personalized review summarization directly since there are rich personalized information (e.g., user preferences and product characteristics) to be considered, which is crucial to the salience estimation of input review. In this paper, we propose a pre-trained personalized review summarization method, which aims to effectively incorporate the personalized information of users and products into the salience estimation of the input reviews. We design a personalized encoder that could identify the salient contents of the input sequence by jointly considering the semantic and personalized information respectively (i.e., ratings, user and product IDs, and linguistic features), yielding personalized representations for the input reviews and history summaries separately. Moreover, we design an interactive information selection mechanism that further identifies the salient contents of the input reviews and selects relative information from the history summaries. The results on real-world datasets show that our method performs better than the state-of-the-art baselines and could generate more readable summaries."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1934,
      "original_url": "https://aclanthology.org/2023.findings-acl.685/",
      "title": "CaPE: Contrastive Parameter Ensembling for Reducing Hallucination in Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.685.pdf",
      "authors": [
        "Prafulla Kumar Choubey",
        "Alex Fabbri",
        "Jesse Vig",
        "Chien-Sheng Wu",
        "Wenhao Liu",
        "Nazneen Rajani"
      ],
      "abstract": "Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on factual errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more effectively, utilizing variations in noise in training samples to reduce hallucination. Starting with a base model fine-tuned on an entire dataset, we additionally train expert and anti-expert models on clean and noisy subsets of the data, respectively. We then adjust the parameters of the base model by adding (subtracting) the parameters of the expert (anti-expert), advancing the recent work on additive parameter ensembling approaches. Trained on a much smaller data subset, expert and anti-expert models only fractionally (<14%) increases the total training time. Further, CaPE uses parameter ensembling and does not increase the inference time. Experimental results show that CaPE improves performance across different automatic factual metrics and human evaluation, with a maximum improvement of 16.69% and 15.38% on summary-level dependency-arc entailment accuracy for the XSUM and CNN/DM datasets. The CaPE model performs comparably to the base model on metrics of informativeness such as ROUGE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1935,
      "original_url": "https://aclanthology.org/2023.findings-acl.686/",
      "title": "OpineSum: Entailment-based self-training for abstractive opinion summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.686.pdf",
      "authors": [
        "Annie Louis",
        "Joshua Maynez"
      ],
      "abstract": "A typical product or place often has hundreds of reviews, and summarization of these texts is an important and challenging problem. Recent progress on abstractive summarization in domains such as news has been driven by supervised systems trained on hundreds of thousands of news articles paired with human-written summaries. However for opinion texts, such large scale datasets are rarely available. Unsupervised methods, self-training, and few-shot learning approaches bridge that gap. In this work, we present a novel self-training approach, OpineSum for abstractive opinion summarization. The self-training summaries in this approach are built automatically using a novel application of textual entailment and capture the consensus of opinions across the various reviews for an item. This method can be used to obtain silver-standard summaries on a large scale and train both unsupervised and few-shot abstractive summarization systems. OpineSum outperforms strong peer systems in both settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1936,
      "original_url": "https://aclanthology.org/2023.findings-acl.687/",
      "title": "A Call for Standardization and Validation of Text Style Transfer Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.687.pdf",
      "authors": [
        "Phil Ostheimer",
        "Mayank Kumar Nagda",
        "Marius Kloft",
        "Sophie Fellenz"
      ],
      "abstract": "Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and automated evaluation. In addition, we also find a validation gap: only few automated metrics have been validated using human experiments. To this end, we thoroughly scrutinize both the standardization and validation gap and reveal the resulting pitfalls. This work also paves the way to close the standardization and validation gap in TST evaluation by calling out requirements to be met by future research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1937,
      "original_url": "https://aclanthology.org/2023.findings-acl.688/",
      "title": "Bridging the Granularity Gap for Acoustic Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.688.pdf",
      "authors": [
        "Chen Xu",
        "Yuhao Zhang",
        "Chengbo Jiao",
        "Xiaoqian Liu",
        "Chi Hu",
        "Xin Zeng",
        "Tong Xiao",
        "Anxiang Ma",
        "Huizhen Wang",
        "Jingbo Zhu"
      ],
      "abstract": "While Transformer has become the de-facto standard for speech, modeling upon the fine-grained frame-level features remains an open challenge of capturing long-distance dependencies and distributing the attention weights. We propose Progressive Down-Sampling (PDS) which gradually compresses the acoustic features into coarser-grained units containing more complete semantic information, like text-level representation. In addition, we develop a representation fusion method to alleviate information loss that occurs inevitably during high compression. In this way, we compress the acoustic features into 1/32 of the initial length while achieving better or comparable performances on the speech recognition task. And as a bonus, it yields inference speedups ranging from 1.20x to 1.47x.By reducing the modeling burden, we also achieve competitive results when training on the more challenging speech translation task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1938,
      "original_url": "https://aclanthology.org/2023.findings-acl.689/",
      "title": "MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.689.pdf",
      "authors": [
        "Libo Qin",
        "Shijue Huang",
        "Qiguang Chen",
        "Chenran Cai",
        "Yudi Zhang",
        "Bin Liang",
        "Wanxiang Che",
        "Ruifeng Xu"
      ],
      "abstract": "Multi-modal sarcasm detection has attracted much recent attention. Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder the development of reliable multi-modal sarcasm detection system: (1) There are some spurious cues in MMSD, leading to the model bias learning; (2) The negative samples in MMSD are not always reasonable. To solve the aforementioned issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings of MMSD, by removing the spurious cues and re-annotating the unreasonable samples. Meanwhile, we present a novel framework called multi-view CLIP that is capable of leveraging multi-grained cues from multiple perspectives (i.e., text, image, and text-image interaction view) for multi-modal sarcasm detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for building reliable multi-modal sarcasm detection systems and multi-view CLIP can significantly outperform the previous best baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1939,
      "original_url": "https://aclanthology.org/2023.findings-acl.690/",
      "title": "Learn to Not Link: Exploring NIL Prediction in Entity Linking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.690.pdf",
      "authors": [
        "Fangwei Zhu",
        "Jifan Yu",
        "Hailong Jin",
        "Lei Hou",
        "Juanzi Li",
        "Zhifang Sui"
      ],
      "abstract": "Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem.NEL takes ambiguous entities as seeds, collects relevant mention context in the Wikipedia corpus, and ensures the presence of mentions linking to NIL by human annotation and entity masking. We conduct a series of experiments with the widely used bi-encoder and cross-encoder entity linking models, results show that both types of NIL mentions in training data have a significant influence on the accuracy of NIL prediction. Our code and dataset can be accessed at https://github.com/solitaryzero/NIL_EL."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1940,
      "original_url": "https://aclanthology.org/2023.findings-acl.691/",
      "title": "On Text-based Personality Computing: Challenges and Future Directions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.691.pdf",
      "authors": [
        "Qixiang Fang",
        "Anastasia Giachanou",
        "Ayoub Bagheri",
        "Laura Boeschoten",
        "Erik-Jan van Kesteren",
        "Mahdi Shafiee Kamalabad",
        "Daniel Oberski"
      ],
      "abstract": "Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the NLP research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, datasets, performance evaluation, modelling choices, as well as ethics and fairness. When addressing each challenge, not only do we combine perspectives from both NLP and social sciences, but also offer concrete suggestions. We hope to inspire more valid and reliable TPC research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1941,
      "original_url": "https://aclanthology.org/2023.findings-acl.692/",
      "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.692.pdf",
      "authors": [
        "Chaofan Tao",
        "Lu Hou",
        "Haoli Bai",
        "Jiansheng Wei",
        "Xin Jiang",
        "Qun Liu",
        "Ping Luo",
        "Ngai Wong"
      ],
      "abstract": "The increasing sizes of large generative Pre-trained Language Models (PLMs) hinder their deploymentin real-world applications. To obtain efficient PLMs, previous studies mostly focus on pruning the attention heads and feed-forward networks (FFNs) of the Transformer. Nevertheless, we find that in generative PLMs, the hidden dimension shared by many other modules (e.g., embedding layer and layer normalization) contains persistent outliers regardless of the network input. This study comprehensively investigates the structured pruning of generative PLMs with all the above compressible components. To identify redundant network structures, we assign learnable masks over compressible components followed by sparse training. Various sizes of PLMs can be flexibly extracted via different thresholds, and are then task-specifically fine-tuned for further improvement. Extensive experiments on language modeling, summarization and machine translation validate the effectiveness of the proposed method. For example, the pruned BART brings 1.51x/6.96x inference speedup on GPU/CPU with 67% size reduction, and can be further combined with quantization for more than 25× compression."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1942,
      "original_url": "https://aclanthology.org/2023.findings-acl.693/",
      "title": "Prompt-Guided Retrieval Augmentation for Non-Knowledge-Intensive Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.693.pdf",
      "authors": [
        "Zhicheng Guo",
        "Sijie Cheng",
        "Yile Wang",
        "Peng Li",
        "Yang Liu"
      ],
      "abstract": "Retrieval-augmented methods have received increasing attention to support downstream tasks by leveraging useful information from external resources. Recent studies mainly focus on exploring retrieval to solve knowledge-intensive (KI) tasks. However, the potential of retrieval for most non-knowledge-intensive (NKI) tasks remains under-explored. There are two main challenges to leveraging retrieval-augmented methods for NKI tasks: 1) the demand for diverse relevance score functions and 2) the dilemma between training cost and task performance. To address these challenges, we propose a two-stage framework for NKI tasks, named PGRA. In the first stage, we adopt a task-agnostic retriever to build a shared static index and select candidate evidence efficiently. In the second stage, we design a prompt-guided reranker to rerank the nearest evidence according to task-specific relevance for the reader. Experimental results show that PGRA outperforms other state-of-the-art retrieval-augmented methods. Our analyses further investigate the influence factors to model performance and demonstrate the generality of PGRA. The code and model will be released for further research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1943,
      "original_url": "https://aclanthology.org/2023.findings-acl.694/",
      "title": "Contextualized Semantic Distance between Highly Overlapped Texts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.694.pdf",
      "authors": [
        "Letian Peng",
        "Zuchao Li",
        "Hai Zhao"
      ],
      "abstract": "Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system’s understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions in their positions. Our metric, Neighboring Distribution Divergence (NDD), represents the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, especially on highly overlapped paired texts. Based on the discovery, we further implement an unsupervised and training-free method for text compression, leading to a significant improvement on the previous perplexity-based method. The high compression rate controlling ability of our method even enables NDD to outperform the supervised state-of-the-art in domain adaption by a huge margin. Further experiments on syntax and semantics analyses verify the awareness of internal sentence structures, indicating the high potential of NDD for further studies."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1944,
      "original_url": "https://aclanthology.org/2023.findings-acl.695/",
      "title": "Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.695.pdf",
      "authors": [
        "Yibin Lei",
        "Liang Ding",
        "Yu Cao",
        "Changtong Zan",
        "Andrew Yates",
        "Dacheng Tao"
      ],
      "abstract": "Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at https://github.com/Yibin-Lei/ReContriever."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1945,
      "original_url": "https://aclanthology.org/2023.findings-acl.696/",
      "title": "Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.696.pdf",
      "authors": [
        "Maitrey Mehta",
        "Vivek Srikumar"
      ],
      "abstract": "Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use. While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators. This norm assumes that multiple annotators are available, which is not the case for highly specialized tasks or low-resource languages. In this paper, we ask: Can we evaluate the quality of a dataset constructed by a single human annotator? To address this question, we propose four weak verifiers to help estimate dataset quality, and outline when each may be employed. We instantiate these strategies for the task of semantic analysis of adpositions in Gujarati, a low-resource language, and show that our weak verifiers concur with a double-annotation study. As an added contribution, we also release the first dataset with semantic annotations in Gujarati along with several model baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1946,
      "original_url": "https://aclanthology.org/2023.findings-acl.697/",
      "title": "Reinforced Active Learning for Low-Resource, Domain-Specific, Multi-Label Text Classification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.697.pdf",
      "authors": [
        "Lukas Wertz",
        "Jasmina Bogojeska",
        "Katsiaryna Mirylenka",
        "Jonas Kuhn"
      ],
      "abstract": "Text classification datasets from specialised or technical domains are in high demand, especially in industrial applications. However, due to the high cost of annotation such datasets are usually expensive to create. While Active Learning (AL) can reduce the labeling cost, required AL strategies are often only tested on general knowledge domains and tend to use information sources that are not consistent across tasks. We propose Reinforced Active Learning (RAL) to train a Reinforcement Learning policy that utilizes many different aspects of the data and the task in order to select the most informative unlabeled subset dynamically over the course of the AL procedure. We demonstrate the superior performance of the proposed RAL framework compared to strong AL baselines across four intricate multi-class, multi-label text classification datasets taken from specialised domains. In addition, we experiment with a unique data augmentation approach to further reduce the number of samples RAL needs to annotate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1947,
      "original_url": "https://aclanthology.org/2023.findings-acl.698/",
      "title": "Improving Classroom Dialogue Act Recognition from Limited Labeled Data with Self-Supervised Contrastive Learning Classifiers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.698.pdf",
      "authors": [
        "Vikram Kumaran",
        "Jonathan Rowe",
        "Bradford Mott",
        "Snigdha Chaturvedi",
        "James Lester"
      ],
      "abstract": "Recognizing classroom dialogue acts has significant promise for yielding insight into teaching, student learning, and classroom dynamics. However, obtaining K-12 classroom dialogue data with labels is a significant challenge, and therefore, developing data-efficient methods for classroom dialogue act recognition is essential. This work addresses the challenge of classroom dialogue act recognition from limited labeled data using a contrastive learning-based self-supervised approach (SSCon). SSCon uses two independent models that iteratively improve each other’s performance by increasing the accuracy of dialogue act recognition and minimizing the embedding distance between the same dialogue acts. We evaluate the approach on three complementary dialogue act recognition datasets: the TalkMoves dataset (annotated K-12 mathematics lesson transcripts), the DailyDialog dataset (multi-turn daily conversation dialogues), and the Dialogue State Tracking Challenge 2 (DSTC2) dataset (restaurant reservation dialogues). Results indicate that our self-supervised contrastive learning-based model outperforms competitive baseline models when trained with limited examples per dialogue act. Furthermore, SSCon outperforms other few-shot models that require considerably more labeled data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1948,
      "original_url": "https://aclanthology.org/2023.findings-acl.699/",
      "title": "Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.699.pdf",
      "authors": [
        "Linjun Li",
        "Tao Jin",
        "Xize Cheng",
        "Ye Wang",
        "Wang Lin",
        "Rongjie Huang",
        "Zhou Zhao"
      ],
      "abstract": "Visual temporal-aligned translation aims to transform the visual sequence into natural words, including important applicable tasks such as lipreading and fingerspelling recognition. However, various performance habits of specific words by different speakers or signers can lead to visual ambiguity, which has become a major obstacle to the development of current methods. Considering the constraints above, the generalization ability of the translation system is supposed to be further explored through the evaluation results on unseen performers. In this paper, we develop a novel generalizable framework named Contrastive Token-Wise Meta-learning (CtoML), which strives to transfer recognition skills to unseen performers. To the best of our knowledge, employing meta-learning methods directly in the image domain poses two main challenges, and we propose corresponding strategies. First, sequence prediction in visual temporal-aligned translation, which aims to generate multiple words autoregressively, is different from the vanilla classification. Thus, we devise the token-wise diversity-aware weights for the meta-train stage, which encourages the model to make efforts on those ambiguously recognized tokens. Second, considering the consistency of word-visual prototypes across different domains, we develop two complementary global and local contrastive losses to maintain inter-class relationships and promote domain-independent. We conduct extensive experiments on the widely-used lipreading dataset GRID and the fingerspelling dataset ChicagoFSWild, and the experimental results show the effectiveness of our proposed CtoML over existing state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1949,
      "original_url": "https://aclanthology.org/2023.findings-acl.700/",
      "title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.700.pdf",
      "authors": [
        "Meng Zhou",
        "Xin Li",
        "Yue Jiang",
        "Lidong Bing"
      ],
      "abstract": "Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. hao and Schütze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of fine-tuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1950,
      "original_url": "https://aclanthology.org/2023.findings-acl.701/",
      "title": "Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.701.pdf",
      "authors": [
        "Alex Mei",
        "Sharon Levy",
        "William Yang Wang"
      ],
      "abstract": "Users’ physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show that FARM obtains state-of-the-art results on the SafeText dataset, showing absolute improvement in safety classification accuracy by 5.9%."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1951,
      "original_url": "https://aclanthology.org/2023.findings-acl.702/",
      "title": "Multijugate Dual Learning for Low-Resource Task-Oriented Dialogue System",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.702.pdf",
      "authors": [
        "Shimin Li",
        "Xiaotian Zhang",
        "Yanjun Zheng",
        "Linyang Li",
        "Xipeng Qiu"
      ],
      "abstract": "Dialogue data in real scenarios tend to be sparsely available, rendering data-starved end-to-end dialogue systems trained inadequately. We discover that data utilization efficiency in low-resource scenarios can be enhanced by mining alignment information uncertain utterance and deterministic dialogue state. Therefore, we innovatively implement dual learning in task-oriented dialogues to exploit the correlation of heterogeneous data. In addition, the one-to-one duality is converted into a multijugate duality to reduce the influence of spurious correlations in dual training for generalization. Without introducing additional parameters, our method could be implemented in arbitrary networks. Extensive empirical analyses demonstrate that our proposed method improves the effectiveness of end-to-end task-oriented dialogue systems under multiple benchmarks and obtains state-of-the-art results in low-resource scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1952,
      "original_url": "https://aclanthology.org/2023.findings-acl.703/",
      "title": "A Class-Rebalancing Self-Training Framework for Distantly-Supervised Named Entity Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.703.pdf",
      "authors": [
        "Qi Li",
        "Tingyu Xie",
        "Peng Peng",
        "Hongwei Wang",
        "Gaoang Wang"
      ],
      "abstract": "Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-training is dominated by the high-performance class in selecting candidates, and deteriorates the low-performance class with the bias of generated pseudo label. To address the class-level imbalance performance, we propose a class-rebalancing self-training framework for improving the distantly-supervised named entity recognition. In candidate selection, a class-wise flexible threshold is designed to fully explore other classes besides the high-performance class. In label generation, injecting the distant label, a hybrid pseudo label is adopted to provide straight semantic information for the low-performance class. Experiments on five flat and two nested datasets show that our model achieves state-of-the-art results. We also conduct extensive research to analyze the effectiveness of the flexible threshold and the hybrid pseudo label."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1953,
      "original_url": "https://aclanthology.org/2023.findings-acl.704/",
      "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.704.pdf",
      "authors": [
        "Swarnadeep Saha",
        "Xinyan Yu",
        "Mohit Bansal",
        "Ramakanth Pasunuru",
        "Asli Celikyilmaz"
      ],
      "abstract": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. The tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1954,
      "original_url": "https://aclanthology.org/2023.findings-acl.705/",
      "title": "Learning by Analogy: Diverse Questions Generation in Math Word Problem",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.705.pdf",
      "authors": [
        "Zihao Zhou",
        "Maizhen Ning",
        "Qiufeng Wang",
        "Jie Yao",
        "Wei Wang",
        "Xiaowei Huang",
        "Kaizhu Huang"
      ],
      "abstract": "Solving math word problem (MWP) with AI techniques has recently made great progress with the success of deep neural networks (DNN), but it is far from being solved. We argue that the ability of learning by analogy is essential for an MWP solver to better understand same problems which may typically be formulated in diverse ways. However most existing works exploit the shortcut learning to train MWP solvers simply based on samples with a single question. In lack of diverse questions, these methods merely learn shallow heuristics. In this paper, we make a first attempt to solve MWPs by generating diverse yet consistent questions/equations. Given a typical MWP including the scenario description, question, and equation (i.e., answer), we first generate multiple consistent equations via a group of heuristic rules. We then feed them to a question generator together with the scenario to obtain the corresponding diverse questions, forming a new MWP with a variety of questions and equations. Finally we engage a data filter to remove those unreasonable MWPs, keeping the high-quality augmented ones. To evaluate the ability of learning by analogy for an MWP solver, we generate a new MWP dataset (called DiverseMath23K) with diverse questions by extending the current benchmark Math23K. Extensive experimental results demonstrate that our proposed method can generate high-quality diverse questions with corresponding equations, further leading to performance improvement on Diverse-Math23K. The code and dataset is available at: https://github.com/zhouzihao501/DiverseMWP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1955,
      "original_url": "https://aclanthology.org/2023.findings-acl.706/",
      "title": "Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs. Continual Pre-training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.706.pdf",
      "authors": [
        "Haode Zhang",
        "Haowen Liang",
        "Li-Ming Zhan",
        "Xiao-Ming Wu",
        "Albert Y.S. Lam"
      ],
      "abstract": "We consider the task of few-shot intent detection, which involves training a deep learning model to classify utterances based on their underlying intents using only a small amount of labeled data. The current approach to address this problem is through continual pre-training, i.e., fine-tuning pre-trained language models (PLMs) on external resources (e.g., conversational corpora, public intent detection datasets, or natural language understanding datasets) before using them as utterance encoders for training an intent classifier. In this paper, we show that continual pre-training may not be essential, since the overfitting problem of PLMs on this task may not be as serious as expected. Specifically, we find that directly fine-tuning PLMs on only a handful of labeled examples already yields decent results compared to methods that employ continual pre-training, and the performance gap diminishes rapidly as the number of labeled data increases. To maximize the utilization of the limited available data, we propose a context augmentation method and leverage sequential self-distillation to boost performance. Comprehensive experiments on real-world benchmarks show that given only two or more labeled samples per class, direct fine-tuning outperforms many strong baselines that utilize external data sources for continual pre-training. The code can be found at https://github.com/hdzhang-code/DFTPlus."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1956,
      "original_url": "https://aclanthology.org/2023.findings-acl.707/",
      "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.707.pdf",
      "authors": [
        "Qinyuan Cheng",
        "Xiaogui Yang",
        "Tianxiang Sun",
        "Linyang Li",
        "Xipeng Qiu"
      ],
      "abstract": "Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings.However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve Contrastive Learning of sentence embeddings from AI Feedback (CLAIF).Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings.Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1957,
      "original_url": "https://aclanthology.org/2023.findings-acl.708/",
      "title": "Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.708.pdf",
      "authors": [
        "Haipeng Sun",
        "Junwei Bao",
        "Youzheng Wu",
        "Xiaodong He"
      ],
      "abstract": "Traditional end-to-end task-oriented dialog systems first convert dialog context into belief state and action state before generating the system response. The system response performance is significantly affected by the quality of the belief state and action state. We first explore what dialog context representation is beneficial to improving the quality of the belief state and action state, which further enhances the generated response quality. To tackle our exploration, we propose Mars, an end-to-end task-oriented dialog system with two contrastive learning strategies to model the relationship between dialog context and belief/action state representations. Empirical results show dialog context representations, which are more different from semantic state representations, are more conducive to multi-turn task-oriented dialog. Moreover, our proposed Mars achieves state-of-the-art performance on the MultiWOZ 2.0, CamRest676, and CrossWOZ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1958,
      "original_url": "https://aclanthology.org/2023.findings-acl.709/",
      "title": "Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.709.pdf",
      "authors": [
        "Pengcheng Jiang",
        "Shivam Agarwal",
        "Bowen Jin",
        "Xuan Wang",
        "Jimeng Sun",
        "Jiawei Han"
      ],
      "abstract": "The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TagReal that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TagReal achieves state-of-the-art performance on two benchmark datasets. We find that TagReal has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1959,
      "original_url": "https://aclanthology.org/2023.findings-acl.710/",
      "title": "Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.710.pdf",
      "authors": [
        "Wei-Jen Ko",
        "Yating Wu",
        "Cutter Dalton",
        "Dananjay Srinivas",
        "Greg Durrett",
        "Junyi Jessy Li"
      ],
      "abstract": "Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Human evaluation results show that QUD dependency parsing is possible for language models trained with this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1960,
      "original_url": "https://aclanthology.org/2023.findings-acl.711/",
      "title": "An Integrated Approach for Political Bias Prediction and Explanation Based on Discursive Structure",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.711.pdf",
      "authors": [
        "Nicolas Devatine",
        "Philippe Muller",
        "Chloé Braud"
      ],
      "abstract": "One crucial aspect of democracy is fair information sharing. While it is hard to prevent biases in news, they should be identified for better transparency. We propose an approach to automatically characterize biases that takes into account structural differences and that is efficient for long texts. This yields new ways to provide explanations for a textual classifier, going beyond mere lexical cues. We show that: (i) the use of discourse-based structure-aware document representations compare well to local, computationally heavy, or domain-specific models on classification tasks that deal with textual bias (ii) our approach based on different levels of granularity allows for the generation of better explanations of model decisions, both at the lexical and structural level, while addressing the challenge posed by long texts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1961,
      "original_url": "https://aclanthology.org/2023.findings-acl.712/",
      "title": "Smart Word Suggestions for Writing Assistance",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.712.pdf",
      "authors": [
        "Chenshuo Wang",
        "Shaoguang Mao",
        "Tao Ge",
        "Wenshan Wu",
        "Xun Wang",
        "Yan Xia",
        "Jonathan Tien",
        "Dongyan Zhao"
      ],
      "abstract": "Enhancing word usage is a desired feature for writing assistance. To further advance research in this area, this paper introduces “Smart Word Suggestions” (SWS) task and benchmark. Unlike other works, SWS emphasizes end-to-end evaluation and presents a more realistic writing assistance scenario. This task involves identifying words or phrases that require improvement and providing substitution suggestions. The benchmark includes human-labeled data for testing, a large distantly supervised dataset for training, and the framework for evaluation. The test data includes 1,000 sentences written by English learners, accompanied by over 16,000 substitution suggestions annotated by 10 native speakers. The training dataset comprises over 3.7 million sentences and 12.7 million suggestions generated through rules. Our experiments with seven baselines demonstrate that SWS is a challenging task. Based on experimental analysis, we suggest potential directions for future research on SWS. The dataset and related codes will be available for research purposes."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1962,
      "original_url": "https://aclanthology.org/2023.findings-acl.713/",
      "title": "JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.713.pdf",
      "authors": [
        "Mo Yu",
        "Yi Gu",
        "Xiaoxiao Guo",
        "Yufei Feng",
        "Xiaodan Zhu",
        "Michael Greenspan",
        "Murray Campbell",
        "Chuang Gan"
      ],
      "abstract": "Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We proposea new commonsense reasoning dataset based on human’s Interactive Fiction (IF) gameplaywalkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hopreasoning. Moreover, the IF game-based construction procedure requires much less humaninterventions than previous ones. Different from existing benchmarks, our dataset focuseson the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively uti-lize such functional knowledge to infer the outcomes of actions, rather than relying solely onmemorizing facts. Experiments show that the introduced dataset is challenging to previousmachine reading models as well as the new large language models with a significant 20%performance gap compared to human experts."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1963,
      "original_url": "https://aclanthology.org/2023.findings-acl.714/",
      "title": "A Study on Knowledge Distillation from Weak Teacher for Scaling Up Pre-trained Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.714.pdf",
      "authors": [
        "Hayeon Lee",
        "Rui Hou",
        "Jongpil Kim",
        "Davis Liang",
        "Sung Ju Hwang",
        "Alexander Min"
      ],
      "abstract": "Distillation from Weak Teacher (DWT) is a method of transferring knowledge from a smaller, weaker teacher model to a larger student model to improve its performance. Previous studies have shown that DWT can be effective in the vision domain and natural language processing (NLP) pre-training stage. Specifically, DWT shows promise in practical scenarios, such as enhancing new generation or larger models using pre-trained yet older or smaller models and lacking a resource budget. However, the optimal conditions for using DWT have yet to be fully investigated in NLP pre-training. Therefore, this study examines three key factors to optimize DWT, distinct from those used in the vision domain or traditional knowledge distillation. These factors are:(i) the impact of teacher model quality on DWT effectiveness, (ii) guidelines for adjusting the weighting value for DWT loss, and (iii) the impact of parameter remapping as a student model initialization technique for DWT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1964,
      "original_url": "https://aclanthology.org/2023.findings-acl.715/",
      "title": "SORTIE: Dependency-Aware Symbolic Reasoning for Logical Data-to-text Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.715.pdf",
      "authors": [
        "Xueliang Zhao",
        "Tingchen Fu",
        "Lemao Liu",
        "Lingpeng Kong",
        "Shuming Shi",
        "Rui Yan"
      ],
      "abstract": "Logical data-to-text generation is a representative task in measuring the capabilities of both language generation and complex reasoning. Despite the introduction of reasoning skills in generation, existing works still rely on neural language models to output the final table description. However, due to the inefficacy of neural language models in complex reasoning, these methods inevitably have difficulty working out key entities in the description and might produce unfaithful descriptions. To alleviate these issues, we propose a dependency-aware symbolic reasoning framework that reasons out each entity in the table description with our designed table-compatible programming language. To figure out the dependency relationship among entities, we devise an entity scheduling mechanism to determine the order of programme synthesis such that the reasoning of an entity only relies on other “resolved” entities. Experiments on three datasets and three backbones show that ours outperforms previous methods not only in surface-level fidelity but also in logical fidelity. Notably, the proposed framework enhances GPT-2, BART and T5 with an absolute improvement of 5.7%~11.5% on SP-Acc."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1965,
      "original_url": "https://aclanthology.org/2023.findings-acl.716/",
      "title": "Boosting Event Extraction with Denoised Structure-to-Text Augmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.716.pdf",
      "authors": [
        "Bo Wang",
        "Heyan Huang",
        "Xiaochi Wei",
        "Ge Shi",
        "Xiao Liu",
        "Chong Feng",
        "Tong Zhou",
        "Shuaiqiang Wang",
        "Dawei Yin"
      ],
      "abstract": "Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction (DAEE), which generates additional training data through the knowledge-based structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for event extraction and achieves comparable results with the state-of-the-art."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1966,
      "original_url": "https://aclanthology.org/2023.findings-acl.717/",
      "title": "Detecting Adversarial Samples through Sharpness of Loss Landscape",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.717.pdf",
      "authors": [
        "Rui Zheng",
        "Shihan Dou",
        "Yuhao Zhou",
        "Qin Liu",
        "Tao Gui",
        "Qi Zhang",
        "Zhongyu Wei",
        "Xuanjing Huang",
        "Menghan Zhang"
      ],
      "abstract": "Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep and narrow local minima of the loss landscape (high sharpness) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter (low sharpness).om the perspective of sharpness via visualizing the input loss landscape of models. Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classification tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1967,
      "original_url": "https://aclanthology.org/2023.findings-acl.718/",
      "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.718.pdf",
      "authors": [
        "Spyridon Mouselinos",
        "Mateusz Malinowski",
        "Henryk Michalewski"
      ],
      "abstract": "Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances. To investigate the effect, we propose the “block of influence” concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1968,
      "original_url": "https://aclanthology.org/2023.findings-acl.719/",
      "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.719.pdf",
      "authors": [
        "Justus Mattern",
        "Fatemehsadat Mireshghallah",
        "Zhijing Jin",
        "Bernhard Schoelkopf",
        "Mrinmaya Sachan",
        "Taylor Berg-Kirkpatrick"
      ],
      "abstract": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend toassign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs.However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1969,
      "original_url": "https://aclanthology.org/2023.findings-acl.720/",
      "title": "CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.720.pdf",
      "authors": [
        "Rahul Madhavan",
        "Rishabh Garg",
        "Kahini Wadhawan",
        "Sameep Mehta"
      ],
      "abstract": "We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1970,
      "original_url": "https://aclanthology.org/2023.findings-acl.721/",
      "title": "Can Diffusion Model Achieve Better Performance in Text Generation ? Bridging the Gap between Training and Inference !",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.721.pdf",
      "authors": [
        "Zecheng Tang",
        "Pinzheng Wang",
        "Keyan Zhou",
        "Juntao Li",
        "Ziqiang Cao",
        "Min Zhang"
      ],
      "abstract": "Diffusion models have been successfully adapted to text generation tasks by mapping the discrete text into the continuous space. However, there exist nonnegligible gaps between training and inference, owing to the absence of the forward process during inference. Thus, the model only predicts based on the previously generated reverse noise rather than the noise computed by the forward process. Besides, the widely-used downsampling strategy in speeding up the inference will cause the mismatch of diffusion trajectories between training and inference. To understand and mitigate the above two types of training-inference discrepancies, we launch a thorough preliminary study. Based on our observations, we propose two simple yet effective methods to bridge the gaps mentioned above, named Distance Penalty and Adaptive Decay Sampling. Extensive experiments on 6 generation tasks confirm the superiority of our methods, which can achieve 100× → 200× speedup with better performance. Our code will be released at https://github.com/CODINNLG/Bridge_Gap_Diffusion."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1971,
      "original_url": "https://aclanthology.org/2023.findings-acl.722/",
      "title": "Topic-Guided Self-Introduction Generation for Social Media Users",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.722.pdf",
      "authors": [
        "Chunpu Xu",
        "Jing Li",
        "Piji Li",
        "Min Yang"
      ],
      "abstract": "Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user’s personal interests. While most prior work profiling users with tags (e.g., ages), we investigate sentence-level self-introductions to provide a more natural and engaging way for users to know each other. Here we exploit a user’s tweeting history to generate their self-introduction. The task is non-trivial because the history content may be lengthy, noisy, and exhibit various personal interests. To address this challenge, we propose a novel unified topic-guided encoder-decoder (UTGED) framework; it models latent topics to reflect salient user interest, whose topic mixture then guides encoding a user’s history and topic words control decoding their self-introduction. For experiments, we collect a large-scale Twitter dataset, and extensive results show the superiority of our UTGED to the advanced encoder-decoder models without topic modeling."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1972,
      "original_url": "https://aclanthology.org/2023.findings-acl.723/",
      "title": "Recyclable Tuning for Continual Pre-training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.723.pdf",
      "authors": [
        "Yujia Qin",
        "Cheng Qian",
        "Xu Han",
        "Yankai Lin",
        "Huadong Wang",
        "Ruobing Xie",
        "Zhiyuan Liu",
        "Maosong Sun",
        "Jie Zhou"
      ],
      "abstract": "Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1973,
      "original_url": "https://aclanthology.org/2023.findings-acl.724/",
      "title": "BLOCSUM: Block Scope-based Source Code Summarization via Shared Block Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.724.pdf",
      "authors": [
        "YunSeok Choi",
        "Hyojun Kim",
        "Jee-Hyong Lee"
      ],
      "abstract": "Code summarization, which aims to automatically generate natural language descriptions from source code, has become an essential task in software development for better program understanding. Abstract Syntax Tree (AST), which represents the syntax structure of the source code, is helpful when utilized together with the sequence of code tokens to improve the quality of code summaries. Recent works on code summarization attempted to capture the sequential and structural information of the source code, but they considered less the property that source code consists of multiple code blocks. In this paper, we propose BLOCSUM, BLOck scope-based source Code SUMmarization via shared block representation that utilizes block-scope information by representing various structures of the code block. We propose a shared block position embedding to effectively represent the structure of code blocks and merge both code and AST.Furthermore, we develop variant ASTs to learn rich information such as block and global dependencies of the source code. To prove our approach, we perform experiments on two real-world datasets, the Java dataset and the Python dataset. We demonstrate the effectiveness of BLOCSUM through various experiments, including ablation studies and a human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1974,
      "original_url": "https://aclanthology.org/2023.findings-acl.725/",
      "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.725.pdf",
      "authors": [
        "Zhengkun Zhang",
        "Wenya Guo",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Yadao Wang",
        "Xin Jiang",
        "Qun Liu",
        "Zhenglu Yang"
      ],
      "abstract": "With the scale and capacity of pretrained models growing rapidly, parameter-efficient language model tuning has emerged as a popular paradigm for solving various NLP and Vision-and-Language (V&L) tasks. In this paper, we design a unified parameter-efficient multitask learning framework that works effectively on both NLP and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings and visual modality as input, and outputs weights for different modules in a pretrained language model, such as the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning.). Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1975,
      "original_url": "https://aclanthology.org/2023.findings-acl.726/",
      "title": "Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.726.pdf",
      "authors": [
        "Zixuan Ling",
        "Xiaoqing Zheng",
        "Jianhan Xu",
        "Jinshu Lin",
        "Kai-Wei Chang",
        "Cho-Jui Hsieh",
        "Xuanjing Huang"
      ],
      "abstract": "We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sentences into fragments, clustering the fragments to abstract away syntactic variations of the same meaning, and predicting predicate-argument relations between the fragments. To better model the statistical dependencies between predicates and their arguments, we further conduct a hierarchical Pitman-Yor process. An improved Metropolis-Hastings merge-split sampler is proposed to speed up the mixing and convergence of Markov chains by leveraging pre-trained distributed representations. The experimental results show that the models achieve better accuracy on both question-answering and relation extraction tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1976,
      "original_url": "https://aclanthology.org/2023.findings-acl.727/",
      "title": "Generating Labeled Data for Relation Extraction: A Meta Learning Approach with Joint GPT-2 Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.727.pdf",
      "authors": [
        "Amir Pouran Ben Veyseh",
        "Franck Dernoncourt",
        "Bonan Min",
        "Thien Nguyen"
      ],
      "abstract": "Relation Extraction (RE) is the task of identifying semantic relation between real-world entities mentioned in text. Despite significant progress in RE research, a remaining challenge for RE concerns the lack of training data for data-hungry deep learning models. Cost of annotation and difficulty of the task are among hindrance to collect a large-scale RE dataset in different domains. To address this limitation, we propose a novel framework to automatically generate labeled data for RE. Our framework presents the pre-trained language model GPT-2 for data generation. In addition, to optimize the generated samples for an RE model, we introduce a meta learning approach to allow the GPT-2 model to be updated during the training process for RE. In particular, to leverage the feedback from the RE model to improve the data generation from GPT-2, we propose a novel reward function to update the GPT-2 model with REINFORCE, seeking to promote the similarity of the RE loss function’s gradients computed for generated data and a meta development set. We conduct extensive experiments on two benchmark datasets to produce state-of-the-art performance for RE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1977,
      "original_url": "https://aclanthology.org/2023.findings-acl.728/",
      "title": "Disfluency Generation for More Robust Dialogue Systems",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.728.pdf",
      "authors": [
        "Benjamin Marie"
      ],
      "abstract": "Disfluencies in user utterances can trigger a chain of errors impacting all the modules of a dialogue system: natural language understanding, dialogue state tracking, and response generation. In this work, we first analyze existing dialogue datasets commonly used in research and show that they only contain a marginal number of disfluent utterances. Due to this relative absence of disfluencies in their training data, dialogue systems may then critically fail when exposed to disfluent utterances. Following this observation, we propose to augment existing datasets with disfluent user utterances by paraphrasing fluent utterances into disfluent ones. Relying on a pre-trained language model, our few-shot disfluent paraphraser guided by a disfluency classifier can generate useful disfluent utterances for training better dialogue systems. We report on improvements for both dialogue state tracking and response generation when the dialogue systems are trained on datasets augmented with our disfluent utterances."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1978,
      "original_url": "https://aclanthology.org/2023.findings-acl.729/",
      "title": "Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.729.pdf",
      "authors": [
        "Chen Chen",
        "Yufei Wang",
        "Aixin Sun",
        "Bing Li",
        "Kwok-Yan Lam"
      ],
      "abstract": "Knowledge Graph Completion (KGC) often requires both KG structural and textual information to be effective. Pre-trained Language Models (PLMs) have been used to learn the textual information, usually under the fine-tune paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly focus on the textual information and overlook structural knowledge. To tackle this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC) which maintains a balance between structural information and textual knowledge. CSProm-KG only tunes the parameters of Conditional Soft Prompts that are generated by the entities and relations representations. We verify the effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR, FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new state-of-the-art on these benchmarks. We conduct further analysis to show (i) the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG, and (iii) the flexibility of CSProm-KG."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1979,
      "original_url": "https://aclanthology.org/2023.findings-acl.730/",
      "title": "Revisiting Pathologies of Neural Models under Input Reduction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.730.pdf",
      "authors": [
        "Canasai Kruengkrai",
        "Junichi Yamagishi"
      ],
      "abstract": "We revisit the question of why neural models tend to produce high-confidence predictions on inputs that appear nonsensical to humans. Previous work has suggested that the models fail to assign low probabilities to such inputs due to model overconfidence. We evaluate various regularization methods on fact verification benchmarks and find that this problem persists even with well-calibrated or underconfident models, suggesting that overconfidence is not the only underlying cause. We also find that regularizing the models with reduced examples helps improve interpretability but comes with the cost of miscalibration. We show that although these reduced examples are incomprehensible to humans, they can contain valid statistical patterns in the dataset utilized by the model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1980,
      "original_url": "https://aclanthology.org/2023.findings-acl.731/",
      "title": "Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.731.pdf",
      "authors": [
        "Fei Yuan",
        "Yinquan Lu",
        "Wenhao Zhu",
        "Lingpeng Kong",
        "Lei Li",
        "Yu Qiao",
        "Jingjing Xu"
      ],
      "abstract": "Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. Existing monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models. In this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT.For a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. The proposed training recipe brings a 28.2× speedup over the conventional multi-way training method.code and data repo: https://github.com/CONE-MT/Lego-MT.git."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1981,
      "original_url": "https://aclanthology.org/2023.findings-acl.732/",
      "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.732.pdf",
      "authors": [
        "Michiel de Jong",
        "Yury Zemlyanskiy",
        "Joshua Ainslie",
        "Nicholas FitzGerald",
        "Sumit Sanghai",
        "Fei Sha",
        "William Cohen"
      ],
      "abstract": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1982,
      "original_url": "https://aclanthology.org/2023.findings-acl.733/",
      "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.733.pdf",
      "authors": [
        "Jason Hoelscher-Obermaier",
        "Julia Persson",
        "Esben Kran",
        "Ioannis Konstas",
        "Fazl Barez"
      ],
      "abstract": "Recent model editing techniques promise to mitigate the problem of memorizing false or outdated associations during LLM training. However, we show that these techniques can introduce large unwanted side effects which are not detected by existing specificity benchmarks. We extend the existing CounterFact benchmark to include a dynamic component and dub our benchmark CounterFact+. Additionally, we extend the metrics used for measuring specificity by a principled KL divergence-based metric. We use this improved benchmark to evaluate recent model editing techniques and find that they suffer from low specificity. Our findings highlight the need for improved specificity benchmarks that identify and prevent unwanted side effects."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1983,
      "original_url": "https://aclanthology.org/2023.findings-acl.734/",
      "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.734.pdf",
      "authors": [
        "Xinze Li",
        "Zhenghao Liu",
        "Chenyan Xiong",
        "Shi Yu",
        "Yu Gu",
        "Zhiyuan Liu",
        "Ge Yu"
      ],
      "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1984,
      "original_url": "https://aclanthology.org/2023.findings-acl.735/",
      "title": "Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.735.pdf",
      "authors": [
        "Xiaocui Yang",
        "Shi Feng",
        "Daling Wang",
        "Qi Sun",
        "Wenfang Wu",
        "Yifei Zhang",
        "Pengfei Hong",
        "Soujanya Poria"
      ],
      "abstract": "We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt. Extensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1985,
      "original_url": "https://aclanthology.org/2023.findings-acl.736/",
      "title": "Predicting Human Translation Difficulty Using Automatic Word Alignment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.736.pdf",
      "authors": [
        "Zheng Wei Lim",
        "Trevor Cohn",
        "Charles Kemp",
        "Ekaterina Vylomova"
      ],
      "abstract": "Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these behavioral measures are costly and do not scale. In this work, we use word alignments computed over large scale bilingual corpora to develop predictors of lexical translation difficulty. We evaluate our approach using behavioural data from translations provided both in and out of context, and report results that improve on a previous embedding-based approach (Thompson et al., 2020). Our work can therefore contribute to a deeper understanding of cross-lingual differences and of causes of translation difficulty."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1986,
      "original_url": "https://aclanthology.org/2023.findings-acl.737/",
      "title": "Know Where You’re Going: Meta-Learning for Parameter-Efficient Fine-Tuning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.737.pdf",
      "authors": [
        "Mozhdeh Gheini",
        "Xuezhe Ma",
        "Jonathan May"
      ],
      "abstract": "A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on if and how such knowledge of the downstream fine-tuning approach calls for complementary measures after pre-training and before fine-tuning. In this work, we show that taking the ultimate choice of fine-tuning into consideration boosts the performance of parameter-efficient fine-tuning. By relying on optimization-based meta-learning using MAML with certain modifications for our distinct purpose, we prime the pre-trained model specifically for parameter-efficient fine-tuning, resulting in gains of up to 4.96 points on cross-lingual NER fine-tuning. Our ablation settings and analyses further reveal that the specific approach we take to meta-learning is crucial for the attained gains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1987,
      "original_url": "https://aclanthology.org/2023.findings-acl.738/",
      "title": "Moving Beyond Downstream Task Accuracy for Information Retrieval Benchmarking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.738.pdf",
      "authors": [
        "Keshav Santhanam",
        "Jon Saad-Falcon",
        "Martin Franz",
        "Omar Khattab",
        "Avi Sil",
        "Radu Florian",
        "Md Arafat Sultan",
        "Salim Roukos",
        "Matei Zaharia",
        "Christopher Potts"
      ],
      "abstract": "Neural information retrieval (IR) systems have progressed rapidly in recent years, in large part due to the release of publicly available benchmarking tasks. Unfortunately, some dimensions of this progress are illusory: the majority of the popular IR benchmarks today focus exclusively on downstream task accuracy and thus conceal the costs incurred by systems that trade away efficiency for quality. Latency, hardware cost, and other efficiency considerations are paramount to the deployment of IR systems in user-facing settings. We propose that IR benchmarks structure their evaluation methodology to include not only metrics of accuracy, but also efficiency considerations such as a query latency and the corresponding cost budget for a reproducible hardware setting. For the popular IR benchmarks MS MARCO and XOR-TyDi, we show how the best choice of IR system varies according to how these efficiency considerations are chosen and weighed. We hope that future benchmarks will adopt these guidelines toward more holistic IR evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1988,
      "original_url": "https://aclanthology.org/2023.findings-acl.739/",
      "title": "AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.739.pdf",
      "authors": [
        "Abhijnan Nath",
        "Sheikh Mannan",
        "Nikhil Krishnaswamy"
      ],
      "abstract": "Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings. In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks. AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on “longer-context” tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cross-document coreference task on a translated version of the ECB+ corpus, where we present a new SOTA result for an LRL. Our source code and evaluation scripts may be found at https://github.com/csu-signal/axomiyaberta."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1989,
      "original_url": "https://aclanthology.org/2023.findings-acl.740/",
      "title": "An Exploratory Study on Model Compression for Text-to-SQL",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.740.pdf",
      "authors": [
        "Shuo Sun",
        "Yuze Gao",
        "Yuchen Zhang",
        "Jian Su",
        "Bin Chen",
        "Yingzhan Lin",
        "Shuqi Sun"
      ],
      "abstract": "Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence Text-to-SQL models. Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1990,
      "original_url": "https://aclanthology.org/2023.findings-acl.741/",
      "title": "FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.741.pdf",
      "authors": [
        "Ziyue Jiang",
        "Qian Yang",
        "Jialong Zuo",
        "Zhenhui Ye",
        "Rongjie Huang",
        "Yi Ren",
        "Zhou Zhao"
      ],
      "abstract": "Stutter removal is an essential scenario in the field of speech editing. However, when the speech recording contains stutters, the existing text-based speech editing approaches still suffer from: 1) the over-smoothing problem in the edited speech; 2) lack of robustness due to the noise introduced by stutter; 3) to remove the stutters, users are required to determine the edited region manually. To tackle the challenges in stutter removal, we propose FluentSpeech, a stutter-oriented automatic speech editing model. Specifically, 1) we propose a context-aware diffusion model that iteratively refines the modified mel-spectrogram with the guidance of context features; 2) we introduce a stutter predictor module to inject the stutter information into the hidden sequence; 3) we also propose a stutter-oriented automatic speech editing (SASE) dataset that contains spontaneous speech recordings with time-aligned stutter labels to train the automatic stutter localization model. Experimental results on VCTK and LibriTTS datasets demonstrate that our model achieves state-of-the-art performance on speech editing. Further experiments on our SASE dataset show that FluentSpeech can effectively improve the fluency of stuttering speech in terms of objective and subjective metrics. Code and audio samples can be found at https://github.com/Zain-Jiang/Speech-Editing-Toolkit."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1991,
      "original_url": "https://aclanthology.org/2023.findings-acl.742/",
      "title": "HyHTM: Hyperbolic Geometry-based Hierarchical Topic Model",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.742.pdf",
      "authors": [
        "Simra Shahid",
        "Tanay Anand",
        "Nikitha Srikanth",
        "Sumit Bhatia",
        "Balaji Krishnamurthy",
        "Nikaash Puri"
      ],
      "abstract": "Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lower-level topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry-based Hierarchical Topic Model - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialize in granularity from generic higher-level topics to specific lower-level topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline. We have made the source code for our algorithm publicly accessible."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1992,
      "original_url": "https://aclanthology.org/2023.findings-acl.743/",
      "title": "KoRC: Knowledge Oriented Reading Comprehension Benchmark for Deep Text Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.743.pdf",
      "authors": [
        "Zijun Yao",
        "Yantao Liu",
        "Xin Lv",
        "Shulin Cao",
        "Jifan Yu",
        "Juanzi Li",
        "Lei Hou"
      ],
      "abstract": "Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRC in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the strongest baseline only achieves 68.3% and 30.0% F1 measure in the IID and OOD test set, respectively. These results indicate that deep text understanding is still an unsolved challenge. We will release our dataset and baseline methods upon acceptance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1993,
      "original_url": "https://aclanthology.org/2023.findings-acl.744/",
      "title": "DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.744.pdf",
      "authors": [
        "Vishal Saley",
        "Rocktim Das",
        "Dinesh Raghu",
        "Mausam"
      ],
      "abstract": "Task-oriented dialog (TOD) agents often ground their responses on external knowledge bases (KBs). These KBs can be dynamic and may be updated frequently. Existing approaches for learning TOD agents assume the KB snapshot contemporary to each individual dialog is available during training. However, in real-world scenarios, only the latest KB snapshot is available during training and as a result, the train dialogs may contain facts conflicting with the latest KB. These dialog-KB inconsistencies in the training data may potentially confuse the TOD agent learning algorithm. In this work, we define the novel problem of learning a TOD agent with dialog-KB inconsistencies in the training data. We propose a Dialog-KB Arbitration Framework (DKAF) which reduces the dialog-KB inconsistencies by predicting the contemporary KB snapshot for each train dialog. These predicted KB snapshots are then used for training downstream TOD agents. As there are no existing datasets with dialog-KB inconsistencies, we systematically introduce inconsistencies in two publicly available dialog datasets. We show that TOD agents trained with DKAF perform better than existing baselines on both these datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1994,
      "original_url": "https://aclanthology.org/2023.findings-acl.745/",
      "title": "Scale-Invariant Infinite Hierarchical Topic Model",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.745.pdf",
      "authors": [
        "Shusei Eshima",
        "Daichi Mochihashi"
      ],
      "abstract": "Hierarchical topic models have been employed to organize a large number of diverse topics from corpora into a latent tree structure. However, existing models yield fragmented topics with overlapping themes whose expected probability becomes exponentially smaller along the depth of the tree. To solve this intrinsic problem, we propose a scale-invariant infinite hierarchical topic model (ihLDA). The ihLDA adaptively adjusts the topic creation to make the expected topic probability decay considerably slower than that in existing models. Thus, it facilitates the estimation of deeper topic structures encompassing diverse topics in a corpus. Furthermore, the ihLDA extends a widely used tree-structured prior (Adams et al., 2010) in a hierarchical Bayesian way, which enables drawing an infinite topic tree from the base tree while efficiently sampling the topic assignments for the words. Experiments demonstrate that the ihLDA has better topic uniqueness and hierarchical diversity thanexisting approaches, including state-of-the-art neural models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1995,
      "original_url": "https://aclanthology.org/2023.findings-acl.746/",
      "title": "RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.746.pdf",
      "authors": [
        "Chulun Zhou",
        "Yunlong Liang",
        "Fandong Meng",
        "Jinan Xu",
        "Jinsong Su",
        "Jie Zhou"
      ],
      "abstract": "Multilingual vision-language (V&L) pre-training has achieved remarkable progress in learning universal representations across different modalities and languages. In spite of recent success, there still remain challenges limiting further improvements of V&L pre-trained models in multilingual settings. Particularly, current V&L pre-training methods rely heavily on strictly-aligned multilingual image-text pairs generated from English-centric datasets through machine translation. However, the cost of collecting and translating such strictly-aligned datasets is usually unbearable. In this paper, we propose Regularized Contrastive Cross-lingual Cross-modal (RC3) pre-training, which further exploits more abundant weakly-aligned multilingual image-text pairs. Specifically, we design a regularized cross-lingual visio-textual contrastive learning objective that constrains the representation proximity of weakly-aligned visio-textual inputs according to textual relevance. Besides, existing V&L pre-training approaches mainly deal with visual inputs by either region-of-interest (ROI) features or patch embeddings. We flexibly integrate the two forms of visual features into our model for pre-training and downstream multi-modal tasks. Extensive experiments on 5 downstream multi-modal tasks across 6 languages demonstrate the effectiveness of our proposed method over competitive contrast models with strong zero-shot capability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1996,
      "original_url": "https://aclanthology.org/2023.findings-acl.747/",
      "title": "Deep Equilibrium Non-Autoregressive Sequence Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.747.pdf",
      "authors": [
        "Zaixiang Zheng",
        "Yi Zhou",
        "Hao Zhou"
      ],
      "abstract": "In this work, we argue that non-autoregressive (NAR) sequence generative models can equivalently be regarded as an iterative refinement process towards the target sequence, implying an underlying dynamical system of NAR model: z = f (z, x) → y. In such a way, the optimal prediction of a NAR model should be the equilibrium state of its dynamics if given infinitely many iterations. However, this is infeasible in practice due to limited computational and memory budgets. To this end, we propose DEQNAR to directly solve for the equilibrium state of NAR models based on deep equilibrium networks (Bai et al., 2019) with black-box root-finding solvers and back-propagate through the equilibrium point via implicit differentiation with constant memory. We conduct extensive experiments on four WMT machine translation benchmarks. Our main findings show that DEQNAR can indeed converge to a more accurate prediction and is a general-purpose framework that consistently helps yield substantial improvement for several strong NAR backbones."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1997,
      "original_url": "https://aclanthology.org/2023.findings-acl.748/",
      "title": "ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.748.pdf",
      "authors": [
        "Yue Yu",
        "Yuchen Zhuang",
        "Rongzhi Zhang",
        "Yu Meng",
        "Jiaming Shen",
        "Chao Zhang"
      ],
      "abstract": "With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further pro- pose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self- consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that ReGen achieves 4.3% gain over the strongest baselines and saves around 70% of the time when compared with baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language models to boost performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1998,
      "original_url": "https://aclanthology.org/2023.findings-acl.749/",
      "title": "Race, Gender, and Age Biases in Biomedical Masked Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.749.pdf",
      "authors": [
        "Michelle Kim",
        "Junghwan Kim",
        "Kristen Johnson"
      ],
      "abstract": "Biases cause discrepancies in healthcare services. Race, gender, and age of a patient affect interactions with physicians and the medical treatments one receives. These biases in clinical practices can be amplified following the release of pre-trained language models trained on biomedical corpora. To bring awareness to such repercussions, we examine social biases present in the biomedical masked language models. We curate prompts based on evidence-based practice and compare generated diagnoses based on biases. For a case study, we measure bias in diagnosing coronary artery disease and using cardiovascular procedures based on bias. Our study demonstrates that biomedical models are less biased than BERT in gender, while the opposite is true for race and age."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 1999,
      "original_url": "https://aclanthology.org/2023.findings-acl.750/",
      "title": "Neighboring Words Affect Human Interpretation of Saliency Explanations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.750.pdf",
      "authors": [
        "Alon Jacovi",
        "Hendrik Schuff",
        "Heike Adel",
        "Ngoc Thang Vu",
        "Yoav Goldberg"
      ],
      "abstract": "Word-level saliency explanations (“heat maps over words”) are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to investigate how the marking of a word’s *neighboring words* affect the explainee’s perception of the word’s importance in the context of a saliency explanation. We find that neighboring words have significant effects on the word’s importance rating. Concretely, we identify that the influence changes based on neighboring direction (left vs. right) and a-priori linguistic and computational measures of phrases and collocations (vs. unrelated neighboring words).Our results question whether text-based saliency explanations should be continued to be communicated at word level, and inform future research on alternative saliency explanation methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2000,
      "original_url": "https://aclanthology.org/2023.findings-acl.751/",
      "title": "HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.751.pdf",
      "authors": [
        "Swaroop Mishra",
        "Elnaz Nouri"
      ],
      "abstract": "Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy Help Me Think where we encourage largelanguage models (such as GPT3 and ChatGPT) to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique Help Me Think on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2001,
      "original_url": "https://aclanthology.org/2023.findings-acl.752/",
      "title": "Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.752.pdf",
      "authors": [
        "Anni Zou",
        "Zhuosheng Zhang",
        "Hai Zhao"
      ],
      "abstract": "Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies primarily rest on grasping either unstructured evidence or potential reasoning paths from structured knowledge bases, yet failing to exploit the benefits of heterogeneous knowledge simultaneously. In light of this, we propose Decker, a commonsense fact verification model that is capable of bridging heterogeneous knowledge by uncovering latent relationships between structured and unstructured knowledge. Experimental results on two commonsense fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the effectiveness of our Decker and further analysis verifies its capability to seize more precious information through reasoning. The official implementation of Decker is available at https://github.com/Anni-Zou/Decker."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2002,
      "original_url": "https://aclanthology.org/2023.findings-acl.753/",
      "title": "DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.753.pdf",
      "authors": [
        "Jinglin Liu",
        "Zhenhui Ye",
        "Qian Chen",
        "Siqi Zheng",
        "Wen Wang",
        "Zhang Qinglin",
        "Zhou Zhao"
      ],
      "abstract": "Recently, binaural audio synthesis (BAS) has emerged as a promising research field for its applications in augmented and virtual realities. Binaural audio helps ususers orient themselves and establish immersion by providing the brain with interaural time differences reflecting spatial information. However, existing BAS methods are limited in terms of phase estimation, which is crucial for spatial hearing. In this paper, we propose the DopplerBAS method to explicitly address the Doppler effect of the moving sound source. Specifically, we calculate the radial relative velocity of the moving speaker in spherical coordinates, which further guides the synthesis of binaural audio. This simple method introduces no additional hyper-parameters and does not modify the loss functions, and is plug-and-play: it scales well to different types of backbones. DopperBAS distinctly improves the representative WarpNet and BinauralGrad backbones in the phase error metric and reaches a new state of the art (SOTA): 0.780 (versus the current SOTA 0.807). Experiments and ablation studies demonstrate the effectiveness of our method."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2003,
      "original_url": "https://aclanthology.org/2023.findings-acl.754/",
      "title": "Easy-to-Hard Learning for Information Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.754.pdf",
      "authors": [
        "Chang Gao",
        "Wenxuan Zhang",
        "Wai Lam",
        "Lidong Bing"
      ],
      "abstract": "Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human learning process. In this paper, we propose a unified easy-to-hard learning framework consisting of three stages, i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking the human learning process. By breaking down the learning process into multiple stages, our framework facilitates the model to acquire general IE task knowledge and improve its generalization ability. Extensive experiments across four IE tasks demonstrate the effectiveness of our framework. We achieve new state-of-the-art results on 13 out of 17 datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2004,
      "original_url": "https://aclanthology.org/2023.findings-acl.755/",
      "title": "SConE: Simplified Cone Embeddings with Symbolic Operators for Complex Logical Queries",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.755.pdf",
      "authors": [
        "Chau Nguyen",
        "Tim French",
        "Wei Liu",
        "Michael Stewart"
      ],
      "abstract": "Geometric representation of query embeddings (using points, particles, rectangles and cones) can effectively achieve the task of answering complex logical queries expressed in first-order logic (FOL) form over knowledge graphs, allowing intuitive encodings. However, current geometric-based methods depend on the neural approach to model FOL operators (conjunction, disjunction and negation), which are not easily explainable with considerable computation cost. We overcome this challenge by introducing a symbolic modeling approach for the FOL operators, emphasizing the direct calculation of the intersection between geometric shapes, particularly sector-cones in the embedding space, to model the conjunction operator. This approach reduces the computation cost as a non-neural approach is involved in the core logic operators. Moreover, we propose to accelerate the learning in the relation projection operator using the neural approach to emphasize the essential role of this operator in all query structures. Although empirical evidence for explainability is challenging, our approach demonstrates a significant improvement in answering complex logical queries (both non-negative and negative FOL forms) over previous geometric-based models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2005,
      "original_url": "https://aclanthology.org/2023.findings-acl.756/",
      "title": "Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.756.pdf",
      "authors": [
        "Peng Qi",
        "Yuyang Zhao",
        "Yufeng Shen",
        "Wei Ji",
        "Juan Cao",
        "Tat-Seng Chua"
      ],
      "abstract": "The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually with multimodal information. Nevertheless, news videos from different perspectives regarding the same event are commonly posted together, which contain complementary or contradictory information and thus can be used to evaluate each other mutually. To this end, we introduce a new and practical paradigm, i.e., cross-sample fake news video detection, and propose a novel framework, Neighbor-Enhanced fakE news video Detection (NEED), which integrates the neighborhood relationship of new videos belonging to the same event. NEED can be readily combined with existing single-sample detectors and further enhance their performances with the proposed graph aggregation (GA) and debunking rectification (DR) modules. Specifically, given the feature representations obtained from single-sample detectors, GA aggregates the neighborhood information with the dynamic graph to enrich the features of independent samples. After that, DR explicitly leverages the relationship between debunking videos and fake news videos to refute the candidate videos via textual and visual consistency. Extensive experiments on the public benchmark demonstrate that NEED greatly improves the performance of both single-modal (up to 8.34% in accuracy) and multimodal (up to 4.97% in accuracy) base detectors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2006,
      "original_url": "https://aclanthology.org/2023.findings-acl.757/",
      "title": "An Annotated Dataset for Explainable Interpersonal Risk Factors of Mental Disturbance in Social Media Posts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.757.pdf",
      "authors": [
        "Muskan Garg",
        "Amirmohammad Shahbandegan",
        "Amrit Chadha",
        "Vijay Mago"
      ],
      "abstract": "With a surge in identifying suicidal risk and its severity in social media posts, we argue that a more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. The success of computational intelligence techniques for inferring mental illness from social media resources, points to natural language processing as a lens for determining Interpersonal Risk Factors (IRF) in human writings. Motivated with limited availability of datasets for social NLP research community, we construct and release a new annotated dataset with human-labelled explanations and classification of IRF affecting mental disturbance on social media: (i) Thwarted Belongingness (TBe), and (ii) Perceived Burdensomeness (PBu). We establish baseline models on our dataset facilitating future research directions to develop real-time personalized AI models by detecting patterns of TBe and PBu in emotional spectrum of user’s historical social media profile."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2007,
      "original_url": "https://aclanthology.org/2023.findings-acl.758/",
      "title": "Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.758.pdf",
      "authors": [
        "Xiang Fan",
        "Yiwei Lyu",
        "Paul Pu Liang",
        "Ruslan Salakhutdinov",
        "Louis-Philippe Morency"
      ],
      "abstract": "Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing NANO, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. NANO achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that NANO is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals’ personal preferences with high sample efficiency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2008,
      "original_url": "https://aclanthology.org/2023.findings-acl.759/",
      "title": "Connectivity Patterns are Task Embeddings",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.759.pdf",
      "authors": [
        "Zhiheng Xi",
        "Rui Zheng",
        "Yuansen Zhang",
        "Xuanjing Huang",
        "Zhongyu Wei",
        "Minlong Peng",
        "Mingming Sun",
        "Qi Zhang",
        "Tao Gui"
      ],
      "abstract": "Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2009,
      "original_url": "https://aclanthology.org/2023.findings-acl.760/",
      "title": "Improving Autoregressive Grammatical Error Correction with Non-autoregressive Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.760.pdf",
      "authors": [
        "Hang Cao",
        "Zhiquan Cao",
        "Chi Hu",
        "Baoyu Hou",
        "Tong Xiao",
        "Jingbo Zhu"
      ],
      "abstract": "Grammatical Error Correction (GEC) aims to correct grammatical errors in sentences. We find that autoregressive models tend to assign low probabilities to tokens that need corrections. Here we introduce additional signals to the training of GEC models so that these systems can learn to better predict at ambiguous positions. To do this, we use a non-autoregressive model as an auxiliary model, and develop a new regularization term of training by considering the difference in predictions between the autoregressive and non-autoregressive models. We experiment with this method on both English and Chinese GEC tasks. Experimental results show that our GEC system outperforms the baselines on all the data sets significantly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2010,
      "original_url": "https://aclanthology.org/2023.findings-acl.761/",
      "title": "SamToNe: Improving Contrastive Loss for Dual Encoder Retrieval Models with Same Tower Negatives",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.761.pdf",
      "authors": [
        "Fedor Moiseev",
        "Gustavo Hernandez Abrego",
        "Peter Dornbach",
        "Imed Zitouni",
        "Enrique Alfonseca",
        "Zhe Dong"
      ],
      "abstract": "Dual encoders have been used for retrieval tasks and representation learning with good results. A standard way to train dual encoders is using a contrastive loss with in-batch negatives. In this work, we propose an improved contrastive learning objective by adding queries or documents from the same encoder towers to the negatives, for which we name it as “contrastive loss with SAMe TOwer NEgatives” (SamToNe). By evaluating on question answering retrieval benchmarks from MS MARCO and MultiReQA, and heterogenous zero-shot information retrieval benchmarks (BEIR), we demonstrate that SamToNe can effectively improve the retrieval quality for both symmetric and asymmetric dual encoders. By directly probing the embedding spaces of the two encoding towers via the t-SNE algorithm (van der Maaten and Hinton, 2008), we observe that SamToNe ensures the alignment between the embedding spaces from the two encoder towers. Based on the analysis of the embedding distance distributions of the top-1 retrieved results, we further explain the efficacy of the method from the perspective of regularisation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2011,
      "original_url": "https://aclanthology.org/2023.findings-acl.762/",
      "title": "On the Strength of Sequence Labeling and Generative Models for Aspect Sentiment Triplet Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.762.pdf",
      "authors": [
        "Shen Zhou",
        "Tieyun Qian"
      ],
      "abstract": "Generative models have achieved great success in aspect sentiment triplet extraction tasks. However, existing methods ignore the mutual informative clues between aspect and opinion terms and may generate false paired triplets. Furthermore, the inherent limitations of generative models, i.e., the token-by-token decoding and the simple structured prompt, prevent models from handling complex structures especially multi-word terms and multi-triplet sentences. To address these issues, we propose a sequence labeling enhanced generative model. Firstly, we encode the dependency between aspect and opinion into two bidirectional templates to avoid false paired triplets. Secondly, we introduce a marker-oriented sequence labeling module to improve generative models’ ability of tackling complex structures. Specifically, this module enables the generative model to capture the boundary information of aspect/opinion spans and provides hints to decode multiple triplets with the shared marker. Experimental results on four datasets prove that our model yields a new state-of-art performance. Our code and data are available at https://github.com/NLPWM-WHU/SLGM."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2012,
      "original_url": "https://aclanthology.org/2023.findings-acl.763/",
      "title": "Revisiting Non-Autoregressive Translation at Scale",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.763.pdf",
      "authors": [
        "Zhihao Wang",
        "Longyue Wang",
        "Jinsong Su",
        "Junfeng Yao",
        "Zhaopeng Tu"
      ],
      "abstract": "In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on the scaled dataset, which can be regarded as a strong baseline for future works. We release code and system outputs at https://github.com/DeepLearnXMU/Scaling4NAT."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2013,
      "original_url": "https://aclanthology.org/2023.findings-acl.764/",
      "title": "Improving Radiology Summarization with Radiograph and Anatomy Prompts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.764.pdf",
      "authors": [
        "Jinpeng Hu",
        "Zhihong Chen",
        "Yang Liu",
        "Xiang Wan",
        "Tsung-Hui Chang"
      ],
      "abstract": "The impression is crucial for the referring physicians to grasp key information since it is concluded from the findings and reasoning of radiologists. To alleviate the workload of radiologists and reduce repetitive human labor in impression writing, many researchers have focused on automatic impression generation. However, recent works on this task mainly summarize the corresponding findings and pay less attention to the radiology images. In clinical, radiographs can provide more detailed valuable observations to enhance radiologists’ impression writing, especially for complicated cases. Besides, each sentence in findings usually focuses on single anatomy, such that they only need to be matched to corresponding anatomical regions instead of the whole image, which is beneficial for textual and visual features alignment. Therefore, we propose a novel anatomy-enhanced multimodal model to promote impression generation. In detail, we first construct a set of rules to extract anatomies and put these prompts into each sentence to highlight anatomy characteristics. Then, two separate encoders are applied to extract features from the radiograph and findings. Afterward, we utilize a contrastive learning module to align these two representations at the overall level and use a co-attention to fuse them at the sentence level with the help of anatomy-enhanced sentence representation. The experimental results on two benchmark datasets confirm the effectiveness of the proposed method, which achieves state-of-the-art results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2014,
      "original_url": "https://aclanthology.org/2023.findings-acl.765/",
      "title": "Explanation Regeneration via Information Bottleneck",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.765.pdf",
      "authors": [
        "Qintong Li",
        "Zhiyong Wu",
        "Lingpeng Kong",
        "Wei Bi"
      ],
      "abstract": "Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM’s results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2015,
      "original_url": "https://aclanthology.org/2023.findings-acl.766/",
      "title": "Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.766.pdf",
      "authors": [
        "Pengzhi Gao",
        "Liwen Zhang",
        "Zhongjun He",
        "Hua Wu",
        "Haifeng Wang"
      ],
      "abstract": "The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and simplicity of CrossConST, we believe it can serve as a strong baseline for future multilingual NMT research."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2016,
      "original_url": "https://aclanthology.org/2023.findings-acl.767/",
      "title": "ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.767.pdf",
      "authors": [
        "Ming Zhong",
        "Siru Ouyang",
        "Minhao Jiang",
        "Vivian Hu",
        "Yizhu Jiao",
        "Xuan Wang",
        "Jiawei Han"
      ],
      "abstract": "Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2017,
      "original_url": "https://aclanthology.org/2023.findings-acl.768/",
      "title": "Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.768.pdf",
      "authors": [
        "Yung-Sung Chuang",
        "Wei Fang",
        "Shang-Wen Li",
        "Wen-tau Yih",
        "James Glass"
      ],
      "abstract": "We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2018,
      "original_url": "https://aclanthology.org/2023.findings-acl.769/",
      "title": "Neural Networks Against (and For) Self-Training: Classification with Small Labeled and Large Unlabeled Sets",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.769.pdf",
      "authors": [
        "Payam Karisani"
      ],
      "abstract": "We propose a semi-supervised text classifier based on self-training using one positive and one negative property of neural networks. One of the weaknesses of self-training is the semantic drift problem, where noisy pseudo-labels accumulate over iterations and consequently the error rate soars. In order to tackle this challenge, we reshape the role of pseudo-labels and create a hierarchical order of information. In addition, a crucial step in self-training is to use the classifier confidence prediction to select the best candidate pseudo-labels. This step cannot be efficiently done by neural networks, because it is known that their output is poorly calibrated. To overcome this challenge, we propose a hybrid metric to replace the plain confidence measurement. Our metric takes into account the prediction uncertainty via a subsampling technique. We evaluate our model in a set of five standard benchmarks, and show that it significantly outperforms a set of ten diverse baseline models. Furthermore, we show that the improvement achieved by our model is additive to language model pretraining, which is a widely used technique for using unlabeled documents."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2019,
      "original_url": "https://aclanthology.org/2023.findings-acl.770/",
      "title": "Inducing Character-level Structure in Subword-based Language Models with Type-level Interchange Intervention Training",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.770.pdf",
      "authors": [
        "Jing Huang",
        "Zhengxuan Wu",
        "Kyle Mahowald",
        "Christopher Potts"
      ],
      "abstract": "Language tasks involving character-level manipulations (e.g., spelling corrections, arithmetic operations, word games) are challenging for models operating on subword units. To address this, we develop a causal intervention framework to learn robust and interpretable character representations inside subword-based language models. Our method treats each character as a typed variable in a causal model and learns such causal structures by adapting the interchange intervention training method of Geiger et al. (2021). We additionally introduce a suite of character-level tasks that systematically vary in their dependence on meaning and sequence-level context. While character-level models still perform best on purely form-based tasks like string reversal, our method outperforms character-level models on more complex tasks that blend form, meaning, and context, such as spelling correction in context and word search games. Compared with standard subword-based models, our approach also significantly improves robustness on unseen token sequences and leads to human-interpretable internal representations of characters."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2020,
      "original_url": "https://aclanthology.org/2023.findings-acl.771/",
      "title": "Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.771.pdf",
      "authors": [
        "Daniel Saggau",
        "Mina Rezaei",
        "Bernd Bischl",
        "Ilias Chalkidis"
      ],
      "abstract": "Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -siamese neural network- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear classification settings on three long document topic classification tasks from the legal and biomedical domains."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2021,
      "original_url": "https://aclanthology.org/2023.findings-acl.772/",
      "title": "QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.772.pdf",
      "authors": [
        "Ziming Li",
        "Yan Zhou",
        "Yaxin Liu",
        "Fuqing Zhu",
        "Chuanpeng Yang",
        "Songlin Hu"
      ],
      "abstract": "Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (i.e., textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2022,
      "original_url": "https://aclanthology.org/2023.findings-acl.773/",
      "title": "Language acquisition: do children and language models follow similar learning stages?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.773.pdf",
      "authors": [
        "Linnea Evanson",
        "Yair Lakretz",
        "Jean Rémi King"
      ],
      "abstract": "During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of human children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some – but not all – learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2023,
      "original_url": "https://aclanthology.org/2023.findings-acl.774/",
      "title": "The Role of Output Vocabulary in T2T LMs for SPARQL Semantic Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.774.pdf",
      "authors": [
        "Debayan Banerjee",
        "Pranav Nair",
        "Ricardo Usbeck",
        "Chris Biemann"
      ],
      "abstract": "In this work, we analyse the role of output vocabulary for text-to-text (T2T) models on the task of SPARQL semantic parsing. We perform experiments within the the context of knowledge graph question answering (KGQA), where the task is to convert questions in natural language to the SPARQL query language. We observe that the query vocabulary is distinct from human vocabulary. Language Models (LMs) are pre-dominantly trained for human language tasks, and hence, if the query vocabulary is replaced with a vocabulary more attuned to the LM tokenizer, the performance of models may improve. We carry out carefully selected vocabulary substitutions on the queries and find absolute gains in the range of 17% on the GrailQA dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2024,
      "original_url": "https://aclanthology.org/2023.findings-acl.775/",
      "title": "UniCOQE: Unified Comparative Opinion Quintuple Extraction As A Set",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.775.pdf",
      "authors": [
        "Zinong Yang",
        "Feng Xu",
        "Jianfei Yu",
        "Rui Xia"
      ],
      "abstract": "Comparative Opinion Quintuple Extraction (COQE) aims to identify comparative opinion sentences in product reviews, extract comparative opinion elements in the sentences, and then incorporate them into quintuples. Existing methods decompose the COQE task into multiple primary subtasks and then solve them in a pipeline manner. However, these approaches ignore the intrinsic connection between subtasks and the error propagation among stages. This paper proposes a unified generative model, UniCOQE, to solve the COQE task in one shot. We design a generative template where all the comparative tuples are concatenated as the target output sequence. However, the multiple tuples are inherently not an ordered sequence but an unordered set. The pre-defined order will force the generative model to learn a false order bias and hinge the model’s training. To alleviate this bias, we introduce a new “predict-and-assign” training paradigm that models the golden tuples as a set. Specifically, we utilize a set-matching strategy to find the optimal order of tuples. The experimental results on multiple benchmarks show that our unified generative model significantly outperforms the SOTA method, and ablation experiments prove the effectiveness of the set-matching strategy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2025,
      "original_url": "https://aclanthology.org/2023.findings-acl.776/",
      "title": "Response-conditioned Turn-taking Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.776.pdf",
      "authors": [
        "Bing’er Jiang",
        "Erik Ekstedt",
        "Gabriel Skantze"
      ],
      "abstract": "Previous approaches to turn-taking and response generation in conversational systems have treated it as a two-stage process: First, the end of a turn is detected (based on conversation history), then the system generates an appropriate response. Humans, however, do not take the turn just because it is likely, but also consider whether what they want to say fits the position. In this paper, we present a model (an extension of TurnGPT) that conditions the end-of-turn prediction on both conversation history and what the next speaker wants to say. We found that our model consistently outperforms the baseline model in a variety of metrics. The improvement is most prominent in two scenarios where turn predictions can be ambiguous solely from the conversation history: 1) when the current utterance contains a statement followed by a question; 2) when the end of the current utterance semantically matches the response. Treating the turn-prediction and response-ranking as a one-stage process, our findings suggest that our model can be used as an incremental response ranker, which can be applied in various settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2026,
      "original_url": "https://aclanthology.org/2023.findings-acl.777/",
      "title": "A Unified One-Step Solution for Aspect Sentiment Quad Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.777.pdf",
      "authors": [
        "Junxian Zhou",
        "Haiqin Yang",
        "Yuxuan He",
        "Hao Mou",
        "JunBo Yang"
      ],
      "abstract": "Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings of existing strong ASQP baselines and therefore propose a unified one-step solution for ASQP, namely One-ASQP, to detect the aspect categories and to identify the aspectopinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds several unique advantages: (1) by separating ASQP into two subtasks and solving them independently and simultaneously, we can avoid error propagation in pipeline-based methods and overcome slow training and inference in generation-based methods; (2) by introducing sentiment-specific horns tagging schema in a token-pair-based two-dimensional matrix, we can exploit deeper interactions between sentiment elements and efficiently decode the AOS triplets; (3) we design \"[NULL]” token can help us effectively identify the implicit aspects or opinions. Experiments on two benchmark datasets and our released two datasets demonstrate the advantages of our One-ASQP. The two new datasets are publicly released at https://www.github.com/Datastory-CN/ASQP-Datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2027,
      "original_url": "https://aclanthology.org/2023.findings-acl.778/",
      "title": "On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.778.pdf",
      "authors": [
        "Chenghao Xiao",
        "Yang Long",
        "Noura Al Moubayed"
      ],
      "abstract": "Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as “spurious contextualization” is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamics with different training temperatures, batch sizes and pooling methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2028,
      "original_url": "https://aclanthology.org/2023.findings-acl.779/",
      "title": "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.779.pdf",
      "authors": [
        "Marius Mosbach",
        "Tiago Pimentel",
        "Shauli Ravfogel",
        "Dietrich Klakow",
        "Yanai Elazar"
      ],
      "abstract": "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations. Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2029,
      "original_url": "https://aclanthology.org/2023.findings-acl.780/",
      "title": "Common Law Annotations: Investigating the Stability of Dialog System Output Annotations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.780.pdf",
      "authors": [
        "Seunggun Lee",
        "Alexandra DeLucia",
        "Nikita Nangia",
        "Praneeth Ganedi",
        "Ryan Guan",
        "Rubing Li",
        "Britney Ngaw",
        "Aditya Singhal",
        "Shalaka Vaidya",
        "Zijun Yuan",
        "Lining Zhang",
        "João Sedoc"
      ],
      "abstract": "Metrics for Inter-Annotator Agreement (IAA), like Cohen’s Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure or reproducibility. While researchers are encouraged to increase annotator agreement, this can lead to specific and tailored annotation guidelines. We hypothesize that this may result in diverging annotations from different groups. To study this, we first propose the Lee et al. Protocol (LEAP), a standardized and codified annotation protocol. LEAP strictly enforces transparency in the annotation process, which ensures reproducibility of annotation guidelines. Using LEAP to annotate a dialog dataset, we empirically show that while research groups may create reliable guidelines by raising agreement, this can cause divergent annotations across different research groups, thus questioning the validity of the annotations. Therefore, we caution NLP researchers against using reliability as a proxy for reproducibility and validity."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2030,
      "original_url": "https://aclanthology.org/2023.findings-acl.781/",
      "title": "HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.781.pdf",
      "authors": [
        "Yuqi Ren",
        "Deyi Xiong"
      ],
      "abstract": "Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating shortcut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify shortcut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple NLP tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on OOD samples, while not undermining the performance on IID data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2031,
      "original_url": "https://aclanthology.org/2023.findings-acl.782/",
      "title": "PMI-Align: Word Alignment With Point-Wise Mutual Information Without Requiring Parallel Training Data",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.782.pdf",
      "authors": [
        "Fatemeh Azadi",
        "Heshaam Faili",
        "Mohammad Javad Dousti"
      ],
      "abstract": "Word alignment has many applications including cross-lingual annotation projection, bilingual lexicon extraction, and the evaluation or analysis of translation outputs. Recent studies show that using contextualized embeddings from pre-trained multilingual language models could give us high quality word alignments without the need of parallel training data. In this work, we propose PMI-Align which computes and uses the point-wise mutual information between source and target tokens to extract word alignments, instead of the cosine similarity or dot product which is mostly used in recent approaches. Our experiments show that our proposed PMI-Align approach could outperform the rival methods on five out of six language pairs. Although our approach requires no parallel training data, we show that this method could also benefit the approaches using parallel data to fine-tune pre-trained language models on word alignments. Our code and data are publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2032,
      "original_url": "https://aclanthology.org/2023.findings-acl.783/",
      "title": "Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.783.pdf",
      "authors": [
        "Riccardo Orlando",
        "Simone Conia",
        "Roberto Navigli"
      ],
      "abstract": "Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs. Conversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives. However, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings – newspaper headlines, dialogues, and tweets, among others. In this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types. Having observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures. We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer. In conclusion, we claim that SRL is far from “solved”, and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates. We release our software and datasets at https://github.com/sapienzanlp/exploring-srl."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2033,
      "original_url": "https://aclanthology.org/2023.findings-acl.784/",
      "title": "DSPM-NLG: A Dual Supervised Pre-trained Model for Few-shot Natural Language Generation in Task-oriented Dialogue System",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.784.pdf",
      "authors": [
        "Yufan Wang",
        "Bowei Zou",
        "Rui Fan",
        "Ai Ti Aw",
        "Tingting He"
      ],
      "abstract": "In few-shot settings, fully conveying the semantic information of the dialogue act is a crucial challenge for Natural Language Generation (NLG) in the task-oriented dialogue system. An interesting fact is that NLG and Spoken Language Understanding (SLU) are a natural dual problem pair. Suppose the response generated by the NLG module can be restored to the corresponding dialogue act by the SLU module, which reflects that the generated response fully conveys the semantic information of the dialogue act. Based on this idea, a novel Dual Supervised Pre-trained Model for a few-shot Natural Language Generation (DSPM-NLG) is proposed to regularize the pre-training process. We adopt a joint model with a dual supervised framework to learn the dual correlation between NLG and SLU from the perspective of probability. In addition, a slot-masked strategy is designed to enable the model to focus better on the key slot-value pairs. DSPM-NLG is continuously trained on existing public large-scale annotated data, which thoroughly learns the duality between two tasks to enhance the semantically controlling and generalization abilities of the pre-trained model. Experiments demonstrate that our proposed model performs outstandingly on the few-shot benchmark dataset and outperforms the previous SOTA results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2034,
      "original_url": "https://aclanthology.org/2023.findings-acl.785/",
      "title": "TEPrompt: Task Enlightenment Prompt Learning for Implicit Discourse Relation Recognition",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.785.pdf",
      "authors": [
        "Wei Xiang",
        "Chao Liang",
        "Bang Wang"
      ],
      "abstract": "Implicit Discourse Relation Recognition (IDRR) aims at classifying the relation sense between two arguments without an explicit connective. Recently, the ConnPrompt (Xiang et al., 2022) has leveraged the powerful prompt learning for IDRR based on the fusion of multi-prompt decisions from three different yet much similar connective prediction templates. Instead of multi-prompt ensembling, we propose to design auxiliary tasks with enlightened prompt learning for the IDRR task. Although an auxiliary task is not used to directly output final prediction, we argue that during the joint training some of its learned features can be useful to boost the main task. In light of such motivations, we propose a task enlightenment prompt learning model, called TEPrompt, to fuse learned features from three related tasks for IDRR. In particular, the TEPrompt contains three tasks, viz., Discourse Relation Recognition (DRR), Sense Semantics Classification (SSC) and Annotated Connective Prediction (ACP), each with a unique prompt template and an answer space. In the training phase, we jointly train three prompt learning tasks with shared argument representation. In the testing phase, we only take the DRR output with fused features as the final IDRR decision. Experiments with the same conditions have shown that the proposed TEPrompt outperforms the ConnPrompt. This can be attributed to the promoted decision features and language models benefited from joint-training of auxiliary tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2035,
      "original_url": "https://aclanthology.org/2023.findings-acl.786/",
      "title": "Evaluating Factuality in Cross-lingual Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.786.pdf",
      "authors": [
        "Mingqi Gao",
        "Wenqing Wang",
        "Xiaojun Wan",
        "Yuemei Xu"
      ],
      "abstract": "Cross-lingual summarization aims to help people efficiently grasp the core idea of the document written in a foreign language. Modern text summarization models generate highly fluent but often factually inconsistent outputs, which has received heightened attention in recent research. However, the factual consistency of cross-lingual summarization has not been investigated yet. In this paper, we propose a cross-lingual factuality dataset by collecting human annotations of reference summaries as well as generated summaries from models at both summary level and sentence level. Furthermore, we perform the fine-grained analysis and observe that over 50% of generated summaries and over 27% of reference summaries contain factual errors with characteristics different from monolingual summarization. Existing evaluation metrics for monolingual summarization require translation to evaluate the factuality of cross-lingual summarization and perform differently at different tasks and levels. Finally, we adapt the monolingual factuality metrics as an initial step towards the automatic evaluation of summarization factuality in cross-lingual settings. Our dataset and code are available at https://github.com/kite99520/Fact_CLS."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2036,
      "original_url": "https://aclanthology.org/2023.findings-acl.787/",
      "title": "On the Correspondence between Compositionality and Imitation in Emergent Neural Communication",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.787.pdf",
      "authors": [
        "Emily Cheng",
        "Mathieu Rita",
        "Thierry Poibeau"
      ],
      "abstract": "Compositionality is a hallmark of human language that not only enables linguistic generalization, but also potentially facilitates acquisition. When simulating language emergence with neural networks, compositionality has been shown to improve communication performance; however, its impact on imitation learning has yet to be investigated. Our work explores the link between compositionality and imitation in a Lewis game played by deep neural agents. Our contributions are twofold: first, we show that the learning algorithm used to imitate is crucial: supervised learning tends to produce more average languages, while reinforcement learning introduces a selection pressure toward more compositional languages. Second, our study reveals that compositional languages are easier to imitate, which may induce the pressure toward compositional languages in RL imitation settings."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2037,
      "original_url": "https://aclanthology.org/2023.findings-acl.788/",
      "title": "The Coreference under Transformation Labeling Dataset: Entity Tracking in Procedural Texts Using Event Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.788.pdf",
      "authors": [
        "Kyeongmin Rim",
        "Jingxuan Tu",
        "Bingyang Ye",
        "Marc Verhagen",
        "Eben Holderness",
        "James Pustejovsky"
      ],
      "abstract": "We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accurately link entities in anaphoric and coreference relations without an understanding of the transformations those entities undergo. We show how adding event semantics helps to better model entity coreference. We argue that all transformation predicates, not just creation verbs, introduce a new entity into the discourse, as a kind of generalized Result Role, which is typically not textually mentioned. This allows us to model procedural texts as process graphs and to compute the coreference type for any two entities in the recipe. We present our annotation methodology and the corpus generated as well as describe experiments on coreference resolution of entity mentions under a process-oriented model of events."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2038,
      "original_url": "https://aclanthology.org/2023.findings-acl.789/",
      "title": "Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.789.pdf",
      "authors": [
        "Tianjian Li",
        "Kenton Murray"
      ],
      "abstract": "Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. Although the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68% and improves the ROUGE-L score by 1.5 on average."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2039,
      "original_url": "https://aclanthology.org/2023.findings-acl.790/",
      "title": "Distractor Generation based on Text2Text Language Models with Pseudo Kullback-Leibler Divergence Regulation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.790.pdf",
      "authors": [
        "Hui-Juan Wang",
        "Kai-Yu Hsieh",
        "Han-Cheng Yu",
        "Jui-Ching Tsou",
        "Yu An Shih",
        "Chen-Hua Huang",
        "Yao-Chung Fan"
      ],
      "abstract": "In this paper, we address the task of cloze-style multiple choice question (MCQs) distractor generation. Our study is featured by the following designs. First, we propose to formulate the cloze distractor generation as a Text2Text task. Second, we propose pseudo Kullback-Leibler Divergence for regulating the generation to consider the item discrimination index in education evaluation. Third, we explore the candidate augmentation strategy and multi-tasking training with cloze-related tasks to further boost the generation performance. Through experiments with benchmarking datasets, our best perfomring model advances the state-of-the-art result from 10.81 to 22.00 (p@1 score)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2040,
      "original_url": "https://aclanthology.org/2023.findings-acl.791/",
      "title": "Lexical Translation Inconsistency-Aware Document-Level Translation Repair",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.791.pdf",
      "authors": [
        "Zhen Zhang",
        "Junhui Li",
        "Shimin Tao",
        "Hao Yang"
      ],
      "abstract": "Following the idea of “one translation per discourse”, in this paper we aim to improve translation consistency via document-level translation repair (DocRepair), i.e., automatic post-editing on translations of documents. To this end, we propose a lexical translation inconsistency-aware DocRepair to explicitly model translation inconsistency. First we locate the inconsistency in automatic translation. Then we provide translation candidates for those inconsistency. Finally, we propose lattice-like input to properly model inconsistent tokens and phrases and their candidates. Experimental results on three document-level translation datasets show that based on G-Transformer, a state-of-the-art document-to-document (Doc2Doc) translation model, our Doc2Doc DocRepair achieves significant improvement on translation quality in BLEU scores, but also greatly improves lexical translation consistency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2041,
      "original_url": "https://aclanthology.org/2023.findings-acl.792/",
      "title": "CausalDialogue: Modeling Utterance-level Causality in Conversations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.792.pdf",
      "authors": [
        "Yi-Lin Tuan",
        "Alon Albalak",
        "Wenda Xu",
        "Michael Saxon",
        "Connor Pryor",
        "Lise Getoor",
        "William Yang Wang"
      ],
      "abstract": "Despite their widespread adoption, neural conversation models have yet to exhibit natural chat capabilities with humans. In this research, we examine user utterances as causes and generated responses as effects, recognizing that changes in a cause should produce a different effect. To further explore this concept, we have compiled and expanded upon a new dataset called CausalDialogue through crowd-sourcing. This dataset includes multiple cause-effect pairs within a directed acyclic graph (DAG) structure. Our analysis reveals that traditional loss functions struggle to effectively incorporate the DAG structure, leading us to propose a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models. To evaluate the needs of considering causality in dialogue generation, we built a comprehensive benchmark on CausalDialogue dataset using different models, inference, and training methods. Through experiments, we find that a causality-inspired loss like ExMATE can improve the diversity and agility of conventional loss function and there is still room for improvement to reach human-level quality on this new dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2042,
      "original_url": "https://aclanthology.org/2023.findings-acl.793/",
      "title": "Towards Unified Spoken Language Understanding Decoding via Label-aware Compact Linguistics Representations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.793.pdf",
      "authors": [
        "Zhihong Zhu",
        "Xuxin Cheng",
        "Zhiqi Huang",
        "Dongsheng Chen",
        "Yuexian Zou"
      ],
      "abstract": "Joint intent detection and slot filling models have shown promising success in recent years due to the high correlations between the two tasks. However, previous works independently decode the two tasks, which could result in misaligned predictions for both tasks. To address this shortcoming, we propose a novel method named Label-aware Compact Linguistics Representation (LCLR), which leverages label embeddings to jointly guide the decoding process. Concretely, LCLR projects both task-specific hidden states into a joint label latent space, where both task-specific hidden states could be concisely represented as linear combinations of label embeddings. Such feature decomposition of task-specific hidden states increases the representing power for the linguistics of utterance. Extensive experiments on two single- and multi-intent SLU benchmarks prove that LCLR can learn more discriminative label information than previous separate decoders, and consistently outperform previous state-of-the-art methods across all metrics. More encouragingly, LCLR can be applied to boost the performance of existing approaches, making it easy to be incorporated into any existing SLU models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2043,
      "original_url": "https://aclanthology.org/2023.findings-acl.794/",
      "title": "Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.794.pdf",
      "authors": [
        "Liyan Tang",
        "Yifan Peng",
        "Yanshan Wang",
        "Ying Ding",
        "Greg Durrett",
        "Justin Rousseau"
      ],
      "abstract": "A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, “less likely brainstorming,” that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generation method that uses a novel contrastive learning strategy to encourage models to differentiate between generating likely and less likely outputs according to humans. We compare our method with several state-of-the-art controlled text generation models via automatic and human evaluations and show that our models’ capability of generating less likely outputs is improved."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2044,
      "original_url": "https://aclanthology.org/2023.findings-acl.795/",
      "title": "Language Modeling with Latent Situations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.795.pdf",
      "authors": [
        "Belinda Z. Li",
        "Maxwell Nye",
        "Jacob Andreas"
      ],
      "abstract": "Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SITUATIONSUPERVISION has two components: an *auxiliary situation modeling* task that trains models to predict entity state representations in context, and a *latent state inference* procedure that imputes these states from partially annotated training data. SITUATIONSUPERVISION can be applied via fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, it requires only a small number of state annotations to produce substantial coherence improvements (up to an 16% reduction in errors), showing that standard LMs can be efficiently adapted to explicitly model language and aspects of its meaning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2045,
      "original_url": "https://aclanthology.org/2023.findings-acl.796/",
      "title": "Can Cross-Lingual Transferability of Multilingual Transformers Be Activated Without End-Task Data?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.796.pdf",
      "authors": [
        "Zewen Chi",
        "Heyan Huang",
        "Xian-Ling Mao"
      ],
      "abstract": "Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when end-task data are unavailable. In this work, we explore whether the cross-lingual transferability can be activated without end-task data. We propose a cross-lingual transfer method, named PlugIn-X. PlugIn-X disassembles monolingual and multilingual Transformers into sub-modules, and reassembles them to be the multilingual end-task model. After representation adaptation, PlugIn-X finally performs cross-lingual transfer in a plug-and-play style. Experimental results show that PlugIn-X successfully activates the cross-lingual transferability of multilingual Transformers without accessing end-task data. Moreover, we analyze how the cross-model representation alignment affects the cross-lingual transferability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2046,
      "original_url": "https://aclanthology.org/2023.findings-acl.797/",
      "title": "Focus-aware Response Generation in Inquiry Conversation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.797.pdf",
      "authors": [
        "Yiquan Wu",
        "Weiming Lu",
        "Yating Zhang",
        "Adam Jatowt",
        "Jun Feng",
        "Changlong Sun",
        "Fei Wu",
        "Kun Kuang"
      ],
      "abstract": "Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in inquiry conversation by taking the focus into consideration. We propose a novel Focus-aware Response Generation (FRG) method by jointly optimizing a multi-level encoder and a set of focal decoders to generate several candidate responses that correspond to different focuses. Additionally, a focus ranking module is proposed to predict the next focus and rank the candidate responses. Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and human evaluation compared to the state-of-the-art approaches."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2047,
      "original_url": "https://aclanthology.org/2023.findings-acl.798/",
      "title": "A Hierarchical Explanation Generation Method Based on Feature Interaction Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.798.pdf",
      "authors": [
        "Yiming Ju",
        "Yuanzhe Zhang",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "The opaqueness of deep NLP models has motivated efforts to explain how deep models predict. Recently, work has introduced hierarchical attribution explanations, which calculate attribution scores for compositional text hierarchically to capture compositional semantics. Existing work on hierarchical attributions tends to limit the text groups to a continuous text span, which we call the connecting rule. While easy for humans to read, limiting the attribution unit to a continuous span might lose important long-distance feature interactions for reflecting model predictions. In this work, we introduce a novel strategy for capturing feature interactions and employ it to build hierarchical explanations without the connecting rule. The proposed method can convert ubiquitous non-hierarchical explanations (e.g., LIME) into their corresponding hierarchical versions. Experimental results show the effectiveness of our approach in building high-quality hierarchical explanations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2048,
      "original_url": "https://aclanthology.org/2023.findings-acl.799/",
      "title": "Jointly Reparametrized Multi-Layer Adaptation for Efficient and Private Tuning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.799.pdf",
      "authors": [
        "Umang Gupta",
        "Aram Galstyan",
        "Greg Ver Steeg"
      ],
      "abstract": "Efficient finetuning of pretrained language transformers is becoming increasingly prevalent for solving natural language processing tasks. While effective, it can still require a large number of tunable parameters. This can be a drawback for low-resource applications and training with differential-privacy constraints, where excessive noise may be introduced during finetuning. To this end, we propose a novel language transformer finetuning strategy that introduces task-specific parameters in multiple transformer layers. These parameters are derived from fixed random projections of a single trainable vector, enabling finetuning with significantly fewer parameters while maintaining performance. We achieve within 5% of full finetuning performance on GLUE tasks with as few as 4,100 parameters per task, outperforming other parameter-efficient finetuning approaches that use a similar number of per-task parameters. Besides, the random projections can be precomputed at inference, avoiding additional computational latency. All these make our method particularly appealing for low-resource applications. Finally, our method achieves the best or comparable utility compared to several recent finetuning methods when training with the same privacy constraints, underscoring its effectiveness and potential real-world impact."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2049,
      "original_url": "https://aclanthology.org/2023.findings-acl.800/",
      "title": "A Diffusion Model for Event Skeleton Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.800.pdf",
      "authors": [
        "Fangqi Zhu",
        "Lin Zhang",
        "Jun Gao",
        "Bing Qin",
        "Ruifeng Xu",
        "Haiqin Yang"
      ],
      "abstract": "Event skeleton generation, aiming to induce an event schema skeleton graph with abstracted event nodes and their temporal relations from a set of event instance graphs, is a critical step in the temporal complex event schema induction task. Existing methods effectively address this task from a graph generation perspective but suffer from noise-sensitive and error accumulation, e.g., the inability to correct errors while generating schema. We, therefore, propose a novel Diffusion Event Graph Model (DEGM) to address these issues. Our DEGM is the first workable diffusion model for event skeleton generation, where the embedding and rounding techniques with a custom edge-based loss are introduced to transform a discrete event graph into learnable latent representations. Furthermore, we propose a denoising training process to maintain the model’s robustness. Consequently, DEGM derives the final schema, where error correction is guaranteed by iteratively refining the latent representations during the schema generation process. Experimental results on three IED bombing datasets demonstrate that our DEGM achieves better results than other state-of-the-art baselines. Our code and data are available at https://github.com/zhufq00/EventSkeletonGeneration."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2050,
      "original_url": "https://aclanthology.org/2023.findings-acl.801/",
      "title": "Nonparametric Decoding for Generative Retrieval",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.801.pdf",
      "authors": [
        "Hyunji Lee",
        "JaeYoung Kim",
        "Hoyeon Chang",
        "Hanseok Oh",
        "Sohee Yang",
        "Vladimir Karpukhin",
        "Yi Lu",
        "Minjoon Seo"
      ],
      "abstract": "The generative retrieval model depends solely on the information encoded in its model parameters without external memory, its information capacity is limited and fixed. To overcome the limitation, we propose Nonparametric Decoding (Np Decoding) which can be applied to existing generative retrieval models. Np Decoding uses nonparametric contextualized vocab embeddings (external memory) rather than vanilla vocab embeddings as decoder vocab embeddings. By leveraging the contextualized vocab embeddings, the generative retrieval model is able to utilize both the parametric and nonparametric space. Evaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document retrieval task shows that applying Np Decoding to generative retrieval models significantly improves the performance. We also show that Np Decoding is data- and parameter-efficient, and shows high performance in the zero-shot setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2051,
      "original_url": "https://aclanthology.org/2023.findings-acl.802/",
      "title": "Aspect-aware Unsupervised Extractive Opinion Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.802.pdf",
      "authors": [
        "Haoyuan Li",
        "Somnath Basu Roy Chowdhury",
        "Snigdha Chaturvedi"
      ],
      "abstract": "Extractive opinion summarization extracts sentences from users’ reviews to represent the prevalent opinions about a product or service. However, the extracted sentences can be redundant and may miss some important aspects, especially for centroid-based extractive summarization models (Radev et al., 2004). To alleviate these issues, we introduce TokenCluster– a method for unsupervised extractive opinion summarization that automatically identifies the aspects described in the review sentences and then extracts sentences based on their aspects. It identifies the underlying aspects of the review sentences using roots of noun phrases and adjectives appearing in them. Empirical evaluation shows that TokenCluster improves aspect coverage in summaries and achieves strong performance on multiple opinion summarization datasets, for both general and aspect-specific summarization. We also perform extensive ablation and human evaluation studies to validate the design choices of our method. The implementation of our work is available at https://github.com/leehaoyuan/TokenCluster"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2052,
      "original_url": "https://aclanthology.org/2023.findings-acl.803/",
      "title": "GNN-SL: Sequence Labeling Based on Nearest Examples via GNN",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.803.pdf",
      "authors": [
        "Shuhe Wang",
        "Yuxian Meng",
        "Rongbin Ouyang",
        "Jiwei Li",
        "Tianwei Zhang",
        "Lingjuan Lyu",
        "Guoyin Wang"
      ],
      "abstract": "To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and resultscomparable to SOTA performances on NER datasets, and POS datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2053,
      "original_url": "https://aclanthology.org/2023.findings-acl.804/",
      "title": "Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.804.pdf",
      "authors": [
        "Xinyi Wang",
        "Zitao Wang",
        "Wei Hu"
      ],
      "abstract": "Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2054,
      "original_url": "https://aclanthology.org/2023.findings-acl.805/",
      "title": "Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.805.pdf",
      "authors": [
        "Haw-Shiuan Chang",
        "Zonghai Yao",
        "Alolika Gon",
        "Hong Yu",
        "Andrew McCallum"
      ],
      "abstract": "Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently. Based on the finding, we propose several softmax alternatives by simplifying the pointer networks and accelerating the word-by-word rerankers. In GPT-2, our proposals are significantly better and more efficient than mixture of softmax, a state-of-the-art softmax alternative. In summarization experiments, without very significantly decreasing its training/testing speed, our best method based on T5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and improves MAUVE scores by 30% in BookSum paragraph-level dataset."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2055,
      "original_url": "https://aclanthology.org/2023.findings-acl.806/",
      "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.806.pdf",
      "authors": [
        "Linyi Yang",
        "Shuibai Zhang",
        "Libo Qin",
        "Yafu Li",
        "Yidong Wang",
        "Hanmeng Liu",
        "Jindong Wang",
        "Xing Xie",
        "Yue Zhang"
      ],
      "abstract": "Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2056,
      "original_url": "https://aclanthology.org/2023.findings-acl.807/",
      "title": "Investigating the Saliency of Sentiment Expressions in Aspect-Based Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.807.pdf",
      "authors": [
        "Joachim Wagner",
        "Jennifer Foster"
      ],
      "abstract": "We examine the behaviour of an aspect-based sentiment classifier built by fine-tuning the BERT BASE model on the SemEval 2016 English dataset. In a set of masking experiments, we examine the extent to which the tokens identified as salient by LIME and a gradient-based method are being used by the classifier. We find that both methods are able to produce faithful rationales, with LIME outperforming the gradient-based method. We also identify a set of manually annotated sentiment expressions for this dataset, and carry out more masking experiments with these as human rationales. The enhanced performance of a classifier that only sees the relevant sentiment expressions suggests that they are not being used to their full potential. A comparison of the LIME and gradient rationales with the sentiment expressions reveals only a moderate level of agreement. Some disagreements are related to the fixed length of the rationales and the tendency of the rationales to contain content words related to the aspect itself."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2057,
      "original_url": "https://aclanthology.org/2023.findings-acl.808/",
      "title": "DMLM: Descriptive Masked Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.808.pdf",
      "authors": [
        "Edoardo Barba",
        "Niccolò Campolungo",
        "Roberto Navigli"
      ],
      "abstract": "Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word in a context, being provided with the word’s definition. For instance, given the sentence “I was going to the _”, if we provided as definition “financial institution”, the model would have to predict the word “bank”; if, instead, we provided “sandy seashore”, the model should predict “beach”. Our evaluation highlights the effectiveness of DMLM in comparison with standard MLM, showing improvements on a number of well-established NLU benchmarks, as well as other semantics-focused tasks, e.g., Semantic Role Labeling. Furthermore, we demonstrate how it is possible to take full advantage of DMLM to embed explicit semantics in downstream tasks, explore several properties of DMLM-based contextual representations and suggest a number of future directions to investigate."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2058,
      "original_url": "https://aclanthology.org/2023.findings-acl.809/",
      "title": "Reproducibility in NLP: What Have We Learned from the Checklist?",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.809.pdf",
      "authors": [
        "Ian Magnusson",
        "Noah A. Smith",
        "Jesse Dodge"
      ],
      "abstract": "Scientific progress in NLP rests on the reproducibility of researchers’ claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist’s introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44% of submissions that gather new data are 5% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2% lower relative to the rest. We find that only 46% of submissions claim to open-source their code, though submissions that do have 8% higher reproducibility score relative to those that do not, the most for any item. We discuss what can be inferred about the state of reproducibility in NLP, and provide a set of recommendations for future conferences, including: a) allowing submitting code and appendices one week after the deadline, and b) measuring dataset reproducibility by a checklist of data collection practices."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2059,
      "original_url": "https://aclanthology.org/2023.findings-acl.810/",
      "title": "Domain Generalization via Switch Knowledge Distillation for Robust Review Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.810.pdf",
      "authors": [
        "You Zhang",
        "Jin Wang",
        "Liang-Chih Yu",
        "Dan Xu",
        "Xuejie Zhang"
      ],
      "abstract": "Applying neural models injected with in-domain user and product information to learn review representations of unseen or anonymous users incurs an obvious obstacle in content-based recommender systems. For the generalization of the in-domain classifier, most existing models train an extra plain-text model for the unseen domain. Without incorporating historical user and product information, such a schema makes unseen and anonymous users dissociate from the recommender system. To simultaneously learn the review representation of both existing and unseen users, this study proposed a switch knowledge distillation for domain generalization. A generalization-switch (GSwitch) model was initially applied to inject user and product information by flexibly encoding both domain-invariant and domain-specific features. By turning the status ON or OFF, the model introduced a switch knowledge distillation to learn a robust review representation that performed well for either existing or anonymous unseen users. The empirical experiments were conducted on IMDB, Yelp-2013, and Yelp-2014 by masking out users in test data as unseen and anonymous users. The comparative results indicate that the proposed method enhances the generalization capability of several existing baseline models. For reproducibility, the code for this paper is available at: https://github.com/yoyo-yun/DG_RRR."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2060,
      "original_url": "https://aclanthology.org/2023.findings-acl.811/",
      "title": "On Search Strategies for Document-Level Neural Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.811.pdf",
      "authors": [
        "Christian Herold",
        "Hermann Ney"
      ],
      "abstract": "Compared to sentence-level systems, document-level neural machine translation (NMT) models produce a more consistent output across a document and are able to better resolve ambiguities within the input. There are many works on document-level NMT, mostly focusing on modifying the model architecture or training strategy to better accommodate the additional context-input. On the other hand, in most works, the question on how to perform search with the trained model is scarcely discussed, sometimes not mentioned at all. In this work, we aim to answer the question how to best utilize a context-aware translation model in decoding. We start with the most popular document-level NMT approach and compare different decoding schemes, some from the literature and others proposed by us. In the comparison, we are using both, standard automatic metrics, as well as specific linguistic phenomena on three standard document-level translation benchmarks. We find that most commonly used decoding strategies perform similar to each other and that higher quality context information has the potential to further improve the translation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2061,
      "original_url": "https://aclanthology.org/2023.findings-acl.812/",
      "title": "Causal Intervention for Mitigating Name Bias in Machine Reading Comprehension",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.812.pdf",
      "authors": [
        "Jiazheng Zhu",
        "Shaojuan Wu",
        "Xiaowang Zhang",
        "Yuexian Hou",
        "Zhiyong Feng"
      ],
      "abstract": "Machine Reading Comprehension (MRC) is to answer questions based on a given passage, which has made great achievements using pre-trained Language Models (LMs). We study the robustness of MRC models to names which is flexible and repeatability. MRC models based on LMs may overuse the name information to make predictions, which causes the representation of names to be non-interchangeable, called name bias. In this paper, we propose a novel Causal Interventional paradigm for MRC (CI4MRC) to mitigate name bias. Specifically, we uncover that the pre-trained knowledge concerning names is indeed a confounder by analyzing the causalities among the pre-trained knowledge, context representation and answers based on a Structural Causal Model (SCM). We develop effective CI4MRC algorithmic implementations to constrain the confounder based on the neuron-wise and token-wise adjustments. Experiments demonstrate that our proposed CI4MRC effectively mitigates the name bias and achieves competitive performance on the original SQuAD. Moreover, our method is general to various pre-trained LMs and performs robustly on the adversarial datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2062,
      "original_url": "https://aclanthology.org/2023.findings-acl.813/",
      "title": "Counterfactual Probing for the Influence of Affect and Specificity on Intergroup Bias",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.813.pdf",
      "authors": [
        "Venkata Subrahmanyan Govindarajan",
        "David Beaver",
        "Kyle Mahowald",
        "Junyi Jessy Li"
      ],
      "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts — thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR reliably use affect in classification, the model’s usage of specificity is inconclusive."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2063,
      "original_url": "https://aclanthology.org/2023.findings-acl.814/",
      "title": "SongRewriter: A Chinese Song Rewriting System with Controllable Content and Rhyme Scheme",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.814.pdf",
      "authors": [
        "Yusen Sun",
        "Liangyou Li",
        "Qun Liu",
        "Dit-Yan Yeung"
      ],
      "abstract": "Although lyrics generation has achieved significant progress in recent years, it has limited practical applications because the generated lyrics cannot be performed without composing compatible melodies. In this work, we bridge this practical gap by proposing a song rewriting system which rewrites the lyrics of an existing song such that the generated lyrics are compatible with the rhythm of the existing melody and thus singable. In particular, we propose SongRewriter, a controllable Chinese lyric generation and editing system which assists users without prior knowledge of melody composition. The system is trained by a randomized multi-level masking strategy which produces a unified model for generating entirely new lyrics or editing a few fragments. To improve the controllabiliy of the generation process, we further incorporate a keyword prompt to control the lexical choices of the content and propose novel decoding constraints and a vowel modeling task to enable flexible end and internal rhyme schemes. While prior rhyming metrics are mainly for rap lyrics, we propose three novel rhyming evaluation metrics for song lyrics. Both automatic and human evaluations show that the proposed model performs better than the state-of-the-art models in both contents and rhyming quality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2064,
      "original_url": "https://aclanthology.org/2023.findings-acl.815/",
      "title": "Triplet-Free Knowledge-Guided Response Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.815.pdf",
      "authors": [
        "Dongming Li",
        "Jianfeng Liu",
        "Baoyuan Wang"
      ],
      "abstract": "Generating vivid and informative responses (e.g., comments for social posts and utterances for dialogues) is challenging without giving relevant knowledge. Prior works focus on constructing the ”latent” knowledge first and then learning how to ”ground” it based on pseudo (context, knowledge, response) triplets. However, the retrieval between real responses and their latent knowledge is difficult in nature. In this paper, instead of focusing on how to ground knowledge given the responses, we take a different perspective to optimize the final responses for given guided knowledge directly. This allows us to re-formulate the entire problem in a simplified yet more scalable way. Specifically, we pretrain a response language model (LM) to measure the relevance and consistency between any context and response, then use search engines to collect the top-ranked passages to serve as the guiding knowledge without explicitly optimizing the ‘‘best” latent knowledge that corresponds to a given response. The final response generation model is trained through reinforcement learning by taking both the response LM prior and knowledge-injection rate as rewards. For better evaluations, we construct a new Chinese benchmark, ”IceKC”, using fresh multimodal online social posts. Both automatic evaluations and human evaluations show our zero-resource approach performs significantly better than prior works."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2065,
      "original_url": "https://aclanthology.org/2023.findings-acl.816/",
      "title": "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.816.pdf",
      "authors": [
        "Matthew Raffel",
        "Lizhong Chen"
      ],
      "abstract": "Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment’s attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2066,
      "original_url": "https://aclanthology.org/2023.findings-acl.817/",
      "title": "Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.817.pdf",
      "authors": [
        "Wanlong Liu",
        "Shaohuan Cheng",
        "Dingyi Zeng",
        "Qu Hong"
      ],
      "abstract": "Document-level event argument extraction poses new challenges of long input and cross-sentence inference compared to its sentence-level counterpart. However, most prior works focus on capturing the relations between candidate arguments and the event trigger in each event, ignoring two crucial points: a) non-argument contextual clue information; b) the relevance among argument roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling and latent Role Guidance) model, which contains two novel and effective modules for the above problem. The Span-Trigger-based Contextual Pooling (STCP) adaptively selects and aggregates the information of non-argument clue words based on the context attention weights of specific argument-trigger pairs from pre-trained model. The Role-based Latent Information Guidance (RLIG) module constructs latent role representations, makes them interact through role-interactive encoding to capture semantic relevance, and merges them into candidate arguments. Both STCP and RLIG introduce no more than 1% new parameters compared with the base model and can be easily applied to other event extraction models, which are compact and transplantable. Experiments on two public datasets show that our SCPRG outperforms previous state-of-the-art methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents respectively. Further analyses illustrate the interpretability of our model."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2067,
      "original_url": "https://aclanthology.org/2023.findings-acl.818/",
      "title": "Exploring the Impact of Vision Features in News Image Captioning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.818.pdf",
      "authors": [
        "Junzhe Zhang",
        "Xiaojun Wan"
      ],
      "abstract": "The task of news image captioning aims to generate a detailed caption which describes the specific information of an image in a news article. However, we find that recent state-of-art models can achieve competitive performance even without vision features. To resolve the impact of vision features in the news image captioning task, we conduct extensive experiments with mainstream models based on encoder-decoder framework. From our exploration, we find 1) vision features do contribute to the generation of news image captions; 2) vision features can assist models to better generate entities of captions when the entity information is sufficient in the input textual context of the given article; 3) Regions of specific objects in images contribute to the generation of related entities in captions."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2068,
      "original_url": "https://aclanthology.org/2023.findings-acl.819/",
      "title": "Using Collostructional Analysis to evaluate BERT’s representation of linguistic constructions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.819.pdf",
      "authors": [
        "Tim Veenboer",
        "Jelke Bloem"
      ],
      "abstract": "Collostructional analysis is a technique devised to find correlations between particular words and linguistic constructions in order to analyse meaning associations of these constructions. Contrasting collostructional analysis results with output from BERT might provide insights into the way BERT represents the meaning of linguistic constructions. This study tests to what extent English BERT’s meaning representations correspond to known constructions from the linguistics literature by means of two tasks that we propose. Firstly, by predicting the words that can be used in open slots of constructions, the meaning associations of more lexicalized constructions can be observed. Secondly, by finding similar sequences using BERT’s output embeddings and manually reviewing the resulting sentences, we can observe whether instances of less lexicalized constructions are clustered together in semantic space. These two methods show that BERT represents constructional meaning to a certain extent, but does not separate instances of a construction from a near-synonymous construction that has a different form."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2069,
      "original_url": "https://aclanthology.org/2023.findings-acl.820/",
      "title": "Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.820.pdf",
      "authors": [
        "Xingdi Yuan",
        "Tong Wang",
        "Yen-Hsiang Wang",
        "Emery Fine",
        "Rania Abdelghani",
        "Hélène Sauzéon",
        "Pierre-Yves Oudeyer"
      ],
      "abstract": "Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches, namely round-trip and prompt-based score, to selecting high-quality questions from a set of LLM-generated candidates. Our method works without the need to modify the underlying model, nor does it rely on human-annotated references — both of which are realistic constraints for real-world deployment of LLMs. With automatic as well as human evaluations, we empirically demonstrate that our approach can effectively select questions of higher qualities than greedy generation."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2070,
      "original_url": "https://aclanthology.org/2023.findings-acl.821/",
      "title": "Sentiment Knowledge Enhanced Self-supervised Learning for Multimodal Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.821.pdf",
      "authors": [
        "Fan Qian",
        "Jiqing Han",
        "Yongjun He",
        "Tieran Zheng",
        "Guibin Zheng"
      ],
      "abstract": "Multimodal Sentiment Analysis (MSA) has made great progress that benefits from extraordinary fusion scheme. However, there is a lack of labeled data, resulting in severe overfitting and poor generalization for supervised models applied in this field. In this paper, we propose Sentiment Knowledge Enhanced Self-supervised Learning (SKESL) to capture common sentimental patterns in unlabeled videos, which facilitates further learning on limited labeled data. Specifically, with the help of sentiment knowledge and non-verbal behavior, SKESL conducts sentiment word masking and predicts fine-grained word sentiment intensity, so as to embed sentiment information at the word level into pre-trained multimodal representation. In addition, a non-verbal injection method is also proposed to integrate non-verbal information into the word semantics. Experiments on two standard benchmarks of MSA clearly show that SKESL significantly outperforms the baseline, and achieves new State-Of-The-Art (SOTA) results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2071,
      "original_url": "https://aclanthology.org/2023.findings-acl.822/",
      "title": "Theory of Mind in Freely-Told Children’s Narratives: A Classification Approach",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.822.pdf",
      "authors": [
        "Bram van Dijk",
        "Marco Spruit",
        "Max van Duijn"
      ],
      "abstract": "Children are the focal point for studying the link between language and Theory of Mind (ToM) competence. Language and ToM are often studied with younger children and standardized tests, but as both are social competences, data and methods with higher ecological validity are critical. We leverage a corpus of 442 freely-told stories by Dutch children aged 4-12, recorded in their everyday classroom environments, to study language and ToM with NLP-tools. We labelled stories according to the mental depth of story characters children create, as a proxy for their ToM competence ‘in action’, and built a classifier with features encoding linguistic competences identified in existing work as predictive of ToM.We obtain good and fairly robust results (F1-macro = .71), relative to the complexity of the task for humans. Our results are explainable in that we link specific linguistic features such as lexical complexity and sentential complementation, that are relatively independent of children’s ages, to higher levels of character depth. This confirms and extends earlier work, as our study includes older children and socially embedded data from a different domain. Overall, our results support the idea that language and ToM are strongly interlinked, and that in narratives the former can scaffold the latter."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2072,
      "original_url": "https://aclanthology.org/2023.findings-acl.823/",
      "title": "Better Language Models of Code through Self-Improvement",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.823.pdf",
      "authors": [
        "Hung To",
        "Nghi Bui",
        "Jin L.C. Guo",
        "Tien Nguyen"
      ],
      "abstract": "Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which is then used for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs’ performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2073,
      "original_url": "https://aclanthology.org/2023.findings-acl.824/",
      "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.824.pdf",
      "authors": [
        "Mirac Suzgun",
        "Nathan Scales",
        "Nathanael Schärli",
        "Sebastian Gehrmann",
        "Yi Tay",
        "Hyung Won Chung",
        "Aakanksha Chowdhery",
        "Quoc Le",
        "Ed Chi",
        "Denny Zhou",
        "Jason Wei"
      ],
      "abstract": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the tasks for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2074,
      "original_url": "https://aclanthology.org/2023.findings-acl.825/",
      "title": "Score It All Together: A Multi-Task Learning Study on Automatic Scoring of Argumentative Essays",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.825.pdf",
      "authors": [
        "Yuning Ding",
        "Marie Bexte",
        "Andrea Horbach"
      ],
      "abstract": "When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. On the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from additional information about context, writing prompt and argument type. We then explore the different combinations of three tasks: automated span detection, type and quality prediction. Results show that a multi-task learning approach combining the three tasks outperforms sequential approaches that first learn to segment and then predict the quality/type of a segment."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2075,
      "original_url": "https://aclanthology.org/2023.findings-acl.826/",
      "title": "Data Sampling and (In)stability in Machine Translation Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.826.pdf",
      "authors": [
        "Chi-kiu Lo",
        "Rebecca Knowles"
      ],
      "abstract": "We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling procedure as separate from questions about human annotator consistency. We provide evidence that the latest data sampling approach used at WMT skews the annotated data toward shorter documents, not necessarily representative of the full test set. Lastly, we examine a new data sampling method that uses the available labour budget to sample data in a more representative manner, with the goals of improving representation of various document lengths in the sample and producing more stable rankings of system translation quality."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2076,
      "original_url": "https://aclanthology.org/2023.findings-acl.827/",
      "title": "Probing Graph Decomposition for Argument Pair Extraction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.827.pdf",
      "authors": [
        "Yang Sun",
        "Bin Liang",
        "Jianzhu Bao",
        "Yice Zhang",
        "Geng Tu",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "abstract": "Argument pair extraction (APE) aims to extract interactive argument pairs from two passages within a discussion. The key challenge of APE is to effectively capture the complex context-aware interactive relations of arguments between the two passages. In this paper, we elicit relational semantic knowledge from large-scale pre-trained language models (PLMs) via a probing technique. The induced sentence-level relational probing graph can help capture rich explicit interactive relations between argument pairs effectively. Since the relevance score of a sentence pair within a passage is generally larger than that of the sentence pair from different passages, each sentence would prefer to propagate information within the same passage and under-explore the interactive relations between two passages. To tackle this issue, we propose a graph decomposition method to decompose the probing graph into four sub-graphs from intra- and inter-passage perspectives, where the intra-passage graphs can help detect argument spans within each passage and the inter-passage graphs can help identify the argument pairs between the review and rebuttal passages. Experimental results on two benchmark datasets show that our method achieves substantial improvements over strong baselines for APE."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2077,
      "original_url": "https://aclanthology.org/2023.findings-acl.828/",
      "title": "DiffuSum: Generation Enhanced Extractive Summarization with Diffusion",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.828.pdf",
      "authors": [
        "Haopeng Zhang",
        "Xiao Liu",
        "Jiawei Zhang"
      ],
      "abstract": "Extractive summarization aims to form a summary by directly extracting sentences from the source document. Existing works mostly formulate it as a sequence labeling problem by making individual sentence label predictions. This paper proposes DiffuSum, a novel paradigm for extractive summarization, by directly generating the desired summary sentence representations with diffusion models and extracting sentences based on sentence representation matching. In addition, DiffuSum jointly optimizes a contrastive sentence encoder with a matching loss for sentence representation alignment and a multi-class contrastive loss for representation diversity. Experimental results show that DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail with ROUGE scores of 44.83/22.56/40.56. Experiments on the other two datasets with different summary lengths and cross-dataset evaluation also demonstrate the effectiveness of DiffuSum. The strong performance of our framework shows the great potential of adapting generative models for extractive summarization."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2078,
      "original_url": "https://aclanthology.org/2023.findings-acl.829/",
      "title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.829.pdf",
      "authors": [
        "Erica Kido Shimomoto",
        "Edison Marrese-Taylor",
        "Hiroya Takamura",
        "Ichiro Kobayashi",
        "Hideki Nakayama",
        "Yusuke Miyao"
      ],
      "abstract": "This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. Recent works tackled this task by improving query inputs with large pre-trained language models (PLM), at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs. Therefore, this paper studies the role of query sentence representation with PLMs in TVG and assesses the applicability of parameter-efficient training with NLP adapters. We couple popular PLMs with a selection of existing approaches and test different adapters to reduce the impact of the additional parameters. Our results on three challenging datasets show that, with the same visual inputs, TVG models greatly benefited from the PLM integration and fine-tuning, stressing the importance of the text query representation in this task. Furthermore, adapters were an effective alternative to full fine-tuning, even though they are not tailored to our task, allowing PLM integration in larger TVG models and delivering results comparable to SOTA models. Finally, our results shed light on which adapters work best in different scenarios."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2079,
      "original_url": "https://aclanthology.org/2023.findings-acl.830/",
      "title": "A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.830.pdf",
      "authors": [
        "Vladimir Araujo",
        "Alvaro Soto",
        "Marie-Francine Moens"
      ],
      "abstract": "Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms’ importance for memory models."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2080,
      "original_url": "https://aclanthology.org/2023.findings-acl.831/",
      "title": "Pay Attention to Implicit Attribute Values: A Multi-modal Generative Framework for AVE Task",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.831.pdf",
      "authors": [
        "Yupeng Zhang",
        "Shensi Wang",
        "Peiguang Li",
        "Guanting Dong",
        "Sirui Wang",
        "Yunsen Xian",
        "Zhoujun Li",
        "Hongzhi Zhang"
      ],
      "abstract": "Attribute Value Extraction (AVE) boosts many e-commerce platform services such as targeted recommendation, product retrieval and question answering. Most previous studies adopt an extractive framework such as named entity recognition (NER) to capture subtokens in the product descriptions as the corresponding values of target attributes. However, in the real world scenario, there also exist implicit attribute values that are not mentioned explicitly but embedded in the image information and implied text meaning of products, for which the power of extractive methods is severely constrained. To address the above issues, we exploit a unified multi-modal AVE framework named DEFLATE (a multi-modal unifieD framEwork For impLicit And expliciT AVE) to acquire implicit attribute values in addition to the explicit ones. DEFLATE consists of a QA-based generation model to produce candidate attribute values from the product information of different modalities, and a discriminative model to ensure the credibility of the generated answers. Meanwhile, to provide a testbed that close to the real world, we collect and annotate a multi-modal dataset with parts of implicit attribute values. Extensive experiments conducted on multiple datasets demonstrate that DEFLATE significantly outperforms previous methods on the extraction of implicit attribute values, while achieving comparable performance for the explicit ones."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2081,
      "original_url": "https://aclanthology.org/2023.findings-acl.832/",
      "title": "CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.832.pdf",
      "authors": [
        "Yijiang Dong",
        "Lara Martin",
        "Chris Callison-Burch"
      ],
      "abstract": "Story generation and understanding—as with all NLG/NLU tasks—has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for many flaws that neural networks have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re³) with minimal hand engineering. This work highlights the usefulness of code-based symbolic representations for enabling LLMs to better perform story reasoning tasks."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2082,
      "original_url": "https://aclanthology.org/2023.findings-acl.833/",
      "title": "Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.833.pdf",
      "authors": [
        "Yuval Reif",
        "Roy Schwartz"
      ],
      "abstract": "NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2083,
      "original_url": "https://aclanthology.org/2023.findings-acl.834/",
      "title": "Context-Aware Document Simplification",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.834.pdf",
      "authors": [
        "Liam Cripwell",
        "Joël Legrand",
        "Claire Gardent"
      ],
      "abstract": "To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of sentence-level simplification operations) and using this plan to guide sentence-level simplification downstream. However, this is still limited in that the simplification model has no direct access to the local inter-sentence document context, likely having a negative impact on surface realisation. We explore various systems that use document context within the simplification process itself, either by iterating over larger text units or by extending the system architecture to attend over a high-level representation of document context. In doing so, we achieve state-of-the-art performance on the document simplification task, even when not relying on plan-guidance. Further, we investigate the performance and efficiency tradeoffs of system variants and make suggestions of when each should be preferred."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2084,
      "original_url": "https://aclanthology.org/2023.findings-acl.835/",
      "title": "Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.835.pdf",
      "authors": [
        "Qianglong Chen",
        "Guohai Xu",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Luo Si",
        "Yin Zhang"
      ],
      "abstract": "Existing knowledge-enhanced methods have achieved remarkable results in certain Q&A tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into the contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanation using acquired symbolic knowledge and prompt as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledge for downstream task enhancement. We conduct a series of experiments on three widely-used question-answering datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the help of generated contrastive explanation, our CPACE model achieves new SOTA on CSQA (89.8% on the testing set, 0.9% higher than human performance), and gains impressive improvement on QASC and OBQA (4.2% and 3.5%, respectively)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2085,
      "original_url": "https://aclanthology.org/2023.findings-acl.836/",
      "title": "Abstract then Play: A Skill-centric Reinforcement Learning Framework for Text-based Games",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.836.pdf",
      "authors": [
        "Anjie Zhu",
        "Peng-Fei Zhang",
        "Yi Zhang",
        "Zi Huang",
        "Jie Shao"
      ],
      "abstract": "Text-based games present an exciting test-bed for reinforcement learning algorithms in the natural language environment. In these adventure games, an agent must learn to interact with the environment through text in order to accomplish tasks, facing large and combinational action space as well as partial observability issues. However, existing solutions fail to decompose the task and abstract the action autonomously, which either pre-specify the subtasks or pre-train on the human gameplay dataset. In this work, we introduce a novel skill-centric reinforcement learning framework, which is capable of abstracting the action in an end-to-end manner. To learn a more disentangled skill, we focus on the informativeness and distinguishability of the skill in accordance with the information bottleneck principle. Specifically, we introduce a discriminator to enable the skill to reflect the trajectory and push their representations onto the unit hypersphere to distribute uniformly. Moreover, a self-predictive mechanism is employed to learn inverse and forward dynamics, and a self-recovery mechanism is leveraged to refine the action representation, thus resulting in a more comprehensive perception of dynamics and more effective representations of textual state and action. Empirical experiments are carried out on the Jericho environment and the results validate the superiority against state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2086,
      "original_url": "https://aclanthology.org/2023.findings-acl.837/",
      "title": "SSP: Self-Supervised Post-training for Conversational Search",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.837.pdf",
      "authors": [
        "Quan Tu",
        "Shen Gao",
        "Xiaolong Wu",
        "Zhao Cao",
        "Ji-Rong Wen",
        "Rui Yan"
      ],
      "abstract": "Conversational search has been regarded as the next-generation search paradigm. Constrained by data scarcity, most existing methods distill the well-trained ad-hoc retriever to the conversational retriever. However, these methods, which usually initialize parameters by query reformulation to discover contextualized dependency, have trouble in understanding the dialogue structure information and struggle with contextual semantic vanishing. In this paper, we propose {pasted macro ‘FULLMODEL’} ({pasted macro ‘MODEL’}) which is a new post-training paradigm with three self-supervised tasks to efficiently initialize the conversational search model to enhance the dialogue structure and contextual semantic understanding. Furthermore, the {pasted macro ‘MODEL’} can be plugged into most of the existing conversational models to boost their performance. To verify the effectiveness of our proposed method, we apply the conversational encoder post-trained by {pasted macro ‘MODEL’} on the conversational search task using two benchmark datasets: CAsT-19 and CAsT-20.Extensive experiments that our {pasted macro ‘MODEL’} can boost the performance of several existing conversational search methods. Our source code is available at https://github.com/morecry/SSP."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2087,
      "original_url": "https://aclanthology.org/2023.findings-acl.838/",
      "title": "Towards Reference-free Text Simplification Evaluation with a BERT Siamese Network Architecture",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.838.pdf",
      "authors": [
        "Xinran Zhao",
        "Esin Durmus",
        "Dit-Yan Yeung"
      ],
      "abstract": "Text simplification (TS) aims to modify sentences to make their both content and structure easier to understand. Traditional n-gram matching-based TS evaluation metrics heavily rely on the exact token match and human-annotated simplified sentences. In this paper, we present a novel neural-network-based reference-free TS metric BETS that leverages pre-trained contextualized language representation models and large-scale paraphrasing datasets to evaluate simplicity and meaning preservation. We show that our metric, without collecting any costly human simplification reference, correlates better than existing metrics with human judgments for the quality of both overall simplification (+7.7%) and its key aspects, i.e., comparative simplicity (+11.2%) and meaning preservation (+9.2%)."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2088,
      "original_url": "https://aclanthology.org/2023.findings-acl.839/",
      "title": "Causal interventions expose implicit situation models for commonsense language understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.839.pdf",
      "authors": [
        "Takateru Yamakoshi",
        "James McClelland",
        "Adele Goldberg",
        "Robert Hawkins"
      ],
      "abstract": "Accounts of human language processing have long appealed to implicit “situation models” that enrich comprehension with relevant but unstated world knowledge. Here, we apply causal intervention techniques to recent transformer models to analyze performance on the Winograd Schema Challenge (WSC), where a single context cue shifts interpretation of an ambiguous pronoun. We identify a relatively small circuit of attention heads that are responsible for propagating information from the context word that guides which of the candidate noun phrases the pronoun ultimately attends to. We then compare how this circuit behaves in a closely matched “syntactic” control where the situation model is not strictly necessary. These analyses suggest a distinct pathway through which implicit situation models may be constructed to guide pronoun resolution"
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2089,
      "original_url": "https://aclanthology.org/2023.findings-acl.840/",
      "title": "Iterative Nearest Neighbour Machine Translation for Unsupervised Domain Adaptation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.840.pdf",
      "authors": [
        "Hui Huang",
        "Shuangzhi Wu",
        "Xinnian Liang",
        "Zefan Zhou",
        "Muyun Yang",
        "Tiejun Zhao"
      ],
      "abstract": "Unsupervised domain adaptation of machine translation, which adapts a pre-trained translation model to a specific domain without in-domain parallel data, has drawn extensive attention in recent years. However, most existing methods focus on the fine-tuning based techniques, which is non-extensible. In this paper, we propose a new method to perform unsupervised domain adaptation in a non-parametric manner. Our method only resorts to in-domain monolingual data, and we jointly perform nearest neighbour inference on both forward and backward translation directions. The forward translation model creates nearest neighbour datastore for the backward direction, and vice versa, strengthening each other in an iterative style. Experiments on multi-domain datasets demonstrate that our method significantly improves the in-domain translation performance and achieves state-of-the-art results among non-parametric methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2090,
      "original_url": "https://aclanthology.org/2023.findings-acl.841/",
      "title": "PruMUX: Augmenting Data Multiplexing with Model Compression",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.841.pdf",
      "authors": [
        "Yushan Su",
        "Vishvak Murahari",
        "Karthik Narasimhan",
        "Kai Li"
      ],
      "abstract": "As language models increase in size by the day, methods for efficient inference are critical to leveraging their capabilities for various applications. Prior work has investigated techniques like model pruning, knowledge distillation, and data multiplexing to increase model throughput without sacrificing accuracy. In this paper, we combine two such methods – structured pruning and data multiplexing – to compound the speedup gains obtained by either method. Our approach, PruMUX, obtains up to 7.5-29.5X throughput improvement over BERT-base model with accuracy threshold from 80% to 74%. We further study various combinations of parameters (such as sparsity and multiplexing factor) in the two techniques to provide a comprehensive analysis of the tradeoff between accuracy and throughput in the resulting models. We then propose Auto-PruMUX, a meta-level model that can predict the high-performance parameters for pruning and multiplexing given a desired accuracy loss budget, providing a practical method to leverage the combination effectively."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2091,
      "original_url": "https://aclanthology.org/2023.findings-acl.842/",
      "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.842.pdf",
      "authors": [
        "Nihar Sahoo",
        "Niteesh Mallela",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "In this paper, we describe our work on social bias detection in a low-resource multilingual setting in which the languages are from two very divergent families- Indo-European (English, Hindi, and Italian) and Altaic (Korean). Currently, the majority of the social bias datasets available are in English and this inhibits progress on social bias detection in low-resource languages. To address this problem, we introduce a new dataset for social bias detection in Hindi and investigate multilingual transfer learning using publicly available English, Italian, and Korean datasets. The Hindi dataset contains 9k social media posts annotated for (i) binary bias labels (bias/neutral), (ii) binary labels for sentiment (positive/negative), (iii) target groups for each bias category, and (iv) rationale for annotated bias labels (a short piece of text). We benchmark our Hindi dataset using different multilingual models, with XLM-R achieving the best performance of 80.8 macro-F1 score. Our results show that the detection of social biases in resource-constrained languages such as Hindi and Korean may be improved with the use of a similar dataset in English. We also show that translating all datasets into English does not work effectively for detecting social bias, since the nuances of source language are lost in translation. All the scripts and datasets utilized in this study will be publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2092,
      "original_url": "https://aclanthology.org/2023.findings-acl.843/",
      "title": "Don’t Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.843.pdf",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Xin Lu",
        "Bing Qin"
      ],
      "abstract": "As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings and thoughts of others via other-awareness. However, they ignore to include self-awareness to consider the own views of the self in their responses, which is a crucial process to achieve the empathy. To this end, we propose to generate Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically, three stages, self-other differentiation, self-other modulation and self-other generation, are devised to clearly maintain, regulate and inject the self-other aware information into the process of empathetic response generation. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of EmpSOA to generate more empathetic responses. Our source code will be publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2093,
      "original_url": "https://aclanthology.org/2023.findings-acl.844/",
      "title": "Are Layout-Infused Language Models Robust to Layout Distribution Shifts? A Case Study with Scientific Documents",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.844.pdf",
      "authors": [
        "Catherine Chen",
        "Zejiang Shen",
        "Dan Klein",
        "Gabriel Stanovsky",
        "Doug Downey",
        "Kyle Lo"
      ],
      "abstract": "Recent work has shown that infusing layout features into language models (LMs) improves processing of visually-rich documents such as scientific papers. Layout-infused LMs are often evaluated on documents with familiar layout features (e.g., papers from the same publisher), but in practice models encounter documents with unfamiliar distributions of layout features, such as new combinations of text sizes and styles, or new spatial configurations of textual elements. In this work we test whether layout-infused LMs are robust to layout distribution shifts. As a case study we use the task of scientific document structure recovery, segmenting a scientific paper into its structural categories (e.g., “title”, “caption”, “reference”). To emulate distribution shifts that occur in practice we re-partition the GROTOAP2 dataset. We find that under layout distribution shifts model performance degrades by up to 20 F1. Simple training strategies, such as increasing training diversity, can reduce this degradation by over 35% relative F1; however, models fail to reach in-distribution performance in any tested out-of-distribution conditions. This work highlights the need to consider layout distribution shifts during model evaluation, and presents a methodology for conducting such evaluations."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2094,
      "original_url": "https://aclanthology.org/2023.findings-acl.845/",
      "title": "Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.845.pdf",
      "authors": [
        "Yang Lin",
        "Xin Gao",
        "Xu Chu",
        "Yasha Wang",
        "Junfeng Zhao",
        "Chao Chen"
      ],
      "abstract": "Efforts have been made to apply topic seed words to improve the topic interpretability of topic models. However, due to the semantic diversity of natural language, supervisions from seed words could be ambiguous, making it hard to be incorporated into the current neural topic models. In this paper, we propose SeededNTM, a neural topic model enhanced with supervisions from seed words on both word and document levels. We introduce a context-dependency assumption to alleviate the ambiguities with context document information, and an auto-adaptation mechanism to automatically balance between multi-level information. Moreover, an intra-sample consistency regularizer is proposed to deal with noisy supervisions via encouraging perturbation and semantic consistency. Extensive experiments on multiple datasets show that SeededNTM can derive semantically meaningful topics and outperforms the state-of-the-art seeded topic models in terms of topic quality and classification accuracy."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2095,
      "original_url": "https://aclanthology.org/2023.findings-acl.846/",
      "title": "Learning from Children: Improving Image-Caption Pretraining via Curriculum",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.846.pdf",
      "authors": [
        "Hammad Ayyubi",
        "Rahul Lokesh",
        "Alireza Zareian",
        "Bo Wu",
        "Shih-Fu Chang"
      ],
      "abstract": "Image-caption pretraining has been quite successfully used for downstream vision tasks like zero-shot image classification and object detection. However, image-caption pretraining is still a hard problem – it requires multiple concepts (nouns) from captions to be aligned to several objects in images. To tackle this problem, we go to the roots – the best learner, children. We take inspiration from cognitive science studies dealing with children’s language learning to propose a curriculum learning framework. The learning begins with easy-to-align image caption pairs containing one concept per caption. The difficulty is progressively increased with each new phase by adding one more concept per caption. Correspondingly, the knowledge acquired in each learning phase is utilized in subsequent phases to effectively constrain the learning problem to aligning one new concept-object pair in each phase. We show that this learning strategy improves over vanilla image-caption training in various settings – pretraining from scratch, using a pretrained image or/and pretrained text encoder, low data regime etc."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2096,
      "original_url": "https://aclanthology.org/2023.findings-acl.847/",
      "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.847.pdf",
      "authors": [
        "Ethan Perez",
        "Sam Ringer",
        "Kamile Lukosiute",
        "Karina Nguyen",
        "Edwin Chen",
        "Scott Heiner",
        "Craig Pettit",
        "Catherine Olsson",
        "Sandipan Kundu",
        "Saurav Kadavath",
        "Andy Jones",
        "Anna Chen",
        "Benjamin Mann",
        "Brian Israel",
        "Bryan Seethor",
        "Cameron McKinnon",
        "Christopher Olah",
        "Da Yan",
        "Daniela Amodei",
        "Dario Amodei",
        "Dawn Drain",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Guro Khundadze",
        "Jackson Kernion",
        "James Landis",
        "Jamie Kerr",
        "Jared Mueller",
        "Jeeyoon Hyun",
        "Joshua Landau",
        "Kamal Ndousse",
        "Landon Goldberg",
        "Liane Lovitt",
        "Martin Lucas",
        "Michael Sellitto",
        "Miranda Zhang",
        "Neerav Kingsland",
        "Nelson Elhage",
        "Nicholas Joseph",
        "Noemi Mercado",
        "Nova DasSarma",
        "Oliver Rausch",
        "Robin Larson",
        "Sam McCandlish",
        "Scott Johnston",
        "Shauna Kravec",
        "Sheer El Showk",
        "Tamera Lanham",
        "Timothy Telleen-Lawton",
        "Tom Brown",
        "Tom Henighan",
        "Tristan Hume",
        "Yuntao Bai",
        "Zac Hatfield-Dodds",
        "Jack Clark",
        "Samuel R. Bowman",
        "Amanda Askell",
        "Roger Grosse",
        "Danny Hernandez",
        "Deep Ganguli",
        "Evan Hubinger",
        "Nicholas Schiefer",
        "Jared Kaplan"
      ],
      "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2097,
      "original_url": "https://aclanthology.org/2023.findings-acl.848/",
      "title": "Cross-Domain Argument Quality Estimation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.848.pdf",
      "authors": [
        "Michael Fromm",
        "Max Berrendorf",
        "Evgeniy Faerman",
        "Thomas Seidl"
      ],
      "abstract": "Argumentation is one of society’s foundational pillars, and, sparked by advances in NLP, and the vast availability of text data, automated mining of arguments receives increasing attention. A decisive property of arguments is their strength or quality. While there are works on the automated estimation of argument strength, their scope is narrow:They focus on isolated datasets and neglect the interactions with related argument-mining tasks, such as argument identification and evidence detection. In this work, we close this gap by approaching argument quality estimation from multiple different angles:Grounded on rich results from thorough empirical evaluations, we assess the generalization capabilities of argument quality estimation across diverse domains and the interplay with related argument mining tasks. We find that generalization depends on a sufficient representation of different domains in the training part. In zero-shot transfer and multi-task experiments, we reveal that argument quality is among the more challenging tasks but can improve others. We publish our code at https://github.com/fromm-m/acl-cross-domain-aq."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2098,
      "original_url": "https://aclanthology.org/2023.findings-acl.849/",
      "title": "DiaASQ: A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.849.pdf",
      "authors": [
        "Bobo Li",
        "Hao Fei",
        "Fei Li",
        "Yuhan Wu",
        "Jinsong Zhang",
        "Shengqiong Wu",
        "Jingye Li",
        "Yijiang Liu",
        "Lizi Liao",
        "Tat-Seng Chua",
        "Donghong Ji"
      ],
      "abstract": "The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinion mining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely DiaASQ, aiming to detect the quadruple of target-aspect-opinion-sentiment in a dialogue. We manually construct a large-scale high-quality DiaASQ dataset in both Chinese and English languages. We deliberately develop a neural model to benchmark the task, which advances in effectively performing end-to-end quadruple prediction, and manages to incorporate rich dialogue-specific and discourse feature representations for better cross-utterance quadruple extraction. We hope the new benchmark will spur more advancements in the sentiment analysis community."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2099,
      "original_url": "https://aclanthology.org/2023.findings-acl.850/",
      "title": "GeoDRL: A Self-Learning Framework for Geometry Problem Solving using Reinforcement Learning in Deductive Reasoning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.850.pdf",
      "authors": [
        "Shuai Peng",
        "Di Fu",
        "Yijun Liang",
        "Liangcai Gao",
        "Zhi Tang"
      ],
      "abstract": "Ensuring both interpretability and correctness is a great challenge in automated geometry problem solving (GPS), and the scarcity of labeled data hinders learning mathematical reasoning from samples. Therefore, we present GeoDRL, a self-learning geometry problem solving framework that integrates logic graph deduction and Deep Reinforcement Learning (DRL) to optimize geometry reasoning as a Markov Decision Process. GeoDRL employs a Graph Neural Network on a Geometry Logic Graph, updating the problem state using a symbolic system. Incorporating DRL into deductive reasoning enables GeoDRL to achieve unsupervised self-learning while maintaining correctness. GeoDRL, through unsupervised learning, exhibits enhanced accuracy in the Geometry3K dataset, improving by 11.1% over previous SOTA methods, and simultaneously boosts efficiency and interpretability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2100,
      "original_url": "https://aclanthology.org/2023.findings-acl.851/",
      "title": "Uncertainty-Aware Unlikelihood Learning Improves Generative Aspect Sentiment Quad Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.851.pdf",
      "authors": [
        "Mengting Hu",
        "Yinhao Bai",
        "Yike Wu",
        "Zhen Zhang",
        "Liqi Zhang",
        "Hang Gao",
        "Shiwan Zhao",
        "Minlie Huang"
      ],
      "abstract": "Recently, aspect sentiment quad prediction has received widespread attention in the field of aspect-based sentiment analysis. Existing studies extract quadruplets via pre-trained generative language models to paraphrase the original sentence into a templated target sequence. However, previous works only focus on what to generate but ignore what not to generate. We argue that considering the negative samples also leads to potential benefits. In this work, we propose a template-agnostic method to control the token-level generation, which boosts original learning and reduces mistakes simultaneously. Specifically, we introduce Monte Carlo dropout to understand the built-in uncertainty of pre-trained language models, acquiring the noises and errors. We further propose marginalized unlikelihood learning to suppress the uncertainty-aware mistake tokens. Finally, we introduce minimization entropy to balance the effects of marginalized unlikelihood learning. Extensive experiments on four public datasets demonstrate the effectiveness of our approach on various generation templates."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2101,
      "original_url": "https://aclanthology.org/2023.findings-acl.852/",
      "title": "Adversarial Knowledge Stimulated Contrastive Prompting for Few-shot Language Learners",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.852.pdf",
      "authors": [
        "Kai Zheng",
        "Qingfeng Sun",
        "Yaming Yang",
        "Tengchao Lv",
        "Yeyong Pi",
        "Changlin Zhao",
        "Fei Xu",
        "Qi Zhang"
      ],
      "abstract": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models(PLMs) on few-shot Natural Language Understanding (NLU) tasks by employing task-specific prompts. Yet, PLMsare unfamiliar with prompt-style expressionsduring pre-training, which limits the few-shotlearning performance on downstream tasks. It would be desirable if the models can stimulate prompting knowledge while adaptation to specific NLU tasks. We present the Adversarial Knowledge Stimulated Contrastive Prompting (AKSCP) framework, leading to better few-shot NLU tasks for language models by implicitly stimulate knowledge from pretrained language model. In AKSCP, a novel paradigm Cloze-driven prompt is proposed for joint prompt tuning across word cloze task and prompt-based learning, forcing PLMs to stimulate prompting knowledge. We further design an Adversarial Contrastive learning method to improve the generalization ability of PLM for different downstream tasks. Experiments over a variety of NLU tasks show that AKSCP consistently outperforms state-of-the-arts for prompt-based fine-tuning."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2102,
      "original_url": "https://aclanthology.org/2023.findings-acl.853/",
      "title": "Making Pre-trained Language Models Better Learn Few-Shot Spoken Language Understanding in More Practical Scenarios",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.853.pdf",
      "authors": [
        "Yufan Wang",
        "Jie Mei",
        "Bowei Zou",
        "Rui Fan",
        "Tingting He",
        "Ai Ti Aw"
      ],
      "abstract": "Most previous few-shot Spoken Language Understanding (SLU) models typically need to be trained on a set of data-rich source domains and adapt to the target domain with a few examples. In this paper, we explore a more practical scenario for few-shot SLU, in which we only assume access to a pre-trained language model and a few labeled examples without any other source domain data. We concentrate on understanding how far the few-shot SLU could be pushed in this setting. To this end, we develop a prompt-based intent detection model in few-shot settings, which leverages the BERT original pre-training next sentence prediction task and the prompt template to detect the user’s intent. For slot filling, we propose an approach of reconstructing slot labels, which reduces the training complexity by reducing the number of slot labels in few-shot settings. To evaluate the few-shot SLU for a more practical scenario, we present two benchmarks, FewShotATIS and FewShotSNIPS. And a dynamic sampling strategy is designed to construct the two datasets according to the learning difficulty of each intent and slot. Experiments on FewShotATIS and FewShotSNIPS demonstrate that our proposed model achieves state-of-the-art performance."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2103,
      "original_url": "https://aclanthology.org/2023.findings-acl.854/",
      "title": "Typology Guided Multilingual Position Representations: Case on Dependency Parsing",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.854.pdf",
      "authors": [
        "Tao Ji",
        "Yuanbin Wu",
        "Xiaoling Wang"
      ],
      "abstract": "Recent multilingual models benefit from strong unified semantic representation models. However, due to conflict linguistic regularities, ignoring language-specific features during multilingual learning may suffer from negative transfer. In this work, we analyze the relationbetween a language’s position space and its typological characterization, and suggest deploying different position spaces for different languages. We develop a position generation network which combines prior knowledge from typology features and existing position vectors. Experiments on the multilingual dependency parsing task show that the learned position vectors exhibit meaningful hidden structures, and they can help achieving the best multilingual parsing results."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2104,
      "original_url": "https://aclanthology.org/2023.findings-acl.855/",
      "title": "Learning Event-aware Measures for Event Coreference Resolution",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.855.pdf",
      "authors": [
        "Yao Yao",
        "Zuchao Li",
        "Hai Zhao"
      ],
      "abstract": "Researchers are witnessing knowledge-inspired natural language processing shifts the focus from entity-level to event-level, whereas event coreference resolution is one of the core challenges. This paper proposes a novel model for within-document event coreference resolution. On the basis of event but not entity as before, our model learns and integrates multiple representations from both event alone and event pair. For the former, we introduce multiple linguistics-motivated event alone features for more discriminative event representations. For the latter, we consider multiple similarity measures to capture the distinction of event pair. Our proposed model achieves new state-of-the-art on the ACE 2005 benchmark, demonstrating the effectiveness of our proposed framework."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2105,
      "original_url": "https://aclanthology.org/2023.findings-acl.856/",
      "title": "Second Language Acquisition of Neural Language Models",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.856.pdf",
      "authors": [
        "Miyu Oba",
        "Tatsuki Kuribayashi",
        "Hiroki Ouchi",
        "Taro Watanabe"
      ],
      "abstract": "With the success of neural language models (LMs), their language acquisition has gained much attention. This work sheds light on the second language (L2) acquisition of LMs, while previous work has typically explored their first language (L1) acquisition. Specifically, we trained bilingual LMs with a scenario similar to human L2 acquisition and analyzed their cross-lingual transfer from linguistic perspectives. Our exploratory experiments demonstrated that the L1 pretraining accelerated their linguistic generalization in L2, and language transfer configurations (e.g., the L1 choice, and presence of parallel texts) substantially affected their generalizations. These clarify their (non-)human-like L2 acquisition in particular aspects."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2106,
      "original_url": "https://aclanthology.org/2023.findings-acl.857/",
      "title": "On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.857.pdf",
      "authors": [
        "SongYang Gao",
        "Shihan Dou",
        "Qi Zhang",
        "Xuanjing Huang",
        "Jin Ma",
        "Ying Shan"
      ],
      "abstract": "Detecting adversarial samples that are carefully crafted to fool the model is a critical step to socially-secure applications. However, existing adversarial detection methods require access to sufficient training data, which brings noteworthy concerns regarding privacy leakage and generalizability. In this work, we validate that the adversarial sample generated by attack algorithms is strongly related to a specific vector in the high-dimensional inputs. Such vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated without original training data. Based on this discovery, we propose a data-agnostic adversarial detection framework, which induces different responses between normal and adversarial samples to UAPs. Experimental results show that our method achieves competitive detection performance on various text classification tasks, and maintains an equivalent time consumption to normal inference."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2107,
      "original_url": "https://aclanthology.org/2023.findings-acl.858/",
      "title": "Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.858.pdf",
      "authors": [
        "Fangyi Yu",
        "Lee Quartey",
        "Frank Schilder"
      ],
      "abstract": "The use of large language models (LLMs) for zero- or few-shot prompting in natural language processing has given rise to a new research area known as prompt engineering. Recent studies have demonstrated that Chain-of-Thought (CoT) prompts can lead to significant improvements in tasks such as arithmetic and common-sense reasoning. This paper explores the use of such approaches in legal reasoning tasks by conducting experiments on the COLIEE entailment task, which is based on the Japanese Bar exam. We evaluate zero-shot/few-shot and fine-tuning approaches with and without explanations, as well as various prompting strategies. Our results indicate that while CoT prompting and fine-tuning with explanations can improve performance, the best results are achieved with prompts derived from specific legal reasoning techniques, such as IRAC (Issue, Rule, Application, Conclusion). In addition, we observe that few-shot learning where the demonstrations are derived from clustering past training data consistently yields high performance on the COLIEE entailment task for both the years of the data that we tested. Through our experiments, we improve the previous best result on the 2021 COLIEE task from 0.7037 to 0.8025 and surpass the best system from 2022 with an accuracy of 0.789."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2108,
      "original_url": "https://aclanthology.org/2023.findings-acl.859/",
      "title": "End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.859.pdf",
      "authors": [
        "Yuanhe Tian",
        "Weidong Chen",
        "Bo Hu",
        "Yan Song",
        "Fei Xia"
      ],
      "abstract": "End-to-end Aspect-based Sentiment Analysis (EASA) is a natural language processing (NLP) task that involves extracting aspect terms and identifying the sentiments for them, which provides a fine-grained level of text analysis and thus requires a deep understanding of the running text. Many previous studies leverage advanced text encoders to extract context information and use syntactic information, e.g., the dependency structure of the input sentence, to improve the model performance. However, such models may reach a bottleneck since the dependency structure is not designed to provide semantic information of the text, which is also important for identifying the sentiment and thus leave room for further improvement. Considering that combinatory categorial grammar (CCG) is a formalism that expresses both syntactic and semantic information of a sentence, it has the potential to be beneficial to EASA. In this paper, we propose a novel approach to improve EASA with CCG supertags, which carry the syntactic and semantic information of the associated words and serve as the most important part of the CCG derivation. Specifically, our approach proposes a CCG supertag decoding process to learn the syntactic and semantic information carried by CCG supertags and use the information to guide the attention over the input words so as to identify important contextual information for EASA. Furthermore, a gate mechanism is used in incorporating the weighted contextual information into the backbone EASA decoding process. We evaluate our approach on three publicly available English datasets for EASA, and show that it outperforms strong baselines and achieves state-of-the-art results on all datasets."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2109,
      "original_url": "https://aclanthology.org/2023.findings-acl.860/",
      "title": "ConKI: Contrastive Knowledge Injection for Multimodal Sentiment Analysis",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.860.pdf",
      "authors": [
        "Yakun Yu",
        "Mingjun Zhao",
        "Shi-ang Qi",
        "Feiran Sun",
        "Baoxun Wang",
        "Weidong Guo",
        "Xiaoli Wang",
        "Lei Yang",
        "Di Niu"
      ],
      "abstract": "Multimodal Sentiment Analysis leverages multimodal signals to detect the sentiment of a speaker. Previous approaches concentrate on performing multimodal fusion and representation learning based on general knowledge obtained from pretrained models, which neglects the effect of domain-specific knowledge. In this paper, we propose Contrastive Knowledge Injection (ConKI) for multimodal sentiment analysis, where specific-knowledge representations for each modality can be learned together with general knowledge representations via knowledge injection based on an adapter architecture. In addition, ConKI uses a hierarchical contrastive learning procedure performed between knowledge types within every single modality, across modalities within each sample, and across samples to facilitate the effective learning of the proposed representations, hence improving multimodal sentiment predictions. The experiments on three popular multimodal sentiment analysis benchmarks show that ConKI outperforms all prior methods on a variety of performance metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2110,
      "original_url": "https://aclanthology.org/2023.findings-acl.861/",
      "title": "On Degrees of Freedom in Defining and Testing Natural Language Understanding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.861.pdf",
      "authors": [
        "Saku Sugawara",
        "Shun Tsugita"
      ],
      "abstract": "Natural language understanding (NLU) studies often exaggerate or underestimate the capabilities of systems, thereby limiting the reproducibility of their findings. These erroneous evaluations can be attributed to the difficulty of defining and testing NLU adequately. In this position paper, we reconsider this challenge by identifying two types of researcher degrees of freedom. We revisit Turing’s original interpretation of the Turing test and reveal that an effective test of NLU does not provide an operational definition; it merely provides inductive evidence that the test subject understands the language sufficiently well to meet stakeholder objectives. In other words, stakeholders are free to arbitrarily define NLU through their objectives. To use the test results as inductive evidence, stakeholders must carefully assess if the interpretation of test scores is valid or not. However, designing and using NLU tests involve other degrees of freedom, such as specifying target skills and defining evaluation metrics. As a result, achieving consensus among stakeholders becomes difficult. To resolve this issue, we propose a validity argument, which is a framework comprising a series of validation criteria across test components. By demonstrating that current practices in NLU studies can be associated with those criteria and organizing them into a comprehensive checklist, we prove that the validity argument can serve as a coherent guideline for designing credible test sets and facilitating scientific communication."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2111,
      "original_url": "https://aclanthology.org/2023.findings-acl.862/",
      "title": "AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.862.pdf",
      "authors": [
        "Yuxiang Nie",
        "Heyan Huang",
        "Wei Wei",
        "Xian-Ling Mao"
      ],
      "abstract": "Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ULQA), aiming to generate high-quality long-document QA instances in an unsupervised manner. Besides, we propose AttenWalker, a novel unsupervised method to aggregate and generate answers with long-range dependency so as to construct long-document QA pairs. Specifically, AttenWalker is composed of three modules, i.e. span collector, span linker and answer aggregator. Firstly, the span collector takes advantage of constituent parsing and reconstruction loss to select informative candidate spans for constructing answers. Secondly, with the help of the attention graph of a pre-trained long-document model, potentially interrelated text spans (that might be far apart) could be linked together via an attention-walking algorithm. Thirdly, in the answer aggregator, linked spans are aggregated into the final answer via the mask-filling ability of a pre-trained model. Extensive experiments show that AttenWalker outperforms previous methods on NarrativeQA and Qasper. In addition, AttenWalker also shows strong performance in the few-shot learning setting."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2112,
      "original_url": "https://aclanthology.org/2023.findings-acl.863/",
      "title": "Adaptive Ordered Information Extraction with Deep Reinforcement Learning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.863.pdf",
      "authors": [
        "Wenhao Huang",
        "Jiaqing Liang",
        "Zhixu Li",
        "Yanghua Xiao",
        "Chuanjun Ji"
      ],
      "abstract": "Information extraction (IE) has been studied extensively. The existing methods always follow a fixed extraction order for complex IE tasks with multiple elements to be extracted in one instance such as event extraction. However, we conduct experiments on several complex IE datasets and observe that different extraction orders can significantly affect the extraction results for a great portion of instances, and the ratio of sentences that are sensitive to extraction orders increases dramatically with the complexity of the IE task. Therefore, this paper proposes a novel adaptive ordered IE paradigm to find the optimal element extraction order for different instances, so as to achieve the best extraction results. We also propose an reinforcement learning (RL) based framework to generate optimal extraction order for each instance dynamically. Additionally, we propose a co-training framework adapted to RL to mitigate the exposure bias during the extractor training phase. Extensive experiments conducted on several public datasets demonstrate that our proposed method can beat previous methods and effectively improve the performance of various IE tasks, especially for complex ones."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2113,
      "original_url": "https://aclanthology.org/2023.findings-acl.864/",
      "title": "Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.864.pdf",
      "authors": [
        "Zihao Wang",
        "Weizhi Fei",
        "Hang Yin",
        "Yangqiu Song",
        "Ginny Wong",
        "Simon See"
      ],
      "abstract": "Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learningbased models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scorning functions motivated by local comparison or global transport, this work investigates the local and global trade-off with unbalanced optimal transport theory. Specifically, we embed sets as bounded measures in R endowed with a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a design also facilitates closed-form set operators in the embedding space. Moreover, we introduce a convolution-based algorithm for linear time computation and a block diagonal kernel to enforce the trade-off. Results show that WFRE is capable of outperforming existing query embedding methods on standard datasets, evaluation sets with combinatorially complex queries, and hierarchical knowledge graphs. Ablation study shows that finding a better local and global trade-off is essential for performance improvement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2114,
      "original_url": "https://aclanthology.org/2023.findings-acl.865/",
      "title": "RISE: Leveraging Retrieval Techniques for Summarization Evaluation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.865.pdf",
      "authors": [
        "David Uthus",
        "Jianmo Ni"
      ],
      "abstract": "Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained as a retrieval task using a dual-encoder retrieval setup, and can then be subsequently utilized for evaluating a generated summary given an input document, without gold reference summaries. RISE is especially well suited when working on new datasets where one may not have reference summaries available for evaluation. We conduct comprehensive experiments on the SummEval benchmark (Fabbri et al., 2021) and a long document summarization benchmark. The results show that RISE consistently achieves higher correlation with human evaluations compared to many past approaches to summarization evaluation. Furthermore, RISE also demonstrates data-efficiency and generalizability across languages."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2115,
      "original_url": "https://aclanthology.org/2023.findings-acl.866/",
      "title": "On the Difference of BERT-style and CLIP-style Text Encoders",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.866.pdf",
      "authors": [
        "Zhihong Chen",
        "Guiming Chen",
        "Shizhe Diao",
        "Xiang Wan",
        "Benyou Wang"
      ],
      "abstract": "Masked language modeling (MLM) has been one of the most popular pretraining recipes in natural language processing, e.g., BERT, one of the representative models. Recently, contrastive language-image pretraining (CLIP) has also attracted attention, especially its vision models that achieve excellent performance on a broad range of vision tasks. However, few studies are dedicated to studying the text encoders learned by CLIP. In this paper, we analyze the difference between BERT-style and CLIP-style text encoders from three experiments: (i) general text understanding, (ii) vision-centric text understanding, and (iii) text-to-image generation. Experimental analyses show that although CLIP-style text encoders underperform BERT-style ones for general text understanding tasks, they are equipped with a unique ability, i.e., synesthesia, for the cross-modal association, which is more similar to the senses of humans."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2116,
      "original_url": "https://aclanthology.org/2023.findings-acl.867/",
      "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.867.pdf",
      "authors": [
        "Marc Brinner",
        "Sina Zarrieß"
      ],
      "abstract": "Concurrent with the rapid progress in neural network-based models in NLP, the need for creating explanations for the predictions of these black-box models has risen steadily. Yet, especially for complex inputs like texts or images, existing interpretability methods still struggle with deriving easily interpretable explanations that also accurately represent the basis for the model’s decision. To this end, we propose a new, model-agnostic method to generate extractive explanations for predictions made by neural networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The masking is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness, and compactness of the generated explanation. Our method achieves state-of-the-art results in a challenging paragraph-level rationale extraction task, showing that this task can be performed without training a specialized model. We further apply our method to image inputs and obtain high-quality explanations for image classifications, which indicates that the objectives for optimizing explanation masks in text generalize to inputs of other modalities."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2117,
      "original_url": "https://aclanthology.org/2023.findings-acl.868/",
      "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.868.pdf",
      "authors": [
        "Samuel Cahyawijaya",
        "Holy Lovenia",
        "Alham Fikri Aji",
        "Genta Winata",
        "Bryan Wilie",
        "Fajri Koto",
        "Rahmad Mahendra",
        "Christian Wibisono",
        "Ade Romadhony",
        "Karissa Vincentio",
        "Jennifer Santoso",
        "David Moeljadi",
        "Cahya Wirawan",
        "Frederikus Hudi",
        "Muhammad Satrio Wicaksono",
        "Ivan Parmonangan",
        "Ika Alfina",
        "Ilham Firdausi Putra",
        "Samsul Rahmadani",
        "Yulianti Oenang",
        "Ali Septiandri",
        "James Jaya",
        "Kaustubh Dhole",
        "Arie Suryani",
        "Rifki Afina Putri",
        "Dan Su",
        "Keith Stevens",
        "Made Nindyatama Nityasya",
        "Muhammad Adilazuarda",
        "Ryan Hadiwijaya",
        "Ryandito Diandaru",
        "Tiezheng Yu",
        "Vito Ghifari",
        "Wenliang Dai",
        "Yan Xu",
        "Dyah Damapuspita",
        "Haryo Wibowo",
        "Cuk Tho",
        "Ichwanul Karo Karo",
        "Tirana Fatyanosa",
        "Ziwei Ji",
        "Graham Neubig",
        "Timothy Baldwin",
        "Sebastian Ruder",
        "Pascale Fung",
        "Herry Sujaini",
        "Sakriani Sakti",
        "Ayu Purwarianti"
      ],
      "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments.NusaCrowd’s data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2118,
      "original_url": "https://aclanthology.org/2023.findings-acl.869/",
      "title": "Transcribing Vocal Communications of Domestic Shiba lnu Dogs",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.869.pdf",
      "authors": [
        "Jieyi Huang",
        "Chunhao Zhang",
        "Mengyue Wu",
        "Kenny Zhu"
      ],
      "abstract": "How animals communicate and whether they have languages is a persistent curiosity of human beings. However, the study of animal communications has been largely restricted to data from field recordings or in a controlled environment, which is expensive and limited in scale and variety. In this paper, we take domestic Shiba Inu dogs as an example, and extract their vocal communications from large amount of YouTube videos of Shiba Inu dogs. We classify these clips into different scenarios and locations, and further transcribe the audio into phonetically symbolic scripts through a systematic process. We discover consistent phonetic symbols among their expressions, which indicates that Shiba Inu dogs can have systematic verbal communication patterns. This reusable framework produces the first-of-its-kind Shiba Inu vocal communication dataset that will be valuable to future research in both zoology and linguistics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2119,
      "original_url": "https://aclanthology.org/2023.findings-acl.870/",
      "title": "SkillQG: Learning to Generate Question for Reading Comprehension Assessment",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.870.pdf",
      "authors": [
        "Xiaoqiang Wang",
        "Bang Liu",
        "Siliang Tang",
        "Lingfei Wu"
      ],
      "abstract": "We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. Existing question generation systems widely differentiate questions by literal information such as question words and answer types to generate semantically relevant questions for a given context. However, they rarely consider the comprehension nature of questions, i.e., the different comprehension capabilities embodied by different questions. In comparison, our SkillQG is able to tailor a fine-grained assessment and improvement to the capabilities of questions answering models built on it. Specifically, we first frame the comprehension type of questions based on a hierarchical skill-based schema. We then formulate SkillQG as a skill-conditioned question generator. Furthermore, to improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models. Empirical results demonstrate that SkillQG outperforms baselines in terms of quality, relevance, and skill-controllability while showing a promising performance boost in downstream question answering task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2120,
      "original_url": "https://aclanthology.org/2023.findings-acl.871/",
      "title": "Improving Long Dialogue Summarization with Semantic Graph Representation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.871.pdf",
      "authors": [
        "Yilun Hua",
        "Zhaoyuan Deng",
        "Kathleen McKeown"
      ],
      "abstract": "Although Large Language Models (LLMs) are successful in abstractive summarization of short dialogues, summarization of long dialogues remains challenging. To address this challenge, we propose a novel algorithm that processes complete dialogues comprising thousands of tokens into topic-segment-level Abstract Meaning Representation (AMR) graphs, which explicitly capture the dialogue structure, highlight salient semantics, and preserve high-level information. We also develop a new text-graph attention to leverage both graph semantics and a pretrained LLM that exploits the text. Finally, we propose an AMR node selection loss used jointly with conventional cross-entropy loss, to create additional training signals that facilitate graph feature encoding and content selection. Experiments show that our system outperforms the state-of-the-art models on multiple long dialogue summarization datasets, especially in low-resource settings, and generalizes well to out-of-domain data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2121,
      "original_url": "https://aclanthology.org/2023.findings-acl.872/",
      "title": "Model Intrinsic Features of Fine-tuning based Text Summarization Models for Factual Consistency",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.872.pdf",
      "authors": [
        "Jongyoon Song",
        "Nohil Park",
        "Bongkyu Hwang",
        "Jaewoong Yun",
        "Seongho Joe",
        "Youngjune Gwon",
        "Sungroh Yoon"
      ],
      "abstract": "In this study, we analyze the model intrinsic features of a summarization model by varying the fine-tuning objectives and datasets. We fine-tune BART models combining three fine-tuning objectives (negative log-likelihood, unlikelihood, and contrastive loss) and two datasets (CNN/DailyMail and XSum) and provide shuffled or aligned documents to observe changes in the model predictions and intrinsic features. We find that (i) the inductive bias for factual consistency during the fine-tuning procedure depends on both the objectives and datasets, and (ii) summarization models with relatively low factual consistency are more likely to model summaries that are not conditional to the documents. We demonstrate that splitting data based on the unconditional and conditional summary modeling difficulty affects the factual consistency and intrinsic features of the summarization models. Our experimental results highlight the importance of studying the inductive bias during fine-tuning for factual consistency."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2122,
      "original_url": "https://aclanthology.org/2023.findings-acl.873/",
      "title": "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.873.pdf",
      "authors": [
        "Tiannan Wang",
        "Wangchunshu Zhou",
        "Yan Zeng",
        "Xinsong Zhang"
      ],
      "abstract": "Pre-trained vision-language models (VLMs) have achieved impressive results in a range of vision-language tasks. However, popular VLMs usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and deployment in real-world applications due to space, memory, and latency constraints. In this work, we introduce a distilling then pruning framework to compress large vision-language models into smaller, faster, and more accurate ones. We first shrink the size ofa pre-trained large VLM and apply knowledge distillation in the vision-language pre-training stage to obtain a task-agnostic compact VLM. Then we propose a modal-adaptive pruning algorithm to automatically infer the importance of vision and language modalities for different downstream tasks and adaptively remove redundant structures and neurons in different encoders with controllable target sparsity. We apply our framework to train EfficientVLM, a fast and accurate vision-language model consisting of 6 vision layers, 3 text layers, and 3 cross-modal fusion layers, accounting for only 93 million parameters in total, which is 44.3% of the teacher model. EfficientVLM retains 98.4% performance of the teacher model and accelerates its inference speed by 2.2×. EfficientVLM achieves a large absolute improvement over previous SoTA efficient VLMs of similar sizes by a large margin on various vision-language tasks, including VQAv2 (+4.9%), NLVR2 (+5.6%), ITR (R@1 on TR +17.2%, on IR + 15.6% ) and COCO caption generation (CIDEr +6.5), demonstrating a large potential on training lightweight VLMs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2123,
      "original_url": "https://aclanthology.org/2023.findings-acl.874/",
      "title": "DP-BART for Privatized Text Rewriting under Local Differential Privacy",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.874.pdf",
      "authors": [
        "Timour Igamberdiev",
        "Ivan Habernal"
      ],
      "abstract": "Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privacy guarantees, privatization of only individual words, as well as a lack of transparency and reproducibility. In this paper, we propose a new system ‘DP-BART’ that largely outperforms existing LDP systems. Our approach uses a novel clipping method, iterative pruning, and further training of internal representations which drastically reduces the amount of noise required for DP guarantees. We run experiments on five textual datasets of varying sizes, rewriting them at different privacy guarantees and evaluating the rewritten texts on downstream text classification tasks. Finally, we thoroughly discuss the privatized text rewriting approach and its limitations, including the problem of the strict text adjacency constraint in the LDP paradigm that leads to the high noise requirement."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2124,
      "original_url": "https://aclanthology.org/2023.findings-acl.875/",
      "title": "Robustness of Learning from Task Instructions",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.875.pdf",
      "authors": [
        "Jiasheng Gu",
        "Hongyu Zhao",
        "Hanzi Xu",
        "Liangyu Nie",
        "Hongyuan Mei",
        "Wenpeng Yin"
      ],
      "abstract": "Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-specific examples. This paradigm seriously hinders the development of task generalization since preparing a task-specific example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the definition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: first, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust system for task generalization should be able to handle any new tasks regardless of the variability of instructions. However, the system robustness in dealing with instruction-driven task generalization is still unexplored. This work investigates the system robustness when the instructions of new tasks are (i) manipulated, (ii) paraphrased, or (iii) from different levels of conciseness. To our knowledge, this is the first work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2125,
      "original_url": "https://aclanthology.org/2023.findings-acl.876/",
      "title": "Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.876.pdf",
      "authors": [
        "Gábor Berend"
      ],
      "abstract": "In this paper, we propose an alternative to the classic masked language modeling (MLM) pre-training paradigm, where the objective is altered from the reconstruction of the exact identity of randomly selected masked subwords to the prediction of their latent semantic properties. We coin the proposed pre-training technique masked latent semantic modeling (MLSM for short). In order to make the contextualized determination of the latent semantic properties of the masked subwords possible, we rely on an unsupervised technique which uses sparse coding. Our experimental results reveal that the fine-tuned performance of those models that we pre-trained via MLSM is consistently and significantly better compared to the use of vanilla MLM pretraining and other strong baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2126,
      "original_url": "https://aclanthology.org/2023.findings-acl.877/",
      "title": "Detection and Mitigation of the Negative Impact of Dataset Extractivity on Abstractive Summarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.877.pdf",
      "authors": [
        "Yubin Ge",
        "Sullam Jeoung",
        "Ly Dinh",
        "Jana Diesner"
      ],
      "abstract": "In text summarization, extractivity is defined as a measurement of the degree of overlap between a source document and its summary. Previous research has shown that the extractivity level of training data can influence both output extractivity and the amount of factual information (i.e. faithfulness) in outputs for abstractive summarization. However, it remains unclear if and how extractivity impacts the performance of abstractive models. In this work, we investigate the relationship between dataset extractivity and model performance by comparing the performance of trained models under different degrees of extractivity. We find that while low levels of extractivity can improve performance, as extractivity increases, performance is negatively impacted. Furthermore, through an analysis of the model’s copy continuity of content, we discover that higher extractivity leads to a greater tendency for the model to copy text continuously from the source document rather than identifying and summarizing important content that should be covered in the target summary. To address these issues, we propose a simple and effective method to design copy labels for fixing the model’s copying behaviors and train the model with a copy mechanism. The experimental results illustrate the effectiveness of our strategy in alleviating the negative impact on model performance resulting from high dataset extractivity, and that our method outperforms several competitive baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2127,
      "original_url": "https://aclanthology.org/2023.findings-acl.878/",
      "title": "Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.878.pdf",
      "authors": [
        "Siwei Wu",
        "Xiangqing Shen",
        "Rui Xia"
      ],
      "abstract": "The nodes in the commonsense knowledge graph (CSKG) are normally represented by free-form short text (e.g., word or phrase). Different nodes may represent the same concept. This leads to the problems of edge sparsity and node redundancy, which challenges CSKG representation and completion. On the one hand, edge sparsity limits the performance of graph representation learning; On the other hand, node redundancy makes different nodes corresponding to the same concept have inconsistent relations with other nodes. To address the two problems, we propose a new CSKG completion framework based on Contrastive Pretraining and Node Clustering (CPNC). Contrastive Pretraining constructs positive and negative head-tail node pairs on CSKG and utilizes contrastive learning to obtain better semantic node representation. Node Clustering aggregates nodes with the same concept into a latent concept, assisting the task of CSKG completion. We evaluate our CPNC approach on two CSKG completion benchmarks (CN-100K and ATOMIC), where CPNC outperforms the state-of-the-art methods. Extensive experiments demonstrate that both Contrastive Pretraining and Node Clustering can significantly improve the performance of CSKG completion. The source code of CPNC is publicly available on https://github.com/NUSTM/CPNC."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2128,
      "original_url": "https://aclanthology.org/2023.findings-acl.879/",
      "title": "Incorporating Factuality Inference to Identify Document-level Event Factuality",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.879.pdf",
      "authors": [
        "Heng Zhang",
        "Peifeng Li",
        "Zhong Qian",
        "Xiaoxu Zhu"
      ],
      "abstract": "Document-level Event Factuality Identification (DEFI) refers to identifying the degree of certainty that a specific event occurs in a document. Previous studies on DEFI failed to link the document-level event factuality with various sentence-level factuality values in the same document. In this paper, we innovatively propose an event factuality inference task to bridge the sentence-level and the document-level event factuality semantically. Specifically, we present a Sentence-to-Document Inference Network (SDIN) that contains a multi-layer interaction module and a gated aggregation module to integrate the above two tasks, and employ a multi-task learning framework to improve the performance of DEFI. The experimental results on the public English and Chinese DLEF datasets show that our model outperforms the SOTA baselines significantly."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2129,
      "original_url": "https://aclanthology.org/2023.findings-acl.880/",
      "title": "Hybrid and Collaborative Passage Reranking",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.880.pdf",
      "authors": [
        "Zongmeng Zhang",
        "Wengang Zhou",
        "Jiaxin Shi",
        "Houqiang Li"
      ],
      "abstract": "In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2130,
      "original_url": "https://aclanthology.org/2023.findings-acl.881/",
      "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.881.pdf",
      "authors": [
        "Haoran Li",
        "Mingshi Xu",
        "Yangqiu Song"
      ],
      "abstract": "Sentence-level representations are beneficial for various natural language processing tasks. It is commonly believed that vector representations can capture rich linguistic properties. Currently, large language models (LMs) achieve state-of-the-art performance on sentence embedding. However, some recent works suggest that vector representations from LMs can cause information leakage. In this work, we further investigate the information leakage issue and propose a generative embedding inversion attack (GEIA) that aims to reconstruct input sequences based only on their sentence embeddings. Given the black-box access to a language model, we treat sentence embeddings as initial tokens’ representations and train or fine-tune a powerful decoder model to decode the whole sequences directly. We conduct extensive experiments to demonstrate that our generative inversion attack outperforms previous embedding inversion attacks in classification metrics and generates coherent and contextually similar sentences as the original inputs."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2131,
      "original_url": "https://aclanthology.org/2023.findings-acl.882/",
      "title": "Learning Query Adaptive Anchor Representation for Inductive Relation Prediction",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.882.pdf",
      "authors": [
        "Zhiwen Xie",
        "Yi Zhang",
        "Jin Liu",
        "Guangyou Zhou",
        "Jimmy Huang"
      ],
      "abstract": "Relation prediction on knowledge graphs (KGs) attempts to infer the missing links between entities. Most previous studies are limited to the transductive setting where all entities must be seen during the training, making them unable to perform reasoning on emerging entities. Recently, the inductive setting is proposed to handle the entities in the test phase to be unseen during training, However, it suffers from the inefficient reasoning under the enclosing subgraph extraction issue and the lack of effective entity-independent feature modeling. To this end, we propose a novel Query Adaptive Anchor Representation (QAAR) model for inductive relation prediction. First, we extract one opening subgraph and perform reasoning by one time for all candidate triples, which is more efficient when the number of candidate triples is large. Second, we define some query adaptive anchors which are independent on any specific entity. Based on these anchors, we take advantage of the transferable entity-independent features (relation-aware, structure-aware and distance features) that can be used to produce entity embeddings for emerging unseen entities. Such entity-independent features is modeled by a query-aware graph attention network on the opening subgraph. Experimental results demonstrate that our proposed QAAR outperforms state-of-the-art baselines in inductive relation prediction task."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2132,
      "original_url": "https://aclanthology.org/2023.findings-acl.883/",
      "title": "Context or Knowledge is Not Always Necessary: A Contrastive Learning Framework for Emotion Recognition in Conversations",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.883.pdf",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Ruibin Mao",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "abstract": "Emotion recognition in conversations (ERC) aims to detect the emotion of utterances in conversations. Existing efforts generally focus on modeling context- and knowledge-sensitive dependencies. However, it is observed that the emotions of many utterances can be correctly detected without context or external knowledge. In such cases, blindly leveraging the context and external knowledge may impede model training. Based on this, we propose a novel framework based on contrastive learning (CL), called CKCL (including the contrastive learning scenarios among Context and Knowledge), to distinguish the above utterances for better vector representations. The CKCL framework defines context- and knowledge-independent utterances, as the positive sample, whose predicted results are unchanged even masking context and knowledge representations, otherwise, the negative sample. This can obtain a latent feature reflecting the impact degree of context and external knowledge on predicted results, thus effectively denoising irrelevant context and knowledge during training. Experimental results on four datasets show the performance of CKCL-based models is significantly boosted and outperforms state-of-the-art methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2133,
      "original_url": "https://aclanthology.org/2023.findings-acl.884/",
      "title": "Exploring Speaker-Related Information in Spoken Language Understanding for Better Speaker Diarization",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.884.pdf",
      "authors": [
        "Luyao Cheng",
        "Siqi Zheng",
        "Zhang Qinglin",
        "Hui Wang",
        "Yafeng Chen",
        "Qian Chen"
      ],
      "abstract": "Speaker diarization is a classic task in speech processing and is crucial in multi-party scenarios such as meetings and conversations. Current mainstream speaker diarization approaches consider acoustic information only, which result in performance degradation when encountering adverse acoustic environment. In this paper, we propose methods to extract speaker-related information from semantic content in multi-party meetings, which, as we will show, can further benefit speaker diarization. We introduce two sub-tasks, Dialogue Detection and Speaker-Turn Detection, in which we effectively extract speaker information from conversational semantics. We also propose a simple yet effective algorithm to jointly model acoustic and semantic information and obtain speaker-identified texts. Experiments on both AISHELL-4 and AliMeeting datasets show that our method achieves consistent improvements over acoustic-only speaker diarization systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2134,
      "original_url": "https://aclanthology.org/2023.findings-acl.885/",
      "title": "Cross-Lingual Knowledge Distillation for Answer Sentence Selection in Low-Resource Languages",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.885.pdf",
      "authors": [
        "Shivanshu Gupta",
        "Yoshitomo Matsubara",
        "Ankit Chadha",
        "Alessandro Moschitti"
      ],
      "abstract": "While impressive performance has been achieved on the task of Answer Sentence Selection (AS2) for English, the same does not hold for languages that lack large labeled datasets. In this work, we propose Cross-Lingual Knowledge Distillation (CLKD) from a strong English AS2 teacher as a method to train AS2 models for low-resource languages in the tasks without the need of labeled data for the target language. To evaluate our method, we introduce 1) Xtr-WikiQA, a translation-based WikiQA dataset for 9 additional languages, and 2) TyDi-AS2, a multilingual AS2 dataset with over 70K questions spanning 8 typologically diverse languages. We conduct extensive experiments on Xtr-WikiQA and TyDi-AS2 with multiple teachers, diverse monolingual and multilingual pretrained language models (PLMs) as students, and both monolingual and multilingual training. The results demonstrate that CLKD either outperforms or rivals even supervised fine-tuning with the same amount of labeled data and a combination of machine translation and the teacher model. Our method can potentially enable stronger AS2 models for low-resource languages, while TyDi-AS2 can serve as the largest multilingual AS2 dataset for further studies in the research community."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2135,
      "original_url": "https://aclanthology.org/2023.findings-acl.886/",
      "title": "Run Like a Girl! Sport-Related Gender Bias in Language and Vision",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.886.pdf",
      "authors": [
        "Sophia Harrison",
        "Eleonora Gualdoni",
        "Gemma Boleda"
      ],
      "abstract": "Gender bias in Language and Vision datasets and models has the potential to perpetuate harmful stereotypes and discrimination. We analyze gender bias in two Language and Vision datasets. Consistent with prior work, we find that both datasets underrepresent women, which promotes their invisibilization. Moreover, we hypothesize and find that a bias affects human naming choices for people playing sports: speakers produce names indicating the sport (e.g. “tennis player” or “surfer”) more often when it is a man or a boy participating in the sport than when it is a woman or a girl, with an average of 46% vs. 35% of sports-related names for each gender. A computational model trained on these naming data reproduces thebias. We argue that both the data and the model result in representational harm against women."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2136,
      "original_url": "https://aclanthology.org/2023.findings-acl.887/",
      "title": "People and Places of Historical Europe: Bootstrapping Annotation Pipeline and a New Corpus of Named Entities in Late Medieval Texts",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.887.pdf",
      "authors": [
        "Vit Novotny",
        "Kristina Luger",
        "Michal Štefánik",
        "Tereza Vrabcova",
        "Ales Horak"
      ],
      "abstract": "Although pre-trained named entity recognition (NER) models are highly accurate on modern corpora, they underperform on historical texts due to differences in language OCR errors. In this work, we develop a new NER corpus of 3.6M sentences from late medieval charters written mainly in Czech, Latin, and German.We show that we can start with a list of known historical figures and locations and an unannotated corpus of historical texts, and use information retrieval techniques to automatically bootstrap a NER-annotated corpus. Using our corpus, we train a NER model that achieves entity-level Precision of 72.81–93.98% with 58.14–81.77% Recall on a manually-annotated test dataset. Furthermore, we show that using a weighted loss function helps to combat class imbalance in token classification tasks. To make it easy for others to reproduce and build upon our work, we publicly release our corpus, models, and experimental code."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2137,
      "original_url": "https://aclanthology.org/2023.findings-acl.888/",
      "title": "Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.888.pdf",
      "authors": [
        "Gengyu Wang",
        "Kate Harwood",
        "Lawrence Chillrud",
        "Amith Ananthram",
        "Melanie Subbiah",
        "Kathleen McKeown"
      ],
      "abstract": "We present a new fact-checking benchmark, Check-COVID, that requires systems to verify claims about COVID-19 from news using evidence from scientific articles. This approach to fact-checking is particularly challenging as it requires checking internet text written in everyday language against evidence from journal articles written in formal academic language. Check-COVID contains 1, 504 expert-annotated news claims about the coronavirus paired with sentence-level evidence from scientific journal articles and veracity labels. It includes both extracted (journalist-written) and composed (annotator-written) claims. Experiments using both a fact-checking specific system and GPT-3.5, which respectively achieve F1 scores of 76.99 and 69.90 on this task, reveal the difficulty of automatically fact-checking both claim types and the importance of in-domain data for good performance. Our data and models are released publicly at https://github.com/posuer/Check-COVID."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2138,
      "original_url": "https://aclanthology.org/2023.findings-acl.889/",
      "title": "Early Exit with Disentangled Representation and Equiangular Tight Frame",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.889.pdf",
      "authors": [
        "Yixin Ji",
        "Jikai Wang",
        "Juntao Li",
        "Qiang Chen",
        "Wenliang Chen",
        "Min Zhang"
      ],
      "abstract": "Dynamic early exit has demonstrated great potential in coping with the sharply increasing number of pre-trained language model parameters, which can achieve a good trade-off between performance and efficiency. The existing early exit paradigm relies on training parametrical internal classifiers at each intermediate layer to complete specific tasks. Based on the predictions of these internal classifiers, different methods are designed to decide when to exit. Under this circumstance, each intermediate layer takes on both generic language representation learning and task-specific feature extraction, which makes each intermediate layer struggle to balance two types of backward loss signals during training. To break this dilemma, we propose an adapter method to decouple the two distinct types of representation and further introduce a non-parametric simplex equiangular tight frame classifier (ETF) for improvement. Extensive experiments on monolingual and multilingual tasks demonstrate that our method gains significant improvements over strong PLM backbones and early exit methods."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2139,
      "original_url": "https://aclanthology.org/2023.findings-acl.890/",
      "title": "Tokenization with Factorized Subword Encoding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.890.pdf",
      "authors": [
        "David Samuel",
        "Lilja Øvrelid"
      ],
      "abstract": "In recent years, language models have become increasingly larger and more complex. However, the input representations for these models continue to rely on simple and greedy subword tokenization methods. In this paper, we propose a novel tokenization method that factorizes subwords onto discrete triplets using a VQ-VAE model. The effectiveness of the proposed tokenization method, referred to as the Factorizer, is evaluated on language modeling and morpho-syntactic tasks for 7 diverse languages. Results indicate that this method is more appropriate and robust for morphological tasks than the commonly used byte-pair encoding (BPE) tokenization algorithm."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2140,
      "original_url": "https://aclanthology.org/2023.findings-acl.891/",
      "title": "Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.891.pdf",
      "authors": [
        "James Michaelov",
        "Benjamin Bergen"
      ],
      "abstract": "How well do language models deal with quantification? In this study, we focus on ‘few’-type quantifiers, as in ‘few children like toys’, which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and ‘few’-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on ‘few’-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2141,
      "original_url": "https://aclanthology.org/2023.findings-acl.892/",
      "title": "“A Little is Enough”: Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.892.pdf",
      "authors": [
        "Akshay Batheja",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of QE framework to extracting quality parallel corpus from the pseudo-parallel corpus.. By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system’s performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model. The baseline model is the one that is trained on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model. This demonstrates the promise of transfer learning in the setting under discussion. QE systems typically require in the order of (7K-25K) of training data. Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40th of the normal requirement and achieves comparable performance. All the scripts and datasets utilized in this study will be publicly available."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2142,
      "original_url": "https://aclanthology.org/2023.findings-acl.893/",
      "title": "How effective is machine translation on low-resource code-switching? A case study comparing human and automatic metrics",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.893.pdf",
      "authors": [
        "Li Nguyen",
        "Christopher Bryant",
        "Oliver Mayeux",
        "Zheng Yuan"
      ],
      "abstract": "This paper presents an investigation into the differences between processing monolingual input and code-switching (CSW) input in the context of machine translation (MT). Specifically, we compare the performance of three MT systems (Google, mBART-50 and M2M-100-big) in terms of their ability to translate monolingual Vietnamese, a low-resource language, and Vietnamese-English CSW respectively. To our knowledge, this is the first study to systematically analyse what might happen when multilingual MT systems are exposed to CSW data using both automatic and human metrics. We find that state-of-the-art neural translation systems not only achieve higher scores on automatic metrics when processing CSW input (compared to monolingual input), but also produce translations that are consistently rated as more semantically faithful by humans. We further suggest that automatic evaluation alone is insufficient for evaluating the translation of CSW input. Our findings establish a new benchmark that offers insights into the relationship between MT and CSW."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2143,
      "original_url": "https://aclanthology.org/2023.findings-acl.894/",
      "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.894.pdf",
      "authors": [
        "Sherzod Hakimov",
        "David Schlangen"
      ],
      "abstract": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input – but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model’s output by providing a means of tracing the output back through the verbalised image content."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2144,
      "original_url": "https://aclanthology.org/2023.findings-acl.895/",
      "title": "On the Expressivity Role of LayerNorm in Transformers’ Attention",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.895.pdf",
      "authors": [
        "Shaked Brody",
        "Uri Alon",
        "Eran Yahav"
      ],
      "abstract": "Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models. In this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. This is in contrast to the common belief that LayerNorm’s only role is to normalize the activations during the forward pass, and their gradients during the backward pass. We consider a geometric interpretation of LayerNorm and show that it consists of two components: (a) projection of the input vectors to a d-1 space that is orthogonal to the [1,1,...,1] vector, and(b) scaling of all vectors to the same norm of d. We show that each of these components is important for the attention layer that follows it in Transformers:(a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation in the attention; and(b) scaling allows each key to potentially receive the highest attention, and prevents keys from being “un-select-able”.We show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as “majority”. Our code is available at https://github.com/tech-srl/layer_norm_expressivity_role ."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2145,
      "original_url": "https://aclanthology.org/2023.findings-acl.896/",
      "title": "DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.896.pdf",
      "authors": [
        "ChaeHun Park",
        "Seungil Lee",
        "Daniel Rim",
        "Jaegul Choo"
      ],
      "abstract": "Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DENSITY, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier. Our metric measures how likely a response would appear in the distribution of human conversations. Moreover, to improve the performance of DENSITY, we utilize contrastive learning to further compress the feature space. Experiments on multiple response evaluation datasets show that DENSITY correlates better with human evaluations than the existing metrics."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2146,
      "original_url": "https://aclanthology.org/2023.findings-acl.897/",
      "title": "Fixing MoE Over-Fitting on Low-Resource Languages in Multilingual Machine Translation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.897.pdf",
      "authors": [
        "Maha Elbayad",
        "Anna Sun",
        "Shruti Bhosale"
      ],
      "abstract": "Sparsely gated Mixture of Experts (MoE) models have been shown to be a compute-efficient method to scale model capacity for multilingual machine translation. However, for low-resource tasks, MoE models severely over-fit. We show effective regularization strategies, namely dropout techniques for MoE layers in EOM and FOM, Conditional MoE Routing and Curriculum Learning methods that prevent over-fitting and improve the performance of MoE models on low-resource tasks without adversely affecting high-resource tasks. On a massively multilingual machine translation benchmark, our strategies result in about +1 chrF++ improvement in very low resource language pairs. We perform an extensive analysis of the learned MoE routing to better understand the impact of our regularization methods and how we can improve them."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2147,
      "original_url": "https://aclanthology.org/2023.findings-acl.898/",
      "title": "Intent Discovery with Frame-guided Semantic Regularization and Augmentation",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.898.pdf",
      "authors": [
        "Yajing Sun",
        "Rui Zhang",
        "Jingyuan Yang",
        "Wei Peng"
      ],
      "abstract": "Most existing intent discovery methods leverage representation learning and clustering to transfer the prior knowledge of known intents to unknown ones. The learned representations are limited to the syntactic forms of sentences, therefore, fall short of recognizing adequate variations under the same meaning of unknown intents. This paper proposes an approach utilizing frame knowledge as conceptual semantic guidance to bridge the gap between known intents representation learning and unknown intents clustering. Specifically, we employ semantic regularization to minimize the bidirectional KL divergence between model predictions for frame-based and sentence-based samples. Moreover, we construct a frame-guided data augmenter to capture intent-friendly semantic information and implement contrastive clustering learning for unsupervised sentence embedding. Extensive experiments on two benchmark datasets show that our method achieves substantial improvements in accuracy (5%+) compared to solid baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2148,
      "original_url": "https://aclanthology.org/2023.findings-acl.899/",
      "title": "An Empirical Comparison of LM-based Question and Answer Generation Methods",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.899.pdf",
      "authors": [
        "Asahi Ushio",
        "Fernando Alva-Manchego",
        "Jose Camacho-Collados"
      ],
      "abstract": "Question and answer generation (QAG) consists of generating a set of question-answer pairs given a context (e.g. a paragraph). This task has a variety of applications, such as data augmentation for question answering (QA) models, information retrieval and education. In this paper, we establish baselines with three different QAG methodologies that leverage sequence-to-sequence language model (LM) fine-tuning. Experiments show that an end-to-end QAG model, which is computationally light at both training and inference times, is generally robust and outperforms other more convoluted approaches. However, there are differences depending on the underlying generative LM. Finally, our analysis shows that QA models fine-tuned solely on generated question-answer pairs can be competitive when compared to supervised QA models trained on human-labeled data."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2149,
      "original_url": "https://aclanthology.org/2023.findings-acl.900/",
      "title": "Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.900.pdf",
      "authors": [
        "Qian Li",
        "Shafiq Joty",
        "Daling Wang",
        "Shi Feng",
        "Yifei Zhang",
        "Chengwei Qin"
      ],
      "abstract": "With the evolution of Knowledge Graphs (KGs), new entities emerge which are not seen before. Representation learning of KGs in such an inductive setting aims to capture and transfer the structural patterns from existing entities to new entities. However, the performance of existing methods in inductive KGs are limited by sparsity and implicit transfer. In this paper, we propose VMCL, a Contrastive Learning (CL) framework with graph guided Variational autoencoder on Meta-KGs in the inductive setting. We first propose representation generation to capture the encoded and generated representations of entities, where the generated variations can densify representations with complementary features. Then, we design two CL objectives that work across entities and meta-KGs to simulate the transfer mode. With extensive experiments we demonstrate that our proposed VMCL can significantly outperform previous state-of-the-art baselines."
    },
    {
      "conference": "ACL",
      "year": 2023,
      "order": 2150,
      "original_url": "https://aclanthology.org/2023.findings-acl.901/",
      "title": "Decouple knowledge from paramters for plug-and-play language modeling",
      "pdf_url": "https://aclanthology.org/2023.findings-acl.901.pdf",
      "authors": [
        "Xin Cheng",
        "Yankai Lin",
        "Xiuying Chen",
        "Dongyan Zhao",
        "Rui Yan"
      ],
      "abstract": "Pre-trained language models (PLM) have made impressive results in a wide range of NLP tasks and it has been revealed that one of the key factors to their success is the parameters of these models implicitly learn various types of knowledge in the pre-training corpus. However, encoding knowledge implicitly in the model parameters has two fundamental drawbacks. First, the knowledge is neither editable nor scalable once the model is trained, which is especially problematic in that knowledge is consistently evolving. Second, it lacks interpretability and prevents us from understanding what kind of knowledge PLM needs to solve a certain task. In this paper, we introduce {pasted macro ‘MODEL’}, a pre-training model with differentiable plug-in memory (DPM). The key intuition behind is to decouple the knowledge storage from model parameters with an editable and scalable key-value memory and leverage knowledge in an explainable manner by knowledge retrieval in the {pasted macro ‘MEMORY’}. We conduct extensive experiments under various settings to justify this design choice. In domain adaptation setting, {pasted macro ‘MODEL’} could be easily adapted to different domains with pluggable in-domain memory—obtaining 3.95 F1 improvements across four domains, without any in-domain training. {pasted macro ‘MODEL’} could also keep absorbing new knowledge after pre-training is done by knowledge updating operation in the {pasted macro ‘MEMORY’} without re-training. Finally, we show that by incorporating training samples into {pasted macro ‘MEMORY’} with knowledge prompting, {pasted macro ‘MODEL’} could further be improved by the instruction of in-task knowledge."
    }
  ],
  "metadata": {
    "crawl_date": "2025-04-25 23:52:22",
    "source_url": "https://papers.cool/venue/ACL.2023",
    "actual_papers": 2150
  }
}